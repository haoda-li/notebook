{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"index.html","title":"Courses","text":"Vision and GraphicsMachine LearningGeneral Computer ScienceMathematicsStatistics <ul> <li> <p>Learning for 3D Vision     UCB CS294-173 (2023W)</p> <p> Angjoo Kanazawa</p> <p> Course Site</p> </li> <li> <p>Intro. Image Understanding     CSC420 (2019F)</p> <p> Babak Taati</p> <p> Course Site</p> </li> <li> <p>Intro. Visual Computing      CSC320 (2019F)</p> <p> Yawen Ma</p> </li> <li> <p>Geometry Processing      CSC419/CSC2520 (2020F)</p> <p> Alec Jacobson</p> <p> Course Site</p> </li> <li> <p>Physics-based Animation     CSC417/CSC2549 (2020F)</p> <p> David I.W. Levin </p> <p> Course Site</p> </li> <li> <p>Computer Graphics and Imaging     UCB CS284A/CS184A (2023W)</p> <p> Ren Ng, James F. O'Brien </p> <p> Course Site</p> </li> <li> <p>Intro. Computer Graphics      CSC418/CSC2504 (2020S)</p> <p> Sarah Kushner</p> <p> Course Site</p> </li> </ul> <ul> <li> <p>Neural Networks     CSC413/CSC2516 (2020W)</p> <p> Jimmy Ba </p> <p> Course Site</p> </li> <li> <p>Probabilistic Learning and Reasoning     CSC412/CSC2506 (2020W)</p> <p> David K. Duvenaud</p> <p> Course Site</p> </li> <li> <p>Intro. Machine Learning     CSC311 (2019F)</p> <p> Richard Zemel</p> <p> Course Site</p> </li> </ul> <ul> <li> <p>Applications of Parallel Computers     UCB CS 267 (2023W)</p> <p> Ayd\u0131n Bulu\u00e7, James Demmel, Laura Grigori</p> <p> Course Site</p> </li> <li> <p>Numerical Methods      CSC336 (2019F)</p> <p> Kenneth R. Jackson </p> </li> <li> <p>Parallel Programming      CSC367 (2021F)</p> <p> Maryam Mehri Dehnavi</p> </li> <li> <p>Principles of Programming Languages     CSC324 (2022W)</p> <p> Gary Baumgartner</p> <p> Instructor's Notes (David Liu)</p> </li> <li> <p>Algorithms Design, Analysis and Complexity      CSC373 (2019W)</p> <p> Francois Pitt </p> </li> <li> <p>Enriched Data Structures and Analysis      CSC265 (2018F)</p> <p> Aleksandar Nikolov </p> </li> <li> <p>Enriched Theory of Computation     CSC240 (2018W)</p> <p> Faith Ellen</p> <p> Instructor's Notes (Vassos Hadzilacos)</p> </li> <li> <p>Databases      CSC343 (2019W)</p> <p> Daniela Rosu</p> </li> </ul> <ul> <li> <p>Geometry of Curves and Surfaces     MAT363 (2022W)</p> <p> Alexander Nabutovsky</p> </li> <li> <p>Complex Variables      MAT334 (2021F)</p> <p> Daniel Johnstone</p> </li> <li> <p>Intro. Real Analysis     MAT337 (2019S)</p> <p> Tomas Kojar</p> </li> </ul> <ul> <li> <p>Theory of Statistical Practice      STA355 (2020W)</p> <p> Keith Knight</p> </li> <li> <p>Time Series Analysis      STA457/2202 (2019S)</p> <p> Jen Wen Lin</p> </li> <li> <p>Methods of Data Analysis      STA303/STA1002 (2019S)</p> <p> Shivon Sue-Chee</p> </li> <li> <p>Probability      STA347 (2018F)</p> <p> Gun Ho Jang</p> </li> </ul>"},{"location":"apm462/calculus_review.html","title":"Calculus Review","text":""},{"location":"apm462/calculus_review.html#limit-and-continuity","title":"Limit and Continuity","text":"<p>Let \\(f, g\\) be functions \\(\\mathbb R^n \\rightarrow \\mathbb R\\)</p>"},{"location":"apm462/calculus_review.html#claim-1","title":"Claim 1","text":"<p>If \\(f\\) is continuous and \\(g\\) is discontinuous at \\(a\\in \\mathbb R^n\\), then \\(f+g\\) must be discontinuous at \\(a\\). </p> <p>proof. Assume \\(f\\) is continuous and \\(g\\) is discontinuous at \\(a\\in \\mathbb R^n\\). Let \\(L\\in\\mathbb R\\) be arbitrary. By the definition of discontinuity, take \\(\\epsilon_g &gt; 0\\) s.t. \\(\\forall \\delta &gt; 0. \\exists x\\in \\mathbb R^n. |x-a|&lt;\\delta \\land |g(x) -g(a)| \\geq \\epsilon_g\\). By the definition of continuity, take \\(\\delta_f &gt; 0\\) s.t. \\(\\forall x\\in \\mathbb R^n. |x-a| &lt; \\delta_f\\Rightarrow |f(x) -f(a)| &lt; \\epsilon_g/2\\) Then, take \\(\\epsilon = \\epsilon_g/2\\). Let \\(\\delta &gt; 0\\) be arbitrary. Take \\(x\\in\\mathbb R^n. |x-a| &lt; \\min(\\delta, \\delta_f)\\).  Therefore, </p> \\[\\begin{align*} &amp;\\quad\\|f(x) - f(a) + g(x) - g(a)\\| \\\\&amp;= \\|(f(x) - f(a)) - (g(a) - g(x))\\|  \\\\ &amp;\\geq \\big\\|\\|f(x) - f(a)\\| - \\|g(a) - g(x)\\|\\big\\| &amp;\\text{reverse triangle ineq.}\\\\ &amp;\\geq \\epsilon_g/2\\\\ &amp;= \\epsilon \\end{align*}\\] <p>Therefore, we prove that </p> \\[\\forall L\\in \\mathbb R. \\exists \\epsilon &gt; 0, \\forall \\delta &gt; 0. \\exists x\\in \\mathbb R^n. \\|x-a\\|&lt;\\delta \\land \\|f(x)+g(x)-(f(a)+g(a))\\| \\geq \\epsilon\\]"},{"location":"apm462/calculus_review.html#claim-2","title":"Claim 2","text":"<p>If both \\(f, g\\) are discontinuous at \\(a\\in \\mathbb R^n\\), then \\(f+g\\) can be either continuous or discontinuous.   </p> <p>proof. Consider the following examples,  Take \\(f_1(x) := \\begin{cases}1 &amp;x = a\\\\0&amp;x\\neq a\\end{cases}\\) and \\(g_1(x):= \\begin{cases}-1 &amp;x = a\\\\0&amp;x\\neq a\\end{cases}\\). Obviously \\(f_1+g_1\\) is continuous at \\(a\\). Take \\(f_2(x) := \\begin{cases}1 &amp;x = a\\\\0&amp;x\\neq a\\end{cases}\\) and \\(g_2(x):= \\begin{cases}1 &amp;x = a\\\\0&amp;x\\neq a\\end{cases}\\). Obviously \\(f_2+g_2\\) is discontinuous at \\(a\\). </p>"},{"location":"apm462/calculus_review.html#claim-3","title":"Claim 3","text":"<p>If \\(f\\) is continuous and \\(g\\) is discontinuous at \\(a\\in \\mathbb R^n\\), then \\(f\\times g\\) can be either continuous or discontinuous.   </p> <p>proof. Consider the following examples,   </p> <p>One of \\(f, g\\) is continuous: Take \\(f_1(x) := 0\\) and \\(g_1(x):= \\begin{cases}1 &amp;x = a\\\\0&amp;x\\neq a\\end{cases}\\). \\(f_1(x) g_1(x) = 0\\) is continuous at \\(a\\).  Take \\(f_2(x) := 1\\) and \\(g_2(x):= \\begin{cases}1 &amp;x = a\\\\0&amp;x\\neq a\\end{cases}\\). \\(f_2(x)g_2(x) = g_2(x)\\) is discontinuous at \\(a\\). </p> <p>Both \\(f\\) and \\(g\\) are discontinuous: Take \\(f_3(x) := \\begin{cases}1 &amp;x = a\\\\0&amp;x\\neq a\\end{cases}\\) and \\(g_3(x):= \\begin{cases}0 &amp;x = a\\\\1&amp;x\\neq a\\end{cases}\\). Obviously \\(f_1(x)g_1(x) = 0\\) is continuous at \\(a\\).  Take \\(f_4(x) := \\begin{cases}1 &amp;x = a\\\\0&amp;x\\neq a\\end{cases}\\) and \\(g_4(x):= \\begin{cases}1 &amp;x = a\\\\0&amp;x\\neq a\\end{cases}\\). Obviously \\(f_4(x)g_4(x) = f_4(x)\\) is continuous at \\(a\\).  </p>"},{"location":"apm462/calculus_review.html#claim-4","title":"Claim 4","text":"<p>if \\(f:\\mathbb R^n\\rightarrow \\mathbb R\\) is continuous, then \\(S = \\{x\\in\\mathbb R^n:f(x)&gt;0\\}\\) is open.</p> <p>proof. Let \\(f:\\mathbb R^n\\rightarrow \\mathbb R\\) be continuous. Let \\(x_0\\in\\mathbb R^n\\) be arbitrary, assume \\(f(x_0) = L &gt; 0\\). By continuity of \\(f\\), take \\(\\delta &gt; 0\\) s.t. \\(\\forall x\\in\\mathbb R^n. |x-x_0| &lt; \\delta \\Rightarrow |f(x) - f(x_0)| &lt; L/2\\). Then, note that \\(|f(x) - f(x_0)| &lt; L/2 \\Rightarrow f(x) &gt; f(x_0) - L/2 = L/2 &gt; 0\\).  This implies that \\(\\forall x \\in B(\\delta, x_0). x\\in S\\). Therefore, we have shown that \\(\\forall x_0 \\in S. \\exists \\delta &gt; 0. s.t. B(\\delta, x_0) \\subset S\\), which means \\(S\\) is open. </p>"},{"location":"apm462/calculus_review.html#examples-1","title":"Examples 1","text":"<p>Find \\(a\\) s.t. \\(\\forall \\delta &gt; 0, \\exists x \\in S. 0 &lt; |x-a| &lt; \\delta\\)</p>"},{"location":"apm462/calculus_review.html#examples-11","title":"Examples 1.1","text":"<p>\\(S = \\{(x, y)\\in\\mathbb R^2. x^2 +y^2 &lt; 1\\}\\cup\\{(0, 2)\\}\\). </p> <p>Claim. \\(\\forall a\\in \\{(x, y)\\in\\mathbb R^2. x^2 +y^2 &lt; 1\\}\\), the condition is satisfied. </p> <p>proof. Let \\(a = (x_0, y_0)\\) s.t. \\(x_0^2 + y_0^2 = c &lt; 1\\) be arbitrary.  Take \\(\\epsilon_0 = 1-c, 0 &lt; \\epsilon_0 &lt; 1\\). Let \\(\\delta &gt; 0\\) be arbitrary. Take \\(\\epsilon = \\min(\\frac{\\epsilon_0}4, \\frac{\\epsilon_0}{4|x_0 + y_0|}, \\frac{\\delta}2)\\), then note that </p> \\[|2\\epsilon(x_0 + y_0)| \\leq 2\\frac{\\epsilon_0}{4|x_0+y_0|}|x_0+y_0| \\leq \\epsilon_0/2\\] \\[2\\epsilon^2 \\leq 2\\frac{\\epsilon_0^2}{4^2} = \\epsilon_0^2/8\\] <p>so that </p> \\[\\begin{align*} (x_0 + \\epsilon)^2 + (y_0 + \\epsilon)^2 &amp;\\leq x^2_0 + y^2_0  + |2\\epsilon (x_0 + y_0)| + 2\\epsilon^2\\\\ &amp;\\leq x_0^2 + y_0^2 + \\epsilon_0/2 + \\epsilon_0^2/8\\\\ &amp;\\leq x_0^2 + y_0^2 + \\epsilon_0/2 + \\epsilon_0/8 &amp;0&lt;\\epsilon_0&lt;1\\\\ &amp;&lt; x_0^2 + y_0^2 + \\epsilon_0\\\\ &amp;= c + 1 - c \\\\ &amp; = 1\\\\ &amp;\\Rightarrow (x_0 + \\epsilon, y_0 + \\epsilon) \\in S \\end{align*}\\] <p>Also, </p> \\[|(x_0, y_0) - (x_0 + \\epsilon, y_0 + \\epsilon)| = \\sqrt 2\\epsilon \\leq \\sqrt 2\\frac{\\delta}2 = \\delta/\\sqrt 2 &lt; \\delta\\] <p>Therefore, \\(\\forall a\\in S_1. \\forall \\delta &gt; 0, \\exists x\\in S. |x-a| &lt; \\delta\\)</p> <p>Claim. \\(a = (0, 2)\\) does not satisfy the condition. </p> <p>proof. Let \\((x_0, y_0)\\in S - \\{(0, 2)\\} = \\{(x, y)\\in\\mathbb R^2: x^2 +y^2 &lt; 1\\}\\).  Note that \\(0 &lt; x_0^2 + y_0^2 &lt; 1\\), so that \\(-\\sqrt1 &lt; y_0 &lt; \\sqrt 1\\). Therefore, \\((y_0-2)^2 &gt; (1-2)^2 = 1\\)</p> \\[\\begin{align*} |(x_0, y_0) - (0, 2)| &amp;= \\sqrt{(x_0-0)^2 + (y_0-2)^2} \\\\ &amp;\\geq \\sqrt{(y_0-2)^2}\\\\ &amp;\\geq 1 \\end{align*}\\] <p>Therefore, \\(\\forall x\\in S. |x-(0, 2)| \\leq 0 \\lor |x-(0, 2)|\\geq 1\\)</p>"},{"location":"apm462/calculus_review.html#examples-12","title":"Examples 1.2","text":"<p>\\(S = \\{(x, y)\\in\\mathbb R^2. x^2 +y^2 &lt; 1\\}\\cup\\{(0, 1)\\}\\).</p> <p>Claim. \\(\\forall a\\in S\\), the condition is satisfied.   </p> <p>proof. We only need to show that </p> \\[\\forall \\delta &gt; 0. \\exists x\\in S. 0 &lt; |x-(0, 1)| &lt; \\delta\\] <p>The rest are proven in (1). </p> <p>Let \\(\\delta &gt; 0\\). Take \\(\\delta_0 = \\min(0.1, \\delta/2)\\). Then, </p> \\[|(0, 1-\\delta_0) - (0, 1)| = 1-\\delta_0 - 1 = \\delta_0\\] <p>Note that \\(\\delta_0 \\leq 2/\\delta &lt; \\delta\\) and \\(0 + (1-\\delta_0)^2 = 1 - 2\\delta_0 + \\delta_0^2 \\leq 1 - 0.2 + 0.01 &lt; 1\\) Therefore, \\((0, 1-\\delta_0)\\in S\\) and \\(0 &lt; |(0, 1-\\delta_0) - (0, 1)| &lt; \\delta\\). </p>"},{"location":"apm462/calculus_review.html#examples-13","title":"Examples 1.3","text":"<p>\\(S = \\{(0, 2^{-n})\\in\\mathbb R^2:n\\in\\mathbb N\\}\\)</p> <p>Claim. \\(\\forall a\\in S\\), the condition does NOT satisfy. </p> <p>proof. Let \\(a = (0, 2^{-n})\\in S\\) for some \\(n\\in\\mathbb N\\). Take \\(\\delta = 2^{-(n+1)} &gt; 0\\). Then, let \\(m\\in\\mathbb N, m\\neq n\\). </p> \\[|(0, 2^{-n}) - (0, 2^{-m})| = |2^{-n}- 2^{-m}| = 2^{-n}|1-2^{n-m}|\\] <p>Suppose \\(m &gt; n\\), then \\(2^{n-m} \\leq 2^{-1}, |1-2^{n-m}|\\geq 1/2\\). Suppose \\(n &gt; m\\), then \\(2^{n-m} \\geq 2, |1-2^{n-m}| \\geq 1\\). </p> <p>Therefore, \\(|(0, 2^{-n}) - (0, 2^{-m})| \\geq 2^{-n}/2 = 2^{-(n+1)}\\)</p>"},{"location":"apm462/calculus_review.html#examples-14","title":"Examples 1.4","text":"<p>\\(S = \\{(0, 2^{-n})\\in\\mathbb R^2:n\\in \\mathbb N\\}\\cup \\{(0, 0)\\}\\)</p> <p>Claim. \\(a = (0, 0)\\) satisfies the condition</p> <p>proof. For any \\(\\delta &gt; 0\\), we can find some \\(n\\in\\mathbb N\\) s.t. \\(2^{-n} &lt; \\delta\\) so that \\(0 &lt; |(0, 2^{-n}) - (0, 0)| = 2^{-n} &lt; \\delta\\)</p> <p>Claim. \\(\\forall a\\in \\{(0, 2^{-n})\\in\\mathbb R^2. n\\in\\mathbb N\\}, a\\) does not satisfy the condition. </p> <p>proof. In addition to points in (3), further notice that  \\(\\forall n\\in\\mathbb N\\). \\(|(0, 2^{-n}) - (0, 0)| = 2^{-n} &gt; 2^{-(n+1)}\\)</p>"},{"location":"apm462/calculus_review.html#claim-5","title":"Claim 5","text":"<p>For \\(f:\\mathbb R^n\\rightarrow \\mathbb R^k\\) and \\(g: \\mathbb R^k\\rightarrow\\mathbb R^l\\), if \\(f\\) is continuous at \\(a\\in\\mathbb R^n\\) and \\(g\\) is continuous at \\(f(a)\\), then \\(g\\circ f\\) is continuous at \\(a\\). </p> <p>proof. Let \\(\\epsilon &gt; 0\\). Because \\(g\\) is continuous at \\(f(a)\\), take \\(\\delta_1 &gt; 0\\) s.t. \\(\\forall y\\in\\mathbb R^k. |y-f(a)| &lt; \\delta_1 \\Rightarrow |g(y) - g(f(a))| &lt; \\epsilon\\). Because \\(f\\) is continuous at \\(a\\), take \\(\\delta_2 &gt; 0\\) s.t. \\(\\forall x\\in\\mathbb R^n. |x - a|  &lt; \\delta_2 \\Rightarrow |f(x) - f(a)| &lt; \\delta_1\\) Therefore, for any \\(x\\in\\mathbb R^n. |x-a| &lt;\\delta_2\\Rightarrow |f(x) - f(a)| &lt; \\delta_1\\Rightarrow |g(f(x)) - g(f(a))| &lt; \\epsilon\\). </p>"},{"location":"apm462/calculus_review.html#sequences-and-completeness","title":"Sequences and completeness","text":""},{"location":"apm462/calculus_review.html#claim-1_1","title":"Claim 1","text":"<p>\\(\\vec a_j\\rightarrow \\vec a \\land \\vec b_j\\rightarrow \\vec b\\Rightarrow \\vec a_j\\cdot \\vec b_j \\rightarrow \\vec a \\cdot \\vec b\\)</p> <p>proof. First note that \\(\\vec a_j \\rightarrow \\vec a\\Rightarrow \\forall i\\in \\{1, 2, ..., n\\}. a_{ij}\\rightarrow a_{i\\cdot}, b_{ij}\\rightarrow b_{i\\cdot}\\), where \\(a_{ij}\\) is the \\(i\\)th component of \\(\\vec a_j\\) and \\(a_{i\\cdot}\\) is the \\(i\\)th component of \\(\\vec a\\). Then, note that </p> \\[\\vec a_j\\cdot \\vec b_j = \\sum_{i=1}^n a_{ij}b_{ij}\\] <p>Therefore, apply limit laws for addition and multiplication for 1-D case, </p> \\[\\lim_{j\\rightarrow\\infty}\\sum_{i=1}^n a_{ij}b_{ij} = \\sum_{i=1}^n a_{i\\cdot}b_{i\\cdot} = \\vec a\\cdot \\vec b\\]"},{"location":"apm462/calculus_review.html#claim-2_1","title":"Claim 2","text":"<p>For \\(\\{a_j\\}_j\\subset \\mathbb R^n\\) and function \\(f:\\mathbb R^n\\rightarrow\\mathbb R^k\\). If \\(a_j\\rightarrow a\\) and \\(f\\) continuous and \\(a\\), then \\(f(a_j)\\rightarrow f(a)\\). </p> <p>proof. Let \\(\\epsilon &gt; 0\\) be arbitrary. Since \\(f\\) is continuous at \\(a\\), take \\(\\delta &gt; 0\\) s.t. \\(\\forall x\\in\\mathbb R^n. |x-a|&lt;\\delta\\Rightarrow |f(x)-f(a)| &lt; \\epsilon\\).  Since \\(a_j\\rightarrow a\\), take \\(N\\in\\mathbb N^+\\) s.t. \\(\\forall n \\in\\mathbb N^+. n &gt; N\\Rightarrow |a_n - a| &lt; \\delta\\). Therefore, \\(|a_n - a| &lt; \\delta \\Rightarrow |f(a_n) - f(a)| &lt; \\epsilon\\). We have proven that \\(\\forall \\epsilon &gt; 0. \\exists N\\in\\mathbb N^+. \\forall n \\in\\mathbb N^+ . n &gt; N \\Rightarrow |f(a_n) - f(a)| &lt; \\epsilon\\). By definition of convergence, \\(\\lim_{j\\rightarrow \\infty}f(a_j) = f(a)\\)</p>"},{"location":"apm462/calculus_review.html#compactness-and-applications","title":"Compactness and applications","text":""},{"location":"apm462/calculus_review.html#claim-1_2","title":"Claim 1","text":"<p>If \\(S \\subseteq \\mathbb R^n\\) is closed and \\(\\{x_j\\}_j \\subseteq\\mathbb R^n\\) converges to \\(x\\not\\in S\\), then \\(\\exists j\\in\\mathbb N^+. x_j\\not\\in S\\). </p> <p>proof. First, note that \\(x\\not\\in S\\Rightarrow x\\in S^c\\) and \\(S\\) is closed \\(\\Rightarrow S^c\\) is open. By the definition of open set, take \\(\\epsilon &gt; 0\\) s.t. \\(B(\\epsilon, x) \\subset S^c\\). Then, note that \\(x_j\\rightarrow x\\), by the definition of convergent sequence, take \\(N\\in\\mathbb N^+\\) s.t. \\(\\forall n \\in\\mathbb N^+. n \\geq N\\Rightarrow |x_n - x| &lt; \\epsilon\\). Therefore, note that \\(|x_N - x|&lt; \\epsilon \\Rightarrow x_N \\in B(\\epsilon, x) \\subset S^c\\Rightarrow x_N\\not\\in S\\)</p>"},{"location":"apm462/calculus_review.html#claim-2_2","title":"Claim 2","text":"<p>If \\(S\\) is unbounded, then exists some \\(\\{x_j\\}_j\\subset S\\) that has no convergent subsequence. </p> <p>proof. Construct \\(\\{x_j\\}_j\\) by the following recursive procedure. Let \\(x_1\\in S, x_1\\neq 0\\) be arbitrary, let \\(r_1 = \\|x_1\\|\\). Given \\(x_n, r_n\\), let \\(x_{n+1} \\in S - B(2r_n, 0), r_{n+1} = \\|x_{n+1}\\|\\). Note that such \\(x_{n+1}\\) always exists since \\(S\\) is unbounded.  Also, note that \\(r_{n+1} = \\|x_{n+1}\\| \\geq 2r_n \\geq 2^{n-1} r_1\\). </p> <p>Then, we will show such sequence has no convergent subsequence.  Given any subsequence \\(\\{x_{k_j}\\}_j\\), Let \\(L\\in S\\) be arbitrary, let \\(J &gt; 0\\) be arbitrary. Take some \\(j \\geq J\\) s.t. \\(2^{k_j-1}r_1  \\geq 1 + \\|L\\|\\), since \\(r_1 &gt; 0\\) we can always find such \\(j\\).  Therefore, </p> \\[\\begin{align*} \\|x_{k_j} - L\\| &amp;\\geq \\big\\| \\|x_{k_j}\\| - \\|L\\|\\big\\| &amp;\\text{reverse triangle ineq.} \\\\ &amp;= 2^{k_j-1}r_1 - \\|L\\| \\\\ &amp;\\geq 1 + \\|L\\| - \\|L\\|\\\\ &amp; = 1 \\end{align*}\\] <p>Therefore, \\(\\{x_{k_j}\\}_j\\) diverges. </p>"},{"location":"apm462/calculus_review.html#lemma-3","title":"Lemma 3","text":"<p>Any subsequence of convergent sequence will also converge to the same limit.  </p> <p>proof. Let \\(\\{a_j\\}_j\\) be convergent to some \\(L\\). Let \\(\\{a_{k_j}\\}_j\\) be some subsequence.   Let \\(\\epsilon &gt; 0\\) be arbitrary, take \\(J\\in\\mathbb N^+\\) s.t. \\(\\forall j &gt; J. \\|a_j-L\\| &lt; \\epsilon\\). Take \\(J_2 \\in \\mathbb N^+\\) s.t. \\(k_{J_2} \\geq J\\) so that \\(\\forall j &gt; J_2. k_j &gt; k_{J_2} \\geq J\\). Therefore, \\(\\forall j &gt; J_2. \\|a_{k_j}- L\\| &lt; \\epsilon\\). </p>"},{"location":"apm462/calculus_review.html#claim-4_1","title":"Claim 4","text":"<p>The Cartesian Product of two compact sets is compact. </p> <p>proof. Let \\(K_1 \\subseteq \\mathbb R^n, K_2 \\subseteq \\mathbb R^m\\)  be compact. Let \\(\\{(x_j, y_j)\\}_j\\) be a sequence in \\(K_1\\times K_2\\) so that \\(\\{x_j\\}_j\\in K_1, \\{y_j\\}_j \\in K_2\\). By compactness of \\(K_1\\), take subsequence \\(\\{x_{k_j}\\}_j\\) that converge to some \\(L_1\\in K_1\\). By compactness of \\(K_2\\), further take \\(\\{y_{l_{k_j}}\\}_j\\) be a subsequence of \\(\\{y_{k_j}\\}_j\\) that converge to some \\(L_2\\in K_2\\). Then, since \\(\\{x_{l_{k_j}}\\}_j\\) is a subsequence of \\(\\{x_{k_j}\\}_j\\), \\(\\{x_{l_{k_j}}\\}_j\\) will also converge to \\(L_1\\) (using lemma above). Therefore, \\(\\{(x_{l_{k_j}}, y_{l_{k_j}})\\}_{j}\\rightarrow (L_1, L_2) \\in K_1\\times K_2\\) By definition of compactness, \\(K_1\\times K_2\\) is compact. </p>"},{"location":"apm462/calculus_review.html#claim-5_1","title":"Claim 5","text":"<p>The closed subset of a compact set is compact.  </p> <p>proof 1.  If \\(S\\) is compact, then by BW theorem \\(S\\) is closed and bounded. so that \\(\\forall s\\in S. s\\in B(R, 0)\\) for some \\(R &gt; 0\\). Then, for any closed subset \\(A \\subseteq S\\), \\(s\\in A\\Rightarrow s\\in S\\Rightarrow s\\in B(R, 0)\\) so that \\(A\\) is also bounded. Therefore, \\(A\\) is also closed and bounded, hence compact. </p>"},{"location":"apm462/calculus_review.html#chain-rule","title":"Chain Rule","text":""},{"location":"apm462/calculus_review.html#claim-1_3","title":"Claim 1","text":"<p>\\(q(x): = |x|^2. \\nabla q(x) = 2x\\)</p> <p>proof For any \\(x\\in\\mathbb R^n\\), </p> \\[\\begin{align*} \\lim_{h\\rightarrow 0} \\frac{q(x+h) - q(x) - 2x\\cdot h}{|h|} &amp;= \\lim_{h\\rightarrow 0} \\frac{|x+h|^2 - |x|^2 - 2x\\cdot h}{|h|}\\\\ &amp;= \\lim_{h\\rightarrow 0}\\frac{|x|^2 + 2x\\cdot h + |h|^2 -|x|^2-2x\\cdot h}{|h|}\\\\ &amp;= \\lim_{h\\rightarrow 0} \\frac{|h|^2}{|h|}\\\\ &amp;= \\lim_{h\\rightarrow} h\\\\ &amp;= 0 \\end{align*}\\] <p>Therefore, by the definition of derivative \\(\\nabla q = 2x\\)</p>"},{"location":"apm462/calculus_review.html#example-1","title":"Example 1","text":"<p>Prove chain rule for \\(g: \\mathbb R\\rightarrow \\mathbb R^2, f: \\mathbb R^2 \\rightarrow \\mathbb R. \\phi = f\\circ g\\)</p> <p>Given that </p> \\[\\begin{align*} \\phi(t+h) - \\phi(t) = &amp;[f(x(t+h), y(t+h)) - f(x(t+h), y(t))] &amp;(i)\\\\ &amp;\\quad+ [f(x(t+h), y(t)) - f(x(t), y(t))] &amp;(ii) \\end{align*}\\] <p>First, write \\(g(x)=f(x, y)\\) so that</p> \\[\\begin{align*} (i) &amp;= g(x(t+h)) - f(x(t)) \\\\ &amp;= (x(t+h) - x(t))g'(x(t)+\\theta_{11}(x(t+h) - x(t)))\\\\ &amp;= h(x'(t+\\theta_{12}h))g'(x(t)+\\theta_{11}(x(t+h) - x(t)))\\\\ &amp;= h\\frac{\\partial x}{\\partial t}(t+\\theta_{12}h)\\frac{\\partial f}{\\partial x}(x(t) - \\theta_{11}(x(t+h) - x(t))) \\end{align*}\\] <p>by MVT, where \\(\\theta_{11}, \\theta_{12}\\in (0, 1)\\). Similarly, </p> \\[(ii) = h\\frac{\\partial y}{\\partial t}(t+\\theta_{22}h)\\frac{\\partial f}{\\partial y}(y(t) - \\theta_{21}(y(t+h) - y(t)))\\] <p>Therefore, </p> \\[\\begin{align*} \\phi'(t) &amp;= \\lim_{h\\rightarrow 0} h^{-1} \\phi(t+h) - \\phi(t)\\\\ &amp;= \\lim_{h\\rightarrow 0} h^{-1}\\\\ &amp;\\quad\\bigg[h\\frac{\\partial x}{\\partial t}(t+\\theta_{12}h)\\frac{\\partial f}{\\partial x}(x(t) - \\theta_{11}(x(t+h) - x(t))) \\\\ &amp; \\quad + h\\frac{\\partial y}{\\partial t}(t+\\theta_{22}h)\\frac{\\partial f}{\\partial y}(y(t) - \\theta_{21}(y(t+h) - y(t)))\\bigg]\\\\ &amp;= \\lim_{h\\rightarrow 0} \\\\ &amp;\\quad\\bigg[\\frac{\\partial x}{\\partial t}(t+\\theta_{12}h)\\frac{\\partial f}{\\partial x}(x(t) - \\theta_{11}(x(t+h) - x(t))) \\\\ &amp; \\quad + \\frac{\\partial y}{\\partial t}(t+\\theta_{22}h)\\frac{\\partial f}{\\partial y}(y(t) - \\theta_{21}(y(t+h) - y(t)))\\bigg] &amp;(*) \\end{align*}\\] <p>Then, by our assumption, \\(x, y\\) are differentiable, hence continuous, by continuous mapping theorem, </p> \\[\\lim_{h\\rightarrow 0} x(t+h) = x(t). \\lim_{h\\rightarrow 0} y(t+h) = y(t)\\] <p>In addition, \\(\\theta_{11}, \\theta_{12}, \\theta_{21}, \\theta_{22}\\in (0, 1)\\). Therefore, the limit above exists and equals to</p> \\[(*) = \\frac{\\partial x}{\\partial t}(t)\\frac{\\partial f}{\\partial x}(x(t)) + \\frac{\\partial y}{\\partial t}(t)\\frac{\\partial f}{\\partial y}(y(t)) = \\frac{\\partial f}{\\partial x}\\frac{\\partial x}{\\partial t} + \\frac{\\partial f}{\\partial y}\\frac{\\partial y}{\\partial t}\\]"},{"location":"apm462/co_functional.html","title":"Constrainted Optimization on Calulus of Variations","text":""},{"location":"apm462/co_functional.html#isoperimetric-constraints","title":"Isoperimetric Constraints","text":""},{"location":"apm462/co_functional.html#thrm-first-order-necessary-conditions","title":"Thrm. First Order Necessary Conditions","text":"<p>Define functionals</p> \\[\\begin{align*} F[u] := \\int_a^b L^F(x, u(x), u'(x)) \\, dx \\\\ G[u] L= \\int_a^b L^G(x, u(x), u'(x)) \\, dx. \\end{align*}\\] <p>Our problem is of the form</p> \\[\\begin{align*} \\min_{u \\in \\mathcal{A}} \\qquad &amp;F[u] \\\\ \\text{subject to } &amp;G[u] = \\mathrm{const} \\\\ \\mathcal{A} &amp;= \\{ u \\in C^1([a, b], \\mathbb R) : u(a) = A, u(b) = B \\}. \\end{align*}\\] <p>Suppose \\(u_*\\) is a regular point of \\(G\\) (i.e. \\(\\frac{\\delta G}{\\delta u}(u_*) \\neq 0\\)) which is a minimizer of \\(F\\) subject to the equality constraint. Then there is a \\(\\lambda \\in \\mathbb R\\) such that</p> \\[\\frac{\\delta F}{\\delta u}(u_*) + \\lambda \\frac{\\delta G}{\\delta u}(u_*) = 0\\] <p>The completed proof is too long, you can see a proof on one case in the Additional Examples</p> <p>Recall that</p> \\[\\frac{\\delta F}{\\delta u}(u_*) = -\\frac{d}{dx} L_p^F + L_z^F\\] <p>and similarly for \\(\\frac{\\delta G}{\\delta u}(u_*)\\), so we can write the first order necessary conditions that we just described as </p> \\[-\\frac{d}{dx} \\left[ (L^F + \\lambda L^G)_p \\right] + \\left( L^F + \\lambda L^G \\right)_z = 0\\] <p>where it is understood at which point the partials of the Lagrangians are evaluated.</p>"},{"location":"apm462/co_functional.html#holonomic-constraints","title":"Holonomic Constraints","text":"<p>We consider a functional \\(F\\) depending on three functions:</p> \\[F[\\underbrace{x(\\cdot), y(\\cdot), z(\\cdot)}_{u(\\cdot)}] = \\int_a^b L(t, \\underbrace{x(t), y(t), z(t)}_{u(t)}, \\underbrace{x'(t), y'(t), z'(t)}_{u'(t)}) \\, dx\\] <p>subject to a \\(C^1\\)constraint \\(H(x(t),y(t),z(t)) = 0\\) for \\(t \\in [a, b]\\) with \\(\\nabla H \\neq 0\\) on a space </p> \\[\\mathcal{A} = \\{ u : [a, b] \\to \\mathbb R^3 : u \\text{ is } C^1, u(a) = A, u(b) = B \\}\\] <p>(So the function \"curves\" lie on a surface in \\(\\mathbb R^3\\).) We would like to minimize \\(F[u(\\cdot)]\\) where \\(H(u(t)) = 0\\) and \\(u \\in \\mathcal{A}\\). The Euler-Lagrange equations in this case are</p> \\[\\begin{pmatrix} \\frac{\\delta F}{\\delta x}(u)(t) \\\\ \\frac{\\delta F}{\\delta y}(u)(t) \\\\ \\frac{\\delta F}{\\delta z}(u)(t) \\end{pmatrix} + \\lambda(t) \\begin{pmatrix} H_x(u(t)) \\\\ H_y(u(t)) \\\\ H_z(u(t)) \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix}\\] <p>We may succinctly write this as</p> \\[\\frac{\\delta F}{\\delta u} + \\lambda (t) \\nabla H = 0\\] <p>which parallels nicely with the finite-dimensional case, if we consider variational problems to be infinite-dimensional problems.</p>"},{"location":"apm462/co_functional.html#example-motion-of-a-spherical-pendulum","title":"Example: Motion of a Spherical Pendulum","text":"<p>Consider again the example of the spherical pendulum. The Lagrangian function is</p> \\[L(t, x,y,z, \\dot{x}, \\dot{y}, \\dot{z}) = \\frac{1}{2} m (\\dot{x}^2 + \\dot{y}^2 + \\dot{z}^2) - mgz\\] <p>(Kinetic energy minus potential energy.) Thus the functional we wish to minimize is</p> \\[F[x(\\cdot), y(\\cdot), z(\\cdot)] = \\int_a^b L(t, x(t), y(t), \\dot{x}(t), \\dot{y}(t), \\dot{z}(t)) \\, dt\\] <p>subject to the constraint that \\((x(t), y(t), z(t))\\) is on the sphere of radius \\(\\ell\\), i.e.</p> \\[H(x(t), y(t), z(t)) = \\frac{1}{2}(x(t)^2 + y(t)^2 + z(t)^2) - \\frac{1}{2} \\ell^2 = 0\\] <p>This is a holonomic constraints problem. Last week we solved this by parametrizing the sphere with spherical coordinates, freeing ourselves of the constraint. Now, we can use the first order necessary constraints to solve this. </p> <p>Since the pendulum takes the path which minimizes the net difference in kinetic and potential energy, solutions to this problem will be the paths the pendulum takes depending on the initial conditions. </p> <p>To match the notation we had before, we have</p> \\[\\begin{align*} L(t, z_1,z_2,z_3, p_1,p_2,p_3) &amp;= \\frac{1}{2} (p_1^2 + p_2^2 + p_3^2) - mgz_3 \\\\ H(x,y,z) &amp;= \\frac{1}{2} (x^2+y^2+z^2) - \\frac{1}{2} \\ell^2. \\end{align*}\\] <p>The partials we want are are</p> \\[\\begin{align*} L_{p_1} &amp;= mp_1 &amp;L_{p_2} &amp;= mp_2 &amp;L_{p_3} &amp;= mp_3 \\\\ L_{z_1} &amp;= 0 &amp;L_{z_2} &amp;= 0 &amp;L_{z_3} &amp;= -mg \\\\ H_x &amp;= x &amp;H_y &amp;= y &amp;H_z &amp;= z \\end{align*}\\] <p>We have</p> \\[\\begin{align*} \\frac{\\delta F}{\\delta x} = -m \\ddot{x}(t) \\quad\\frac{\\delta F}{\\delta y} = -m \\ddot{y}(t) \\quad\\frac{\\delta F}{\\delta y} = -m \\ddot{z}(t) - mg. \\end{align*}\\] <p>The Euler-Lagrange equations are</p> \\[\\begin{pmatrix} -m \\ddot{x}(t) \\\\ -m \\ddot{y}(t) \\\\ -m \\ddot{z}(t) - mg \\end{pmatrix} + \\lambda (t) \\begin{pmatrix} x(t) \\\\ y(t) \\\\ z(t) \\end{pmatrix} \\equiv \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix}\\] <p>subject to the constraint \\(x(t)^2 + y(t)^2 + z(t)^2 = \\ell^2\\). There are four equations are four unknowns, so, in principle, the system may be solved.</p> <p>We will solve this in the special case that \\(y \\equiv 0\\), \\(\\ell = 1\\), and \\(m = 1\\). We lie in the \\(xy\\)-plane. We obtain</p> \\[\\begin{align*} \\ddot{x}(t) &amp;= \\lambda(t)x(t) \\\\ \\ddot{z}(t) &amp;= \\lambda(t)z(t) - g \\\\ x(t)^2 + z(t)^2 &amp;= 1. \\end{align*}\\] <p>Parametrize by \\(x(t) = \\sin \\theta(t)\\) and \\(z(t) = -\\cos \\theta(t)\\), so that</p> \\[\\begin{align*} \\dot{x} &amp;= \\dot{\\theta} \\cos \\theta \\\\ \\dot{z} &amp;= \\dot{\\theta} \\sin \\theta \\end{align*}\\] <p>and</p> \\[\\begin{align*} \\ddot{x} &amp;= \\ddot{\\theta} \\cos \\theta - \\dot{\\theta}^2 \\sin \\theta, \\\\ \\ddot{z} &amp;= \\ddot{\\theta} \\sin \\theta + \\dot{\\theta}^2 \\cos \\theta. \\end{align*}\\] <p>Plugging these into the Euler-Lagrange equations gives</p> \\[\\begin{align*} \\ddot{\\theta} \\cos \\theta - \\dot{\\theta}^2 \\sin \\theta &amp;= \\lambda \\sin \\theta \\\\ \\ddot{\\theta} \\sin \\theta + \\dot{\\theta}^2 \\cos\\theta &amp;= -\\lambda \\cos \\theta - g. \\end{align*}\\] <p>Multiplying the second equation by \\(\\sin \\theta\\) gives, after a bit of simplifying,</p> \\[\\ddot{\\theta} \\sin^2 \\theta + \\dot{\\theta}^2 \\sin \\theta \\cos \\theta = -\\ddot{\\theta} \\cos^2 \\theta + \\dot{\\theta}^2 \\sin \\theta \\cos \\theta - g \\sin \\theta\\] <p>This simplifies to</p> \\[\\ddot{\\theta} = -g \\sin \\theta\\] <p>the equation of motion of a planar pendulum.</p>"},{"location":"apm462/co_functional.html#sufficient-conditions-and-convexity","title":"Sufficient Conditions and Convexity","text":"<p>Recall that if \\(u_* \\in \\mathcal{A} = \\{ u \\in C^1([a, b]) : u(a) = A, u(b) = B \\}\\) is a minimizer of the functional</p> \\[F[u(\\cdot)] := \\int_a^b L(x, u(x), u'(x)) \\, dx\\] <p>Then it satisfies the \"primitive\" Euler-Lagrange equation</p> \\[\\int_a^b \\Bigg[ L_z(x, u_*(x), u_*'(x))v(x) +  L_p(x, u_*(x), u_*'(x))v'(x) \\Bigg] \\, dx = 0\\] <p>for all test functions \\(v\\) on \\([a, b]\\).</p> <p>Lemma Assume the conditions above. Suppose \\(L = L(x,z,p)\\) is a \\(C^1\\) function, and that for each \\(x \\in [a, b]\\), \\(L(x, \\cdot, \\cdot)\\) is convex. If \\(u_*\\) satisfies (*) above, then \\(F[u_*(\\cdot)] \\leq F[u_*(\\cdot) + v(\\cdot)]\\) for all test functions \\(v\\) on \\([a, b]\\).</p> <p>The lemma states, roughly, that if \\(u_*\\) is a minimizer, then it must be a global minimizer.</p> <p>proof. Recall the \\(C^1\\) criterion for convexity of a function \\(f : \\mathbb R^n \\to \\mathbb R\\): \\(f\\) is convex if and only if \\(f(a + b) \\geq f(a) + \\nabla f(a) \\cdot b\\) for all \\(a,b \\in \\mathbb R^n\\). We will apply this criterion to the convex function \\(L(x, \\cdot, \\cdot) : \\mathbb R \\times \\mathbb R \\to \\mathbb R\\). Directly applying the criterion gives</p> \\[L(x, z + \\tilde{z}, p + \\tilde{p}) \\geq L(x,z,p) + \\underbrace{\\nabla_{(z,p)}L(z,z,p) \\begin{pmatrix} \\tilde{z} \\\\ \\tilde{p} \\end{pmatrix}}_{=L_z(x,z,p)\\tilde{z} + L_p(x,z,p)\\tilde{p}}\\] <p>Then</p> \\[\\begin{align*} F[u_*(\\cdot) + v(\\cdot)] &amp;= \\int_a^b L(x, u_*(x) + v(x), u_*'(x) + v'(x)) \\, dx \\\\ &amp;\\geq \\int_a^b L(x, u_*(x), u_*'(x)) \\, dx +  \\underbrace{\\int_a^b \\Bigg[ L_z(\\cdots)v(x) + L_p(\\cdots)v'(x) \\Bigg] \\, dx}_{=0} \\\\ &amp;= \\int_a^b L(x, u_*(x), u_*'(x)) \\, dx \\\\ &amp;= L[u_*(\\cdot)]. \\end{align*}\\]"},{"location":"apm462/co_functional.html#thrm-convex-domains","title":"Thrm. Convex Domains","text":"<p>Recall that we were originally looking at convex functions \\(f : \\Omega \\to \\mathbb R\\), where \\(f\\) is \\(C^1\\) and convex and \\(\\Omega \\subseteq \\mathbb R^n\\) is convex. We had a theorem:</p> <p>Claim. Assume the above conditions. \\(x_*\\) is a minimizer of \\(f\\) on \\(\\Omega\\) if and only if \\(\\nabla f(x_*)(x - x_*) \\geq 0\\) for all \\(x \\in \\Omega\\). </p> <p>Apply on variational </p> <p>Consider a functional</p> \\[F[u(\\cdot)] := \\int_a^b L(x, u(x), u'(x)) \\, dx\\] <p>Let \\(\\mathcal{A} = \\{ u \\in C^1([a, b]) : u(a) = A, u(b) = B \\}\\). Let \\(\\Omega\\) be a convex subset of \\(\\mathcal{A}\\), and suppose that \\(u_*\\) is a minimizer of \\(F\\) on \\(\\Omega\\). Then \\(F[u_*(\\cdot) + sv(\\cdot)] \\geq F[u_*(\\cdot)]\\) for all \\(s\\) and all test functions \\(v\\) on \\([a, b]\\), and</p> \\[\\int_a^b \\frac{\\delta F}{\\delta u}(u_*)(x)(u(x) - u_*(x))\\, dx \\int_a^b \\frac{\\delta F}{\\delta u}(u_*)(x)v(x)\\,dx = \\left. \\frac{d}{ds} \\right|_{s=0} F[u_*(\\cdot) + sv(\\cdot)] \\geq 0\\] <p>for all \\(u \\in \\Omega\\). </p>"},{"location":"apm462/co_functional.html#example","title":"Example","text":"<p>Suppose \\(F[u(\\cdot)] = \\int_0^1 \\frac{1}{2} \\left( u'(t)^2 + u(t) \\right) \\, dt\\) where \\(\\mathcal{A}\\) is the set of \\(C^1\\) functions on \\([0, 1]\\) that are zero at the endpoints, and \\(\\Omega = \\{ u \\in \\mathcal{A} : u(t) \\geq \\sin^2(\\pi t) \\}\\). One can check that \\(\\Omega\\) is convex. The Lagrangian function is \\(L(x,z,p) = \\frac{1}{2}(p^2 + z)\\). This is a convex function of \\((z,p)\\), so it satisfies the conditions of the first lemma. Then if \\(u_*\\) minimizes \\(F\\) on \\(\\Omega\\), let \\(f(s) = F[(1 - s)u_*(\\cdot) + su(\\cdot)]\\). We are asked to show that \\(f'(0) \\geq 0\\). </p>"},{"location":"apm462/co_functional_q.html","title":"Examples: Constrainted Optimization on Calulus of Variations","text":""},{"location":"apm462/co_functional_q.html#example-1-proof-of-equality-constraint-el-equation","title":"Example 1 (Proof of equality constraint EL Equation)","text":"<p>Question</p> <p>Consider the problem</p> \\[\\begin{align*} \\text{minimize}\\quad &amp;I[x(\\cdot)] = \\frac12\\int_0^\\pi [x'(t)]^2 dt\\\\ \\text{subject to} \\quad &amp; J[x(\\cdot)] = \\int_0^\\pi [x(t)]^2 dt = 1\\\\ \\text{with conditions} \\quad &amp;x\\in \\mathcal A :=\\{x:[0,\\pi]\\rightarrow \\mathbb R\\mid x(0) = x(\\pi) = 0\\} \\end{align*}\\] <p>Suppose \\(x\\) is a \\(C^2\\) function that solve the problem, let \\(y\\in \\mathcal A\\) be \\(C^2\\), define</p> \\[a(s) = \\bigg[\\int_0^\\pi(x(t) + sy(t))^2 dt\\bigg]^{1/2}\\] \\[i(s) = I[\\frac{x(\\cdot) + sy(\\cdot))}{a(s)}]\\] <p>Part (a)</p> <p>Show that \\(a(0) = 1, i'(0) = 0\\)</p> <p>proof. </p> \\[a(0) = (\\int_0^\\pi (x(t) + 0y(t)^2)dt)^{1/2} = (\\int_0^\\pi x(t)^2dt)^{1/2}\\] <p>Since \\(x\\) solve the problem, it must follows the constraint \\(\\int_0^\\pi x(t) = 1\\)</p> \\[a(0) = (\\int_0^\\pi x(t)^2dt)^{1/2} = 1\\] <p>Since \\(x\\) is the minimizer, FONC gives that </p> \\[\\frac{d}{ds}\\mid_{s=0}I[x(\\cdot) + sy(\\cdot)] = 0\\] <p>for any test function \\(y\\in\\mathcal A\\), note that </p> \\[\\frac{d}{ds}\\mid_{s=0}I[x(\\cdot) + sy(\\cdot)] = \\frac{d}{ds}\\mid_{s=0}I[\\frac{x(\\cdot) + sy(\\cdot)}{1}] = i'(0) = 0\\] <p>Part (b)</p> <p>Show that</p> \\[i'(0) = \\int_0^\\pi x'(t)y'(t)dt - \\lambda \\int_0^\\pi x(t)y(t)dt\\] <p>for constant \\(\\lambda\\) in terms of \\(x(t)\\).</p> <p>proof.  By multiplication rule</p> \\[i'(s) = -2a(s)^{-3}a'(s)I[x + sy] + a(s)^{-2}\\frac{d}{ds}I[x+sy]\\] <p>Evaluate at \\(s=0\\) with \\(a(0) = 1\\), we have</p> \\[i'(0) = -2a'(0)I[x(\\cdot)] + \\left.\\frac{d}{ds}\\right\\vert_{s=0}I[x(\\cdot) + sy(\\cdot)]\\] <p>Then, note that </p> \\[\\begin{align*} a'(s) &amp;= \\frac{1}{2}\\bigg(\\int_0^\\pi(x(t) + sy(t))^2 dt\\bigg)^{-1/2}\\\\&amp;\\quad\\frac{d}{ds}\\int_0^\\pi(x(t) + sy(t))^2dt\\\\ &amp;= \\frac{1}{2}a(s)^{-1} 2\\int_0^\\pi (x(t) + sy(t))y(t)dt &amp;\\text{Leibniz rule}\\\\ a'(0) &amp;= \\int_0^\\pi(x(t)+0y(t))y(t)dt \\\\ &amp;= \\int_0^\\pi x(t)y(t)dt\\\\ \\frac{d}{ds}I[x + sy]&amp;= \\frac{1}{2}\\frac{d}{ds}\\int_0^\\pi(\\frac d{dt} x(t)+sy(t))^2 dt\\\\ &amp;= \\frac12\\frac{d}{ds}\\int_0^\\pi (x'(t) + sy'(t))^2dt\\\\ &amp;= \\frac12\\int_0^\\pi \\frac{d}{ds}(x'(t) + sy'(t))^2dt&amp;\\text{Leibniz rule}\\\\ &amp;= \\int_0^\\pi (x'(t)+sy'(t))y'(t)dt\\\\ \\left.\\frac{d}{ds}\\right\\vert_{s=0}I[x(\\cdot) + sy(\\cdot)]&amp;= \\int_0^\\pi(x'(t) + 0 y'(t))y'(t)dt \\\\ &amp;= \\int_0^\\pi x'(t)y'(t)dt\\\\ i'(0) &amp;= -2I[x(\\cdot)]\\int_0^\\pi x(t)y(t)dt + \\int_0^\\pi x'(t)y'(t)dt \\end{align*}\\] <p>so that let \\(\\lambda = -2 I[x(\\cdot)]\\) we have </p> \\[i'(0) = \\int_0^\\pi x'(t)y'(t)dt -\\lambda \\int_0^\\pi x(t)y(t)dt\\] <p>Part (c)</p> <p>Show that \\(x''(t) + \\lambda x(t) = 0\\) for \\(0 &lt;t&lt;\\pi\\).</p> <p>proof. Note that \\(i'(0) = 0\\) so that </p> \\[\\begin{align*} 0 &amp;= \\int_0^\\pi x'(t)y'(t)dt -\\lambda \\int_0^\\pi x(t)y(t)dt\\\\ &amp;= x'(t)y(t)\\vert^\\pi_0 - \\int_0^\\pi x''(t)y(t)dt -\\lambda\\int_0^\\pi x(t)y(t)dt&amp;\\text{integration by parts}\\\\ &amp;= 0 - \\int_0^\\pi x''(t)y(t) + \\lambda x(t)y(t)dt\\\\ 0 &amp;= \\int_0^\\pi (x''(t) + \\lambda x(t))y(t)dt \\end{align*}\\] <p>To satisfy this equation for all \\(y\\in\\mathcal A\\), we must have \\(x''(t) + \\lambda x(t) = 0\\)</p>"},{"location":"apm462/co_functional_q.html#example-2","title":"Example 2","text":"<p>Question</p> <p>Let \\(\\mathcal A = \\{u:[0,1]\\rightarrow\\mathbb R^3\\mid u\\in C^1 u(0)=A, u(1)=B\\}\\), consider the \"holonomic constraints\" problem</p> \\[\\begin{align*} \\text{minimize}\\quad F[u(\\cdot)]:=\\int_0^1 \\sqrt{u'_1(t)^2 + u'_2(t)^2 + u'_3(t)^2}dt\\\\ \\text{subject to}\\quad u\\in\\mathcal A, G(u(t)) = u_1(t)^2 + u_2(t)^2 = 1 \\end{align*}\\] <p>Find EL equations for this problem.</p> \\[\\begin{align*} L(t, z_1,z_2,z_3,p_1,p_2,p_3) &amp;= (p_1^2 + p_2^2+p_3^2)^{1/2}\\\\ L_z &amp;= 0\\\\ L_{p_i} &amp;= p_i(p_1^2 + p_2^2+p_3^2)^{-1/2} \\\\ &amp;= \\frac{u'_i(t)}{\\sqrt{u'_1(t)^2 + u'_2(t)^2 + u'_3(t)^2}}\\\\ \\nabla_u G &amp;= (2u_1(t), 2u_2(t), 0)\\\\ \\frac{\\partial F}{\\partial u_1} &amp;= 0 - \\frac{d}{dt}L_{p_i}\\\\ &amp;= -\\frac{d}{dt} \\frac{u'_i(t)}{\\sqrt{u'_1(t)^2 + u'_2(t)^2 + u'_3(t)^2}} \\end{align*}\\] <p>Therefore, for some \\(\\lambda:[0, 1]\\rightarrow \\mathbb R\\), the EL equations is </p> \\[\\begin{bmatrix} -\\frac{d}{dt} \\frac{u'_1(t)}{\\sqrt{u'_1(t)^2 + u'_2(t)^2 + u'_3(t)^2}}\\\\ -\\frac{d}{dt} \\frac{u'_2(t)}{\\sqrt{u'_1(t)^2 + u'_2(t)^2 + u'_3(t)^2}}\\\\ -\\frac{d}{dt} \\frac{u'_3(t)}{\\sqrt{u'_1(t)^2 + u'_2(t)^2 + u'_3(t)^2}} \\end{bmatrix} + \\lambda(t) \\begin{bmatrix} 2u_1(t)\\\\ 2u_2(t)\\\\ 0 \\end{bmatrix} =\\begin{bmatrix} 0\\\\0\\\\0 \\end{bmatrix}\\]"},{"location":"apm462/co_functional_q.html#example-3","title":"Example 3","text":"<p>Question</p> <p>Let \\(\\mathcal A = \\{u:[0,1]\\rightarrow\\mathbb R^2\\mid u\\in C^1, u(0)=A, u(1)=B\\}\\), consider the \"holonomic constraints\" problem.</p> \\[\\begin{align*} \\text{minimize}\\quad F[u(\\cdot)]:=\\int_0^1 \\sqrt{u'_1(t)^2 + u'_2(t)^2}dt\\\\ \\text{subject to}\\quad u\\in\\mathcal A, G(u(t)) = u_1(t) + u_2(t)^2 = 1 \\end{align*}\\] <p>Part (a)</p> <p>Find the EL Equation</p> <p>We can easily have the EL equations being</p> \\[\\begin{bmatrix} -\\frac{d}{dt} \\frac{u'_1(t)}{\\sqrt{u'_1(t)^2 + u'_2(t)^2}}\\\\ -\\frac{d}{dt} \\frac{u'_2(t)}{\\sqrt{u'_1(t)^2 + u'_2(t)^2}} \\end{bmatrix} + \\lambda(t) \\begin{bmatrix} 1\\\\ 2u_2(t) \\end{bmatrix} =\\begin{bmatrix} 0\\\\0 \\end{bmatrix}\\] <p>Part (b)</p> <p>Solve the problem as a unconstrainted problem</p> <p>Consider \\(u_2\\in \\{v:[0, 1]\\rightarrow \\mathbb R\\mid v(0) = a, v(1) = b, v\\in C^1\\}\\), the original problem is equivalent to minimize </p> \\[\\begin{align*} F[v(\\cdot)] &amp;= \\int_0^1 \\sqrt{u_2'(t)^2 + \\big[\\frac{d}{dt}(1-u_2(t)^2)\\big]^2}dt\\\\ &amp;= \\int_0^1 \\sqrt{u_2'(t)^2 + (-2u_2(t)u_2'(t))^2}\\\\ &amp;= \\int_0^1 \\sqrt{(1 + 4u_2(t)^2)u_2'(t)^2}dt\\\\ L(x, z, p) &amp;= \\sqrt{(1+4z^2)p^2}\\\\ L_z &amp;= ((1+4z^2)p^2)^{-1/2}(4p^2z)\\\\ &amp;= \\frac{4u_2(t)u_2'(t)^2}{\\sqrt{u_2'(t)^2 + \\big[\\frac{d}{dt}(1-u_2(t)^2)\\big]^2}}\\\\ L_p &amp;= ((1+4z^2)p^2)^{-1/2}(1+4z^2)p\\\\ \\frac{d}{dt}L_p &amp;= \\frac{d}{dt}\\frac{(1 + 4u_2(t)^2)u_2'(t)}{\\sqrt{u_2'(t)^2 + \\big[\\frac{d}{dt}(1-u_2(t)^2)\\big]^2}} \\end{align*}\\] <p>so that the EL equations give </p> \\[- \\frac{d}{dt}\\frac{(1 + 4u_2(t)^2)u_2'(t)}{\\sqrt{u_2'(t)^2 + \\big[\\frac{d}{dt}(1-u_2(t)^2)\\big]^2}} + \\frac{4u_2(t)u_2'(t)^2}{\\sqrt{u_2'(t)^2 + \\big[\\frac{d}{dt}(1-u_2(t)^2)\\big]^2}}= 0 \\] <p>Part (c)</p> <p>Show that (a) and (b) gives the same answer</p> <p>From (a), we have </p> \\[\\lambda(t) = \\frac{d}{dt}\\frac{u'_1(t)}{\\sqrt{u'_1(t)^2 + u'_2(t)^2}}\\] <p>From (b), note that \\(u_1' = \\frac{d}{dt}(1-u_2^2) = -2u_2u_2'\\) so that we can write</p> \\[\\begin{align*} - \\frac{d}{dt}\\frac{(1 + 4u_2(t)^2)u_2'(t)}{\\sqrt{u_2'(t)^2 + \\big[\\frac{d}{dt}(1-u_2(t)^2)\\big]^2}} + \\frac{4u_2(t)u_2'(t)^2}{\\sqrt{u_2'(t)^2 + \\big[\\frac{d}{dt}(1-u_2(t)^2)\\big]^2}}&amp;= 0 \\\\ -\\frac{d}{dt}\\frac{u_2'}{\\sqrt{u_1'(t)^2 + u_2'(t)^2}} + \\frac{d}{dt}\\frac{2u_2(t)u_1'(t)}{{\\sqrt{u_1'(t)^2 + u_2'(t)^2}}} + \\frac{2u_2'(t)u_1'(t)}{{\\sqrt{u_1'(t)^2 + u_2'(t)^2}}}&amp;=0\\\\ -\\frac{d}{dt}\\frac{u_2'}{\\sqrt{u_1'(t)^2 + u_2'(t)^2}} + 2u_2(t)\\frac{d}{dt}\\frac{u_1'(t)}{{\\sqrt{u_1'(t)^2 + u_2'(t)^2}}} &amp;= 0\\\\ -\\frac{d}{dt}\\frac{u_2'}{\\sqrt{u_1'(t)^2 + u_2'(t)^2}} + 2u_2(t)\\lambda(t) &amp;= 0\\\\ \\end{align*}\\]"},{"location":"apm462/co_functional_q.html#example-4","title":"Example 4","text":"<p>Question</p> <p>Let \\(u\\in \\mathcal A = \\{u:[0, 1]\\rightarrow \\mathbb R\\mid u\\in C^1, u(0) = 0\\}\\).</p> \\[\\begin{align*}\\text{minimize}\\quad &amp;F[u] = \\int_0^1L^F(x, u(x), u'(x))dx\\\\ \\text{subject to}\\quad &amp;G[u] =\\int_0^1 u'(x)dx = a \\end{align*}\\] <p>Part (a)</p> <p>Write the EL Equation</p> \\[\\begin{align*} \\frac{\\partial F}{\\partial u} &amp;= L^F_z(x, u(x), u'(x)) - \\frac{d}{dx}L^F_p(x, u(x), u'(x))\\\\ L^G(x, z, p) &amp;= p\\\\ \\frac{d}{dt}L^G_p &amp;= \\frac{d}{dt}1 = 0\\\\ \\frac{\\partial G}{\\partial u} &amp;= 0 \\end{align*}\\] \\[L^F_z(x, u(x), u'(x)) - \\frac{d}{dx}L^F_p(x, u(x), u'(x)) - \\lambda\\times 0= 0\\] <p>And \\(G[u] = \\int_0^1 u'(x)dx = u(x)\\mid^1_0 = u(1) - 0 = a\\), note that this constraint makes the two end points of \\(u\\) fixed, so EL equation is the only needed first order necessary condition. So that we have </p> \\[L^F_z(x, u(x), u'(x)) - \\frac{d}{dx}L^F_p(x, u(x), u'(x))= 0\\] \\[u(1) = a\\] <p>Part (b)</p> <p>Formulated the problem as a unconstrianed problem and solve it</p> <p>Note that \\(G[u] = u(1) - u(0) = u(1) = a\\) so that the problem is to minimize \\(F[u]\\) on \\(u\\in \\{u:[0, 1]\\rightarrow \\mathbb R\\mid u\\in C^1, u(0) = 0, u(1) =a\\}\\), hence the EL equation is simply </p> \\[L^F_z(x, u(x), u'(x)) - \\frac{d}{dx}L^F_p(x, u(x), u'(x)) = 0\\] <p>Part (c)</p> <p>Show (a) and (b) gives the same answer</p> <p>Trivially, </p> \\[L^F_z(x, u(x), u'(x)) - \\frac{d}{dx}L^F_p(x, u(x), u'(x))= 0\\] \\[u(1) = a\\] \\[u\\in \\{u:[0, 1]\\rightarrow \\mathbb R\\mid u\\in C^1, u(0) = 0\\}\\] <p>Is the same as </p> \\[L^F_z(x, u(x), u'(x)) - \\frac{d}{dx}L^F_p(x, u(x), u'(x))= 0\\] \\[u\\in \\{u:[0, 1]\\rightarrow \\mathbb R\\mid u\\in C^1, u(0) = 0, u(1) = a\\}\\]"},{"location":"apm462/co_functional_q.html#example-5","title":"Example 5","text":"<p>Question</p> <p>Consider the minimization problem in Q4, but the two endpoints are both unfixed, find the FONC.</p> <p>Note that the constraint \\(G[u] = \\int_0^1 u'(x)dx = u(1) - u(0) = a\\), so that we are optimize \\(F[u]\\) on \\(\\mathcal A := \\{u[0, 1]\\rightarrow \\mathbb R\\mid u\\in C^1, u(1)-u(0) = a\\}\\). Consider test function \\(v\\) s.t. \\(v(0) = v(1) = 0\\). Assume \\(u_*\\) is a minimizer, define </p> \\[f(s, t):\\mathbb R^2\\rightarrow \\mathbb R := F[u_*(\\cdot) + sv(\\cdot) + t] = \\int_0^1 L(x, u(x)+sv(x)+t, u'(x) + sv'(x))dx\\] <p>so that </p> \\[\\mathcal A = \\{u_*+sv+t\\mid s,t\\in\\mathbb R\\}\\] <p>Since \\(u_*\\) is a minimizer, we must have \\(\\nabla f = 0\\)</p> \\[\\frac{\\partial}{\\partial s}\\mid_{(s,t)=(0, 0)}F = \\int_0^1 L_z^F(\\cdots)v(x) + L_p^G(\\cdots)v'(x)dx = \\int_0^1(L_z - \\frac{d}{dx}L_p)v(x)dx\\] <p>The computation of the above equation is identical to the proof of Euler Lagrange equation, and by fundamental lemma, we have must </p> \\[L_z(x, u(x), u'(x)) - \\frac{d}{dx}L_p(x, u(x), u'(x)) = 0\\] <p>Then, we also need to have</p> \\[\\frac{\\partial}{\\partial t}\\mid_{(s,t) =(0,0)}F = \\int_0^1 L_z^F(x, u(x), u'(x)) dx=0\\] <p>so that FONC are </p> \\[L_z(x, u(x), u'(x)) - \\frac{d}{dx}L_p(x, u(x), u'(x)) = 0\\] \\[\\int_0^1 L_z^F(x, u(x), u'(x)) dx=0\\]"},{"location":"apm462/co_functional_q.html#example-6","title":"Example 6","text":"<p>Question</p> <p>Prove Euler-Lagrange equation for isoperimetric problems.</p> <p>proof. Take \\(v_2\\) s.t. \\(\\int_a^b \\frac{\\partial G}{\\partial u}(u_*)(x)v_2(x)dx \\neq 0\\). Let \\(f(s, t) = F[u_* +sv_1 + tv_2]\\) and \\(g(s, t) = G[u_*+sv_1+tv_2]\\).  Note that </p> \\[\\begin{align*} \\frac{\\partial}{\\partial s}g(0, 0) &amp;= \\int_a^b \\frac{\\partial}{\\partial s}\\mid_{(s, t)=(0, 0)}L^G(x, u_* + sv_1 + tv_2, u_*'+sv_1'+tv_2')dx\\\\ &amp;= \\int_a^b L_z^G(\\cdots)\\frac{\\partial}{\\partial s}(u_*+sv_1+tv_2) + L_p^G(\\cdots)\\frac{\\partial}{\\partial s}(u_*'+sv_1'+tv_2')\\\\ &amp;= \\int_a^b L_z^G(\\cdots)v_1(x) + L_p^G(\\cdots)v_1'(x)\\\\ &amp;=\\int_a^b (L_z^G(x, u_*, u_*') - \\frac{d}{dx}L_p^G(x, u_*, u_*'))v_1(x)dx\\\\ &amp;= \\int_{a}^b \\frac{\\partial G}{\\partial u}(u_*)(x)v_1(x)dx \\end{align*}\\] <p>The above equation is obtained by integration by parts, the steps are identical to the computation for \\(\\frac{d}{ds}\\mid_{s=0}F[u+sv]\\). With the similar derivations, we can show that </p> \\[\\frac{\\partial}{\\partial t}g(0, 0) = \\int_{a}^b \\frac{\\partial G}{\\partial u}(u_*)(x)v_2(x)dx\\] \\[\\frac{\\partial}{\\partial t}f(0,0) = \\int_{a}^b \\frac{\\partial F}{\\partial u}(u_*)(x)v_1(x)dx\\] \\[\\frac{\\partial}{\\partial t}f(0,0) = \\int_{a}^b \\frac{\\partial F}{\\partial u}(u_*)(x)v_1(x)dx\\] <p>Note that by our assumption, \\(\\frac{\\partial}{\\partial t}g(0,0) = \\int_{a}^b \\frac{\\partial G}{\\partial u}(u_*)(x)v_2(x)dx\\neq 0\\) so that \\(\\nabla g(0, 0) \\neq 0\\), therefore we can safely apply Lagrange multipliers, i.e. for some \\(\\lambda\\)</p> \\[\\begin{align*}0 &amp;= \\int_{a}^b \\frac{\\partial F}{\\partial u}(u_*)(x)v_1(x)dx + \\lambda \\int_{a}^b \\frac{\\partial G}{\\partial u}(u_*)(x)v_1(x)dx \\\\ &amp;= \\int_a^b (\\frac{\\partial F}{\\partial u} + \\lambda \\frac{\\partial G}{\\partial u})v_1(x)dx\\\\ 0 &amp;= \\int_{a}^b \\frac{\\partial F}{\\partial u}(u_*)(x)v_2(x)dx + \\lambda \\int_{a}^b \\frac{\\partial G}{\\partial u}(u_*)(x)v_2(x)dx \\\\ &amp;= \\int_a^b (\\frac{\\partial F}{\\partial u} + \\lambda \\frac{\\partial G}{\\partial u})v_2(x)dx\\\\ \\end{align*}\\] <p>By fundamental lemma, both equations lead to </p> \\[\\frac{\\partial F}{\\partial u} + \\lambda \\frac{\\partial G}{\\partial u} = 0\\] <p>a.k.a</p> \\[L_z^F - \\frac{d}{dx}L_p^F + \\lambda(L_z^G - \\frac{d}{dx}L_p^G) = 0\\] \\[-\\frac{d}{dx}(L^G+\\lambda L^G)_p(x, u_*, u'_*) + (L^F+\\lambda L^G)_z(x, u_*,u_*') = 0\\]"},{"location":"apm462/conjugate.html","title":"Conjugate Directions and Conjugate Gradients","text":""},{"location":"apm462/conjugate.html#method-of-conjugate-directions","title":"Method of Conjugate Directions","text":""},{"location":"apm462/conjugate.html#defn-q-conjugate","title":"Defn. Q-Conjugate","text":"<p>Let \\(Q\\) be symmetric. We say that \\(d,d'\\) are \\(Q\\)-conjugate or \\(Q\\)-orthogonal if \\(d^TQd' = 0\\). A finite set \\(d_0, \\dots, d_k\\) of vectors is called \\(Q\\)-orthogonal if \\(d_i^TQd_j = 0\\) for all \\(i \\geq j\\).</p> <p>For example, if \\(Q = I\\), then \\(Q\\)-orthogonality is equivalent to regular orthogonality. For another example, if \\(Q\\) has more than one distinct eigenvalue, let \\(d\\) and \\(d'\\) be eigenvectors corresponding to distinct eigenvalues. Then \\(d^TQd' = \\lambda' d^Td' = 0\\), since the distinct eigenspaces of a symmetric matrix are orthogonal subspaces.</p> <p>Recall that any symmetric matrix \\(Q\\) may the orthogonally diagonalized; there exists an orthonormal basis \\(d_0, \\dots, d_{n-1}\\) of eigenvectors of \\(Q\\). These eigenvectors are also \\(Q\\)-orthogonal. Hence to any symmetric matrix is a basis of orthonormal vectors that are also orthogonal with respect to the matrix, as just defined.</p>"},{"location":"apm462/conjugate.html#examples-of-q-conjugate-set","title":"Examples of Q-conjugate set","text":"<p>Claim. If \\(Q\\) is symmetric and positive definite, then any set of non-zero \\(Q\\)-orthogonal vectors \\(\\{d_i\\}\\) is linearly independent.</p> <p>proof. If \\(\\sum \\alpha_i d_i = 0\\), then left-multiplying by \\(d_j^TQ\\) gives \\(\\alpha_j d_j^T Q d_j = 0\\). Positive definiteness implies \\(\\alpha_j = 0\\). </p> <p>Let \\(Q\\) be an \\(n \\times n\\) symmetric positive definite matrix. Recall that \\(f(x) = \\frac{1}{2}x^TQx - b^Tx\\) has the unique global minimizer \\(x_* = Q^{-1}b\\). Let \\(d_0, \\dots, d_{n-1}\\) be non-zero \\(Q\\)-orthogonal vectors. Then \\(d_0, \\dots, d_{n-1}\\) form a basis of \\(\\mathbb R^n\\). Thus there are scalars \\(\\alpha_0, \\dots, \\alpha_{n-1}\\) such that \\(x_* = \\sum \\alpha_i d_i\\). We would like a formula for the \\(\\alpha_i\\)'s.</p> <p>Multiplying both sides of the sum \\(x_* = \\sum \\alpha_i d_i\\) by \\(d_j^TQ\\) implies that \\(d_j^TQx_* = \\alpha_j d_j^TQd_j\\), implying that</p> \\[\\alpha_j = \\frac{d_j^T b}{d_j^TQd_j}\\] <p>Therefore</p> \\[x_* = \\sum_{i=1}^{n-1} \\frac{d_i^Tb}{d_i^TQd_i}  d_i\\] <p>This implies that we can actually solve for \\(x_*\\) by computing the \\(d_0, \\dots, d_{n-1}\\) and the coefficients above. Computationally, computing inner products is very easy. The disadvantage is that we do not know how to find the vectors \\(d_0, \\dots, d_{n-1}\\).</p>"},{"location":"apm462/conjugate.html#thrm-conjugate-directions","title":"Thrm. Conjugate Directions","text":"<p>Claim. Let \\(d_0, \\dots, d_{n-1}\\) be a set of non-zero \\(Q\\)-orthogonal vectors. For a starting point \\(x_0 \\in \\mathbb R^n\\), consider the sequence \\(\\{x_l\\}\\) defined by </p> \\[x_{k+1} = x_k + \\alpha_k d_k\\] <p>where </p> \\[\\alpha_k = -\\frac{g_k^Td_k}{d_k^TQd_k}, g_k = Qx_k - b\\] <p>The sequence \\(\\{x_k\\}\\) converges to the minimizer \\(x_*\\) it at most \\(n\\) steps; \\(x_n = x_*\\).</p> <p>proof. Write \\(x_* - x_0 = \\alpha_0' d_0 + \\cdots + \\alpha_{n-1}'d_{n-1}\\). Multiply both sides by \\(d_i^TQ\\) to get</p> \\[d_i^TQ(x_* - x_0) = \\alpha_i d_i^TQd_i\\] <p>giving us the expression</p> \\[\\alpha_i' = \\frac{d_i^TQ(x_*-x_0)}{d_i^TQd_i}\\:(*)\\] <p>Note that</p> \\[\\begin{align*} x_1 &amp;= x_0 + \\alpha_0 d_0 \\\\ x_2 &amp;= x_0 + \\alpha_0 d_0 + \\alpha_1 d_1 \\\\ &amp;\\vdots \\\\ x_k &amp;= x_0 + \\alpha_0 d_0 + \\cdots + \\alpha_{k-1}d_{k-1}, \\end{align*}\\] <p>implying that</p> \\[x_k - x_0 = \\alpha_0 d_0 + \\cdots + \\alpha_{k-1}d_{k-1}\\] <p>Multiplying both sides by \\(d_k^TQ\\) gives \\(d_k^TQ(x_k-x_0) = 0\\). By (*) we have</p> \\[\\alpha_k' = \\frac{d_k^T Q(x_* - x_0) - d_k^TQ(x_k - x_0)}{d_k^TQd_k} = \\frac{d_k^TQ(x_* - x_k)}{d_k^TQd_k} = -\\frac{(Qx_k - Qx_*)^T d_k}{d_k^TQd_k}\\] <p>simplifying to</p> \\[\\alpha_k' = -\\frac{g_k^T d_k}{d_k^TQd_k} = \\alpha_k\\] \\[\\implies x_* = x_0 + \\alpha_0 d_0 + \\cdots + \\alpha_{n-1}d_{n-1} = x_n\\] <p>So after \\(n\\) steps, we reach the minimizer.</p>"},{"location":"apm462/conjugate.html#thrm-geometric-interpretation","title":"Thrm. Geometric Interpretation","text":"<p>Let \\(d_0, \\dots, d_{n-1}\\) be a set of non-zero \\(Q\\)-orthogonal vectors in \\(\\mathbb R^n\\), where \\(Q\\) is symmetric and positive definite. Note that these vectors are linearly independent by a result from last lecture. Let \\(B_k\\) denote the subspace spanned by the first \\(k\\) vectors. We have an increasing sequence</p> \\[B_0 \\subsetneq B_1 \\subsetneq \\cdots \\subsetneq B_n\\] <p>and \\(\\dim(B_k) = k\\).</p> <p>Lemma. Let \\(f\\) be a \\(C^1\\) convex function defined on a convex domain \\(\\Omega \\subseteq \\mathbb R^n\\). Suppose there is an \\(x_* \\in \\Omega\\) such that \\(\\nabla f(x_*) \\cdot (y - x_*) \\geq 0\\) for all \\(y \\in \\Omega\\). Then \\(x_*\\) is a global minimizer of \\(f\\) on \\(\\Omega\\). The converse is obviously true.</p> <p>Geometrically, this means that if we move in any feasible direction from the point \\(x_*\\), the function is increasing. Hence \\(x_*\\) is a local minimizer; convexity implies it is global. With this result in mind, we prove the theorem.</p> <p>proof. The affine subspace \\(\\Omega = x_0 + B_k\\) is convex. \\textbf{(This proof could not be finished as attention had to be diverted from the lecture.)}</p> <p>Corollary. \\(x_n\\) minimizes \\(f(x)\\) on \\(\\mathbb R^n\\). That is, \\(x_n = x_*\\); the method of conjugate directions for this function \\(f\\) terminates in at most \\(n\\) steps. </p> <p>Claim. The sequence \\(\\{x_k\\}_{k=0}^\\infty\\) generated from \\(x_0\\) by the method of conjugate directions has the property that \\(x_k\\) minimizes \\(f(x) = \\frac{1}{2}x^TQx - b^Tx\\) on the affine subspace \\(x_0 + B_k\\).</p> <p>When \\(Q = I\\), then \\(q(x)\\) is half the distance squared from \\(x\\) to \\(x_*\\). What if \\(Q \\neq I\\). \\(q\\) is still a metric on \\(\\mathbb R^n\\). Thus \\(x_k\\) is the point \"closest\" to \\(x_*\\) on the affine subspace \\(x_0 + B_k\\).</p>"},{"location":"apm462/conjugate.html#conjugate-gradients","title":"Conjugate Gradients","text":"<p>Start at \\(x_0 \\in \\mathbb R^n\\). Choose \\(d_0 = -g_0 = -\\nabla f(x_0) = b - Qx_0\\). Recursively define \\(d_{k+1} = -g_{k+1} + \\beta_k d_k\\), where \\(g_{k+1} = Qx_{k+1} - b\\) and</p> \\[\\beta_k = \\frac{g_{k+1}^T Q d_k}{d_k^TQd_k}\\] <p>and</p> \\[x_{k+1} = x_k + \\alpha_k d_k\\] <p>where</p> \\[\\alpha_k = -\\frac{g_k^T d_k}{d_k^T Q d_k}\\] <p>Given an initial point \\(x_0\\), take \\(d_0 = -g_0 = b - Qx_0\\). By definition, \\(x_1 = x_0 + \\alpha_0 d_0\\); we need to find \\(\\alpha_0\\). This is</p> \\[\\alpha_0 = -\\frac{g_0^Td_0}{g_0^TQg_0}\\] <p>Then \\(x_2 = x_1 + \\alpha_1 d_1\\). By definition, \\(\\alpha_1 = -\\frac{g_1^T d_1}{d_1^TQd_1}\\), where \\(d_1 = -g_1 + \\beta_0 d_0\\), where \\(\\beta_0 = \\frac{g_1^TQd_0}{d_0^TQd_0}\\).</p> <p>Some remarks:</p> <ul> <li>Like the other conjugate direction methods, this method converges to the minimizer \\(x_*\\) in \\(n\\) steps.</li> <li>We have a procedure to find the direction vectors \\(d_k\\).</li> <li>This method makes good uniform progress towards the solution at every step.</li> </ul>"},{"location":"apm462/conjugate.html#thrm-bound-on-convergence","title":"Thrm. Bound on Convergence","text":"<p>Claim. consider \\(q(x) = \\frac{1}{2}(x-x_*)^TQ(x-x_*) = f(x) + \\text{const}\\). It's better to look at \\(q\\) rather than \\(f\\), since \\(q\\) behaves like a distance function relative to \\(x_*\\).</p> \\[q(x_{k+1}) \\leq \\left( \\max_{\\substack{\\lambda \\\\ \\text{eigval of Q}}} (1 + \\lambda P_k(\\lambda))^2 \\right) q(x_k)\\] <p>where \\(P_k\\) is any polynomial of degree \\(k\\).</p> <p>For example, suppose \\(Q\\) has \\(m \\leq n\\) distinct eigenvalues. Choose a polynomial \\(P_{m-1}\\) such that \\(1 + \\lambda P_{m-1}(\\lambda)\\) has its \\(m\\) zeroes at the \\(m\\) eigenvalues of \\(Q\\). With such a polynomial, we would get \\(q(x_m) \\leq 0\\), implying that \\(q(x_m) = 0\\); the conjugate gradient method terminates at the \\(m\\)th step, i.e. \\(x_m=x_*\\).</p>"},{"location":"apm462/conjugateq.html","title":"Examples: Conjugate Directions and Conjugate Gradients","text":""},{"location":"apm462/conjugateq.html#example-1","title":"Example 1","text":"<p>Question</p> <p>Let \\(c\\in\\mathbb R^n - \\{0\\}, f(x) = \\frac12x^TQx-b^Tx, Q = I +cc^T\\). Using conjugate gradient method, what's the smallest \\(k\\) that guarantees \\(x_k\\) is the minimizer of \\(f\\).</p> <p>Claim. \\(k=2\\)</p> <p>proof. First, consider some eigenvalues \\(\\lambda\\) and corresponding eigenvector \\(x\\), by definition, we have </p> \\[\\begin{align*} \\lambda x &amp;= Qx\\\\ \\lambda x &amp;= (I + cc^T)x\\\\ \\lambda x &amp;= x + cc^Tx\\\\ (\\lambda - 1)x &amp;= (c^Tx)c &amp;\\text{Note that }c^Tx\\in\\mathbb R \\end{align*}\\] <p>If \\(\\lambda - 1 = 0\\), we must have \\(c^Tx = 0\\), so that \\(\\lambda = 1\\) is a eigenvalue, If \\(\\lambda - 1 \\neq 0\\), then \\(x\\) and \\(c\\) are linearly dependent, hence the eigenvector is \\(c\\) and we have </p> \\[\\begin{align*} \\lambda c &amp;= (I + cc^T)c\\\\ \\lambda c &amp;= c + cc^Tc\\\\ \\lambda &amp;= 1 + c^Tc \\end{align*}\\] <p>Therefore, there are only 2 distinct eigenvalues for \\(Q = I + cc^T\\).  </p> <p>Then, we can take \\(P_1(\\lambda) = a + b\\lambda\\) be a polynomial of degree 1 s.t. </p> \\[\\begin{bmatrix} 1&amp;1\\\\ 1&amp;1+c^Tc \\end{bmatrix}\\begin{bmatrix} a\\\\b \\end{bmatrix}=\\begin{bmatrix} -1\\\\-\\frac{1}{1+c^Tc} \\end{bmatrix}\\] <p>so that \\(1+P(1) = 0\\) and \\(1+(1+cc^T)P(1+cc^T) =0\\).  Therefore, we have </p> \\[q(x_2) \\leq \\max_{\\lambda \\in \\{1, 1+c^Tc\\}}(1+\\lambda P_1(\\lambda))q(x_0) = 0\\] <p>Therefore, \\(k = 2\\) guarantees \\(x_2\\) is the minimizer of \\(f\\). </p>"},{"location":"apm462/conjugateq.html#example-2","title":"Example 2","text":"<p>Question</p> <p>Let \\(Q = \\begin{bmatrix}2&amp;-5\\\\-5&amp;2\\end{bmatrix}, b = \\begin{bmatrix}25&amp;8\\end{bmatrix}, f = \\frac{1}{2}x^TQx - b^Tx\\).</p> <p>Part (a)</p> <p>Find eigenvalues \\(\\lambda_0\\leq \\lambda_1\\) of \\(Q\\) and corresponding eigenvectors.</p> <p>First, find its characteristic polynomial as </p> \\[(2-\\lambda)^2 - 25 = \\lambda^2 - 4\\lambda + 4 -25 = \\lambda^2 - 4\\lambda+21=(\\lambda-7)(\\lambda+3)\\] <p>set the equation to \\(0\\) and solve to get</p> \\[\\lambda_0 = -3, \\lambda_1 = 7\\] <p>Then, </p> \\[\\begin{align*} (Q - \\lambda_0I)d_0 &amp;= 0\\\\ \\begin{bmatrix}5&amp;-5\\\\-5&amp;5\\end{bmatrix}d_0 &amp;= 0\\\\ d_0&amp;= \\begin{bmatrix}1\\\\1\\end{bmatrix}\\\\ (Q - \\lambda_1I)d_1 &amp;= 0\\\\ \\begin{bmatrix}-5&amp;-5\\\\-5&amp;-5\\end{bmatrix}d_1 &amp;= 0\\\\ d_1&amp;= \\begin{bmatrix}1\\\\-1\\end{bmatrix} \\end{align*}\\] <p>Part (b)</p> <p>Compute the steps of conjugate directions method given directions \\(d_0, d_1\\).</p> \\[\\begin{align*} g_0 &amp;= \\begin{bmatrix}2&amp;-5\\\\-5&amp;2\\end{bmatrix}\\begin{bmatrix}25\\\\5\\end{bmatrix} - \\begin{bmatrix}25\\\\8\\end{bmatrix}= \\begin{bmatrix}0\\\\-123\\end{bmatrix}\\\\ a_0 &amp;= -(\\begin{bmatrix}0\\\\-123\\end{bmatrix}^T\\begin{bmatrix}1\\\\1\\end{bmatrix}) / (\\begin{bmatrix}1\\\\1\\end{bmatrix}^T \\begin{bmatrix}2&amp;-5\\\\-5&amp;2\\end{bmatrix}\\begin{bmatrix}1\\\\1\\end{bmatrix})= -\\frac{41}2\\\\ x_1 &amp;= \\begin{bmatrix}25\\\\5\\end{bmatrix}-\\frac{41}2\\begin{bmatrix}1\\\\1\\end{bmatrix}= \\begin{bmatrix}4.5\\\\-15.5\\end{bmatrix}\\\\ g_1 &amp;=  \\begin{bmatrix}2&amp;-5\\\\-5&amp;2\\end{bmatrix} \\begin{bmatrix}4.5\\\\-15.5\\end{bmatrix} -  \\begin{bmatrix}25\\\\8\\end{bmatrix}= \\begin{bmatrix}61.5\\\\-61.5\\end{bmatrix}\\\\ a_1 &amp;= -(\\begin{bmatrix}61.5\\\\-61.5\\end{bmatrix}^T\\begin{bmatrix}1\\\\-1\\end{bmatrix}) / (\\begin{bmatrix}1\\\\-1\\end{bmatrix}^T \\begin{bmatrix}2&amp;-5\\\\-5&amp;2\\end{bmatrix}\\begin{bmatrix}1\\\\-1\\end{bmatrix})= -\\frac{123}{14}\\\\ x_2 &amp;= \\begin{bmatrix}-4.5\\\\15.5\\end{bmatrix}-\\frac{123}{14}\\begin{bmatrix}1\\\\-1\\end{bmatrix}= \\begin{bmatrix}-\\frac{30}7\\\\-\\frac{47}7\\end{bmatrix} \\end{align*}\\] <p>Part (c)</p> <p>Compute the steps of conjugate directions method given directions \\(d_1, d_0\\).</p> \\[\\begin{align*} g_0 &amp;= \\begin{bmatrix}2&amp;-5\\\\-5&amp;2\\end{bmatrix}\\begin{bmatrix}25\\\\5\\end{bmatrix} - \\begin{bmatrix}25\\\\8\\end{bmatrix}= \\begin{bmatrix}0\\\\-123\\end{bmatrix}\\\\ a_0 &amp;= -(\\begin{bmatrix}0\\\\-123\\end{bmatrix}^T\\begin{bmatrix}1\\\\-1\\end{bmatrix}) / (\\begin{bmatrix}1\\\\-1\\end{bmatrix}^T \\begin{bmatrix}2&amp;-5\\\\-5&amp;2\\end{bmatrix}\\begin{bmatrix}1\\\\-1\\end{bmatrix})= -\\frac{123}{14}\\\\ x_1 &amp;= \\begin{bmatrix}25\\\\5\\end{bmatrix}-\\frac{123}{14}\\begin{bmatrix}1\\\\-1\\end{bmatrix}= \\begin{bmatrix}\\frac{227}{14}\\\\\\frac{193}{14}\\end{bmatrix}\\\\ g_1 &amp;=  \\begin{bmatrix}2&amp;-5\\\\-5&amp;2\\end{bmatrix} \\begin{bmatrix}\\frac{227}{14}\\\\\\frac{193}{14}\\end{bmatrix} -  \\begin{bmatrix}25\\\\8\\end{bmatrix}= \\begin{bmatrix}61.5\\\\-61.5\\end{bmatrix}\\\\ a_1 &amp;= -(\\begin{bmatrix}61.5\\\\-61.5\\end{bmatrix}^T\\begin{bmatrix}1\\\\1\\end{bmatrix}) / (\\begin{bmatrix}1\\\\1\\end{bmatrix}^T \\begin{bmatrix}2&amp;-5\\\\-5&amp;2\\end{bmatrix}\\begin{bmatrix}1\\\\1\\end{bmatrix})= -\\frac{41}{2}\\\\ x_2 &amp;= \\begin{bmatrix}\\frac{227}{14}\\\\\\frac{193}{14}\\end{bmatrix}-\\frac{41}{2}\\begin{bmatrix}1\\\\1\\end{bmatrix}= \\begin{bmatrix}-\\frac{30}7\\\\-\\frac{47}7\\end{bmatrix} \\end{align*}\\] <p>Part (d)</p> <p>Compute the steps of conjugate gradients.</p> \\[\\begin{align*} g_0 &amp;= \\begin{bmatrix}2&amp;-5\\\\-5&amp;2\\end{bmatrix}\\begin{bmatrix}25\\\\5\\end{bmatrix} - \\begin{bmatrix}25\\\\8\\end{bmatrix}= \\begin{bmatrix}0\\\\-123\\end{bmatrix}\\\\ d_0 &amp;= \\begin{bmatrix}0\\\\123\\end{bmatrix}\\\\ a_0 &amp;= (\\begin{bmatrix}0\\\\123\\end{bmatrix}\\cdot\\begin{bmatrix}0\\\\123\\end{bmatrix}) /  \\begin{bmatrix}0\\\\123\\end{bmatrix}\\begin{bmatrix}2&amp;-5\\\\-5&amp;2\\end{bmatrix}\\begin{bmatrix}0\\\\123\\end{bmatrix} = 0.5\\\\ x_1 &amp;= \\begin{bmatrix}25\\\\5\\end{bmatrix} + 0.5\\begin{bmatrix}0\\\\123\\end{bmatrix}= \\begin{bmatrix}25\\\\66.5\\end{bmatrix}\\\\ g_1 &amp;=\\begin{bmatrix}2&amp;-5\\\\-5&amp;2\\end{bmatrix}\\begin{bmatrix}25\\\\66.5\\end{bmatrix} - \\begin{bmatrix}25\\\\8\\end{bmatrix}= \\begin{bmatrix}-307.5\\\\0\\end{bmatrix}\\\\ \\beta_0 &amp;= (\\begin{bmatrix}-307.5\\\\0\\end{bmatrix}^T\\begin{bmatrix}2&amp;-5\\\\-5&amp;2\\end{bmatrix}\\begin{bmatrix}0\\\\123\\end{bmatrix}) / (\\begin{bmatrix}0\\\\123\\end{bmatrix}^T\\begin{bmatrix}2&amp;-5\\\\-5&amp;2\\end{bmatrix}\\begin{bmatrix}0\\\\123\\end{bmatrix})=6.25\\\\ d_1 &amp;= \\begin{bmatrix}307.5\\\\0\\end{bmatrix} + 6.25\\begin{bmatrix}0\\\\123\\end{bmatrix} = \\begin{bmatrix}307.5\\\\768.75\\end{bmatrix}\\\\ a_1 &amp;= (\\begin{bmatrix}-307.5\\\\0\\end{bmatrix}\\cdot\\begin{bmatrix}307.5\\\\768.75\\end{bmatrix}) /  \\begin{bmatrix}307.5\\\\768.75\\end{bmatrix}\\begin{bmatrix}2&amp;-5\\\\-5&amp;2\\end{bmatrix}\\begin{bmatrix}307.5\\\\768.75\\end{bmatrix} = -\\frac{2}{21}\\\\ x_2 &amp;= \\begin{bmatrix}25\\\\66.5\\end{bmatrix} + \\frac{2}{21}\\begin{bmatrix}307.5\\\\768.75\\end{bmatrix} = \\begin{bmatrix}-\\frac{30}7\\\\-\\frac{47}7\\end{bmatrix}\\\\ g_2 &amp;= \\begin{bmatrix}2&amp;-5\\\\-5&amp;2\\end{bmatrix}\\begin{bmatrix}-\\frac{30}7\\\\-\\frac{47}7\\end{bmatrix} - \\begin{bmatrix}25\\\\8\\end{bmatrix} = \\begin{bmatrix}0\\\\0\\end{bmatrix}\\\\ \\beta_1 &amp;= \\frac{0}{d_1^TQd_1} = 0\\\\ d_2 &amp;= -g_2 + 0 = \\begin{bmatrix}0\\\\0\\end{bmatrix}\\\\ x_3 &amp;= x_2 + a_2\\begin{bmatrix}0\\\\0\\end{bmatrix} = x_2 = \\begin{bmatrix}-\\frac{30}7\\\\-\\frac{47}7\\end{bmatrix} \\end{align*}\\]"},{"location":"apm462/conjugateq.html#example-3","title":"Example 3","text":"<p>Question</p> <p>Prove that the Gram-Schmidt procedure generate a sequence of Q-conjugate directions given a linear independent set of vector \\(p_0,...,p_{n-1}\\in\\mathbb R^n\\).</p> <p>proof. Note that this statement is equal to say that \\(\\forall k\\in\\{0,...,n-1\\}. \\forall j &lt; k. d_k^TQd_j = 0\\), and I'll prove this statement by strong induction.</p> <p>First, since \\(Q\\) is symmetric \\(d_k^TQd_j = d_j^TQd_k\\) for any \\(d_j,d_k\\).  Also note that since \\(d_k\\)'s are linear combinations of \\(p_0, ..., p_{n-1}\\), \\(d_k\\neq 0, \\forall k\\in\\{0,...,n-1\\}\\), and since \\(Q\\) is positive definite \\(d_K^TQd_k &gt; 0\\). </p> <p>Then, note that for \\(k = 0\\), the statement is vacuously true.  Fir \\(k \\in \\{1, ..., n-1\\}\\), assume that \\(\\forall m &lt; k. \\forall j &lt; m. d_m^TQd_j = d_j^TQd_m = 0\\), i.e. \\(\\forall j,m &lt; k, j\\neq m. d^T_mQd_j = 0\\) Then, for some \\(i &lt; k\\), we have</p> \\[\\begin{align*} d_{k}^TQd_{j} &amp;= (p_k - \\sum_{i=0}^{k-1}\\frac{p_k^TQ d_i}{d_i^TQd_i}d_i)^TQd_j\\\\ &amp;= p_k^TQd_j- \\sum_{i=0}^{k-1}\\frac{p_k^TQ d_i}{d_i^TQd_i}d_i^TQd_j\\\\ &amp;= p_k^TQd_j - \\frac{p_k^TQ d_i}{d_j^TQd_j}d_j^TQd_j\\\\ &amp;= p_k^TQd_j - p_k^TQd_j\\\\ &amp;= 0 \\end{align*}\\]"},{"location":"apm462/conjugateq.html#example-4","title":"Example 4","text":"<p>Question</p> <p>Let \\(Q\\) be positive definite, \\(f(x) = \\frac12 x^TQx - b^Tx\\). Let \\(x_1=\\arg\\min_{x\\in S_1}f(x), x_2=\\arg\\min_{x\\in S_2}f(x)\\), where \\(S_1,S_2\\subset E^n\\) and \\(d\\in S_1\\cap S_2\\), assume \\(f(x_1) &lt; f(x_2)\\). Show that \\((x_1-x_2)^TQd = 0\\).</p> <p>proof. Since \\(x_1\\) is a minimizer of \\(S_1\\), we must have \\(\\nabla f(x_1)^T = 0\\), otherwise we can have some \\(\\epsilon &gt; 0, f(x_1 - \\epsilon d) &lt; f(x_1)\\) and \\(x_1-\\epsilon d \\in S_1\\) since \\(x_1\\in S_1, d\\in S_1\\). Note that the equation is expanded as</p> \\[\\begin{align*} \\nabla f(x_1)^T d &amp;= (Qx_1 - b)^Td \\\\ &amp;= x_1^TQ^Td - b^Td \\\\ &amp;= x_1^TQd - b^Td &amp;Q\\text{ is symmetric}\\\\ &amp;= 0 \\end{align*}\\] <p>and similarly we have \\(\\nabla f(x_2)^T d = x_2^TQd - b^Td= 0\\).    </p> <p>Therefore, we have </p> \\[\\begin{align*} (x_1-x_2)^TQd &amp;= x_1^TQd - x_2^TQd \\\\ &amp;= b^Td - b^Td \\\\ &amp;= 0 \\end{align*}\\]"},{"location":"apm462/conjugateq.html#example-5","title":"Example 5","text":"<p>Question</p> <p>Let \\(f = \\frac12x^TQx - b^Tx\\) where \\(Q = diag(\\lambda_1,...,\\lambda_n)\\) being a diagonal, positive definite and symmetric matrix.</p> <p>Part (a)</p> <p>Show that standard basis vectors form a Q-orthogonal set.</p> <p>proof. Let \\(i, j \\in\\{1, ...,n\\}, i\\neq j\\). Denote \\(e_{mk}\\) be the \\(k\\)th entry of \\(e_m\\), \\(Q_{ij}\\) be the entry on \\(i\\)th row and \\(j\\)th column of \\(Q\\)</p> \\[e_i^TQe_j = \\sum_{p=1}^n\\sum_{q=1}^nQ_{pq}e_{ip}e_{jq}\\] <p>Note that \\(Q_{pp} = \\lambda_i, Q_{pq}=0\\), for any \\(p,q\\in\\{1,...,n\\}, p\\neq q\\). \\(e_{ii}=1, e_{ip}=0\\) for \\(p\\in\\{1,...,n\\}-\\{i\\}\\) \\(e_{jj}=1, e_{jq}=0\\) for \\(q\\in\\{1,...,n\\}-\\{j\\}\\) Therefore, consider each term of the summation, when \\(p\\neq q, Q_{pq}=0\\), when \\(p=q\\), at least one of \\(e_{ip},e_{jq}\\) equals 0. Therefore, all terms in the summation are 0, \\(e_i^TQe_j = 0\\), hence \\(\\{d_0,...,d_{n-1}\\} = \\{e_1,...,e_n\\}\\) forms a Q-orthogonal set. </p> <p>Part (b)</p> <p>Prove \\(x_k = (\\frac{b_1}{\\lambda_1},...,\\frac{b_k}{\\lambda_k}, a_{k+1},...,a_n)\\) is the \\(k\\)th step of Conjugate direction method, starting from \\(x_0 = (a_1,...,a_n)\\).</p> <p>proof. I'll prove by induction. Let \\(k\\in \\{1,...,n-2\\}\\), assume \\(x_{k} = (\\frac{b_1}{\\lambda_1},...,\\frac{b_k}{\\lambda_k}, a_{k+1},...,a_n)\\). Consider the \\((k+1)\\)th step of conjugate direction method.  </p> \\[g_k = Qx_k - b =  \\begin{bmatrix} \\lambda_1\\frac{b_1}{\\lambda_1} - b_1\\\\ \\cdots\\\\ \\lambda_k\\frac{b_k}{\\lambda_k} - b_k\\\\ \\lambda_{k+1}a_{k+1} - b_{k+1}\\\\ \\cdots\\\\ \\lambda_{n}a_n - b_{n} \\end{bmatrix} =  \\begin{bmatrix} 0\\\\ \\cdots\\\\ 0\\\\ \\lambda_{k+1}a_{k+1} - b_{k+1}\\\\ \\cdots\\\\ \\lambda_{n}a_n - b_{n} \\end{bmatrix}\\] \\[a_k = -\\frac{g_k^Td_k}{d_k^TQd_k}=-\\frac{g_k^Te_{k+1}}{e_{k+1}^TQe_{k+1}}-\\frac{\\lambda_{k+1}a_{k+1} - b_{k+1}}{\\lambda_{k+1}} = -a_{k+1}+\\frac{b_{k+1}}{\\lambda_{k+1}}\\] \\[x_{k+1} = x_k + a_kd_k = x_k + a_ke_{k+1}  \\begin{bmatrix} \\frac{b_1}{\\lambda_1}\\\\ \\cdots\\\\ \\frac{b_k}{\\lambda_k}\\\\ a_{k+1} -a_{k+1}+\\frac{b_{k+1}}{\\lambda_{k+1}}\\\\ a_{k+2}\\\\ \\cdots\\\\ a_n \\end{bmatrix} =  \\begin{bmatrix} \\frac{b_1}{\\lambda_1}\\\\ \\cdots\\\\ \\frac{b_{k+1}}{\\lambda_{k+1}}\\\\a_{k+2}\\\\ \\cdots\\\\ a_n \\end{bmatrix}\\] <p>Part (c)</p> <p>Prove that \\(\\forall k \\geq 1, x_k\\) is the minimizer of \\(f\\) in the set \\(x_0 + \\mathcal B_k, \\mathcal B_k = span\\{d_0,...,d_{k-1}\\} = span\\{e_1,...,e_k\\}\\).</p> <p>proof. Let </p> \\[\\begin{align*} \\phi(y_1,...,y_k)&amp;=f(x_0 + \\sum_{i=1}^k{y_ie_i})\\\\ &amp;=(x_0 + \\sum_{i=1}^k{y_ie_i})^TQ(x_0 + \\sum_{i=1}^k{y_ie_i})-b^T(x_0 + \\sum_{i=1}^k{y_ie_i}) \\end{align*}\\] <p>Therefore, the problem is equivalent to minimize \\(\\phi\\) on \\(\\in\\mathbb R^k\\). Note that </p> \\[\\frac{\\partial\\phi}{\\partial y_i} = Q_{i\\cdot}(x_{0i}+y_i) - b_i = \\lambda_i(a_i+y_i) - b_i\\] <p>for \\(i=1,2,..,k\\), Therefore, set the derivative to \\(0\\) to satisfy the FONC, we have \\(k\\) equations </p> \\[\\lambda_i(a_i+y_i)-b_i=0\\Rightarrow y_i =\\frac{b_i}{\\lambda_i}-a_i\\] <p>Then, note that \\(\\frac{\\partial^{2}\\phi}{\\partial y_i^2} = \\lambda_i, \\frac{\\partial^{2}\\phi}{\\partial y_i y_j} = 0\\) for \\(i,j\\in\\{1,...,k\\}, i\\neq j\\), we have \\(\\nabla^2\\phi = diag(\\lambda_1,...,\\lambda_k)\\), i.e. the top-left \\(k\\times k\\) submatrix of \\(Q\\), since \\(Q\\) is positive definite, \\(\\nabla^2\\phi\\) is also positive definite, SOC also holds and </p> \\[x_0 + \\sum_{i-1}^k(\\lambda_i(a_i+y_i)-b_i)e_i = (\\frac{b_1}{\\lambda_1},...,\\frac{b_k}{\\lambda_k}, a_{k+1}, ..., a_n) = x_k\\] <p>is the minimizer of \\(\\phi\\).</p>"},{"location":"apm462/conjugateq.html#example-6","title":"Example 6","text":"<p>Question</p> <p>Let \\(Q\\) be a positive definite symmetric matrix.</p> <p>Part (a)</p> <p>Prove \\(d(x, y) = [(x-y)^TQ(x-y)]^{1/2}\\) is a metric.</p> <p>proof. Let \\(x,y\\in\\mathbb R^n\\).  </p> <p>positive definite Since \\(Q\\) is positive definite, </p> \\[\\forall a\\in\\mathbb R^n. a^TQa \\geq 0\\land a^TQa = 0\\Leftrightarrow a = 0\\] \\[\\implies (x-y)^TQ(x-y) \\geq 0\\land (x-y)^TQ(x-y) = 0\\Leftrightarrow x-y = 0\\Leftrightarrow x=y\\] <p>Therefore, </p> \\[d(x,y)=[(x-y)^TQ(x-y)]^{1/2} \\geq 0\\land [(x-y)^TQ(x-y)]^{1/2} = 0\\Leftrightarrow x=y\\] <p>symmetric </p> \\[\\begin{align*} d(x,y) &amp;= [(x-y)^TQ(x-y)]^{1/2} \\\\ &amp;= [(-(y-x))^TQ(-(y-x))]^{1/2}\\\\ &amp;= [-1(-1)(y-x)^TQ(y-x)]^{1/2}\\\\ &amp;= [(y-x)^TQ(y-x)]^{1/2}\\\\ &amp;= d(y,x) \\end{align*}\\] <p>triangular inequality </p> \\[\\begin{align*} d(x,z)&amp;= [(x-z)^TQ(x-z)]^{1/2}\\\\ &amp;= [((x-y)+(y-z))^TQ((x-y)+(y-z))]^{1/2}\\\\ &amp;= [(x-y)^TQ(x-y) + (y-z)^TQ(y-z)]^{1/2}\\\\ &amp;= (d(x,y)^{2} + d(y,z)^2)^{1/2}\\\\ &amp;\\text{by triangular inequality on Euclidean norm of real numbers}\\\\ &amp;\\leq (d(x,y)^{2})^{1/2} + (d(x,y)^{2})^{1/2} \\\\ &amp;= d(x,y) + d(y,z) \\end{align*}\\] <p>Part (b)</p> <p>For \\(x^*\\in\\mathbb R^2, a\\in\\mathbb R\\), for \\(x\\) on the line \\(L = \\{x\\in\\mathbb R^2\\mid x=(t,at), t\\in\\mathbb R\\}\\), find \\(x\\) that minimizes \\(d(x,x^*)\\).</p> <p>Define </p> \\[f(x, y) = d((x, y), (x^*, y^*)) = \\frac12 \\begin{bmatrix}x-x^*\\\\y-y^*\\end{bmatrix}^TQ\\begin{bmatrix}x-x^*\\\\y-y^*\\end{bmatrix}\\] <p>therefore minimizing \\(d((x,y), (x^*, y^*))\\) on \\(L\\) is equivalent to </p> \\[\\begin{align*}&amp;\\text{minimize } &amp;f(x,y)\\\\ &amp;\\text{subject to} &amp;l(x,y) = ax-y = 0 \\end{align*}\\] <p>Note that \\(\\nabla f = Q\\begin{bmatrix}x-x^*\\\\y-y^*\\end{bmatrix}, \\nabla l = \\begin{bmatrix}a\\\\-1\\end{bmatrix}\\) using Lagrange multiplier, we have equations </p> \\[\\begin{align*} Q\\begin{bmatrix}t-x^*\\\\at-y^*\\end{bmatrix} + \\lambda\\begin{bmatrix}a\\\\-1\\end{bmatrix}= 0  \\end{align*}\\] <p>Therefore, since \\(Q\\) is symmetric, write \\(Q = \\begin{bmatrix}p&amp;m\\\\m&amp;q\\end{bmatrix}\\)we can solve for </p> \\[t = \\frac{(p+m)x^* + (q+m)x^*}{a^2q + 2am + p}\\] <p>Since \\(Q\\) is positive definite, this solution is the minimum. </p>"},{"location":"apm462/ecfdo.html","title":"Equality Constrained Finite Dimension Optimization","text":""},{"location":"apm462/ecfdo.html#problem-definition","title":"Problem Definition","text":"<p>Consider the minimization problem</p> \\[\\begin{align*} \\text{min}\\quad &amp;f(x,y) \\\\ \\text{subject to}\\quad &amp;h(x,y) = x^2 + y^2 - 1 = 0 \\end{align*}\\] <p>Suppose \\((x_0, y_0)\\) is a local minimizer. Two cases: 1. \\(\\nabla f(x_0, y_0) \\neq 0\\): we claim that \\(\\nabla f(x_0, y_0)\\) is perpendicular to the tangent space to the unit circle \\(h^{-1}(\\{0\\})\\) at \\((x_0, y_0)\\). If this is not the case, then we obtain a contradiction by looking at the level sets of \\(f\\), to which \\(\\nabla f\\) is perpendicular. Therefore \\(\\nabla f(x_0, y_0) = \\lambda \\nabla h(x_0, y_0)\\) for some \\(\\lambda\\).</p> <ol> <li>\\(\\nabla f(x_0, y_0) = 0\\): as in the previous case, \\(\\lambda = 0\\).</li> </ol> <p>In either case, at a local minimizer, the gradient of the function to be minimized is parallel to the gradient of the constraints.</p>"},{"location":"apm462/ecfdo.html#defn-surface","title":"Defn. Surface","text":"<p>For us, a surface is the set of common zeroes of a finite set of \\(C^1\\) functions. </p>"},{"location":"apm462/ecfdo.html#defn-differentiable-curve","title":"Defn. Differentiable Curve","text":"<p>For us, a differentiable curve on the surface \\(M \\subseteq \\mathbb R^n\\) is the image of a \\(C^1\\) function \\(x : (a, b) \\to M\\).</p>"},{"location":"apm462/ecfdo.html#defn-tangent-vector","title":"Defn. Tangent Vector","text":"<p>Let \\(x(s)\\) be a differentiable curve on \\(M\\) that passes through \\(x_0 \\in M\\) at time \\(x(0) = x_0\\). The velocity vector \\(v = \\left. \\frac{d}{ds} \\right|_{s=0} x(s)\\) of \\(x(s)\\) at \\(x_0\\) is, for us, said to be a tangent vector to the surface \\(M\\) at \\(x_0\\). The set of all tangent vectors to \\(M\\) at \\(x_0\\) is called the tangent space to \\(M\\) at \\(x_0\\) and is denoted by \\(T_{x_0}M\\).</p>"},{"location":"apm462/ecfdo.html#defn-regular-point","title":"Defn. Regular Point","text":"<p>Let \\(M = \\{x \\in \\mathbb R^n : h_1(x) = \\cdots = h_k(x) = 0\\}\\) be a surface. If \\(\\nabla h_1(x_0), \\dots, \\nabla h_k(x_0)\\) are all linearly independent, then \\(x_0\\) is said to be a regular point of \\(M\\).</p>"},{"location":"apm462/ecfdo.html#claim-1","title":"Claim 1","text":"<p>At a regular point \\(x_0 \\in M\\), the tangent space \\(T_{x_0} M\\) is given by</p> \\[T_{x_0} M = \\{ y \\in \\mathbb R^n : \\nabla \\mathbf{h}(x_0)y = 0 \\}\\]"},{"location":"apm462/ecfdo.html#claim-2","title":"Claim 2","text":"<p>Let \\(f, h_1, \\dots, h_k\\) be \\(C^1\\) functions on the open set \\(\\Omega \\subseteq \\mathbb R^n\\). Let \\(x_0 \\in M = \\{ x \\in \\Omega : h_1(x) = \\cdots = h_k(x) = 0 \\}\\). Suppose \\(x_0\\) is a local minimizer of \\(f\\) subject to the constraints \\(h_i(x) = 0\\). Then \\(\\nabla f(x_0)\\) is perpendicular to \\(T_{x_0}M\\).</p> <p>proof. Without loss of generality, suppose \\(\\Omega = \\mathbb R^n\\). Let \\(v \\in T_{x_0}M\\). Then \\(v = \\left. \\frac{d}{ds} \\right|_{s=0}x(s)\\) for some differentiable curve \\(x(s)\\) in \\(M\\) with \\(x(0) = x_0\\). Since \\(x_0\\) is a local minimizer of \\(f\\), \\(0\\) is a local minimizer of \\(f \\circ x\\), so \\(\\nabla f(x_0) \\cdot x'(0) = \\nabla f(x_0) \\cdot v = 0\\).</p>"},{"location":"apm462/ecfdo.html#thrm-first-order-necessary-conditions-lagrange-multipliers","title":"Thrm. First Order Necessary Conditions (Lagrange Multipliers)","text":"<p>Claim. Let \\(f, h_1, \\dots, h_k\\) be \\(C^1\\) functions on some open \\(\\Omega \\subseteq \\mathbb R^n\\). Suppose \\(x_0\\) is a local minimizer of \\(f\\) subject to the constraints \\(h_1(x), \\dots, h_k(x) = 0\\), which is also a regular point of these constraints. Then there are \\(\\lambda_1, \\dots, \\lambda_k \\in \\mathbb R\\) (\"Lagrange multipliers\") such that</p> \\[\\nabla f(x_0) + \\lambda_1 \\nabla h_1(x_0) + \\cdots + \\lambda_k \\nabla h_k(x_0) = 0\\] <p>proof. Since \\(x_0\\) is regular, \\(T_{x_0}M = \\mathrm{span}(\\{ \\nabla h_1(x_0), \\dots, \\nabla h_k(x_0) \\})^\\perp\\). By a lemma from last class, \\(\\nabla f(x_0) \\in (T_{x_0}M)^\\perp\\). Therefore \\(\\nabla f(x_0) \\in \\mathrm{span}(\\{ \\nabla h_1(x_0), \\dots, \\nabla h_k(x_0) \\})\\), since we are dealing with a finite dimensional vector space. We are done.</p>"},{"location":"apm462/ecfdo.html#example-max-volume-of-box-with-constrained-surface-area","title":"Example: Max volume of Box with constrained surface area","text":"<p>Given a fixed area \\(A &gt; 0\\), how do we construct a box of maximum volume with surface area \\(A\\)? Suppose the volume is \\(V(x,y,z) = xyz\\) and the area is \\(A(x,y,z) = 2(xy+xz+yz)\\). Our problem is stated as a maximization problem, so we have to convert it to a minimization problem. Let \\(f = -V\\). We are therefore dealing with the problem</p> \\[\\begin{align*} \\text{minimize } &amp;f(x,y,z) = -xyz \\\\ \\text{subject to } &amp;h(x,y,z) = A(x,y,z) - A = 0, x,y,z \\geq 0 \\end{align*}\\] <p>But we don't know how to deal with inequality constraints right now, so we have to make some changes. Note that if any one of \\(x,y,z\\) is zero, then the volume is zero. Therefore the problem we want to consider is really the problem</p> \\[\\begin{align*} \\text{minimize } &amp;f(x,y,z) \\\\ \\text{subject to } &amp;h(x,y,z) = 0, x,y,z &gt; 0 \\end{align*}\\] <p>Now, if \\(\\Omega = \\{(x,y,z) \\in \\mathbb R^3 : x,y,z &gt; 0\\}\\), then the above minimization problem may be solved using the first order necessary condition we gave above, for the set \\(\\Omega\\) is open.</p> <p>Suppose \\((x_0, y_0, z_0)\\) is a local minimizer of \\(f\\) subject to the constraint \\(h(x,y,z) = 0\\). This point is regular because we are only considering points whose coordinates are all positive. Then there is a \\(\\lambda \\in \\mathbb R\\) such that \\(\\nabla f(x_0, y_0, z_0) + \\lambda \\nabla h(x_0, y_0, z_0) = 0\\). Therefore [ (-y_0z_0, -x_0z_0, -x_0y_0) + \\lambda (2y_0 + 2z_0, 2x_0 + 2z_0, 2x_0 + 2y_0) = (0,0,0). ] Equivalently, </p> \\[\\begin{align*} 2\\lambda (y_0 + z_0) &amp;= y_0z_0 \\\\ 2\\lambda (x_0 + z_0) &amp;= x_0z_0 \\\\ 2\\lambda (x_0 + y_0) &amp;= x_0y_0 \\end{align*}\\] <p>Add all of these equations together: [ 2\\lambda( 2x_0 + 2y_0 + 2z_0 ) = x_0z_0 + x_0y_0 + y_0z_0 = \\frac{A}{2} &gt; 0 ] implying that \\(\\lambda &gt; 0\\). The first two equations tell us that</p> \\[\\begin{align*} 2\\lambda x_0 (y_0 + z_0) &amp;= x_0y_0z_0 \\\\ 2\\lambda y_0 (x_0 + z_0) &amp;= x_0y_0z_0. \\end{align*}\\] <p>Subtracting these two equations gives \\(2\\lambda (x_0z_0 - y_0z_0) = 0\\). Cancelling the \\(z_0\\)'s gives \\(2\\lambda (x_0 - y_0) = 0\\), and since \\(\\lambda &gt; 0\\), we have \\(x_0 = y_0\\). Since we could have done the same thing with the other pairs of equations, we get \\(x_0 = y_0 = z_0\\). </p> <p>Physically, this tells us that in order to maximize the volume of a rectangular solid of fixed area, we must make a cube. Note that we haven't actually solved the maximization problem; we've only figured out what form its solutions must take.</p>"},{"location":"apm462/ecfdo.html#thrm-second-order-necessary-conditions","title":"Thrm. Second Order Necessary Conditions","text":"<p>Claim. Let \\(f, h_1, \\dots, h_k\\) be \\(C^2\\) on some open set \\(\\Omega \\subseteq \\mathbb R^n\\). Suppose \\(x_0\\) is a regular point which is a local minimizer of \\(f\\) subject to the constraints. Then 1. There are \\(\\lambda_1, \\dots, \\lambda_k \\in \\mathbb R\\) such that</p> <pre><code>$$\\nabla f(x_0) + \\lambda_1 \\nabla h_1(x_0) + \\cdots + \\lambda_k \\nabla h_k(x_0) = 0$$\n</code></pre> <ol> <li> <p>The \"Lagrangian\"</p> \\[L(x_0) = \\nabla^2 f(x_0) + \\sum \\lambda_i \\nabla^2 h_i(x_0)\\] <p>is positive semi-definite on the tangent space \\(T_{x_0}M\\), where \\(M = h_1^{-1}(\\{0\\}) \\cap \\cdots \\cap h_k^{-1}(\\{0\\})\\)</p> </li> </ol> <p>proof. Let \\(x(s)\\) be a smooth curve with \\(x(0) = 0\\) in \\(M\\). Recall that, by the product rule,</p> \\[\\begin{align*} \\frac{d}{ds} f(x(s)) &amp;= \\nabla f(x(s)) \\cdot x'(s) \\\\ \\frac{d^2}{ds^2} f(x(s)) &amp;= x'(s) \\cdot \\nabla^2 f(x(s)) x'(s) + \\nabla f(x(s)) \\cdot x''(s). \\end{align*}\\] <p>By the second order Taylor approximation, we have</p> \\[0 \\leq f(x(s)) - f(x(0)) = s \\left. \\frac{d}{ds} \\right|_{s=0} f(x(s)) + \\frac{1}{2}s^2 \\left. \\frac{d^2}{ds^2} \\right|_{s=0} f(x(s)) + o(s^2)\\] <p>This is, equivalently, </p> \\[0 \\leq f(x(s)) - f(x(0)) = s \\nabla f(x_0) \\cdot \\underbrace{x'(0)}_{\\in T_{x_0}M} + \\frac{1}{2}s^2 \\left. \\frac{d^2}{ds^2} \\right|_{s=0} f(x(s)) + o(s^2)\\] <p>Since the gradient at a regular local minimizer is perpendicular to the tangent space there, the first-order term above vanishes. We have</p> \\[0 \\leq \\frac{1}{2}s^2 \\left. \\frac{d^2}{ds^2} \\right|_{s=0} f(x(s)) + o(s^2)\\] <p>By the definition of \\(M\\), we may write the above as</p> \\[0 \\leq \\frac{1}{2}s^2 \\left. \\frac{d^2}{ds^2} \\right|_{s=0} \\left[ f(x(s)) + \\sum \\lambda_i h_i(x(s)) \\right] + o(s^2)\\] <p>Or</p> \\[0 \\leq \\frac{1}{2} s^2 x'(0) \\cdot \\underbrace{\\left(\\nabla^2 f(x_0) + \\sum \\lambda_i \\nabla^2 h(x_0)\\right)}_{=L(x_0)}x'(0) + \\frac{1}{2} s^2 \\underbrace{\\left(\\nabla f(x_0) + \\sum \\lambda_i \\nabla h_i(x_0)\\right)}_{=0} \\cdot x''(0) + o(s^2)\\] <p>Divide by \\(s^2\\):</p> \\[0 \\leq \\frac{1}{2} x'(0) \\cdot L(x_0) x'(0) + \\frac{o(s^2)}{s^2}\\] <p>By taking \\(s\\) small it follows that \\(0 \\leq \\frac{1}{2} x'(0) \\cdot L(x_0) x'(0)\\). Since any tangent vector \\(v \\in T_{x_0}M\\) can be described as the tangent vector to a curve in \\(M\\) through \\(x_0\\), it follows that \\(L(x_0)\\) is positive semi-definite on \\(T_{x_0}M\\).</p>"},{"location":"apm462/ecfdo.html#thrm-second-order-sufficient-conditions","title":"Thrm. Second Order Sufficient Conditions","text":"<p>Claim. Consider functions \\(f, h_1, \\dots, h_k\\) which are \\(C^2\\) on the open \\(\\Omega \\subseteq \\mathbb R^n\\). Suppose \\(x_0\\) is a regular point of the constraints given by \\(h_1(x) = \\cdots = h_k(x) = 0\\). Let \\(M = \\bigcap h_i^{-1}(\\{0\\})\\). Suppose there exist \\(\\lambda_1, \\dots, \\lambda_k \\in \\mathbb R\\) such that</p> <ol> <li>\\(\\nabla f(x_0) + \\sum \\lambda_i \\nabla h_i(x_0) = 0\\)</li> <li>\\(L(x_0) = \\nabla^2 f(x_0) + \\sum \\lambda_i \\nabla^2 h_i(x_0)\\)</li> </ol> <p>is positive definite on \\(T_{x_0}M\\).  </p> <p>Then \\(x_0\\) is a strict local minimizer of \\(f\\) on \\(M\\).</p> <p>proof. Recall that if \\(L(x_0)\\) is positive definite on \\(T_{x_0}M\\), then there is an \\(a &gt; 0\\) such that \\(v \\cdot L(x_0)v \\geq a\\|v\\|^2\\) for all \\(v \\in T_{x_0}M\\). (This is very easily proven by diagonalizing the matrix.) Let \\(x(s)\\) be a smooth curve in \\(M\\) such that \\(x(0) = x_0\\), and normalize the curve so that \\(\\|x'(0)\\| = 1\\). We have which becomes</p> \\[\\begin{align*} f(x(s)) - f(x(0)) &amp;= s \\left. \\frac{d}{ds} \\right|_{s=0} f(x(s)) + \\frac{1}{2} s^2 \\left. \\frac{d^2}{ds^2} \\right|_{s=0} f(x(s)) + o(s^2) \\\\ &amp;= s \\left. \\frac{d}{ds} \\right|_{s=0} \\left[ f(x(s)) + \\sum \\lambda_i h_i(x(s)) \\right]+ \\frac{1}{2} s^2 \\left. \\frac{d^2}{ds^2} \\right|_{s=0} \\left[ f(x(s)) + \\sum \\lambda_i h_i(x(s)) \\right] + o(s^2) \\\\ &amp;= s \\underbrace{[ \\nabla f(x_0) + \\sum \\lambda_i \\nabla h_i(x_0) ]}_{=0 \\text{ by 1.}} \\cdot x'(0) + \\frac{1}{2} s^2x'(0) \\cdot L(x_0) x'(0)  \\\\ &amp;\\qquad\\qquad\\qquad\\qquad\\qquad + \\frac{1}{2} s^2\\underbrace{[ \\nabla f(x_0) + \\sum \\lambda_i \\nabla h_i(x_0) ]}_{=0 \\text{ by 1.}} \\cdot x''(0) + o(s^2) \\\\ &amp;= \\frac{1}{2} s^2 x'(0)^T L(x_0) x'(0) + o(s^2) \\\\ &amp;\\geq \\frac{1}{2}s^2 a\\|x'(0)\\|^2 + o(s^2) \\\\ &amp;= \\frac{1}{2}s^2 a + o(s^2) \\\\ &amp;= s^2 \\left( \\frac{1}{2}a + \\frac{o(s^2)}{s^2} \\right) \\end{align*}\\] <p>For sufficiently small \\(s\\), the above is positive, so \\(f(x(s)) &gt; f(x_0)\\) for all sufficiently small \\(s\\). Since \\(x(s)\\) was arbitrary, \\(x_0\\) is a strict local minimizer of \\(f\\) on \\(M\\).</p>"},{"location":"apm462/ecfdo.html#example-1","title":"Example 1","text":"<p>Recall the box example: maximizing the volume of a box of sides \\(x,y,z\\geq 0\\) subject to a fixed surface area \\(A &gt; 0\\). We were really minimizing the negative of the volume. We got \\((x_0,y_0,z_0) = (l,l,l)\\), where \\(l = \\sqrt{A/6}\\). Our Lagrange multiplier was \\(\\lambda = \\frac{A}{8(x_0+y_0+z_0)} = \\frac{A}{24 l} &gt; 0\\). We had (after some calculation)</p> \\[L(x_0,y_0,z_0) = (2\\lambda - l)\\begin{pmatrix} 0&amp;1&amp;1 \\\\ 1&amp;0&amp;1 \\\\ 1&amp;1&amp;0 \\end{pmatrix}\\] <p>Here, \\(2\\lambda - l &lt; 0\\). We have </p> \\[T_{(x_0,y_0,z_0)}M = \\mathrm{span}( \\nabla h(x_0,y_0,z_0) )^\\perp = \\{ (u,v,w) \\in \\mathbb R^3 : u+v+w=0 \\}\\] <p>since \\(\\nabla h(x_0,y_0,z_0) = (4l,4l,4l)\\). If \\((u,v,w) \\in T_{(x_0,y_0,z_0)}M\\) is nonzero,</p> \\[\\begin{align*} \\begin{pmatrix} u&amp;v&amp;w \\end{pmatrix}(2\\lambda - l)\\begin{pmatrix} 0&amp;1&amp;1 \\\\ 1&amp;0&amp;1 \\\\ 1&amp;1&amp;0 \\end{pmatrix} \\begin{pmatrix} u\\\\v\\\\w \\end{pmatrix} &amp;= \\begin{pmatrix} u&amp;v&amp;w \\end{pmatrix}(2\\lambda - l) \\begin{pmatrix} v+w \\\\ u+w \\\\ u+v \\end{pmatrix} \\\\ &amp;= (2\\lambda - l)\\begin{pmatrix} u&amp;v&amp;w \\end{pmatrix} \\begin{pmatrix} -u \\\\ -v \\\\ -w \\end{pmatrix} \\\\ &amp;= -(2\\lambda - l)(u^2+v^2+w^2) &gt; 0, \\end{align*}\\] <p>so by the SOSC under equality constraints, our point \\((x_0,y_0,z_0)\\) is a strict local maximizer of the volume. In fact, it is a strict global minimum (which is yet to be seen).</p>"},{"location":"apm462/ecfdo.html#example-2","title":"Example 2","text":"<p>Consider the problem</p> \\[\\begin{align*} \\text{minimize } &amp;f(x,y) = x^2-y^2 \\\\ \\text{subject to } &amp;h(x,y) = y = 0. \\end{align*}\\] <p>Then</p> \\[\\nabla f(x,y) + \\lambda \\nabla h(x,y) = \\begin{pmatrix} 2x \\\\ -2y \\end{pmatrix} + \\lambda \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}\\] <p>implying that \\(\\lambda = 0\\) and that \\((x,y) = (0,0)\\) is our candidate local minimizer. Since \\(\\nabla h(x,y) \\neq (0,0)\\), the candidate is a regular point. We have</p> \\[L(0,0) = \\begin{pmatrix} 2 &amp; 0 \\\\ 0 &amp; -2 \\end{pmatrix} + 0 \\begin{pmatrix} 0 &amp; 0 \\\\ 0 &amp; 0 \\end{pmatrix} = \\begin{pmatrix} 2 &amp; 0 \\\\ 0 &amp; -2 \\end{pmatrix}\\] <p>which is not positive semi-definite everywhere. What about on the tangent space \\(T_{(0,0)}(\\text{x-axis})=(\\text{x-axis})\\)? Clearly it is positive definite on the \\(x\\)-axis, so by the SOSC that we just proved, \\((0,0)\\) is a strict local minimizer of \\(f\\) on the \\(x\\)-axis. Thinking of level sets, this is intuitively true.</p>"},{"location":"apm462/ecfdo.html#example-3","title":"Example 3","text":"<p>Consider the problem</p> \\[\\begin{align*} \\text{minimize } &amp;f(x,y) = (x-a)^2 + (y-b)^2 \\\\ \\text{subject to } &amp;h(x,y) = x^2+y^2-1=0. \\end{align*}\\] <p>Let us assume that \\((a,b)\\) satisfies \\(a^2+b^2&gt;1\\). We have \\(\\nabla h(x,y) = (2x,2y)\\), which is non-zero on \\(S^1\\), implying that every point of \\(S^1\\) is a regular point. Lagrange tells us that</p> \\[\\begin{pmatrix} 2(x-a) \\\\ 2(y-b) \\end{pmatrix} + \\lambda \\begin{pmatrix} 2x \\\\ 2y \\end{pmatrix} = \\begin{pmatrix} 0\\\\0 \\end{pmatrix}\\] <p>as well as \\(x^2+y^2=1\\). This may be written</p> \\[\\begin{align*} (1+\\lambda)x &amp;= a \\\\ (1+\\lambda)y &amp;= b \\\\ x^2+y^2 &amp;= 1 \\end{align*}\\] <p>By our assumption that \\(a^2+b^2&gt;1\\), we have \\(\\lambda \\neq -1\\). Therefore</p> \\[\\begin{pmatrix} x \\\\ y \\end{pmatrix} = \\frac{1}{1+\\lambda}\\begin{pmatrix} a \\\\ b \\end{pmatrix}\\] <p>which implies that</p> \\[\\frac{1}{1+\\lambda} = \\frac{1}{\\sqrt{a^2+b^2}}\\] <p>by the third equation. Therefore</p> \\[\\begin{pmatrix} x_0 \\\\ y_0 \\end{pmatrix} = \\frac{1}{\\sqrt{a^2+b^2}}\\begin{pmatrix} a \\\\ b \\end{pmatrix}\\] <p>Thinking of level sets, this is intuitively true. The Lagrangian is</p> \\[L(x_0,y_0) = \\begin{pmatrix} 2 &amp; 0 \\\\ 0 &amp; 2 \\end{pmatrix} + \\lambda \\begin{pmatrix} 2 &amp; 0 \\\\ 0 &amp; 2 \\end{pmatrix} = \\underbrace{(1+\\lambda)}_{&gt;0}\\begin{pmatrix} 2 &amp; 0 \\\\ 0 &amp; 2 \\end{pmatrix}\\] <p>which, by the SOSC that we proved, proves that \\((x_0, y_0)\\) is a strict local minimizer of \\(f\\) on \\(S^1\\). In fact, this point is a global minimizer of \\(f\\) on \\(S^1\\), which follows immediately by the fact that \\(f\\) necessarily takes on a global minimum on \\(S^1\\) and that it only takes on the point \\((x_0,y_0)\\).</p>"},{"location":"apm462/ecfdo.html#example-4-proof-of-lagrange-multiplier-on-a-special-case","title":"Example 4: Proof of Lagrange Multiplier on A Special Case","text":"<p>For a special case, we will derive the Lagrange multipliers equation. Suppose we are working with \\(C^1\\) functions \\(f,h\\). Our problem is</p> \\[\\begin{align*} \\text{minimize } &amp;f(x,y,z) \\\\ \\text{subject to } &amp;g(x,y,z) = z-h(x,y) = 0. \\end{align*}\\] <p>That is, we are minimizing \\(f(x,y,z)\\) on the graph \\(\\Gamma_h\\) of \\(h\\). The Lagrange equation tells us that</p> \\[\\nabla f(x,y,z) + \\lambda g(x,y,z) = \\begin{pmatrix} \\frac{\\partial  f}{\\partial  x}(x,y,z) \\\\ \\frac{\\partial  f}{\\partial  y}(x,y,z) \\\\ \\frac{\\partial  f}{\\partial  z}(x,y,z) \\end{pmatrix} + \\lambda \\begin{pmatrix} -\\frac{\\partial  h}{\\partial  x}(x,y,z) \\\\ -\\frac{\\partial  y}{\\partial  x}(x,y,z) \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix}\\] <p>We will derive the above formula by expressing it as an unconstrained minimization problem</p> \\[\\text{minimize }_{(x,y) \\in \\mathbb R^2} F(x,y)\\] <p>for some function \\(F\\). We will then find the first order necessary conditions for an unconstrained minimization, and then express it as the equation we would like to prove.</p> <p>Define \\(F(x,y) = f(x,y,f(x,y))\\). The constrained minimization problem is therefore equivalent to the unconstrained problem. By our theory of unconstrained minimization, \\(\\nabla F(x_0,y_0)=(0,0)\\). That is,</p> \\[\\nabla F(x_0,y_0) = \\begin{pmatrix} \\frac{\\partial  f}{\\partial  x} + \\frac{\\partial  f}{\\partial  z}\\frac{\\partial  h}{\\partial  x} \\\\ \\frac{\\partial  f}{\\partial  y} + \\frac{\\partial  f}{\\partial  z}\\frac{\\partial  h}{\\partial  y} \\end{pmatrix} = \\begin{pmatrix} 0\\\\0 \\end{pmatrix}\\] <p>Rather,</p> \\[\\begin{align*} \\frac{\\partial  f}{\\partial  x} + \\frac{\\partial  f}{\\partial  z}\\frac{\\partial  h}{\\partial  x} &amp;= 0 \\\\ \\frac{\\partial  f}{\\partial  y} + \\frac{\\partial  f}{\\partial  z}\\frac{\\partial  h}{\\partial  y} &amp;= 0 \\end{align*}\\] <p>Let \\(\\lambda = -\\frac{\\partial  f}{\\partial  z}\\). The equation becomes</p> \\[\\begin{align*} \\frac{\\partial  f}{\\partial  x} - \\lambda \\frac{\\partial  h}{\\partial  x} &amp;= 0 \\\\ \\frac{\\partial  f}{\\partial  y} - \\lambda \\frac{\\partial  h}{\\partial  y} &amp;= 0 \\\\ \\frac{\\partial  f}{\\partial  z} + \\lambda &amp;= 0 \\end{align*}\\] <p>which is what we wanted.</p>"},{"location":"apm462/ecfdoq.html","title":"Examples: Equality Constrained Finite Dimension Optimization","text":""},{"location":"apm462/ecfdoq.html#example-1","title":"Example 1","text":"<p>Question</p> <p>Show that maximizing logistic likelihood is equivalent to minimizing negative log likelihood.</p> <p>proof. Note that maximizing \\(\\mathcal L\\) is equivalent to maximizing \\(\\log(\\mathcal L)\\) since \\(\\log\\) is a monotone increasing function. Also, minimizing a function's negative is equivalent to maximizing the function. Denote \\(c_i = \\exp(a_i^Ty+\\beta), d_j = \\exp(b_j^Ty+\\beta)\\). The likelihood for logistic classification is </p> \\[\\mathcal L = \\prod_i \\frac{c_i}{1+c_i}\\prod_j (1-\\frac{d_j}{1+d_j}) = \\prod_i \\frac{c_i}{1+c_i}\\prod_j \\frac{1}{1+d_j}\\] <p>Then, the negative log likelihood is </p> \\[\\begin{align*} -\\log (\\mathcal L) &amp;= -\\big[\\sum_{i}\\log(\\frac{c_i}{1+c_i}) + \\sum_j\\log(\\frac{1}{1+d_j})\\big]\\\\ &amp;= - \\big[\\sum_i -\\log(\\frac{1+c_i}{c_i})+ \\sum_j - \\log(1+d_j)\\big]\\\\ &amp;= \\sum_i \\log(\\frac{1+c_i}{c_i})+ \\sum_j \\log(1+d_j)\\\\ &amp;= \\sum_i \\log(1 + \\exp(a_i^Ty +\\beta)^{-1})+ \\sum_j \\log(1+\\exp(b_j^Ty+\\beta))\\\\ &amp;= \\sum_i \\log(1 + \\exp(-a_i^Ty -\\beta))+ \\sum_j \\log(1+\\exp(b_j^Ty+\\beta)) \\end{align*}\\] <p>Therefore, minimize \\(\\mathcal L\\) is quivalent to maximize \\(-\\log(\\mathcal L)\\)</p>"},{"location":"apm462/ecfdoq.html#example-2","title":"Example 2","text":"<p>Question</p> <p>Show that polynomial approximation can be written in a quadratic form.</p> <p>proof. First, the equation is</p> \\[\\sum_{k=1}^m\\big(g(x_k) - \\sum_{i=0}^n a_i x_k^i\\big)^2 = \\sum_{k=1}^mg(x_k)^2 - 2\\sum_{k=1}^m\\big(g(x_k)\\sum_{i=0}^n a_i x_k^i\\big) + \\sum_{k=1}^m\\big(\\sum_{i=0}^n a_ix_k^i\\big)^2\\] <p>The first summation is </p> \\[\\sum_{k=1}^mg(x_k)^2 = c\\] <p>The second summation term is</p> \\[\\begin{align*} 2\\sum_{k=1}^m\\big(g(x_k)\\sum_{i=0}^n a_i x_k^i\\big)&amp;= 2\\sum_{i=0}^na_i\\sum_{k=1}^mg(x_k)x_k^j \\\\ &amp;= 2\\sum a_ib_i\\\\ &amp;= 2b^Ta \\end{align*}\\] <p>Define \\(\\vec x_k = (1, x_k, x_k^2,...,x_k^{n})\\), then then last summation term is </p> \\[\\begin{align*} \\sum_{k=1}^m\\big(\\sum_{i=0}^n a_ix_k^i\\big)^2 &amp;= \\sum_{k=1}^m(\\vec x_k^T a)^T(\\vec x_k^T a)\\\\ &amp;= \\sum_{k=1}^m a^T\\vec x_k\\vec x_k^T a\\\\ &amp;= a^T \\big[\\sum_{k=1}^m \\vec x_k\\vec x_k^T\\big] a \\end{align*}\\] <p>Note that \\(\\vec x_k \\vec x_k^T = Q_k\\) where \\(q_{k_{ij}} = [x_k^{i+j}]\\) and \\(\\sum_{k=1}^m Q_k = Q\\).  </p> <p>Therefore, \\(f(a) = a^TQa - 2b^Ta + c\\)</p>"},{"location":"apm462/ecfdoq.html#example-3","title":"Example 3","text":"<p>Question</p> <p>Find the regular points on \\(M = \\{(x, y) \\in\\mathbb R^2\\mid (x-1)^2(x-y)(x+y) = 0\\}\\).</p> <p>Note that the set of all feasible points are \\(\\{(x,y)\\in\\mathbb R^2\\mid x=1\\}\\cup \\{(x,y)\\in\\mathbb R^2\\mid x=y\\}\\cup \\{(x,y)\\in\\mathbb R^2\\mid x=-y\\}\\). Let \\(h(x, y) = (x-1)^2(x-y)(x+y)\\), then</p> \\[\\frac{\\partial h}{\\partial y} = -2(x-1)^2y, \\frac{\\partial}{\\partial x} = 4x^3 - 6x^2-2xy^2+2x+2y^2\\] <p>Set \\(\\nabla h(x, y) =0\\), we solve \\(\\begin{cases}-2(x-1)^2y = 0\\\\4x^3 - 6x^2 - 2xy^2 +2x+2y^2=0\\end{cases}\\) For the set \\(\\{(x,y)\\in\\mathbb R^2\\mid x=1\\}\\), obtained </p> \\[\\begin{cases}-2(1-1)^2y = 0\\\\4 - 6 - 2y^2 +2+2y^2=0\\end{cases}\\] <p>solves to be \\(y\\in\\mathbb R\\). For the set \\(\\{(x,y)\\in\\mathbb R^2\\mid x=y\\}\\), obtained </p> \\[\\begin{cases}-2(y-1)^2y = 0\\\\4y^3 - 6y^2 - 2y^3 +2y+2y^2=0\\end{cases}\\] <p>solves to have \\(x=y=0\\) and \\(x=y=1\\) For the set \\(\\{(x,y)\\in\\mathbb R^2\\mid x=-y\\}\\), obtained </p> \\[\\begin{cases}-2(-y-1)^2y = 0\\\\4(-y)^3 - 6(-y)^2 + 2y^3 -2y+2y^2=0\\end{cases}\\] <p>solves to have \\(x=y=0\\) and \\(x=1, y=-1\\) Combine the cases together, we obtained that the regular points are \\(\\{(x,y)\\in\\mathbb R^2\\mid x= 1\\}\\cup \\{(0, 0)\\}\\)</p>"},{"location":"apm462/ecfdoq.html#example-4","title":"Example 4","text":"<p>Question</p> <p>Consider the minimization problem </p> \\[\\text{minimize}:f(x,y,z) = (x-\\frac y2)^2 + \\frac34(y-2)^2+z^2-3\\] \\[\\text{subject to}: h_1(x,y,z)=x=0; h_2(x,y,z)=z=0\\] <p>Part (a)</p> <p>Which feasible points are regular.</p> <p>First, obviously the feasible points are \\(\\{(0, y, 0)\\mid y\\in\\mathbb R\\}\\). Also, note that \\(\\nabla h_1 = (1, 0, 0), \\nabla h_2 = (0, 0, 1)\\) are always linearly independent. Therefore, all feasible points are regular </p> <p>Part (b)</p> <p>Find candidates for minimizer.</p> <p>First note that \\(\\nabla f = \\begin{bmatrix}2x-y\\\\2y-x-3\\\\2z\\end{bmatrix}\\). Using the Lagrange multipliers method, let \\(\\lambda_1, \\lambda_2 \\in\\mathbb R\\), we obtain the equations</p> \\[\\begin{bmatrix}2x-y\\\\2y-x-3\\\\2z\\end{bmatrix} + \\lambda_1 \\begin{bmatrix}1\\\\0\\\\0\\end{bmatrix} + \\lambda_2 \\begin{bmatrix}0\\\\0\\\\1\\end{bmatrix} = 0\\] <p>Solve the equations on the constraints \\(x=0, z=0\\), we have </p> \\[\\begin{cases} -y+\\lambda_1 = 0\\\\ 2y-3 = 0\\\\ \\lambda_2 = 0 \\end{cases}\\] <p>Which solves to have \\(y=2/3, \\lambda_1 = 2/3, \\lambda_2 = 0\\) so that the candidate for minimizer is \\((0, 2/3, 0)\\)</p> <p>Part (c)</p> <p>Check 2nd order condition.</p> <p>\\(\\nabla^2 f = \\begin{bmatrix}2&amp;-1&amp;0\\\\-1&amp;2&amp;0\\\\0&amp;0&amp;2\\end{bmatrix}, \\nabla^2 h_1 = 0, \\nabla^2 h_2 = 0\\), \\(\\nabla^2 f + \\lambda_1\\nabla^2 h_1 + \\nabla^2 h_2 = \\nabla^2 f\\) is a positive semidefinite matrix on \\(\\mathbb R^3\\), hence also on \\(M\\). By 2nd order condition, \\((0,3/2, 0)\\) is a minimizer.</p>"},{"location":"apm462/ecfdoq.html#example-5","title":"Example 5","text":"<p>Question</p> <p>Let Q be symmetric \\(n\\times n\\) matrix with eigenvalues \\(\\lambda_1 \\leq ...\\leq \\lambda_n\\) and consider minimize \\(f(x)=\\frac{x^TQx}{x^Tx}\\) subject to \\(\\sum^n i(x_i^2) = 1\\), find the min value in terms of \\(\\lambda\\)'s.</p> <p>First, note that for any \\(x\\in\\mathbb R^n, a\\in\\mathbb R\\), </p> \\[f(ax) = \\frac{ax^TQax}{ax^Tax} = \\frac{a^2x^TQx}{a^2x^Tx} = \\frac{x^TQx}{x^Tx}=f(x)\\] <p>Therefore, let \\(a = \\|x\\|^{-1}\\), we have \\(f(x') = f(\\frac{x}{\\|x\\|})\\) so that \\(f(x') = \\frac{x'^TQx'}{1} = x'^TQx'\\) and \\(\\|x'\\| = 1\\). Then, because \\(Q\\) is symmetric,</p> \\[\\nabla f(x) = (Q+Q^T)x = 2Qx\\] <p>Let \\(h(x) = \\sum_{i=1}^n i(x_i)^2 - 1\\), so that </p> \\[\\nabla h(x) = 2(x_1, 2x_2, ..., nx_n)\\] <p>By Lagrange multiplier, take some \\(\\lambda\\in\\mathbb R\\), and we have</p> \\[\\begin{align*} 2Qx &amp;= \\lambda 2(x_1, 2x_2,...,nx_n) \\end{align*}\\]"},{"location":"apm462/ecfdoq.html#question-6","title":"Question 6","text":"<p>Question</p> <p>optimize \\(f(x,y) = (x-6)^2 + 5(y-7)^2\\) subject to \\(h(x, y) = x^2+y^2 - 25 =0\\).</p> Source code <pre><code>import matplotlib.pyplot as plt\nimport numpy as np\n\nx = np.arange(-6, 10, 0.01)\ny = np.arange(-6, 12, 0.01)\nxx, yy = np.meshgrid(x, y)\nf = (xx - 6) ** 2 + 5 * (yy - 7)**2\n\nplt.figure(figsize=(6, 6))\nplt.contour(xx, yy, f, levels=[3, 10, 20, 40, 80, 160, 320, 640])\nan = np.linspace(0, 2 * np.pi, 100)\nplt.plot(5 * np.cos(an), 5 * np.sin(an), color=\"red\")\nplt.axis(\"equal\")\nplt.xlim(-6, 8)\nplt.ylim(-6, 8)\nplt.savefig(\"../assets/ecfdoq.jpg\")\n</code></pre> <p></p> <p>Question</p> <p>Show that every feasible point is regular.</p> <p>\\(\\nabla h(x, y) = (2x, 2y)\\) can only be \\(0\\) IFF \\(x=y=0\\), while \\((0,0)\\) is not a feasible point. All feasible points satisfies that \\((2x,2y)\\neq 0\\)</p> <p>Question</p> <p>Find all candidates of the the local minimum points.</p> \\[\\nabla f(x, y) = (2(x-6), 10(y-7))\\] <p>By Lagrange multiplier, let \\(\\lambda\\in\\mathbb R\\), the system of equations </p> \\[\\begin{cases} 2(x-6) + \\lambda2x = 0\\\\ 10(y-7) + \\lambda 2y = 0\\\\ x^2 + y^2 = 25 \\end{cases}\\]"},{"location":"apm462/icfdo.html","title":"Inequality Constrained Finite Dimension Optimization","text":""},{"location":"apm462/icfdo.html#problem-definition","title":"Problem Definition","text":"<p>Our problem is of the form</p> \\[\\begin{align*} \\text{minimize } &amp;f(x) \\\\ \\text{subject to } &amp;h_1(x) = \\cdots = h_k(x) = 0 \\\\ &amp;g_1(x), \\dots, g_l(x) \\leq 0. \\end{align*}\\]"},{"location":"apm462/icfdo.html#defn-active-constraint","title":"Defn. Active Constraint","text":"<p>Let \\(x_0\\) satisfy the above constraints. We call the inequality constraint \\(g_i(x) \\leq 0\\) active at \\(x_0\\) if \\(g_i(x_0) = 0\\). Otherwise, it is inactive at \\(x_0\\).</p>"},{"location":"apm462/icfdo.html#defn-regular-point","title":"Defn. Regular point","text":"<p>Suppose there is an index \\(l' \\leq l\\) such that \\(g_1(x_0) =, \\dots, g_{l'}(x_0) = 0\\) are active, and \\(g_{l'+1}(x_0) \\leq 0, \\dots, g_l(x_0) \\leq 0\\) are inactive. We say that \\(x_0\\) is a regular point of these constraints if the vectors \\(\\nabla h_1(x_0), \\dots, \\nabla h_k(x_0), \\nabla g_1(x_0), \\dots, \\nabla g_{l'}(x_0)\\) are linearly independent.</p>"},{"location":"apm462/icfdo.html#thrm-first-order-necessary-conditions-kuhn-tucker-conditions","title":"Thrm. First Order Necessary Conditions (Kuhn-Tucker Conditions)","text":"<p>Claim. Let \\(\\Omega \\subseteq \\mathbb R^n\\) be open and consider \\(C^1\\) functions \\(f, h_1, \\dots, h_k, g_1, \\dots, g_l\\) on \\(\\Omega\\). Suppose \\(x_0\\) is a local minimizer of \\(f\\) subject to the constraints, and that \\(x_0\\) is regular as defined above. Then  1. There exist \\(\\lambda_1, \\dots, \\lambda_k \\in \\mathbb R\\) and \\(\\mu_1, \\dots, \\mu_l \\in \\mathbb R^{\\geq 0}\\) such that  \\(\\nabla f(x_0) + \\sum \\lambda_i \\nabla h_i(x_0) + \\sum \\mu_j \\nabla g_j(x_0) = 0\\) 2. (Complementary slackness conditions) For all \\(j\\), \\(\\mu_j g_j(x_0) = 0\\), or equivalently, \\(\\sum \\mu_j g_j(x_0) = 0\\).</p> <p>Suppose the active constraints at \\(x_0\\) are the first \\(l'\\) constraints. Since each \\(\\mu_j \\geq 0\\), condition (ii) is equivalent to saying that if \\(j \\geq l'+1\\), then \\(\\mu_j = 0\\). </p> <p>proof. If \\(x_0\\) is a local minimizer of \\(f\\) subject to the constraints, then it is certainly a local minimizer of \\(f\\) subject to only the active constraints. That is, \\(x_0\\) is also a local minimizer of \\(f\\) subject to the equality constraints</p> \\[h_1(x) = \\cdots = h_k(x) = g_1(x) = \\cdots = g_{l'} = 0\\] <p>We know how to work with this! Let \\(M\\) be the surface defined by these equality constraints. By the Lagrange multipliers theorem, </p> \\[\\nabla f(x_0) + \\sum \\lambda_i \\nabla h_i(x_0) + \\sum \\mu_j \\nabla g_j(x_0) = 0\\] <p>for some \\(\\lambda_i \\in \\mathbb R, \\mu_j \\in \\mathbb R\\). (Note that we have not yet shown that the \\(\\mu_j\\)'s are non-negative.)</p> <p>Note that \\(g_1(x_0) = \\cdots = g_{l'}(x_0) = 0\\). Therefore \\(\\mu_{l'+1} = \\cdots = \\mu_l = 0\\), so it follows that</p> \\[\\mu_1 g_1(x_0) = 0, \\dots, \\mu_{l'}g_{l'}(x_0) = 0\\] <p>which implies that \\(\\mu_j g_j(x_0) = 0\\) for all \\(j\\). We have proven condition (ii).</p> <p>We must now verify the non-negativity of the \\(\\mu_j\\)'s. Suppose for the sake of contradiction that some \\(\\mu_j &lt; 0\\); WLOG assume \\(j=1\\). Let</p> \\[\\tilde{M} = \\{x \\in \\Omega : h_i(x) = 0, g_i(x) = 0, j \\neq 1\\}\\] <p>Since \\(x_0\\) is a regular point of \\(M\\), \\(x_0\\) is a regular point of \\(\\tilde{M}\\). Therefore</p> \\[T_{x_0}\\tilde{M} = \\mathrm{span}(\\{\\nabla h_1(x_0), \\dots, h_k(x_0), \\nabla g_2(x_0), \\dots,\\nabla g_l(x_0)\\})^\\perp\\] <p>The vector \\(\\nabla g_1(x_0)\\) does not lie in this span, so there is a \\(v \\in T_{x_0}\\tilde{M}\\) such that \\(\\nabla g_1(x_0) \\cdot v &lt; 0\\). That is, \\(g_1\\) is strictly decreasing in the direction of \\(v\\), or in more precise language, \\(g_1(x_0 + sv) &lt; g_1(x_0)\\) for all sufficiently small \\(s\\), as we have</p> \\[\\left. \\frac{d}{ds} \\right|_{s=0} g_1(x_0 + sv) = \\nabla g_1(x_0) \\cdot v &lt; 0\\] <p>Therefore \\(v\\) is a feasible direction for \\(g_1(x) \\leq 0\\) at \\(x_0\\), and also, \\(v\\) is tangential to the other constraints. Since \\(x_0\\) is a regular point of \\(\\tilde{M}\\), we may find a curve \\(x(s)\\) on \\(\\tilde{M}\\) such that \\(x(0) = x_0\\) and \\(x'(0) = v\\). Also, \\(s = 0\\) is a local minimizer of \\(f \\circ x\\), so</p> \\[\\left. \\frac{d}{ds}\\right|_{s=0} f(x(s)) = \\nabla f(x_0) \\cdot v \\geq 0\\] <p>On the other hand, </p> \\[\\nabla f(x_0) + \\sum \\lambda_i \\nabla h_i(x_0) + \\mu_1 \\nabla g_1(x_0)+\\sum_{j=2}^{l'} \\mu_j \\nabla g_j(x_0) = 0\\] <p>Taking the dot product of the above equation by \\(v\\) kills the two sums above and gives</p> \\[\\nabla f(x_0)\\cdot v  + \\mu_1 \\nabla g_1(x_0)\\cdot v = 0\\] <p>implying \\(\\nabla f(x_0)\\cdot v &lt; 0\\), a contradiction. So every \\(\\mu_j \\geq 0\\).</p>"},{"location":"apm462/icfdo.html#thrm-second-order-necessary-conditions","title":"Thrm. Second Order Necessary Conditions","text":"<p>Claim. Suppose \\(f, h_1, \\dots, h_k, g_1, \\dots, g_k \\in C^2(\\Omega)\\), where \\(\\Omega \\subseteq \\mathbb R^n\\). Suppose \\(x_0\\) is a regular point of the constraints. If \\(x_0\\) is a local minimizer of \\(f\\) subject to the constraints, then 1. There are \\(\\lambda_1, \\dots, \\lambda_k \\in \\mathbb R\\) and \\(\\mu_1, \\dots, \\mu_l \\geq 0\\) such that \\(\\nabla f(x_0) + \\sum_i \\lambda_i \\nabla h_i(x_0) + \\sum_j \\mu_j \\nabla g_j(x_0) = 0\\) and \\(\\mu_j g_j(x_0) = 0\\) for each \\(j\\).</p> <ol> <li>The matrix \\(L(x_0) = \\nabla^2 f(x_0) + \\sum_i \\lambda_i \\nabla^2 h_i(x_0) + \\sum_j \\mu_j \\nabla^2 g_j(x_0)\\) is positive semi-definite on the tangent space \\(T_{x_0}\\tilde{M}\\) to the active constraints at \\(x_0\\). (Explicitly, \\(L(x_0)\\) is positive semi-definite on the space \\(T_{x_0}\\tilde{M} = \\{v \\in \\mathbb R^n : \\nabla h_i(x_0) \\cdot v = 0. \\forall i, \\nabla g_j(x_0) \\cdot v = 0 \\text{ where }1 \\leq j \\leq l\\}\\) where the active \\(g\\) constraints are indexed precisely by \\(1, \\dots, l'\\)</li> </ol> <p>proof. \\(x_0\\) is a local minimizer of \\(f\\) subject to the constraints, so it is also a local minimizer of \\(f\\) subject to only the active constraints. Since the Lagrange multiplies of the inactive constraints are zero, our theory of equality-constrained minimization finishes the problem.</p>"},{"location":"apm462/icfdo.html#example-1","title":"Example 1","text":"<p>Consider, for example, the problem</p> \\[\\begin{align*} \\text{minimize } &amp;f(x,y) := -x \\\\ \\text{subject to } &amp;g_1(x,y) := x^2+y^2 \\leq 1 \\\\ &amp;g_2(x,y) := y+x-1 \\leq 0 \\end{align*}\\] <p>The feasible set is the closed unit ball \\(\\overline{B_1(0)}\\) with an open semicircle removed from the top right. Geometrically, it is clear that the minimizer should be the point \\((1,0)\\). It is not hard to check that every feasible point is regular. Let's check that \\((x_0, y_0) = (1,0)\\) satisfies the first order conditions. We look at</p> \\[\\nabla f (x_0, y_0) + \\mu_1 \\nabla g_1 (x_0, y_0) + \\mu_2 \\nabla g_2 (x_0, y_0) = (0,0)\\] <p>This becomes</p> \\[\\begin{pmatrix} -1 \\\\ 0 \\end{pmatrix} + \\mu_1 \\begin{pmatrix} 2 \\\\ 0 \\end{pmatrix} + \\mu_2 \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}\\] <p>or</p> \\[\\begin{align*} 2\\mu_1 + \\mu_2 &amp;= 1 \\\\ \\mu_2 &amp;= 0. \\end{align*}\\] <p>So \\(\\mu_1 = 1/2\\). Also, \\(g_1(1,0) = 1^2 + 0^2 - 1 = 0\\) and \\(g_(1,0) = 0\\) as well, so the complementary slackness conditions are satisfied. Therefore \\((1,0)\\) satisfies the Kuhn-Tucker conditions, and so it is a candidate local minimizer. What about the second order conditions?</p> \\[L(1,0) = \\nabla^2 f (x_0, y_0) + \\mu_1 \\nabla^2 g_1 (x_0, y_0) + \\mu_2 \\nabla^2 g_2 (x_0, y_0) \\lor L(1,0) = I\\] <p>Clearly the second order necessary conditions are satisfied, but let's check the tangent space anyway. We have \\(\\nabla g_1(1,0) = (2, 0)\\) and \\(\\nabla g_2(1,0) = (1,1)\\); they are linearly independent, so the tangent space is a point. Therefore the second order necessary conditions are satisfied.</p>"},{"location":"apm462/icfdo.html#thrm-second-order-sufficient-conditions","title":"Thrm. Second Order Sufficient Conditions","text":"<p>Claim. Suppose \\(\\Omega \\subseteq \\mathbb R^n\\) is open and \\(f, h_1, \\dots, h_k, g_1, \\dots, g_l \\in C^2(\\Omega)\\). Consider the minimization problem</p> \\[\\begin{align*} \\text{minimize } &amp;f(x) \\\\ \\text{subject to } &amp;h_1(x) = \\cdots = h_k(x) = 0 \\\\ &amp;g_1(x) \\leq 0, \\dots, g_l(x) \\leq 0 \\end{align*}\\] <p>Suppose \\(x_0\\) is a feasible point of the constraints. If the following three conditions are satisfied: 1. There exist \\(\\lambda_1, \\dots, \\lambda_k \\in \\mathbb R\\) and \\(\\mu_1, \\dots, \\mu_l \\geq 0\\) such that \\(\\nabla f(x_0) + \\sum_i \\lambda_i \\nabla h_i(x_0) + \\sum_j \\mu_j \\nabla g_j(x_0) = 0\\)</p> <ol> <li> <p>\\(\\mu_j g_j(x_0) = 0\\) for each \\(j\\).</p> </li> <li> <p>The matrix \\(L(x_0) = \\nabla^2 f(x_0) + \\sum_i \\lambda_i \\nabla^2 h_i(x_0) + \\sum_j \\mu_j \\nabla^2 g_j(x_0)\\) is positive definite on the tangent space to the \"strongly active constraints\" at \\(x_0\\). That is, it is positive definite on the space \\(\\tilde{\\tilde{T_{x_0}}} = \\{ v \\in \\mathbb R^n : \\nabla h_i(x_0) = 0 \\forall i\\land \\forall k, 1 \\leq k \\leq l''\\Rightarrow \\nabla g_j(x_0) = 0\\}\\) where \\(\\{1, \\dots, l''\\}\\) is the set of all indices of active constraints whose Lagrange multipliers are positive. \\end{enumerate} then \\(x_0\\) is a strict local minimizer of \\(f\\).</p> </li> </ol> <p>proof.  Suppose \\(x_0\\) is not a strict local minimizer of \\(f\\). We claim that there then exists a unit vector \\(v \\in \\mathbb R^n\\) such that  1. \\(\\nabla f(x_0) \\cdot v \\leq 0\\).  2. \\(\\nabla h_i(x_0) \\cdot v = 0\\) for each \\(i = 1, \\dots, k\\).  3. \\(\\nabla g_j(x_0) \\cdot v \\leq 0\\) for all the active constraints (hereafter labelled by \\(j = 1,\\dots, l'\\))</p> <p>Since \\(x_0\\) is not a strict local minimizer, there exists a sequence \\(x_k\\) of feasible points unequal to \\(x_0\\) converging to \\(x_0\\) such that \\(f(x_k) \\leq f(x_0)\\). Then \\(f(x_k) - f(x_0) \\leq 0\\) for each \\(k\\). Let \\(v_k = \\frac{x_k-x_0}{\\|x_k-x_0\\|}\\), and let \\(s_k = \\|x_k - x_0\\|\\). Then \\(x_k = x_0 + s_kv_k\\), with which we may rewrite the inequality as \\(f(s_kv_k + x_0) - f(x_0) \\leq 0\\). Since each \\(v_k \\in S^1\\), we may assume that the sequence \\(v_k\\) is convergent and that it converges to some \\(v \\in S^1\\). We claim that this vector \\(v\\) has the three desired properties.</p> <p>By Taylor's theorem we have</p> \\[\\begin{align*} 0 \\geq f(s_kv_k + x_0) - f(x_0) &amp;= s_k \\nabla f(x_0) \\cdot v_k + o(s_k) \\tag{A}\\\\ 0 = h_i(s_kv_k + x_0) - h_i(x_0) &amp;= s_k \\nabla h_i(x_0) \\cdot v_k + o(s_k) \\tag{B} \\\\ 0 \\geq g_k(s_kv_k+x_0) - g_j(x_0) &amp;= s_k \\nabla g_j(x_0) \\cdot v_k + o(s_k) \\tag{C} \\end{align*}\\] <p>(The last equation is \\(\\leq 0\\) because \\(g_j(x_0) = 0\\).) Divide everything by \\(s_k\\) and take the limit as \\(k \\to \\infty\\). Then</p> \\[\\begin{align*} 0 &amp;\\geq \\nabla f(x_0) \\cdot v \\tag{a}\\\\ 0 &amp;= \\nabla h_i(x_0) \\cdot v \\tag{b}\\\\ 0 &amp;\\geq \\nabla g_j(x_0) \\cdot v \\tag{c}, \\end{align*}\\] <p>which proves the earlier claim.</p> <p>We now claim that equality actually holds in (c). Suppose for the sake of contradiction that there is some \\(1 \\leq k \\leq l'\\) such that \\(\\nabla g_j(x_0) \\cdot v &lt; 0\\) for some \\(j\\) for which \\(g_j\\) is strongly active at \\(x_0\\). By the first condition of the theorem,</p> \\[0 \\geq \\underbrace{\\nabla f(x_0) \\cdot v}_{\\text{$\\geq 0$ by (a)}} = -\\underbrace{\\sum \\lambda_i \\nabla h_i(x_0)\\cdot v}_{\\text{$=0$ by (b)}} \\underbrace{- \\sum \\mu_j \\nabla g_j(x_0)\\cdot v}_{\\text{$\\geq 0$ by (c)}}\\] <p>and so the right hand side is strictly greater than zero, because we only considered strongly active constraints. This is a contradiction, so we conclude that \\(\\nabla g_j(x_0) = 0\\) for all \\(j\\) such that \\(g_j\\) is strongly active at \\(x_0\\). Therefore \\(v \\in \\tilde{\\tilde{T_{x_0}}}\\).</p> <p>Again, by Taylor's theorem</p> \\[\\begin{align*} 0 \\geq f(s_kv_k + x_0) - f(x_0) &amp;= s_k \\nabla f(x_0) \\cdot v_k + \\frac{1}{2}s_k^2 v_k^T \\nabla^2 f(x_k) \\cdot v_k + o(s_k^2) \\\\ 0 = h_i(s_kv_k + x_0) - h_i(x_0) &amp;= s_k \\nabla h_i(x_0) \\cdot v_k + \\frac{1}{2}s_k^2 v_k^T \\nabla^2 h_i(x_k) \\cdot v_k + o(s_k^2) \\\\ 0 \\geq g_k(s_kv_k+x_0) - g_j(x_0) &amp;= s_k \\nabla g_j(x_0) \\cdot v_k + \\frac{1}{2}s_k^2 v_k^T \\nabla^2 g_j(x_k) \\cdot v_k + o(s_k^2)  \\end{align*}\\] <p>Multiply the second line by \\(\\lambda_i\\) and the third by \\(\\mu_j\\) and add everything up to get</p> \\[0 \\geq s_k \\underbrace{\\left[ \\nabla f(x_0) + \\sum \\lambda_i \\nabla h_i(x_0) + \\sum \\mu_j \\nabla g_j(x_0) \\right]}_{\\text{$=0$ by condition 1}} v_k + \\frac{s_k^2}{2} v_k^T \\underbrace{\\left[ \\nabla^2 f(x_0) + \\sum \\lambda_i \\nabla^2 h_i(x_0) + \\sum \\mu_j \\nabla^2 g_j(x_0) \\right]}_{= L(x_0)}v_k + o(s_k^2)\\] <p>Divide everything by \\(s_k^2\\) to get</p> \\[0 \\geq \\frac{1}{2} v_k^T \\left[ \\nabla^2 f(x_0) + \\sum \\lambda_i \\nabla^2 h_i(x_0) + \\sum \\mu_j \\nabla^2 g_j(x_0) \\right] \\cdot v_k + \\frac{o(s_k^2)}{s_k^2}\\] <p>Taking the limit \\(k \\to \\infty\\) gives</p> \\[0 \\leq v^T L(x_0) \\cdot v\\] <p>which violates condition 3 of the theorem. We have a contradiction, so we conclude that \\(x_0\\) must be a strict local minimizer.</p>"},{"location":"apm462/icfdo.html#example-1_1","title":"Example 1","text":"<p>Given \\((a,b)\\) with \\(a,b &gt; 0\\) and \\(a^2+b^2 &gt; 1\\). Consider the minimization problem:</p> \\[\\begin{align*} \\text{minimize } &amp;f(x,y) := (x-a)^2 + (y-b)^2 \\\\ \\text{subject to } &amp;g_1(x,y) := x^2+y^2-1 \\leq 0 \\end{align*}\\] <p>Our intuition says that the minimizer should be \\(\\left( \\frac{a}{\\sqrt{a^2+b^2}}, \\frac{b}{\\sqrt{a^2+b^2}} \\right)\\). We have \\(\\nabla g(x,y) = (2x, 2y)\\), so clearly all feasible points are regular. The Kuhn-Tucker conditions are</p> \\[\\begin{pmatrix} 2(x-a) \\\\ 2(y-a) \\end{pmatrix} + \\mu \\begin{pmatrix} 2x \\\\ 2y \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}\\] <p>and \\(\\mu g(x,y) = 0\\). That is,</p> \\[\\begin{align*} (1+\\mu)x &amp;= a \\\\ (1+\\mu)y &amp;= b \\\\ \\mu (x^2+y^2 - 1) &amp;= 0, \\mu \\geq 0 \\end{align*}\\] <p>Suppose \\(\\mu = 0\\). Then \\(x = a\\) and \\(y = b\\); since we assumed \\(a^2+b^2 &gt; 1\\), we would have that \\((x,y)\\) is not feasible. Therefore \\(\\mu \\neq 0\\), and so \\(x^2+y^2 = 1\\) by the third equation. Squaring the first two equations and adding them gives</p> \\[(1+\\mu)^2 (x^2+y^2) = a^2+b^2\\] <p>implying that \\(\\mu = -1 + \\sqrt{a^2+b^2}\\) - we took the positive root because \\(\\mu &gt; 0\\). This is actually positive, since \\(a^2+b^2 &gt; 1\\). Those first equations again give us</p> \\[\\begin{pmatrix} x_0 \\\\ y_0 \\end{pmatrix} = \\frac{1}{1+\\mu} \\begin{pmatrix} a \\\\ b \\end{pmatrix} = \\frac{1}{\\sqrt{a^2+b^2}} \\begin{pmatrix} a \\\\ b \\end{pmatrix}\\] <p>as expected. The Lagrangian is</p> \\[L(x_0, y_0) = 2I + 2\\mu I = 2(1+\\mu)I = 2\\sqrt{a^2+b^2} I\\] <p>which is everywhere positive definite. Therefore the second-order sufficient conditions are satisfied. For practice, however, let's compute the tangent space to the \"strongly active constraints\". The only constraint is \\(g\\); since \\(g\\) is active and its Lagrange multiplier \\(\\mu\\) is positive, the constraint \\(g\\) is strongly active at \\((x_0, y_0)\\). Therefore the tangent space we are interested in is the tangent space to \\(S^1\\) at \\((x_0, y_0)\\): that space is \\(\\{v \\in \\mathbb R^2 : av_1 + bv_2 = 0\\}\\).</p>"},{"location":"apm462/icfdo.html#example-2","title":"Example 2","text":"<p>Consider the problem</p> \\[\\begin{align*} \\text{minimize } &amp;f(x,y) := x^3 + y^2 \\\\ \\text{subject to } &amp;g(x,y) := (x+1)^2 + y^2 - 1 \\leq 0. \\end{align*}\\] <p>We have \\(\\nabla g(x,y) = (2(x+1), 2y)\\), which makes it clear that every feasible point is regular. The Kuhn-Tucker conditions are</p> \\[\\begin{pmatrix} 3x^2 \\\\ 2y \\end{pmatrix} + \\mu \\begin{pmatrix} 2(x+1) \\\\ 2y \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}\\] <p>with \\(\\mu((x+1)^2 + y^2 - 1) = 0\\) and \\(\\mu \\geq 0\\).</p> <p>Consider \\((x_0, y_0) = (0,0)\\). The Kuhn-Tucker conditions imply \\(\\mu = 0\\). In particular, \\(g\\) is active at \\((0,0)\\), but not strongly active there. The tangent space to the active constraint at \\((0,0)\\) is the y-axis. The Lagrangian at \\((0,0)\\) is</p> \\[L(0,0) = \\begin{pmatrix} 0 &amp; 0 \\\\ 0 &amp; 2 \\end{pmatrix}\\] <p>which is clearly positive definite on this tangent space. However, we cannot conclude anything, since the constraint \\(g\\) is not strongly active. In fact, it is clear that \\((0,0)\\) is not a local minimizer: for \\(x&lt;0\\) sufficiently close to \\(0\\), \\(f(x,0)\\) is negative, yet it is \\(0\\) at \\((0,0)\\).</p>"},{"location":"apm462/icfdoq.html","title":"Examples: Inequality Constrained Finite Dimension Optimization","text":""},{"location":"apm462/icfdoq.html#example-1","title":"Example 1","text":"<p>Question</p> <p>Claim Let \\(\\lambda_i:\\mathbb R^{n(n+1)/2}\\rightarrow\\mathbb R\\) be the function that assigns to a symmetric, \\(n\\times n\\) matrix A its ith smallest eigenvalue for \\(1\\leq i\\leq n\\). Prove that \\(\\lambda_1\\) is a concave function and \\(\\lambda_n\\) is a convex function over the space of symmetric matrices.</p> <p>lemma For all symmetric matrix \\(A, \\forall x\\in\\mathbb R^n. \\|x\\| = 1, \\lambda_n(A) \\geq x^TAx, \\lambda_1(A) \\leq x^TAx\\). </p> <p>proof. Since \\(A\\) is symmetric, by spectral theorem we decompose \\(A = Q\\Lambda Q^T\\) where \\(\\Lambda = diag(\\lambda_n,...,\\lambda_1)\\) and \\(Q\\) is orthogonal.  Then, for any \\(x\\in\\mathbb R\\), </p> \\[\\begin{align*} x^TAx &amp;= x^TQ\\Lambda Q^Tx\\\\ &amp;= (Q^Tx)^T\\Lambda (Q^Tx)\\\\ &amp;= y^T\\Lambda y &amp;\\text{let } y = Q^Tx\\\\ &amp;= \\sum_{i=1}^n\\lambda_i y_i^2 \\end{align*}\\] \\[\\begin{align*} \\lambda_1\\sum_{i=1}^n y_i^2 \\leq \\sum_{i=1}^n\\lambda_i y_i^2\\leq \\lambda_n \\sum_{i=1}^n y_i^2\\\\ \\lambda_1 y^Ty \\leq x^TAx \\leq \\lambda_n y^Ty\\\\ \\end{align*}\\] <p>Note that \\(y^T y = x^TQQ^Tx = x^Tx = \\|x\\| = 1\\) so that \\(\\lambda_1 \\leq x^TAx \\leq \\lambda_n\\)</p> <p>proof. For two arbitrary \\(n\\times n\\) symmetric matrices \\(A, B\\), for some \\(c\\in [0, 1]\\)</p> \\[\\begin{align*} \\lambda_n(cA + (1-c)B) &amp;= \\sup_{x\\in\\mathbb R^n.\\|x\\|=1}\\{x^T(cA+(1-c)B)x\\} &amp;\\text{by lemma}\\\\ &amp;= \\sup_{x\\in\\mathbb R^n.\\|x\\|=1}\\{cx^TAx+(1-c)x^TBx\\} &amp;\\text{linearity}\\\\ &amp;\\leq \\sup_{x\\in\\mathbb R^n.\\|x\\|=1}\\{cx^TAx\\} + \\sup_{x\\in\\mathbb R^n.\\|x\\|=1}\\{(1-c)x^TBx\\}\\\\ &amp;= c\\sup_{x\\in\\mathbb R^n.\\|x\\|=1}\\{x^TAx\\} + (1-c)\\sup_{x\\in\\mathbb R^n.\\|x\\|=1}\\{x^TBx\\}\\\\ &amp;= c\\lambda_n(A) + (1-c)\\lambda_n(B) \\end{align*}\\] <p>similarly, </p> \\[\\begin{align*} \\lambda_1(cA + (1-c)B) &amp;= \\inf_{x\\in\\mathbb R^n.\\|x\\|=1}\\{x^T(cA+(1-c)B)x\\} &amp;\\text{by lemma}\\\\ &amp;\\geq \\inf_{x\\in\\mathbb R^n.\\|x\\|=1}\\{cx^TAx\\} + \\inf_{x\\in\\mathbb R^n.\\|x\\|=1}\\{(1-c)x^TBx\\}\\\\ &amp;= c\\lambda_1(A) + (1-c)\\lambda_1(B) \\end{align*}\\] <p>By definition of concave and convex functions, \\(\\lambda_1\\) is concave, \\(\\lambda_n\\) is convex.</p>"},{"location":"apm462/icfdoq.html#example-2","title":"Example 2","text":"<p>Question</p> <p>Find the regular points for</p> \\[\\begin{align*} \\text{minimize } &amp;f(x, y) = x+2y\\\\ \\text{subject to } &amp;y^2 - x \\leq 0 \\\\ &amp;(x-1)^2 + y^2 = 1\\\\ &amp;x \\geq 0 \\end{align*}\\] <p>First, consider the feasible points, Since \\(y^2 = 1 - (x - 1)^2\\), </p> \\[\\begin{align*} 1 - (x-1)^2 - x &amp;\\leq 0\\\\  -x^2 + x &amp;\\leq 0\\\\ x &amp;\\leq x^2 \\end{align*}\\] <p>Also, \\(x\\geq 0\\), implies that \\(x = 0\\) or \\(x \\geq1\\). So that the set of the feasible points is </p> \\[S = \\{(0, 0)\\}\\cup \\{(x ,y) \\mid (x-1)^2 + y^2 = 1, x\\geq 1\\}\\] <p>Then, we can simply find minimizer on \\(\\{(x ,y) \\mid (x-1)^2 + y^2 = 1, x\\geq 1\\}\\). Let \\(h(x, y) = (x-1)^2 + y^2 - 1, g(x ,y) = x\\) so that \\(\\nabla h = 2((x-1), y), \\nabla g = (1, 0)\\). All the points on \\(S\\) are regular.</p> <p>By Kuhn-Tucker Conditions, take \\(\\lambda \\in\\mathbb R, \\mu \\geq 0\\) </p> \\[\\begin{align*} \\big(\\begin{matrix}1\\\\2\\end{matrix}\\big) + \\lambda \\big(\\begin{matrix}2(x-1)\\\\2y\\end{matrix}\\big) + \\mu\\big(\\begin{matrix}-1\\\\2y\\end{matrix}\\big) &amp;= 0\\\\ \\mu (y^2 - x)&amp;= 0\\\\ (x-1)^2 + y^2 &amp;= 1 \\end{align*}\\] <p>If \\(g\\) is inactive, then solving </p> \\[\\big(\\begin{matrix}1\\\\2\\end{matrix}\\big) + \\lambda \\big(\\begin{matrix}2(x-1)\\\\2y\\end{matrix}\\big) = 0, (x-1)^2 + y^2= 1\\] <p>gives solution \\((1 + \\sqrt{1/5}, \\sqrt{4/5}), \\lambda = -\\frac{1}{2\\sqrt{1/5}}\\) and \\((1 - \\sqrt{1/5}, -\\sqrt{4/5}), \\lambda = \\frac{1}{2\\sqrt{1/5}}\\) while \\((1 - \\sqrt{1/5}, -\\sqrt{4/5})\\) is not feasible. </p> <p>If \\(g\\) is active, from graphically observations, we consider the point \\((1, -1)\\), which leads to the solution \\(\\lambda = 0, \\mu = 1\\).   </p> <p>The candidates are \\((1, -1), (1 + \\sqrt{1/5}, \\sqrt{4/5})\\)</p> \\[\\begin{align*} L &amp;= \\nabla^2 f + \\lambda \\nabla^2 h + \\mu\\nabla^2 g \\\\ &amp;= 0 + 2\\lambda I + \\mu\\begin{bmatrix}0&amp;0\\\\0&amp;1\\end{bmatrix} \\end{align*}\\] <p>If \\(g\\) is inactive, then </p> \\[L = 2\\lambda I = -\\frac{1}{\\sqrt{1/5}}I\\] <p>is not positive semidefinite on \\(S\\).</p> <p>If \\(g\\) is active, then </p> \\[L = \\begin{bmatrix}0&amp;0\\\\0&amp;1\\end{bmatrix}\\] <p>is positive semidefinite on \\(S\\). </p> <p>The global minimizer is \\((1, -1)\\) and the minimum is \\(1 - 2 = -1\\) </p>"},{"location":"apm462/icfdoq.html#example-3","title":"Example 3","text":"<p>Question</p> <p>maximize \\(f(x, y) = xy\\) on \\(x^2 + y^2 \\leq 1, x, y &gt; 0\\). FONC to find candidates.</p> <p>Note that maximize \\(xy\\) is equivalent to minimize \\(-xy\\),  Since \\(x, y &gt; 0\\) are inactive, let \\(h(x, y) = x^2 + y^2\\), note that \\(h\\) will be active, otherwise \\(f(x, y) =-x y\\) have no global minimum. Therefore, take \\(\\lambda\\in\\mathbb R\\), solves</p> \\[\\begin{align*} \\nabla f + \\lambda \\nabla h = \\begin{pmatrix}-y\\\\-x\\end{pmatrix} + \\lambda \\begin{pmatrix}2x\\\\2y\\end{pmatrix} &amp;= 0\\\\ x^2 + y^2 &amp;= 1 \\end{align*}\\] <p>substitute \\(x = \\pm \\sqrt{1-y^2}, \\lambda = \\frac{y}{2\\sqrt{1-y^2}}\\), then \\(\\sqrt{1-y^2} \\pm \\frac{y^2}{\\sqrt{1-y^2}} = 0\\), which gives four solutions \\((\\pm \\sqrt{1/2}, \\pm\\sqrt{1/2})\\), and the only feasible solution is \\((\\sqrt{1/2}, \\sqrt{1/2}), \\lambda = 1/2\\)</p> <p>SOC to check</p> <p>By SOC, </p> \\[L = \\nabla^2 [-xy] + \\frac{-1}{2}\\nabla^2 [x^2 + y^2] = \\begin{bmatrix}0&amp;-1\\\\-1&amp;0\\end{bmatrix} + I= \\begin{bmatrix}1&amp;-1\\\\-1&amp;1\\end{bmatrix}\\] <p>Note that \\(L\\) is positive semidefinite everywhere as </p> \\[\\begin{bmatrix}x&amp;y\\end{bmatrix} \\begin{bmatrix}1&amp;-1\\\\-1&amp;1\\end{bmatrix}\\begin{bmatrix}x\\\\y\\end{bmatrix} = (x- y)^2 \\geq 0\\] <p>Therefore, \\((\\sqrt{1/2}, \\sqrt{1/2})\\) is the local minimizer of \\(-xy\\), hence maximizer of \\(xy\\)</p> <p>Let \\(x = r\\sin\\theta, y = r\\cos\\theta\\) so that the problem becomes  maximize \\(r^2\\sin\\theta\\cos\\theta\\) on \\(|r| \\leq 1, \\theta\\in (0, \\pi/2)\\), which we can farther assume \\(r = 1\\) since \\(\\sin\\theta\\cos\\theta &gt; 0\\) for \\(0 &lt; \\theta&lt;\\pi/2\\). Therefore, the problem becomes maximizing \\(\\sin\\theta\\cos\\theta\\) on \\(0&lt;\\theta &lt;\\pi/2\\). Note that </p> \\[\\begin{align*} \\frac{d^2}{d^2\\theta}\\sin\\theta\\cos\\theta &amp;= \\frac12\\frac{d}{d\\theta}\\frac{d}{d\\theta}\\sin(2\\theta) &amp;\\text{double angle formula}\\\\ &amp;= -2\\sin(2\\theta) \\end{align*}\\] <p>\\(-2\\sin(2\\theta) &lt; 0\\) for \\(0 &lt; \\theta &lt; \\pi/2\\) so that the function is concave. Therefore, set </p> \\[\\frac{d}{d\\theta}\\sin\\theta\\cos\\theta = \\cos(2\\theta) = 0\\] <p>solve to be \\(\\theta = \\frac{\\pi}{4}\\) on \\(0 &lt; \\theta &lt; \\pi/2\\) and is a maximizer. Therefore, \\(x = \\sin\\frac{\\pi}{4} = \\sqrt{\\frac12}, y = \\cos\\frac{\\pi}4 = \\sqrt{\\frac12}\\) is the maximizer for \\(xy\\)</p>"},{"location":"apm462/icfdoq.html#example-4","title":"Example 4","text":"<p>Question</p> \\[\\begin{align*} \\text{minimize} f(x, y) = (x-a)^2 + (y-b)^2\\\\ \\text{on } g_1(x, y) = x^2 + y^2 \\geq 9, g_2(x, y) = x^2 + y^2 \\leq 25 \\end{align*}\\] Source code <pre><code>import matplotlib.pyplot as plt\nimport numpy as np\n\nx = np.arange(-6, 10, 0.01)\ny = np.arange(-6, 12, 0.01)\nxx, yy = np.meshgrid(x, y)\nc1 = plt.Circle((0, 0), 5, color=\"red\", alpha=.5)\nc2 = plt.Circle((0, 0), 3, color=\"white\")\nfig, axs = plt.subplots(1, 3, figsize=(16, 5))\naxs[0].add_artist(c1);axs[0].add_artist(c2)\naxs[0].contour(xx, yy, (xx) ** 2 + (yy - 6)**2, \n               levels=[0.01, 3, 10, 20, 40, 80, 120], cmap=\"rainbow\")\naxs[0].axis(\"equal\");axs[0].set_xlim(-5, 7);axs[0].set_ylim(-5, 7);\n\nc1 = plt.Circle((0, 0), 5, color=\"red\", alpha=.5)\nc2 = plt.Circle((0, 0), 3, color=\"white\")\naxs[1].add_artist(c1);axs[1].add_artist(c2)\naxs[1].contour(xx, yy, (xx) ** 2 + (yy - 2)**2, \n               levels=[0.01, 3, 10, 20, 40, 80, 120], cmap=\"rainbow\")\naxs[1].axis(\"equal\");axs[1].set_xlim(-5, 7);axs[1].set_ylim(-5, 7);\n\nc1 = plt.Circle((0, 0), 5, color=\"red\", alpha=.5)\nc2 = plt.Circle((0, 0), 3, color=\"white\")\naxs[2].add_artist(c1);axs[2].add_artist(c2)\naxs[2].contour(xx, yy, (xx-2) ** 2 + (yy - 3)**2, \n               levels=[0.01, 3, 10, 20, 40, 80, 120], cmap=\"rainbow\")\naxs[2].axis(\"equal\");axs[2].set_xlim(-5, 7);axs[2].set_ylim(-5, 7)\n\nfig.savefig(\"../assets/icfdoq.jpg\")\n</code></pre> <p></p> <p>Question</p> <p>Show that every feasible point is regular</p> <p>The set of feasible points is \\(S = \\{(x, y)\\mid 9 \\leq x^2 + y^2 \\leq 25\\}\\), a.k.a. a donut. Note that \\(g_1 = g_2\\) so that to make sure their derivative are not linearly independent, only one of them can be active for some \\(a, b\\). Then, note that \\(\\nabla g = (2x, 2y)\\neq 0\\) for \\((x,y)\\neq 0\\), therefore, on our feasible set, all points are regular. </p> <p>Question</p> <p>Find candidates for local minimum points using FONC.</p> <p>The Kuhn-Tucker conditions gives </p> \\[\\begin{align*}\\begin{pmatrix}2(x-a)\\\\2(y-b)\\end{pmatrix} + \\mu_1\\begin{pmatrix}2x\\\\2y\\end{pmatrix}+\\mu_2\\begin{pmatrix}-2x\\\\-2y\\end{pmatrix} &amp;= 0\\\\ \\mu_1(x^2 + y^2 - 25)&amp;= 0\\\\ \\mu_2(-x^2 - y^2 + 9)&amp;= 0 \\end{align*}\\]"},{"location":"apm462/icfdoq.html#case-1","title":"Case 1","text":"<p>Suppose both of them are inactive, then the minimizer is \\((a, b), f(a, b) = 0\\), and for this solution to be feasible, \\((a, b)\\) satisfies that \\(9 \\leq a^2 + b^2 \\leq 25\\) </p>"},{"location":"apm462/icfdoq.html#case-2","title":"Case 2","text":"<p>Suppose \\(g_1\\) is active, solves </p> \\[\\begin{align*} 2(x-a) + 2\\mu_1x &amp;= 0\\\\ 2(y-b) + 2\\mu_1y &amp;= 0\\\\ x^2 + y^2 - 25 &amp;= 0 \\end{align*}\\] <p>solves to be one of </p> \\[(5\\frac{a}{\\sqrt{a^2+b^2}}, 5\\frac{b}{\\sqrt{a^2+b^2}}), \\mu_1 = \\frac{\\sqrt{a^2 +b^2}}{5}-1\\] \\[(-5\\frac{a}{\\sqrt{a^2+b^2}}, -5\\frac{b}{\\sqrt{a^2+b^2}}), \\mu_1 = \\frac{-\\sqrt{a^2 +b^2}}{5}-1\\] <p>Note that \\(\\mu_1 \\geq 0\\), so the only solution is \\((5\\frac{a}{\\sqrt{a^2+b^2}}, 5\\frac{b}{\\sqrt{a^2+b^2}}), \\mu_1 = \\frac{\\sqrt{a^2 +b^2}}{5}-1\\) and this only holds when \\(a^2+b^2 \\geq 25\\)</p>"},{"location":"apm462/icfdoq.html#case-3","title":"Case 3","text":"<p>Suppose \\(g_2\\) is active, solves </p> \\[\\begin{align*} 2(x-a) - 2\\mu_2x &amp;= 0\\\\ 2(y-b) - 2\\mu_2y &amp;= 0\\\\ x^2 + y^2 - 9 &amp;= 0 \\end{align*}\\] <p>solves to be </p> \\[(3\\frac{a}{\\sqrt{a^2+b^2}}, 3\\frac{b}{\\sqrt{a^2+b^2}}), \\mu_2 = 1- \\frac{\\sqrt{a^2 +b^2}}{3}\\] \\[(-3\\frac{a}{\\sqrt{a^2+b^2}}, -3\\frac{b}{\\sqrt{a^2+b^2}}), \\mu_2 = 1 + \\frac{\\sqrt{a^2 +b^2}}{3}\\] <p>Note that \\(\\mu_2\\geq 0\\), so that \\((3\\frac{a}{\\sqrt{a^2+b^2}}, 3\\frac{b}{\\sqrt{a^2+b^2}}), \\mu_2 = 1- \\frac{\\sqrt{a^2 +b^2}}{3}\\) only holds when \\(a^2 + b^2 \\leq 9\\)</p>"},{"location":"apm462/icfdoq.html#case-4","title":"Case 4","text":"<p>\\(g_1, g_2\\) cannot be active at the same time, as discussed previously</p> <p>Question</p> <p>find the tangent space.</p> <p>If \\(g_1\\) is active, for \\(x_0 = \\frac{5}{\\sqrt{a^2+b^2}}(a, b)\\), \\(T_{x_0}M = \\{v\\in\\mathbb R^2 \\mid 2\\times\\frac{5}{\\sqrt{a^2+b^2}}\\begin{pmatrix}a\\\\b\\end{pmatrix}\\cdot v = 0\\} = \\{v\\in\\mathbb R^2 \\mid av_1+bv_2 = 0\\}\\).  </p> <p>If \\(g_2\\) is active, for \\(x_0 = \\frac{3}{\\sqrt{a^2+b^2}}(a, b)\\) or \\(x_0 = -\\frac{3}{\\sqrt{a^2+b^2}}(a, b)\\), it is still a scaled vector of \\((a, b)\\), hence \\(T_{x_0}M = \\{v\\in\\mathbb R^2 \\mid av_1 + bv_2 = 0\\}\\).  </p> <p>If \\(g_1, g_2\\) are both inactive, then is unconstrained and the tangent space is \\(\\mathbb R^2\\). </p> <p>Question</p> <p>Check with SOC.</p> \\[L = \\nabla^2 f + (\\mu_1 + \\mu_2)\\nabla^2 g = 2I + 2\\mu_1I +(-2\\mu_2)I = 2(1+\\mu_1 - \\mu_2)I\\] <p>When \\(9 &lt; a^2 + b^2 &lt; 25, \\mu_1=\\mu_2 =0\\), </p> \\[L = 2I\\] <p>is positive semidefinite everywhere, hence \\((a,b)\\) is a local minimizer. </p> <p>When \\(\\mu_1 = \\frac{\\sqrt{a^2+b^2}}{5} - 1, \\mu_2 = 0, a^2+b^2 \\geq 25\\), </p> \\[L = 2(1+ \\frac{\\sqrt{a^2+b^2}}{5}-1) I = 2(\\frac{\\sqrt{a^2+b^2}}{5}) I\\] <p>is positive semidefinite everywhere, hence \\(\\frac{5}{\\sqrt{a^2+b^2}}(a, b)\\) is a local minimizer</p> <p>When \\(\\mu_2 = 1 - \\frac{\\sqrt{a^2+b^2}}{3}, \\mu_1= 0, a^2 + b^2 \\leq 9\\), </p> \\[L = 2(1 - 1+\\frac{\\sqrt{a^2+b^2}}{3}) = 2(\\frac{\\sqrt{a^2+b^2}}{3}) I\\] <p>is positive semidefinite everywhere, hence \\(\\frac{3}{\\sqrt{a^2+b^2}}(a, b)\\) is a local minimizer</p> <p>When \\(\\mu_2 = 1 + \\frac{\\sqrt{a^2+b^2}}{3}, \\mu_1= 0, a^2 + b^2 \\leq 9\\), </p> \\[L = 2(1 - 1 - \\frac{\\sqrt{a^2+b^2}}{3}) = -2(\\frac{\\sqrt{a^2+b^2}}{3}) I\\] <p>is negative semidefinite everywhere, hence is not a local minimum. </p> <p>In summary the local minimizer is </p> \\[\\begin{cases} (a, b) &amp;9 &lt; a^2 + b^2 &lt; 25\\\\ \\frac{3}{\\sqrt{a^2+b^2}}(a, b) &amp;a^2 + b^2 \\leq 9\\\\ \\frac{5}{\\sqrt{a^2+b^2}}(a, b) &amp;a^2+b^2 \\geq 25 \\end{cases}\\]"},{"location":"apm462/icfdoq.html#example-5","title":"Example 5","text":"<p>Question</p> <p>For \\(Q\\) be an \\(n\\times n\\) positive symmetric definite matrix, \\(a, b\\in\\mathbb R^{n&gt;0}, c \\in\\mathbb R^{&gt;0}\\). minimize \\(\\frac{1}{2}x^TQx - b^Tx\\) subject to \\(a^Tx \\leq c, x &gt; 0\\).</p> <p>Take \\(\\mu_0\\) and Kuhn-Tucker conditions gives equations</p> \\[\\begin{align*} Qx - b + \\mu_0 a  &amp;= 0\\\\ \\mu_0(a^Tx - c) &amp;= 0\\\\ \\mu^T x &amp;= 0 \\end{align*}\\] <p>Because all the constraints on \\(x &gt; 0\\) are inactive, they will be excluded from considerations. </p> <p>Suppose unconstrained, then the minimizer for the quadratic form will be \\(x^* = Q^{-1}b\\), since \\(Q\\) is symmetric positive definite, \\(Q^{-1}\\) is also symmetric positive definite, also \\(b &gt; 0\\) so that \\(x^* &gt; 0\\), hence such minimizer can exist. </p> <p>Suppose \\(\\mu_0 \\neq 0, \\mu_1,...,\\mu_n = 0\\), then we have </p> \\[Qx - b + \\mu_0 a = 0, a^Tx = c\\] <p>it can be solved as </p> \\[\\begin{align*} x &amp;= Q^{-1}(b-\\mu_0 a)\\\\ a^TQ^{-1}(b-\\mu_0 a) &amp;= c\\\\ a^TQ^{-1}b - c &amp;= a^TQ^{-1}a \\mu_0\\\\ \\mu_0 &amp;= \\frac{a^TQ^{-1}b - c}{a^TQ^{-1}a}\\\\ x&amp;= Q^{-1}(b - \\frac{a^TQ^{-1}b - c}{a^TQ^{-1}a} \\times a) \\end{align*}\\] <p>Note that since \\(\\mu_0 &gt; 0\\), we must have \\(a^TQ^{-1}b &gt; c\\). </p> <p>Also, note that for any \\(x\\),</p> \\[L(x) = Q\\] <p>Since \\(Q\\) is positive definite everywhere, any candidate will be a minimizer. </p> <p>In summary The minimizer is </p> \\[\\begin{cases} Q^{-1}b &amp;a^TQ^{-1}b &lt; c\\\\ Q^{-1}(b - \\frac{a^TQ^{-1}b - c}{a^TQ^{-1}a}) &amp;a^TQ^{-1}b \\geq c \\end{cases}\\]"},{"location":"apm462/icfdoq.html#example-6","title":"Example 6","text":"<p>Question</p> <p>minimize \\(f(x) = -\\sum_{i=1}^n \\log(a_i+x_i)\\) subject to \\(x_1,...,x_n\\geq 0, \\sum x_i = 1\\).</p> <p>Part (a)</p> <p>Show \\(x_i = \\max\\{0, \\frac{1}{\\lambda}-a_i\\}\\) for some \\(\\lambda \\in\\mathbb R\\).</p> <p>Take \\(\\lambda\\in\\mathbb R, \\mu = (\\mu_1,...,\\mu_n)\\in\\mathbb R^{n\\geq 0}\\), by  Kuhn-Tucker conditions </p> \\[\\begin{align*}  -\\frac1{a_i+x_i} + \\lambda - \\mu_i &amp;= 0\\\\  \\mu_ix_i &amp;= 0\\\\ \\sum_{i=1}^n x_i  &amp;= 1 \\end{align*}\\] <p>for \\(i = 1,...,n\\) </p> <p>for some \\(i \\in \\{1,..,n\\}\\), if \\(\\mu_i\\) is active, then \\(x_i = 0\\), \\(\\lambda = \\frac{1}{a_i} + \\mu_i\\). if \\(\\mu_i\\) is inactive, then \\(\\lambda - \\frac{1}{a_i + x_i} = 0\\Rightarrow x_i = \\frac{1}{\\lambda} - a_i\\), by our constraint, \\(x_i \\geq 0\\) so that \\(x_i = \\max\\{0, \\frac1{\\lambda} - a_i\\}\\)</p> <p>Part (b)</p> <p>Show that \\(\\lambda\\) is unique for each \\(a\\).</p> <p>Take some \\(\\lambda_1\\) s.t. \\(\\sum_{i=1}^n \\max\\{0, \\frac{1}{\\lambda_1} - a_i\\} = 1\\), For any \\(\\lambda_2 \\neq \\lambda_1\\),</p> <p>Suppose \\(\\frac{1}{\\lambda_2} &lt; \\frac1{\\lambda_1}\\), then \\(\\max\\{0, \\frac{1}{\\lambda_2} - a_i\\}  \\leq \\max\\{0, \\frac{1}{\\lambda_1} - a_i\\}\\). Also, to make \\(\\sum_{i=1}^n \\max\\{0, \\frac{1}{\\lambda_1} - a_i\\} = 1\\), there exists some \\(j \\in \\{1,...,n\\}, \\frac{1}{\\lambda_1} - a_j &gt; 0\\),  so that \\(\\max\\{0, \\frac{1}{\\lambda_2} - a_j\\} &lt; \\frac{1}{\\lambda_1} - a_j\\), therefore </p> \\[\\sum_{i=1}^n \\max\\{0, \\frac{1}{\\lambda_2} - a_i\\} &lt; \\sum_{i=1}^n \\max\\{0, \\frac{1}{\\lambda_1} - a_i\\} = 1\\] <p>Suppose \\(\\frac{1}{\\lambda_2} &gt; \\frac1{\\lambda_1}\\), then \\(\\max\\{0, \\frac{1}{\\lambda_2} - a_i\\}  \\geq \\max\\{0, \\frac{1}{\\lambda_1} - a_i\\}\\), similarly pick \\(j\\) s.t. \\(\\max\\{0, \\frac{1}{\\lambda_2} - a_i\\}  &gt; \\frac1{\\lambda_1} - a_i\\) so that </p> \\[\\sum_{i=1}^n \\max\\{0, \\frac{1}{\\lambda_2} - a_i\\} &gt;\\sum_{i=1}^n \\max\\{0, \\frac{1}{\\lambda_1} - a_i\\} = 1\\] <p>Therefore, I have shown that any other \\(\\lambda_2\\) will not satisfy the constraint, hence such \\(\\lambda\\) is unique.</p>"},{"location":"apm462/icfdoq.html#example-7","title":"Example 7","text":"<p>Question</p> <p>Consider \\(f(a, b) = \\min_{(x,x^2)\\in\\mathbb R^2}|(x-a, y-b)|^2\\). Write the FOC for a minimizer \\((x_0, y_0)\\).</p> <p>For each given \\((a, b)\\), define </p> \\[F(x) = |(x-a, y-b)|^2 = (x-a)^2 + (x^2-b)^2\\] <p>so that \\(\\min_{x\\in\\mathbb R}F(x) = \\min_{(x,x^2)\\in\\mathbb R^2}|(x-a, y-b)|^2\\) and the derivative of \\(F\\) is given as</p> \\[\\frac{dF}{dx} = 2(x-a) + 4x^3 - 4bx\\] <p>since \\(F\\) is defined on \\(\\mathbb R\\), the FOC is that </p> \\[2(x_0-a) + 4x_0^3 - 4bx_0 = 0\\] <p>Question</p> <p>Find the cubic equation \\(x_0\\) must satisfy.</p> <p>\\(x_0\\) must satisfy the FOC, a.k.a. </p> \\[4x_0^3 + (2-4b)x_0 - 2a = 0\\] <p>Question</p> <p>Find conditions on \\(x_0\\) that guarantee that \\((x_0, x^2_0)\\) is a local minimizer.</p> <p>Using SOC, note that </p> \\[\\frac{d^2F}{dx^2} = 12x^2 + 2-4b\\] <p>is postive semidefinite iff \\(12x^2 + 2 - 4b &gt; 0\\Rightarrow |x| &gt; \\sqrt{\\frac{2b-1}{6}}\\)</p>"},{"location":"apm462/icfdoq.html#example-8","title":"Example 8","text":"<p>Question</p> <p>Let \\(A\\) be \\(m\\times n\\) matrix and \\(b \\in\\mathbb R^m, c\\in\\mathbb R^n\\), consider the \"primal problem\".</p> \\[\\max_{Ax \\leq b, x\\geq 0} c^Tx\\] <p>and the \"dual problem</p> \\[\\min_{A^Tp \\geq c, p\\geq 0}b^Tp\\] <p>Part (a)</p> <p>Write the FONC for the primal optimal solution \\(x^*\\).</p> <p>First, maximizing \\(c^Tx\\) is equivalent of minimizing \\(-c^Tx\\). Let \\(f(x) = -c^Tx, \\nabla f(x) = -c\\). Let \\(g_i(x) = A_{i\\cdot}x - b_i\\) for \\(i=1,2,...,m\\) where \\(A_{i\\cdot}\\) is the \\(i\\)th row of \\(A, \\nabla g_i(x) = A_{i\\cdot}\\). Let \\(h_j(x) = -x\\) for \\(j = 1,2,...,n, \\nabla h_j(x) = -e_j\\) where \\(e_j\\) is the elementary vector.  </p> <p>Using FNOC, take \\(p_{1*},..., p_{m*} \\geq 0, \\mu_1,...,\\mu_n\\geq 0\\)</p> \\[\\begin{align*} \\nabla f(x_*) + \\sum_{i=1}^m p_{i*} \\nabla g_i(x_*) + \\sum_{i=1}^n \\mu_i\\nabla h_i(x_*) = 0\\\\ -c + \\sum_{i=1}^m p_{i*} A_{i\\cdot} - \\sum_{i=1}^n \\mu_i e_i = 0\\end{align*}\\] <p>Let \\(p_* = (p_{1*},..., p_{m*})\\in\\mathbb R^m, \\mu = (\\mu_1,...,\\mu_n)\\in\\mathbb R^n\\), the FNOC can be simply written as</p> \\[\\begin{align*} -c + A^Tp_* - \\mu &amp;= 0\\\\ p_* &amp;\\geq 0\\\\ \\mu &amp;\\geq 0\\\\ (Ax_* - b)^Tp_* &amp;= 0\\\\ \\mu^Tx_* &amp;= 0 \\end{align*}\\] <p>Part (b)</p> <p>Show that the Lagrange mutipliers \\(p_*\\) for the optimal primal \\(x_*\\) satisfy the constraints of the dual problem.</p> <p>For some Lagrange multiplier \\(p_*\\), we have \\(A^Tp_* = \\mu+c\\) and since \\(\\mu\\geq 0\\), \\(A^Tp_* \\geq c\\), also \\(p_*\\geq 0\\), which satisfy the constrained of the dual problem.</p> <p>Part (c)</p> <p>Use the comlementary slackness conditions for the primal problem to show that \\(p_*^TAx_* = p_*^Tb\\).</p> <p>For some Lagrange multiplier \\(p_*\\), we have </p> \\[\\begin{align*} (Ax_* - b)^Tp_* &amp;= 0\\\\ x_*^TA^Tp_* - b^Tp_* &amp;= 0\\\\ x_*^TA^Tp_* &amp;= b^Tp_* \\end{align*}\\] <p>taking transpose on both sides,</p> \\[p_*^TAx_* = p_*^Tb\\] <p>Part (d)</p> <p>Write the FONC for the dual optimal solution \\(p^*\\).</p> <p>Let \\(f(p) = b^Tp, \\nabla f(p) = 0\\), Let \\(g(p) = - A^Tp + c, \\nabla g(p) = -A\\) Let \\(h(p) = -p, \\nabla p = -I\\) Using FNOC, take \\(x_*\\in\\mathbb R^n, v\\in\\mathbb R^m\\), we have </p> \\[\\begin{align*} b - Ax_* - v &amp;= 0\\\\ x_* &amp;\\geq 0\\\\ v&amp;\\geq 0\\\\ (-A^Tp_*+c)^Tx_* &amp;= 0\\\\ v^Tp_* &amp;= 0 \\end{align*}\\] <p>Part (e)</p> <p>Show that the Lagrange mutipliers \\(x_*\\) for the optimal dual \\(p_*\\) satisfy the constraints of the primal problem.</p> <p>For some Lagrange multiplier \\(x_*\\), we have \\(Ax_* = b-v\\) and since \\(v\\geq 0\\), \\(Ax_* \\leq b\\), also \\(x_*\\geq 0\\), which satisfy the constrained of the dual problem. </p> <p>Part (f)</p> <p>Use the comlementary slackness conditions for the dual problem to show that \\(p_*^TAx_* = c^Tx_*\\).</p> <p>For some Lagrange multiplier \\(x_*\\), we have </p> \\[\\begin{align*} (-A^Tp_*+c)^Tx_* &amp;= 0\\\\ -p_*^TAx_* + c^Tx_* &amp;= 0\\\\ c^Tx_* &amp;= p_*^TAx_* \\end{align*}\\] <p>Part (g)</p> <p>Use the complementary slackness conditions to show that \\(c^Tx_* = p_*^Tb\\).</p> <p>combine part(c) and part(f), we have </p> \\[c^Tx_* = p_*^TAx_* = p_*^Tb\\]"},{"location":"apm462/icfdoq.html#example-9","title":"Example 9","text":"<p>Question</p> <p>Consider the \"continuous\" optimization problem</p> \\[\\begin{align*} \\text{minimize}\\quad&amp;\\int c(x)h(x)dx\\\\ \\text{subject to} \\quad&amp; 0\\leq h(x)\\leq 1\\\\&amp;\\int h(x)dx = 1 \\end{align*}\\] <p>The solution is given in the way that \\(I_s = \\{x\\mid x(x) \\leq s\\}\\), the volume \\(|I_s| = \\int_{I_s}1dx\\) of \\(I_s\\) is an increasing function of \\(s\\), choose \\(s_0\\) s.t. \\(|I_{s_0}| = 1\\), the optimal solution is</p> \\[h(x) = \\mathbb I(x\\in I_{s_0})\\] <p>In economic terms, \\(h(x)\\) is the density of a given resource as a function of the location \\(x\\) and \\(c(x)\\) is the cost of the resource at location \\(x\\). This problem is about wanting to accumulate total of 1 unit of the resource in such a way as to minimize the cost.</p> <p>Explain why the solution makes sense in economic terms.</p> <p>We are collecting all resources lower than some cost \\(s_0\\) and with this cost, we are able to collect exactly enough resources (1 unit of recourses). </p> <p>Consider the discrete problem</p> \\[\\begin{align*} \\min \\quad&amp;\\sum_{i=1}^nc_i h_i\\\\ \\text{subject to} \\quad &amp;0\\leq h_i\\leq 1\\\\ &amp;\\sum_{n}h_i = N \\end{align*}\\] <p>where \\(n\\geq N\\) and \\(N\\in\\mathbb N\\), \\(c_i\\)'s are all distinct. Prove that except for possibly one \\(i\\), \\(h_i\\) is \\(0\\) or \\(1\\) for all other \\(i\\)'s.</p> <p>Let \\(f(h) = c^Th\\) where \\(c = (c_1,...,c_n), h = (h_1,...,h_n), 0\\leq h \\leq 1, \\nabla f(h) = c\\), Let \\(g(h) = \\vec 1^Th\\) where \\(\\vec 1\\) is the \\(\\mathbb R^n\\) vector with all entries being 1, \\(\\nabla g(h) = \\vec 1\\), Note that \\(0 \\leq h \\leq 1\\) gives to constraints \\(-h \\leq 0, h\\leq 1\\). By Kuhn-Tucker conditions, take \\(\\lambda\\in\\mathbb R, \\mu_1\\in\\mathbb R^n, \\mu_2\\in\\mathbb R^n\\). </p> \\[\\begin{align*} c + \\lambda \\vec 1 + \\mu_1 - \\mu_2 &amp;= \\vec 0\\\\ \\sum_{i=1}^n h_i = \\vec 1^Th &amp;= N\\\\ \\mu_1^Th &amp;= 0\\\\ \\mu_2^T(1-h) &amp;= 0\\\\ \\mu_1 &amp;\\geq 0\\\\ \\mu_2 &amp;\\geq 0 \\end{align*}\\] <p>Suppose for some \\(h_i, h_j, 0 &lt; h_i &lt; h_j &lt; 1\\) then we must have \\(\\mu_{1i} = \\mu_{2i} = \\mu_{1j} = \\mu_{2j} = 0\\), then note that </p> \\[c_i + \\lambda + \\mu_{1i} - \\mu_{2j} = c_i + \\lambda = 0\\] \\[c_j + \\lambda + \\mu_{1j} - \\mu_{2j} = c_j + \\lambda = 0\\] <p>implies that \\(c_i = c_j\\), contradicts with the fact that all \\(c_i\\)'s are distinct.  Therefore, by contradiction, the statement is proven. </p>"},{"location":"apm462/newtons_method_and_steepest_descent.html","title":"Newton's Method and Steepest Descent","text":""},{"location":"apm462/newtons_method_and_steepest_descent.html#newtons-method","title":"Newton's Method","text":"<p>Consider a twice-differentiable function \\(f : I \\to \\mathbb R\\) defined on an interval \\(I \\subseteq \\mathbb R\\). We would like to find the minima of \\(f\\). We shall do so by considering quadratic approximations of \\(f\\). </p> <p>Let us start at a point \\(x_0 \\in I\\). Consider </p> \\[q(x) = f(x_0) + f'(x_0)(x - x_0) + \\frac{1}{2}f''(x_0)(x-x_0)^2\\] <p>the (best) quadratic approximation to \\(f\\) at \\(x_0\\). Note that \\(q(x_0) = f(x_0)\\), \\(q'(x_0) = f'(x_0)\\) and \\(q''(x_0) = f''(x_0)\\). We will now find the local minimizer \\(x_1\\) for the quadratic \\(q\\). That is, we would like to find \\(x_1\\) such that</p> \\[0 = q'(x_1) = f'(x_0) + f''(x_0)(x_1-x_0)\\] <p>implying that, so long as \\(f''(x_0) \\neq 0\\), </p> \\[x_1 = x_0 - \\frac{f'(x_0)}{f''(x_0)}\\] <p>The idea of Newton's method is to iterate this procedure. (Consider the Newton's method for finding roots of functions; this is the same as finding the root of the derivative of the function.)</p>"},{"location":"apm462/newtons_method_and_steepest_descent.html#newtons-method-in-1-dim","title":"Newton's Method in 1-Dim","text":"<p>Precisely, we pick a starting point \\(x_0 \\in I\\). Then we recursively define</p> \\[x_{n+1} = x_n - \\frac{f'(x_n)}{f''(x_n)}\\] <p>We hope that the sequence \\(x_n\\) converges to a minimizer of \\(f\\). For the sake of the rest of the lecture, let \\(g = f'\\). With this notation we may write Newton's method as</p> \\[\\begin{align*} x_0 &amp;\\in I \\\\ x_{n+1} &amp;= x_n - \\frac{g(x_n)}{g'(x_n)}. \\end{align*}\\]"},{"location":"apm462/newtons_method_and_steepest_descent.html#thrm-convergence-of-newtons-method-in-1-dim","title":"Thrm. Convergence of Newton's Method in 1-Dim","text":"<p>Claim. Let \\(g \\in C^2(I)\\) (i.e. \\(f \\in C^3(I)\\)). Suppose there is an \\(x_* \\in I\\) satisfies \\(g(x_*) = 0\\) and \\(g'(x_*) \\neq 0\\). If \\(x_0\\) is sufficiently close to \\(x_*\\), then the sequence \\(x_n\\) generated by Newton's method converges to \\(x_*\\).</p> <p>proof. Since \\(g'(x_0) \\neq 0\\), there is, by continuity of \\(g'\\), an \\(\\alpha &gt; 0\\) such that  \\(|g'(x_1)| &gt; \\alpha\\) for all \\(x_1\\) in a neighbourhood of \\(x_0\\) and \\(|g''(x_2)| &lt; \\frac{1}{\\alpha}\\) for all \\(x_2\\) in the neighbourhood of \\(x_0\\). The proof of the first claim is a simple continuity argument. The proof of the second claim follows from continuity of \\(g''\\) and the extreme value theorem applied to this neighbourhood('s closure). (That is, we can choose an \\(\\alpha\\) to bound \\(|g'|\\) from below, and then shrink it possibly to ensure \\(1/\\alpha\\) bounds \\(|g''|\\) from above.)</p> <p>Since \\(g(x_*) = 0\\), the formula of Newton's method now implies</p> \\[(*)\\:x_{n+1} - x_* = x_n - x_* - \\frac{g(x_n) - g(x_*)}{g'(x_n)} = -\\frac{g(x_n) - g(x_*) - g'(x_n)(x_n-x_*)}{g'(x_n)}\\] <p>By the second order mean value theorem, there exists a \\(\\xi\\) sufficiently close to \\(x_*\\) such that</p> \\[g(x_*) = g(x_n) + g'(x_n)(x_* - x_n) + \\frac{1}{2}g''(\\xi)(x_* - x_n)^2\\] <p>Then \\((*)\\) becomes</p> \\[x_{n+1} - x_* = \\frac{1}{2}\\frac{g''(\\xi)}{g'(x_n)}(x_n - x_*)^2.\\] <p>The bounds on \\(g'\\) and \\(g''\\) we found at the start of the proof imply that</p> \\[(**)\\: |x_{n+1} - x_*| &lt; \\frac{1}{2\\alpha^2} |x_n - x_*|^2\\] <p>Let \\(\\rho\\) be the constant \\(\\rho = \\frac{1}{\\alpha^2}|x_0 - x_*|\\). Choose \\(x_0\\) close enough to \\(x_*\\) so that \\(\\rho &lt; 1\\). Then \\((**)\\) implies</p> \\[|x_1 - x_*| &lt; \\frac{1}{2\\alpha^2}|x_0 - x_*||x_0 - x_*| = \\rho |x_0 - x_*| &lt; |x_0 - x_*|\\] <p>Similarly, \\((**)\\) gives</p> \\[|x_2 - x_*| &lt; \\frac{1}{2\\alpha^2}|x_1 - x_*|^2 &lt; \\frac{1}{2\\alpha^2}\\rho^2 |x_0-x_*|^2 &lt; \\rho^2 |x_0 - x_*|\\] <p>Continuing in the same way we obtain</p> \\[|x_n - x_*| &lt; \\rho^n |x_0 - x_*|\\] <p>implying that Newton's method converges in our neighbourhood.</p>"},{"location":"apm462/newtons_method_and_steepest_descent.html#newtons-method-in-higher-dimensions","title":"Newton's Method in Higher Dimensions","text":"<p>Consider a function \\(f : \\Omega \\to \\mathbb R\\) defined on an open set \\(\\Omega \\subseteq \\mathbb R^n\\). We choose a starting point \\(x_0 \\in \\Omega\\), and recursively define</p> \\[x_{n+1} = x_n - \\nabla^2 f(x_n)^{-1} \\nabla f(x_n)\\] <p>For a general \\(f\\), the algorithm requires that \\(\\nabla^2 f(x_n)\\) is invertible. The algorithm stops if \\(\\nabla f(x_n) = 0\\) at some point (that is, the sequence given by Newton's method becomes constant if \\(\\nabla f(x_n) = 0\\) for some \\(x_n\\).) Our main result is</p>"},{"location":"apm462/newtons_method_and_steepest_descent.html#thrm-convergence-of-newtons-method-in-n-dim","title":"Thrm. Convergence of Newton's Method in N-Dim","text":"<p>Suppose \\(f \\in C^3(\\Omega)\\). Suppose also that there is an \\(x_* \\in \\Omega\\) such that \\(\\nabla f(x_*) = 0\\) and \\(\\nabla^2 f(x_*)\\) is invertible. Then the sequence \\(x_n\\) defined by</p> \\[x_{n+1} = x_n - \\nabla^2 f(x_n)^{-1} \\nabla f(x_n)\\] <p>converges for all \\(x_0\\) sufficiently close to \\(x_*\\).</p> <p>The goal of Newton's method was to find a minimizer of \\(f\\), but it is possible for it to fail, for it only searches for critical points, not necessarily extrema.</p>"},{"location":"apm462/newtons_method_and_steepest_descent.html#things-may-go-wrong","title":"Things May Go Wrong","text":"<p>It is possible for Newton's method to fail to converge even when \\(f\\) has a unique global minimizer \\(x_*\\) and the initial point \\(x_0\\) can be taken arbitrarily close to \\(x_*\\). Consider</p> \\[f(x) = \\frac{2}{3}|x|^{3/2} = \\begin{cases}  \\frac{2}{3}x^{3/2} &amp; x \\geq 0 \\\\ \\frac{2}{3}(-x)^{3/2} &amp; x \\leq 0 \\end{cases}\\] <p>This function is differentiable, and its derivative is \\(f'(x) = \\begin{cases}  x^{1/2} &amp; x \\geq 0 \\\\ -(-x)^{1/2} &amp; x \\leq 0 \\end{cases}\\) and its second derivative is \\(f''(x) = \\begin{cases}  \\frac{1}{2}x^{-1/2} &amp; x &gt; 0 \\\\ \\frac{1}{2}(-x)^{-1/2} &amp; x &lt; 0 \\\\ \\text{N/A} &amp; x = 0 \\end{cases}\\) so \\(f \\not\\in C^3\\) (it is not even \\(C^2\\)). Let \\(x_0 = \\epsilon\\). Then</p> \\[x_1 = \\epsilon - \\frac{f'(\\epsilon)}{f''(\\epsilon)} = \\epsilon - \\frac{\\epsilon^{1/2}}{\\frac{1}{2}\\epsilon^{-1/2}} = \\epsilon - 2\\epsilon = -\\epsilon\\] \\[x_2 = -\\epsilon - \\frac{f'(-\\epsilon)}{f''(-\\epsilon)} = -\\epsilon - \\frac{-\\epsilon^{1/2}}{\\frac{1}{2}\\epsilon^{-1/2}} = -\\epsilon + 2\\epsilon = \\epsilon\\] <p>So Newton's method gives an alternating sequence \\(\\epsilon, -\\epsilon, \\epsilon, -\\epsilon, \\dots\\).  This definitely does not converge. This does not contradict the theorem of convergence because the function in question does not satisfy the conditions of the theorem.</p> <p>Now we consider an example in which the function in question converges, just not to a minimizer. Consider \\(f(x) = x^3\\), which has derivatives \\(f'(x) = 3x^2\\) and \\(f''(x) = 6x\\). Starting at \\(x_0\\), we have</p> \\[x_{n+1} = x_n - \\frac{f'(x_n)}{f''(x_n)} = x_n - \\frac{3x_n^2}{6x_n} = x_n - \\frac{1}{2}x_n = \\frac{1}{2}x_n\\] <p>So Newton's method definitely converges to the critical point \\(0\\), no matter the choice of \\(x_0 \\in \\mathbb R\\). However, the function \\(f\\) in question does not have a global minimizer, so, while Newton's method converges, it does not converge to an extrema of any sorts.</p>"},{"location":"apm462/newtons_method_and_steepest_descent.html#steepest-descent","title":"Steepest Descent","text":"<p>Consider a \\(C^1\\) function \\(f : \\Omega \\to \\mathbb R\\) defined on an open set \\(\\Omega \\subseteq \\mathbb R^n\\). The idea is: at every point in the \"landscape\" of \\(f\\) (the graph of \\(f\\) in \\(\\mathbb R^{n+1}\\)), make a step \"downwards\" in the steepest direction. (If you're on a mountain and want to descend to the bottom as fast as possible, how do you do so? You, at your current position, take a step down in the steepest direction, and repeat until you're done.) </p> <p>Since the gradient \\(\\nabla f(x_0)\\) represents the direction of greatest increase of \\(f\\) at \\(x_0\\), the vector \\(-\\nabla f(x_0)\\) represents the direction of steepest decrease at \\(x_0\\). We would therefore like to move in the direction of the negative gradient. We will do so, with the condition that we move until we have a minimizer in the direction of the negative gradient (at which point we will stop moving and repeat).</p>"},{"location":"apm462/newtons_method_and_steepest_descent.html#algorithm","title":"Algorithm","text":"\\[\\begin{align*} x_0 &amp;\\in \\Omega \\\\ x_{k+1} &amp;= x_k - \\alpha_k \\nabla f(x_k) \\end{align*}\\] <p>where \\(\\alpha_k \\geq 0\\) satisfies \\(f(x_k - \\alpha_k \\nabla f(x_k)) = \\min_{\\alpha \\geq 0} f(x_k - \\alpha \\nabla f(x_k))\\) We call \\(\\alpha_k\\) the optimal step, since it is chosen so that \\(x_{k+1}\\) is the minimum of \\(f\\) sufficiently close to \\(x_k\\). We also call \\(x_{k+1}\\) the minimum point on the half-line \\(x_k - \\alpha \\nabla f(x_k), \\alpha \\geq 0\\). We now describe some properties of the method of steepest descent.</p>"},{"location":"apm462/newtons_method_and_steepest_descent.html#thrm-correctness-of-steepest-descent","title":"Thrm. Correctness of Steepest Descent","text":"<p>Claim. The steepest descent algorithm is actually descending; \\(f(x_{k+1}) &lt; f(x_k)\\) so long as \\(\\nabla f(x_k) \\neq 0\\).</p> <p>proof. We have \\(f(x_{k+1}) = f(x_k - \\alpha_k \\nabla f(x_k)) \\leq f(x_k - s \\nabla f(x_k))\\) for all \\(s \\in [0, \\alpha_k]\\).   Also,</p> \\[\\left. \\frac{d}{ds} \\right|_{s=0} f(x_k - s\\nabla f(x_k)) = \\nabla f(x_k) \\cdot (-\\nabla f(x_k)) = -\\| \\nabla f(x_k) \\|^2 &lt; 0\\] <p>Then for sufficiently small \\(s \\geq 0, f(x_k - s\\nabla f(x_k)) &lt; f(x_k)\\)</p>"},{"location":"apm462/newtons_method_and_steepest_descent.html#thrm-direction-of-gradient","title":"Thrm. Direction of Gradient","text":"<p>Claim. The steepest descent algorithm moves in perpendicular steps; for all \\(k\\), we have \\((x_{k+2} - x_{k+1})\\cdot(x_{k+1} - x_k) = 0\\).</p> <p>proof. We have </p> \\[(x_{k+2} - x_{k+1})\\cdot(x_{k+1} - x_k) = \\alpha_{k+1}\\alpha_k \\nabla f(x_{k+1}) \\cdot \\nabla f(x_k)\\] <p>Recall that \\(\\alpha_k \\geq 0\\). If \\(\\alpha_k = 0\\), then the whole expression is zero and we're done. Consider the possibility that \\(\\alpha_k &gt; 0\\). Then</p> \\[f(x_k - \\alpha_k \\nabla f(x_k)) = \\min_{s &gt; 0} f(x_k - s \\nabla f(x_k))\\] <p>implying that \\(\\alpha_k\\) is a minimizer of the function on the right in the above. Then</p> \\[0 = \\left. \\frac{d}{ds} \\right|_{s=\\alpha_k} f(x_k - s\\nabla f(x_k)) = \\nabla f(x_k - \\alpha_k \\nabla f(x_k)) \\cdot (-\\nabla f(x_k)) = -\\nabla f(x_{k+1}) \\cdot \\nabla f(x_k)\\] <p>proving the claim.</p>"},{"location":"apm462/newtons_method_and_steepest_descent.html#thrm-convergence-of-steepest-descent","title":"Thrm. Convergence of Steepest Descent","text":"<p>Claim. Suppose \\(f\\) is a \\(C^1\\) function on an open set \\(\\Omega \\subseteq \\mathbb R^n\\). Let \\(x_0 \\in \\Omega\\), and let \\(\\{x_k\\}_{k=0}^\\infty\\) be the sequence generated by the method of steepest descent. If there is a compact \\(K \\subseteq \\Omega\\) containing all \\(x_k\\), then every convergent subsequence of \\(\\{x_k\\}_{k=0}^\\infty\\) in \\(K\\) will converge to a critical point \\(x_*\\) of \\(f\\).</p> <p>Choose a convergent subsequence \\(\\{x_{k_i}\\}\\) converging to a point \\(x_* \\in K\\).  Note that \\(\\{ f(x_{k_i}) \\}\\) decreases and converges to \\(f(x_*)\\). Since \\(\\{f(x_k)\\}\\) is a decreasing sequence, it also converges to \\(f(x_*)\\).</p> <p>Suppose for the sake of contradiction that \\(\\nabla f(x_*) \\neq 0\\). Since \\(f\\) is \\(C^1\\), \\(\\nabla f(x_{k_i})\\) converges to \\(\\nabla f(x_*)\\). Define \\(y_{k_i} = x_{k_i} - \\alpha_{k_i} \\nabla f(x_{k_i})\\) (i.e. \\(y_{k_i} = x_{k_1+1}\\)). We may therefore assume without loss of generality that \\(y_{k_i}\\) converges to some \\(y_* \\in K\\). Since \\(\\nabla f(x_*) \\neq 0\\), we may write</p> \\[\\alpha_{k_i} = \\frac{|y_{k_i} - x_{k_i}|}{|\\nabla f(x_{k_i})|}\\] <p>Taking the limit as \\(i \\to \\infty\\), we have</p> \\[\\alpha_* := \\lim_{i \\to \\infty} \\alpha_{k_i} = \\frac{|y_* - x_*|}{|\\nabla f(x_*)|}\\] <p>Taking the same limit in the definition of \\(y_{k_i}\\) we have</p> \\[y_* = x_* - \\alpha_* \\nabla f(x_*)\\] <p>Note that</p> \\[f(y_{k_i}) = f(x_{k_i+1}) = \\min_{\\alpha \\geq 0} f(x_{k_i} - \\alpha \\nabla f(x_{k_i}))\\] <p>Thus \\(f(y_{k_i}) \\leq f(x_{k_i} - \\alpha \\nabla f(x_{k_i}))\\) for all \\(\\alpha \\geq 0\\). For any fixed \\(\\alpha \\geq 0\\), taking the limit \\(i \\to \\infty\\) gives us</p> \\[f(y_*) \\leq f(x_* - \\alpha \\nabla f(x_*))\\] \\[\\implies f(y_*) \\leq \\min_{\\alpha \\geq 0} f(x_* - \\alpha \\nabla f(x_*)) &lt; f(x_*)\\] <p>since the function \\(f\\) decreases in the direction of \\(-\\nabla f(x_*) \\neq 0\\).</p> <p>We can also argue the following: \\(f(x_{k_i+1}) \\to f(x_*)\\). But since \\(x_{k_i+1} = y_{k_i}\\), we have \\(f(y_{k_i}) \\to f(y_*)\\), implying \\(f(x_*) = f(y_*)\\), a contradiction.</p>"},{"location":"apm462/newtons_method_and_steepest_descent.html#steepest-descent-in-the-quadratic-case","title":"Steepest Descent in the Quadratic Case","text":"<p>Consider a function \\(f\\) of the form \\(f(x) = \\frac{1}{2}x^TQx - b^Tx\\) for \\(b,x \\in \\mathbb R^n\\) and \\(Q\\) an \\(n \\times n\\) symmetric positive definite matrix. Let \\(\\lambda = \\lambda_1 \\leq \\cdots \\leq \\lambda_n = \\Lambda\\) be the eigenvalues of \\(Q\\). (Note that they are all strictly positive.) Note that \\(\\nabla^2 f(x) = Q\\) for any \\(x\\), so \\(f\\) is strictly convex. There therefore exists a unique global minimizer \\(x_*\\) of \\(f\\) in \\(\\mathbb R^n\\) such that \\(Qx_* = b\\). </p> <p>Let </p> \\[q(x) = \\frac{1}{2}(x - x_*)^TQ(x-x_*) = f(x) + \\frac{1}{2}x_*^TQx_*\\] <p>So \\(q\\) and \\(f\\) differ by a constant. Therefore it suffices to find the minimizer of \\(q\\), rather than \\(f\\). Note that \\(q(x) \\geq 0\\) for all \\(x\\), since \\(Q\\) is positive definite. So we shall study the minimizer \\(x_*\\) of \\(q\\).</p> <p>Note that \\(\\nabla f(x) = \\nabla q(x) = Qx - b\\); let \\(g(x) = Qx - b\\). The method of steepest descent may therefore be written as</p> \\[x_{k+1} = x_k - \\alpha_k g(x_k)\\] <p>We would like a formula for the optimal step \\(\\alpha_k\\). Recall that \\(\\alpha_k\\) is defined to be the minimizer of the function \\(f(x_k - \\alpha g(x_k))\\) over \\(\\alpha \\geq 0\\). Thus</p> \\[0 = \\left. \\frac{d}{d\\alpha} \\right|_{\\alpha = \\alpha_k} f(x_k - \\alpha g(x_k)) = \\nabla f(x_k - \\alpha_k g(x_k)) \\cdot (-g(x_k))\\] <p>This simplifies to</p> \\[0 = (Q(x_k - \\alpha_k g(x_k)) - b) \\cdot (-g(x_k)) = -(\\underbrace{Qx_k - b}_{=g(x_k)} - \\alpha_k Q g(x_k)) \\cdot g(x_k)\\] <p>giving</p> \\[0 = -|g(x_k)|^2 + \\alpha_k g(x_k)^TQg(x_k)\\] <p>Therefore</p> \\[\\alpha_k = \\frac{|g(x_k)|^2}{g(x_k)^TQg(x_k)} \\:(*)\\]"},{"location":"apm462/newtons_method_and_steepest_descent.html#lemma-1","title":"Lemma 1","text":"<p>Claim. \\(q(x_{k+1}) = \\left( 1 - \\frac{|g(x_k)|^4}{(g(x_k)^T Q g(x_k))(g(x_k)^TQ^{-1}g(x_k))} \\right)q(x_k)\\)</p> <p>proof. </p> \\[\\begin{align*} q(x_{k+1}) &amp;= q(x_k - \\alpha_k g(x_k)) \\\\ &amp;= \\frac{1}{2}(x_k - \\alpha_k g(x_k) - x_*)^T Q (x_k - \\alpha_k g(x_k) - x_*) \\\\ &amp;= \\frac{1}{2}(x_k - x_* - \\alpha_k g(x_k))^T Q (x_k - x_* - \\alpha_k g(x_k)) \\\\ &amp;= \\frac{1}{2}(x_k - x_*)^T Q (x_k - x_*) - \\alpha_k g(x_k)^TQ(x_k - x_*) + \\frac{1}{2} \\alpha_k^2 g(x_k)^T Q g(x_k) \\\\ &amp;= q(x_k) - \\alpha_k g(x_k)^TQ(x_k - x_*) + \\frac{1}{2} \\alpha_k^2 g(x_k)^T Q g(x_k), \\end{align*}\\] <p>implying</p> \\[q(x_k) - q(x_{k+1}) = \\alpha_k g(x_k)^TQ(x_k - x_*) - \\frac{1}{2} \\alpha_k^2 g(x_k)^T Q g(x_k)\\] <p>Dividing by \\(q(x_k)\\) gives</p> \\[\\frac{q(x_k) - q(x_{k+1})}{q(x_k)} = \\frac{\\alpha_k g(x_k)^TQ(x_k - x_*) - \\frac{1}{2} \\alpha_k^2 g(x_k)^T Q g(x_k)}{\\frac{1}{2}(x_k - x_*)^T Q (x_k - x_*)}\\] <p>Let \\(g_k = g(x_k)\\) and \\(y_k - x_k - x_*\\). Then</p> \\[\\frac{q(x_k) - q(x_{k+1})}{q(x_k)} = \\frac{\\alpha_k g_k^T Q y_k - \\frac{1}{2} \\alpha_k^2 g_k^T Q g_k}{\\frac{1}{2} y_k^T Q y_k}\\] <p>Note that \\(g_k = Qx_k - b = Q(x - x_*) = Qy_k\\), so \\(y_k = Q^{-1}g_k\\). The above formula therefore simplifies to</p> \\[\\frac{q(x_k) - q(x_{k+1})}{q(x_k)} = \\frac{2 \\alpha_k |g_k|^2 - \\alpha_k^2 g_k^TQg_k}{g_k^T Q^{-1} g_k}\\] <p>Now recall the formula</p> \\[\\alpha_k = \\frac{|g_k|^2}{g_k^TQg_k} \\:(*)\\] <p>This implies that</p> \\[\\frac{q(x_k) - q(x_{k+1})}{q(x_k)} = \\frac{2 \\frac{|g_k|^4}{g_k^T Q g_k} - \\frac{|g_k|^4}{g_k^T Q g_k}}{g_k^T Q^{-1}g_k} = \\frac{|g_k|^4}{(g_k^T Q g_k)(g_K^T Q^{-1} g_k)}\\] <p>proving the theorem.</p>"},{"location":"apm462/newtons_method_and_steepest_descent.html#thrm-convergence-speed-of-steepest-descent-quadratic-case","title":"Thrm. Convergence Speed of Steepest descent, quadratic case","text":"<p>Claim. For \\(x_0 \\in \\mathbb R^n\\), the method of steepest descent starting at \\(x_0\\) converges to the unique minimizer \\(x_*\\) of the function \\(f\\), and we have \\(q(x_{k+1}) \\leq r q(x_k)\\).</p> <p>proof. We know that \\(q(x_{k+1}) \\leq r^k q(x_0)\\). Since \\(0 \\leq r &lt; 1\\), when \\(k \\to \\infty\\), \\(r^k \\to 0\\). Note that</p> \\[x_k \\in \\{ x \\in \\mathbb R^n : q(x) \\leq r^k q(x_0) \\}\\] <p>This set is a sublevel set of \\(q\\). The sublevel sets of \\(q\\) look like concentric filled-in ellipses centred at \\(x_*\\), and as \\(k \\to \\infty\\), they seem to \"shrink\" into \\(x_*\\). Therefore steepest descent converges in the quadratic case.</p> <p>Note that</p> \\[r = \\frac{(\\Lambda - \\lambda)^2}{(\\Lambda - \\lambda)^2} = \\frac{( \\Lambda / \\lambda - 1 )^2}{(\\Lambda / \\lambda - 1)^2}\\] <p>so \\(r\\) depends only on the ratio \\(\\Lambda / \\lambda\\). This number is called the \\emph{condition number of \\(Q\\)}. (The condition number may be defined as \\(\\|Q\\|\\|Q^{-1}\\|\\) in the operator norm on matrices; it is not hard to see that these numbers agree in our case.) </p> <p>If the condition number \\(\\Lambda / \\lambda \\gg 1\\) (large), then convergence is very slow. If \\(\\Lambda / \\lambda = 1\\), then \\(r = 0\\), and so convergence is achieved in one step.</p>"},{"location":"apm462/ufdo.html","title":"Unconstrained Finite Dimensional Optimization","text":""},{"location":"apm462/ufdo.html#problem-definition","title":"Problem Definition","text":"<p>Our main problem is</p> \\[\\begin{align*} &amp;\\min_{x \\in \\Omega} f(x) \\quad f : \\mathbb R^n \\supseteq \\Omega \\to \\mathbb R, \\end{align*}\\] <p>where \\(\\Omega\\) is one of the following three types:  - \\(\\Omega = \\mathbb R^n\\).  -  \\(\\Omega\\) open.  - \\(\\Omega\\) the closure of an open set.</p> <p>We can consider minimization problems without any loss of generality, since any maximization problem can be converted to a minimization problem by taking the negative of the function in question: that is,</p> \\[\\max_{x \\in \\Omega} f(x) = \\min_{x \\in \\Omega} -f(x)\\]"},{"location":"apm462/ufdo.html#defn-feasible-direction","title":"Defn. Feasible Direction","text":"<p>Given \\(\\Omega \\subseteq \\mathbb R^n\\) and a point \\(x_0 \\in \\Omega\\), we say that the vector \\(v \\in \\mathbb R^n\\) is a feasible direction at \\(x_0\\) if there is an \\(\\overline{s} &gt; 0\\) such that \\(x_0 + sv \\in \\Omega\\) for all \\(s \\in [0, \\overline{s}]\\).</p>"},{"location":"apm462/ufdo.html#thrm-first-order-necessary-condition-for-a-local-minimum-fonc","title":"Thrm. First order necessary condition for a local minimum (FONC)","text":"<p>Claim. If \\(f : \\mathbb R^n \\supseteq \\Omega \\to \\mathbb R\\) is \\(C^1\\) and \\(x_0\\) is a local minimizer of \\(f\\) in the interior of \\(\\Omega\\), then \\(\\nabla f(x_0) = 0\\).</p> <p>proof. If \\(x_0\\) is an interior point of \\(\\Omega\\), then all directions at \\(x_0\\) are feasible. In particular, for any such \\(v\\), we have \\(\\nabla f(x_0) \\cdot (v) \\geq 0\\) and \\(\\nabla f(x_0) \\cdot (-v) \\geq 0\\), which implies \\(\\nabla f(x_0) = 0\\) as all directions are feasible at \\(x_0\\).</p> <p>Claim. Let \\(f : \\mathbb R^n \\supseteq \\Omega \\to \\mathbb R\\) be \\(C^1\\). If \\(x_0 \\in \\Omega\\) is a local minimizer of \\(f\\), then \\(\\nabla f(x_0) \\cdot v \\geq 0\\) for all feasible directions \\(v\\) at \\(x_0\\).</p> <p>proof.  Reduce to a single-variable problem by defining \\(g(s) = f(x_0 + sv)\\), where \\(s \\geq 0\\). Then \\(0\\) is a local minimizer of \\(g\\). Taylor's theorem gives us [ g(s) - g(0) = s g'(0) + o(s) = s \\nabla f(x_0) \\cdot v + o(s). ] If \\(\\nabla f(x_0) \\cdot v &lt; 0\\), then for sufficiently small \\(s\\) the right side is negative. This implies that \\(g(s) &lt; g(0)\\) for those \\(s\\), a contradiction. Therefore \\(\\nabla f(x_0) \\cdot v \\geq 0\\).</p>"},{"location":"apm462/ufdo.html#example-1","title":"Example 1","text":"<p>Consider the optimization problem</p> \\[\\begin{align*} \\min_{x \\in \\Omega} f(x,y) = x^2 - xy + y^2 - 3y \\qquad \\text{over } \\Omega = \\mathbb R^2. \\end{align*}\\] <p>By the corollary to the FONC, we want to find the points \\((x_0, y_0)\\) where \\(\\nabla f(x_0, y_0) = 0\\). We have</p> \\[\\begin{align*} \\nabla f(x,y) = (2x-y, -x+2y-3), \\end{align*}\\] <p>so we want to solve </p> \\[\\begin{align*} 2x - y &amp;= 0 \\\\ -x + 2y &amp;= 3, \\end{align*}\\] <p>which has solution \\((x_0, y_0) = (1,2)\\). Therefore \\((1,2)\\) is the only \\emph{candidate} for a local minimizer. That is, if the function \\(f\\) has a local minimizer in \\(\\mathbb R^2\\), then it must be \\((1,2)\\).</p> <p>It turns out that \\((1,2)\\) is a global minimizer for \\(f\\) on \\(\\Omega = \\mathbb R^2\\). By some work, we have [ f(x,y) = \\left(x - \\frac{y}{2}\\mathbb Right)^2 + \\frac{3}{4}(y-2)^2 - 3. ] In this form, it is obvious that a global minimizer occurs at the point where the squared terms are zero, if such a point exists. That point is \\((1,2)\\).</p>"},{"location":"apm462/ufdo.html#example-2","title":"Example 2","text":"<p>Consider the problem</p> \\[\\begin{align*} \\min_{x \\in \\Omega} f(x,y) = x^2 - x + y + xy \\qquad \\text{over } \\Omega = \\{(x,y) \\in \\mathbb R^2 : x,y \\geq 0\\}. \\end{align*}\\] <p>We have</p> \\[\\nabla f(x,y) = (2x + y - 1, x + 1).\\] <p>To apply the FONC, we'll divide the feasible set \\(\\Omega\\) into four different regions.  Suppose that \\((x_0, y_0)\\) is a local minimizer of \\(f\\) on \\(\\Omega\\).  - \\((x_0, y_0)\\) is an interior point: By the corollary to the FONC, we must have \\(\\nabla f(x_0, y_0) = 0\\). Then \\(x_0 = -1\\), which is not in the interior of \\(\\Omega\\). This case fails.</p> <ul> <li>\\((x_0, y_0)\\) on the positive x-axis: Then we are considering \\((x_0, 0)\\). The feasible directions at \\((x_0, 0)\\) are those vectors \\(v \\in \\mathbb R^2\\) with \\(v_2 \\geq 0\\). The FONC tells us that \\(\\nabla f(x_0,0) \\cdot v \\geq 0\\) for all feasible directions \\(v\\). We then have \\((2x_0 - 1)v_1 + (x_0 + 1)v_2 \\geq 0\\) for all \\(v_1\\) and all \\(v_2 \\geq 0\\). In particular, this holds for \\(v_2 = 0\\), so \\((2x_0 - 1)v_1 \\geq 0\\) for all \\(v_1\\), implying \\(x_0 = 1/2\\). Therefore \\((1/2, 0)\\) is a candidate for a local minimizer of \\(f\\) on \\(\\Omega\\) - this is the only candidate for a local minimizer of \\(f\\) on the positive \\(x\\)-axis.</li> <li>\\((x_0, y_0)\\) on the positive y-axis:  Then we are considering \\((0, y_0)\\). The feasible directions here are \\(v \\in \\mathbb R^2\\) with \\(v_1 \\geq 0\\). Then we have \\((y_0 - 1)v_1 + v_2 \\geq 0\\) for any \\(v_2\\) and \\(v_1 \\geq 0\\). This is a contradiction if we take \\(v_1 = 0\\), so \\(f\\) has no local minimizers along the positive \\(y\\)-axis.</li> <li>\\((x_0, y_0)\\) is the origin:  Then we are considering \\((0,0)\\). The feasible directions here are \\(v \\in \\mathbb R^2\\) with \\(v_1, v_2 \\geq 0\\). Then we have \\(-v_1 + v_2 \\geq 0\\) for all \\(v_1, v_2 \\geq 0\\), a contradiction. Therefore the origin is not a local minimizer of \\(f\\).</li> </ul> <p>We conclude that the only candidate for a local minimizer of \\(f\\) is \\((1/2, 0)\\). It turns out that this is actually a global minimizer of \\(f\\) on \\(\\Omega\\). (This is to be seen.)</p>"},{"location":"apm462/ufdo.html#thrm-second-order-necessary-condition-for-a-local-minimum-sonc","title":"Thrm. Second order necessary condition for a local minimum (SONC)","text":"<p>Claim. Let \\(f : \\mathbb R^n \\supseteq \\Omega \\to \\mathbb R\\) be \\(C^2\\). If \\(x_0 \\in \\Omega\\) is a local minimizer of \\(f\\), then for any feasible direction \\(v\\) at \\(x_0\\) the following conditions hold: - \\(\\nabla f(x_0) \\cdot v \\geq 0\\). - If \\(\\nabla f(x_0) \\cdot v = 0\\), then \\(v^T \\nabla^2 f(x_0) v \\geq 0\\).</p> <p>proof. Fix a feasible direction \\(v\\) at \\(x_0\\). Then \\(f(x_0) \\leq f(x_0 + sv)\\) for sufficiently small \\(s\\). By Taylor's theorem,</p> \\[f(x_0 + sv) = f(x_0) + s \\nabla f(x_0) + \\frac{1}{2} s^2 v^T \\nabla^2 f(x_0) v + o(s^2)\\] <p>so by the FONC,</p> \\[f(x_0 + sv) - f(x_0) = \\frac{1}{2} s^2 v^T \\nabla^2 f(x_0) v + o(s^2)\\] <p>If \\(v^T \\nabla^2 f(x_0) v &lt; 0\\), then for sufficiently small \\(s\\) the right side is negative, implying that \\(f(x_0 + sv) &lt; f(x_0)\\) for such \\(s\\), which contradicts local minimality of \\(f(x_0)\\). Therefore \\(v^T \\nabla^2 f(x_0) \\geq 0\\).</p> <p>Corollary. If \\(f : \\mathbb R^n \\supseteq \\Omega \\to \\mathbb R\\) is \\(C^2\\) and \\(x_0\\) is a local minimizer of \\(f\\) in the interior of \\(\\Omega\\), then \\(\\nabla f(x_0) = 0\\) and \\(\\nabla^2 f(x_0)\\) is positive semidefinite.</p>"},{"location":"apm462/ufdo.html#defn-principal-minor","title":"Defn. Principal Minor","text":"<p>A principal minor of a square matrix \\(A\\) is the determinant of a submatrix of \\(A\\) obtained by removing any \\(k\\) rows and the corresponding \\(k\\) columns, \\(k \\geq 0\\). A leading principal minor of \\(A\\) is the determinant of a submatrix obtained by removing the last \\(k\\) rows and \\(k\\) columns of \\(A\\), \\(k \\geq 0\\).</p>"},{"location":"apm462/ufdo.html#thrm-sylvesters-criterion","title":"Thrm. Sylvester's criterion","text":"<ul> <li>For positive definite self-adjoint matrices If \\(A\\) is a self-adjoint matrix, then \\(A \\succ 0\\) if and only if all of the leading principal minors of \\(A\\) are positive.</li> <li>For positive semidefinite self-adjoint matrices If \\(A\\) is a self-adjoint matrix, then \\(A \\succeq 0\\) if and only if all of the principal minors of \\(A\\) are non-negative.</li> </ul>"},{"location":"apm462/ufdo.html#example-1_1","title":"Example 1","text":"<p>Consider the problem</p> \\[\\begin{align*} \\min_{x \\in \\Omega} f(x,y) = x^2 - xy + y^2 - 3y \\qquad \\text{over } \\Omega = \\mathbb R^2. \\end{align*}\\] <p>Recall that \\((1,2)\\) was the only candidate for a local minimizer of \\(f\\) on \\(\\Omega\\). We now check that the SONC holds. Since \\((1,2)\\) is an interior point of \\(\\Omega\\), we must have \\(\\nabla^2 f(1,2) \\succeq 0\\). We have</p> \\[\\nabla^2 f(1,2) = \\begin{pmatrix} 2 &amp; -1 \\\\ -1 &amp; 2 \\end{pmatrix}\\] <p>All of the leading principal minors of \\(\\nabla^2 f(1,2)\\) are positive, so \\((1,2)\\) satisfies the SONC by Sylvester's criterion. </p>"},{"location":"apm462/ufdo.html#example-2_1","title":"Example 2","text":"<p>Consider the problem</p> \\[\\begin{align*} \\min_{x \\in \\Omega} f(x,y) = x^2 - x + y + xy \\qquad \\text{over } \\Omega = \\{(x,y) \\in \\mathbb R^2 : x,y \\geq 0\\}. \\end{align*}\\] <p>Recall that \\((1/2, 0)\\) was the only candidate for a local minizer of \\(f\\). We have</p> \\[\\nabla^2 f(1/2, 0) = \\begin{pmatrix} 2 &amp; 1 \\\\ 1 &amp; 0 \\end{pmatrix}\\] <p>To satisfy the SONC, we must have \\(v^T \\nabla^2 f(1/2, 0) v \\geq 0\\) for all feasible directions \\(v\\) at \\((1/2, 0)\\) such that \\(\\nabla f(1/2, 0) \\cdot v = 0\\). We have \\(\\nabla f(1/2, 0) = (0, 3/2)\\)  so if \\(v = (v_1, 0)\\), then \\(v\\) is a feasible direction at \\((1/2, 0)\\) with \\(\\nabla f(1,2, 0) \\cdot v = 0\\). Then</p> \\[v^T \\nabla^2 f(1/2, 0) v = \\begin{pmatrix} v_1 &amp; 0 \\end{pmatrix}\\begin{pmatrix} 2 &amp; 1 \\\\ 1 &amp; 0 \\end{pmatrix}\\begin{pmatrix} v_1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} v_1 &amp; 0 \\end{pmatrix} \\begin{pmatrix} 2v_1 \\\\ v_1 \\end{pmatrix} = 2v_1^2 \\geq 0\\] <p>So the SONC is satisfied.</p>"},{"location":"apm462/ufdo.html#completing-the-square","title":"Completing the Square","text":"<p>Let \\(A\\) be a symmetric positive definite \\(n \\times n\\) matrix. Our problem is </p> \\[\\begin{align*} \\min_{x \\in \\Omega} f(x) = \\frac{1}{2} x^T Ax - b \\cdot x \\qquad \\text{over } \\Omega = \\mathbb R^n. \\end{align*}\\] <p>The FONC tells us that if \\(x_0\\) is a local minimizer of \\(f\\), then since \\(x_0\\) is an interior point, \\(\\nabla f(x_0) = 0\\). We thus have \\(Ax_0 = b\\), so since \\(A\\) is invertible (positive eigenvalues), \\(x_0 = A^{-1}b\\). Therefore \\(x_0 = A^{-1}b\\) is the unique candidate for a local minimizer of \\(f\\) on \\(\\Omega\\).</p> <p>The SONC then tells us that \\(\\nabla^2 f(x_0) = A\\), so that \\(\\nabla^2 f(x_0) \\succ 0\\), implying that \\(x_0   = A^{-1}b\\) is a candidate for a local minimizer of \\(f\\) on \\(\\Omega\\).</p> <p>In fact, the candidate \\(x_0\\) is a global minimizer. Why? We will \"complete the square\". We can write</p> \\[f(x) =    \\frac{1}{2} x^T Ax - b \\cdot x = \\frac{1}{2}(x - x_0)^T A(x-x_0) - \\frac{1}{2} x_0^T A x_0\\] <p>this relies on symmetry. (Long rearranging of terms.) In this form it is obvious that \\(x_0\\) is a global minimizer of \\(f\\) over \\(\\Omega\\).</p>"},{"location":"apm462/ufdo.html#thrm-second-order-sufficient-conditions-for-interior-local-minimizers","title":"Thrm. Second order sufficient conditions for interior local minimizers","text":"<p>Lemma If \\(A\\) is symmetric and positive-definite, then  there is an \\(a &gt; 0\\) such that \\(v^T A v \\geq a \\|v\\|^2\\) for all \\(v\\).</p> <p>proof. There is an orthogonal matrix \\(Q\\) with \\(Q^T A Q = \\mathrm{diag}(\\lambda_1, \\dots, \\lambda_n)\\). If \\(v = Qw\\),</p> \\[\\begin{align*} v^T A v &amp;= (Qw)^T A Qw \\\\ &amp;= w^T (Q^T A Q) w \\\\ &amp;= \\lambda_1 w_1^2 + \\cdots + \\lambda_n w_n^2 \\\\ &amp;\\geq \\min\\{\\lambda_1, \\dots, \\lambda_n\\} \\|w\\|^2 \\\\ &amp;= \\min\\{\\lambda_1, \\dots, \\lambda_n\\} \\|v\\|^2 \\qquad \\text{since $Q$ is orthogonal} \\end{align*}\\] <p>Since \\(A\\) is positive-definite, every eigenvalue is positive and we are done.</p> <p>Claim. Let \\(f\\) be \\(C^2\\) on \\(\\Omega \\subseteq \\mathbb R^n\\), and let \\(x_0\\) be an interior point of \\(\\Omega\\) such that \\(\\nabla f(x_0) = 0\\) and \\(\\nabla^2 f(x_0) \\succ 0\\). Then \\(x_0\\) is a strict local minimizer of \\(f\\).</p> <p>proof. The condition \\(\\nabla^2 f(x_0) \\succ 0\\) implies there is an \\(a &gt; 0\\) such that \\(v^T \\nabla^2 f(x_0) v \\geq a \\cdot \\|v\\|^2\\) for all \\(v\\). By Taylor's theorem we have</p> \\[\\begin{align*} f(x_0 + v) - f(x_0) &amp;= \\frac{1}{2} v^T \\nabla^2 f(x_0) v + o(\\|v\\|^2) \\geq \\frac{1}{2} a\\|v\\|^2 + o(\\|v\\|^2)\\\\ &amp;= \\|v\\|^2 \\left( \\frac{a}{2} + \\frac{o(\\|v\\|^2)}{\\|v\\|^2} \\right) \\end{align*}\\] <p>For sufficiently small \\(v\\) the right hand side is positive, so \\(f(x_0 + v) &gt; f(x_0)\\) for all such \\(v\\). Therefore \\(x_0\\) is a strict local minimizer of \\(f\\) on \\(\\Omega\\).</p>"},{"location":"apm462/ufdo.html#example-1_2","title":"Example 1","text":"<p>Consider \\(f(x,y) = xy\\). The gradient is \\(\\nabla f(x,y) = (y,x)\\) and the Hessian is </p> \\[\\nabla^2 f(x,y) = \\begin{pmatrix} 0 &amp; 1 \\\\ 1 &amp; 0 \\end{pmatrix}\\] <p>Suppose we want to minimize \\(f\\) on all of \\(\\Omega = \\mathbb R^2\\). By the FONC, the only candidate for a local minimizer is \\((0,0)\\). The Hessian's eigenvalues are \\(\\pm 1\\), so it is not positive definite. We conclude by the SONC that the origin is not a local minimizer of \\(f\\).</p>"},{"location":"apm462/ufdo.html#example-2_2","title":"Example 2","text":"<p>Consider the same function \\(f(x,y) = xy\\) on \\(\\Omega = \\{(x,y) \\in \\mathbb R^2, x, y \\geq 0\\}\\). We claim that every point of the boundary of \\(\\Omega\\) is a local minimizer of \\(f\\).</p> <p>Consider \\((x,0)\\) with \\(x &gt; 0\\). The feasible directions here are \\(v\\) with \\(v_2 \\geq 0\\). The FONC tells us that \\(\\nabla f(x,0) \\cdot v\\geq 0\\). This dot product is \\(xv_2 \\geq 0\\), so \\((x,0)\\) satisfies the FONC. Therefore every point on the positive x-axis is a candidate for a local minimizer. As for the SONC, \\(\\nabla f(x,0) \\cdot v = xv_2 = 0\\) if and only if \\(v_2 = 0\\). Then \\(v^T \\nabla^2 f(x,0) v = 0\\). Of course, this tells us nothing; we need a sufficient condition that works for boundary points. That's for next lecture.</p> <p>Or, you could just say that \\(f = 0\\) on the boundary of \\(\\Omega\\) and is positive on the interior, so every point of the boundary of \\(\\Omega\\) is a local minimizer (not strict) of \\(f\\).</p>"},{"location":"apm462/ufdoq.html","title":"Examples: Unconstrained Finite Dimensional Optimization","text":""},{"location":"apm462/ufdoq.html#example-1","title":"Example 1","text":"<p>Question</p> <p>Let \\(a\\in\\mathbb R\\) and \\(f_a:\\mathbb R^2\\rightarrow \\mathbb R, f_a(x, y)= x^2 + 2y^2 + axy - y\\).</p> <p>Part (a)</p> <p>Find the points satisfy FOC.    </p> <p>The partial derivative gives </p> \\[\\frac{\\partial f_a}{\\partial x} = 2x+ay, \\frac{\\partial f_a}{\\partial y} = 4y+ax - 1\\] <p>Set the derivatives to 0 to meet the FOC. If \\(a = 0\\), then \\(x = 0, y = 1/4\\) satisfies FOC. If \\(a\\neq 0\\), then to make \\(\\begin{bmatrix}a&amp;4\\\\2&amp;a\\end{bmatrix}\\begin{bmatrix}x\\\\y\\end{bmatrix} = \\begin{bmatrix}1\\\\0\\end{bmatrix}\\) has solutions, the reduced row echelon form gives  \\(\\begin{bmatrix}a&amp;4\\\\0&amp;\\frac{a^2}2- 4\\end{bmatrix}\\begin{bmatrix}x\\\\y\\end{bmatrix} = \\begin{bmatrix}1\\\\-1\\end{bmatrix}\\), If \\(a = \\pm 2\\sqrt{2}\\), then there is no local minimum, otherwise, we have the unique solution \\(y = \\frac{2}{8-a^2}, x = \\frac1a(1-\\frac{8}{8-a^2}) = \\frac{-a}{8-a^2}\\).  To summarize, If \\(a = 0\\), then \\(x=0, y=1/4\\)  satisfies the FOC If \\(a = \\pm 2\\sqrt 2\\), then there is no points satisfy the FOC If \\(a\\neq 0\\) and \\(a\\neq \\pm2\\sqrt2\\), then \\(x = \\frac{-a}{8-a^2}, y = \\frac{2}{8-a^2}\\) satisfies the FOC.</p> <p>Part (b)</p> <p>Find the points satisfy SOC</p> <p>The Hessian matrix gives </p> \\[F_a = \\begin{bmatrix}2&amp;a\\\\a&amp;4\\end{bmatrix}\\] <p>Note that \\(F_a\\) is positive semidefinite iff \\(\\det(F_a) = 2\\times 4-a^2 &gt; 0\\), so that for any \\(a\\in(-2\\sqrt 2, 2\\sqrt 2)\\), the points satisfies the SOC. </p> <p>Part (c)</p> <p>Prove the local minimum is actually global minimum.</p> <p>proof 1. Prove by completing the square Note that \\(f_a(x, y) = \\frac12\\begin{bmatrix}x\\\\y\\end{bmatrix}\\cdot \\begin{bmatrix}2&amp;a\\\\a&amp;4\\end{bmatrix}\\begin{bmatrix}x\\\\y\\end{bmatrix} - \\begin{bmatrix}0\\\\1\\end{bmatrix}\\cdot\\begin{bmatrix}x\\\\y\\end{bmatrix}\\) Therefore, using completing the square method, let \\(x^* = \\begin{bmatrix}x^*\\\\y^*\\end{bmatrix}\\frac{1}{8-a^2}\\begin{bmatrix}4&amp;-a\\\\-a&amp;2\\end{bmatrix}\\begin{bmatrix}0\\\\1\\end{bmatrix} = \\begin{bmatrix}\\frac{-a}{8-a^2}\\\\\\frac{2}{8-a^2}\\end{bmatrix}\\), then </p> \\[f_a(x, y) = \\frac12\\begin{bmatrix}x-x^*\\\\y-y^*\\end{bmatrix}\\cdot \\begin{bmatrix}2&amp;a\\\\a&amp;4\\end{bmatrix}\\begin{bmatrix}x-x^*\\\\y-y^*\\end{bmatrix} - \\frac12\\begin{bmatrix}x^*\\\\y^*\\end{bmatrix}\\cdot \\begin{bmatrix}2&amp;a\\\\a&amp;4\\end{bmatrix}\\begin{bmatrix}x^*\\\\y^*\\end{bmatrix}\\] <p>Note that when \\(a\\in (-2\\sqrt2, 2\\sqrt 2)\\), the matrix \\(\\begin{bmatrix}2&amp;a\\\\a&amp;4\\end{bmatrix}\\) is positive-semidefinite, i.e. for any \\(\\begin{bmatrix}x\\\\y\\end{bmatrix}\\), we have \\(\\begin{bmatrix}x-x^*\\\\y-y^*\\end{bmatrix}\\cdot \\begin{bmatrix}2&amp;a\\\\a&amp;4\\end{bmatrix}\\begin{bmatrix}x-x^*\\\\y-y^*\\end{bmatrix}\\geq 0\\). Therefore, the minimum can only be reached when \\(\\begin{bmatrix}x-x^*\\\\y-y^*\\end{bmatrix} = 0 \\Rightarrow \\begin{bmatrix}x\\\\y\\end{bmatrix} = \\begin{bmatrix}x^*\\\\y^*\\end{bmatrix} = \\begin{bmatrix}\\frac{-a}{8-a^2}\\\\\\frac{2}{8-a^2}\\end{bmatrix}\\) proof 2. Prove by convexity Let \\(\\vec x_1 = (x_1, y_1)\\in \\mathbb R^2, \\vec x_2 = (x_2, y_2) \\in \\mathbb R^2\\), let \\(c\\in [0, 1]\\). For \\(c = 1\\), obviously \\(f_a(1x_1 +0x_2) = 1f_a(x_1) + 0 f_a(x_2)\\), similarly for \\(c=0\\). For \\(c\\in(0,1)\\), denote \\(Q = \\begin{bmatrix}2&amp;a\\\\a&amp;4\\end{bmatrix}\\)</p> \\[\\begin{align*} f(c\\vec x_1 + (1-c)\\vec x_2) &amp;= \\frac 12(c\\vec x_1 + (1-c)\\vec x_2)^T Q (c\\vec x_1 + (1-c)\\vec x_2) - \\begin{bmatrix}0\\\\1\\end{bmatrix}(c\\vec x_1 + (1-c)\\vec x_2)\\\\ &amp;= \\frac12\\big[ c^2\\vec x_1^TQ\\vec x_1 + c(1-c)\\vec x_1^TQ\\vec x_2+c(1-c)\\vec x_2^TQ\\vec x_1 + (1-c)^2\\vec x_2^TQ\\vec x_2 \\big] \\\\ &amp;\\quad- \\begin{bmatrix}0\\\\1\\end{bmatrix}(c\\vec x_1 + (1-c)\\vec x_2) \\end{align*}\\] <p>Therefore, </p> \\[\\begin{align*} &amp;\\quad cf_a(\\vec x_1) + (1-c)f_a(\\vec x_2) - f_a(c_x1+(1-c)x_2)\\\\ &amp;=\\frac{1}{2}(c-c^2)\\vec x_1^TQx_1 - c(1-c)\\vec x_1^TQ\\vec x_2-c(1-c)\\vec x_2^TQ\\vec x_1 +((1-c)-(1-c)^2)\\vec x_2^TQ\\vec x_2\\\\ &amp;= \\frac12c(1-c)[x_1^TQx_1-x_1^TQx_2-x_2^TQx_1+x_2^TQx_2]\\\\ &amp;= \\frac12c(1-c)(\\vec x_1-\\vec x_2)^TQ(\\vec x_1-\\vec x_2) \\end{align*}\\] <p>Note that when \\(a\\in(-2\\sqrt 2, 2\\sqrt 2)\\), \\(Q\\) is positive semidefinite, hence \\(\\frac12c(1-c)(\\vec x_1-\\vec x_2)^TQ(\\vec x_1-\\vec x_2) \\geq 0\\). Therefore, \\(cf_a(\\vec x_1) + (1-c)f_a(\\vec x_2) \\geq f_a(c_x1+(1-c)x_2)\\). By the definition of convex function, \\(f_a\\) is convex, and the local minimum is the global minimum.</p>"},{"location":"apm462/ufdoq.html#example-2","title":"Example 2","text":"<p>Question</p> <p>Find the local minimum point(s) for \\(f(x, y,z ) = (x-\\frac y2)^2 + \\frac34(y-2)^2 + z^2 -3\\) on \\(S = \\{(x,y,z)\\in\\mathbb R^3 \\mid x\\leq 0, y \\geq 0\\}\\), and prove the local minimum is actually a global minimum.</p> <p>First of all, let \\(g(x, y) = f(x, y, 0) = (x-\\frac y2)^2 + \\frac34(y-2)^2-3\\). Obsever that \\(\\forall z\\in\\mathbb R. z^2 \\geq 0\\), hence minimizes \\(f\\) is equivalent to minimize \\(g\\) as having \\(z=0\\). </p> <p>First, the partial derivative</p> \\[\\nabla g = (2x-y, 2y-x-3)\\] <ol> <li>\\(x &lt; 0, y &gt; 0\\)  solves the system of equation that \\(\\nabla f = \\vec0\\), </li> </ol> \\[x=1, y=2, z =0\\] <p>does not lie in the set, there is no local min in the interior of \\(S\\)</p> <ol> <li>\\(x = 0, y &gt; 0\\) Let the feasible direction be \\(v = (v_1, v_2), v_1 \\leq 0\\) and want </li> </ol> \\[\\nabla f \\cdot v = (2\\cdot 0-y)v_1 + (2y-0-3)v_2 =-yv_1 + (2y-3)v_2\\geq 0\\] <p>Note that for any \\(v_1 \\leq 0, -yv_1 \\geq 0\\) so that the condition is equivalent to have \\(2y-3 = 0\\Rightarrow y = \\frac32\\), and the candidate is \\((0, \\frac32)\\)</p> <ol> <li>\\(x &lt; 0, y = 0\\) Let the feasible direction be \\(v = (v_1, v_2), v_2 \\geq 0\\) and want </li> </ol> \\[\\nabla f \\cdot v = (2x-0)v_1 + (2\\cdot 0-x-3)v_2 =2xv_1 - (x+3)v_2\\geq 0\\] <p>Note that for direction \\((1/2, 1), \\nabla f\\cdot v = 2x\\cdot\\frac12 - (x + 3) = -3 &lt; 0\\) for any \\(x\\), hence there is no local min in this case. </p> <ol> <li>\\(x, y = 0\\) Let the feasible direction be \\(v = (v_1, v_2), v_1 \\leq 0, v_2 \\geq 0\\) and want </li> </ol> \\[\\nabla f \\cdot v = 0v_1 + (-3)v_2 = -3v_2\\geq 0\\] <p>does not hold for \\(v_2 \\geq 0\\), hence no local minimum.</p>"},{"location":"apm462/ufdoq.html#example-3","title":"Example 3","text":"<p>Question</p> <p>Show that \\(xx^T\\), where \\(x\\in\\mathbb R^n\\), is positive semidefinite.</p> <p>Let \\(x\\in\\mathbb R^n, a\\in\\mathbb R^n\\).  </p> \\[\\begin{align*} a^T(xx^T)a &amp;= (a^Tx)(x^Ta)\\\\ &amp;= (x^Ta)^T(x^Ta) &amp;\\text{take transpose}\\\\ &amp;= \\|x^Ta\\|^2\\\\ &amp;\\geq 0 \\end{align*}\\] <p>Therefore, by definition of positive semidefinite, \\(xx^T\\) is positive semidefinite </p>"},{"location":"apm462/ufdoq.html#example-4","title":"Example 4","text":"<p>Question</p> <p>Let \\(f(x) = b\\cdot Ax\\), \\(A\\) is \\(n\\times m\\) matrix, \\(x\\in\\mathbb R^m, b\\in\\mathbb R^n\\), show that \\(\\nabla f(x)=A^Tb\\).</p> <p>First, note that</p> \\[\\begin{align*} b\\cdot Ax &amp;=  \\begin{bmatrix}b_1\\\\\\vdots\\\\b_n \\end{bmatrix}\\cdot  \\begin{bmatrix} A_{11}&amp;\\cdots&amp;A_{1m}\\\\ \\vdots&amp;\\ddots&amp;\\vdots\\\\ A_{n1}&amp;\\cdots&amp;A_{nm} \\end{bmatrix} \\begin{bmatrix}x_1\\\\\\vdots\\\\x_m \\end{bmatrix}\\\\ &amp;= \\begin{bmatrix}b_1\\\\\\vdots\\\\b_n \\end{bmatrix} \\cdot  \\begin{bmatrix}\\sum_{i=1}^mA_{1i}x_i\\\\\\vdots\\\\\\sum_{i=1}^mA_{ni}x_i \\end{bmatrix}\\\\ &amp;= \\sum_{j=1}^n b_j\\sum_{i=1}^m A_{ji}{x_i} \\end{align*}\\] <p>Therefore, for each component \\(x_i\\), we can easily derive the partial derivative as </p> \\[\\frac{\\partial f}{\\partial x_i} =\\sum_{j=1}^n b_j A_{ji}\\] <p>and so that \\(\\nabla f = \\begin{bmatrix}\\sum_{j=1}^n b_j A_{j1}\\\\\\vdots\\\\\\sum_{j=1}^n b_j A_{jm}\\end{bmatrix} = A^Tb\\)</p> <p>Part (b)</p> <p>Let \\(f(x) = x\\cdot Ax\\), show that \\(\\nabla f(x) = (A+A^T)x\\).</p> \\[\\begin{align*} \\nabla (x^TAx) &amp;= \\nabla(\\sum_{i=1}^n \\sum_{j=1}^n A_{ij}x_ix_j)\\\\ &amp;= \\begin{bmatrix}\\sum_{i=1}^n A_{i1}x_i + \\sum_{j=1}^n A_{1j}x_j\\\\...\\\\\\sum_{i=1}^n A_{in}x_i + \\sum_{j=1}^n A_{nj}x_j\\end{bmatrix}\\\\ &amp;= Ax + A^Tx\\\\ &amp;= (A+A^T)x \\end{align*}\\]"},{"location":"apm462/ufdoq.html#example-5","title":"Example 5","text":"<p>Question</p> <p>Let \\(f:\\mathbb R^{2n} \\rightarrow \\mathbb R, f(x, y) = \\frac12|Ax-By|^2\\), where \\(A,B\\) are \\(m\\times n\\) matrices, \\(x, y\\in\\mathbb R^n\\)</p> <p>Part (a)</p> <p>Find \\(\\nabla f, \\nabla^2f\\)</p> <p>Note that \\(f(x, y) = \\frac12 (Ax-By)^T(Ax-By)\\), let \\(g(x, y) = Ax-By, h(a) = a^Ta\\) so that \\(f(x, y) = \\frac12h(g(x, y))\\). Therefore, using chain rule, </p> \\[D f(x,y) = \\frac12D (h\\circ g)(x,y) = \\frac12Dh(g(x,y)) \\cdot Dg(x,y)\\] <p>Note that \\(h(a) = a^T a = a^TI a\\) where \\(I\\) is the identity matrix, so that by Question 4 Part (b), \\(Dh(a) = (I+I^T)a = 2a\\). Then, note that \\(\\frac{\\partial g}{\\partial x} = A, \\frac{\\partial g}{\\partial y} = -B\\), hence \\(Df(x, y) = \\begin{bmatrix}[A]&amp;[-B]\\end{bmatrix}\\), i.e. matrix \\(A, -B\\) stacked horizontally. Therefore, </p> \\[\\nabla f(x, y) = \\frac12 \\cdot 2 [Ax-By]\\cdot \\begin{bmatrix}[A]\\\\ [-B]\\end{bmatrix} = \\begin{bmatrix}[A]&amp; [-B]\\end{bmatrix}^T(Ax-By)\\] <p>Then, note that \\((Ax-By)^T\\begin{bmatrix}[A]&amp; [-B]\\end{bmatrix} = \\begin{bmatrix}[A]&amp; [-B]\\end{bmatrix}^T Ax - \\begin{bmatrix}[A]&amp; [-B]\\end{bmatrix}^TBy\\). Therefore, </p> \\[\\frac{\\partial}{\\partial x} \\begin{bmatrix}[A]&amp; [B]\\end{bmatrix}^T Ax - \\begin{bmatrix}[A]&amp; [B]\\end{bmatrix}^TBy = \\begin{bmatrix}[A]&amp; [-B]\\end{bmatrix}^T A\\] \\[\\frac{\\partial}{\\partial y} \\begin{bmatrix}[A]&amp; [B]\\end{bmatrix}^T Ax - \\begin{bmatrix}[A]&amp; [B]\\end{bmatrix}^TBy = \\begin{bmatrix}[-A]&amp; [B]\\end{bmatrix}^T B\\] <p>Rewrite into matrix form, </p> \\[\\nabla^2 f = \\begin{bmatrix}[A^TA]&amp; [-B^TA]\\\\ [-A^TB] &amp; [B^TB]\\end{bmatrix}\\] <p>Part (b)</p> <p>If \\((x_0, y_0)\\) satisfies \\(Ax_0 = By_0\\), then \\((x_0, y_0)\\) is a local minimum.</p> <p>Note that \\(\\nabla f(x_0, y_0) = \\begin{bmatrix}[A]&amp; [B]\\end{bmatrix}^T(Ax_0-By_0) = \\begin{bmatrix}[A]&amp; [B]\\end{bmatrix}^T0 = 0\\) which satisfies FOC.  </p> <p>Also, note that the Hessian matrix can be rewrite as </p> \\[\\nabla^2 f = \\begin{bmatrix}[A^TA]&amp; [-B^TA]\\\\ [-A^TB] &amp; [B^TB]\\end{bmatrix} = \\begin{bmatrix}A\\\\-B\\end{bmatrix}^T\\begin{bmatrix}A\\\\-B\\end{bmatrix}\\] <p>so that it is a positive semidefinite matrix, which satisfy the SOC.</p>"},{"location":"apm462/ufdoq.html#example-6","title":"Example 6","text":"<p>Question</p> <p>Let \\(g\\) be a convex function on \\(\\mathbb R^n\\), \\(f\\) be a linear, nondecreasing function on a single variable.</p> <p>Part (a)</p> <p>Prove \\(F:=f\\circ g\\) is convex.</p> \\[\\begin{align*} F(\\theta x + (1-\\theta) y) = f(g(\\theta x + (1- \\theta) y))) \\end{align*}\\] <p>By convexity of \\(g\\), </p> \\[g(\\theta x + (1- \\theta) y) \\leq \\theta g(x) + (1-\\theta)g(y)\\] <p>By non-decreasing of \\(f\\)</p> \\[f(g(\\theta x + (1- \\theta) y)) \\leq f(\\theta g(x) + (1-\\theta)g(y))\\] <p>By linearity of \\(f\\)</p> \\[f(\\theta g(x) + (1-\\theta)g(y)) = \\theta f(g(x)) + (1-\\theta) f(g(y)) = \\theta F(x) + (1-\\theta) F(y)\\] <p>By the definition of convex, the claim is proven. </p> <p>Part (b)</p> <p>\\(\\nabla^2F(x)\\) is positive semidefinite. </p> \\[\\begin{align*} \\nabla^2F(x) &amp;= \\frac{\\partial}{\\partial x}(\\frac{\\partial f}{\\partial g}\\cdot \\frac{\\partial g}{\\partial x}) &amp;\\text{chain rule}\\\\ &amp;= (\\frac{\\partial}{\\partial x}\\frac{\\partial f}{\\partial g})\\cdot \\frac{\\partial g}{\\partial x} + \\frac{\\partial f}{\\partial g}\\cdot(\\frac{\\partial }{\\partial x}\\frac{\\partial g}{\\partial x})&amp;\\text{product rule}\\\\ &amp;= (\\frac{\\partial^2 f}{\\partial g^2}\\cdot  \\frac{\\partial g}{\\partial x})\\cdot \\frac{\\partial g}{\\partial x} + \\frac{\\partial f}{\\partial g}\\cdot \\frac{\\partial^2 g}{\\partial x^2} &amp;\\text{chain rule} \\end{align*}\\] <p>Rewrite the derivatives with the matrix multiplication notation</p> \\[\\begin{align*} \\nabla^2 F(x) &amp;= [d^2 f(g(x)) \\nabla g(x)]\\nabla g(x)^T + d f(g(x))\\nabla^2 g(x)\\\\ &amp;= d^2 f(g(x))\\nabla g(x)\\nabla g(x)^T +  df(g(x))\\nabla^2 g(x) \\end{align*}\\] <p>Because \\(f\\) is linear, \\(d^2f(g(x)) = 0\\) Because \\(f\\) is non-decreasing, \\(df(g(x)) \\geq 0\\) Because \\(g\\) is convex, \\(\\nabla^2 g(x)\\) is positive semidefinite Also, note that \\(\\nabla g(x) \\nabla g(x)^T\\) is positive semidefinite Therefore, a positive semidefinite matrix scaled by a positive number is still positive semidefinite. </p>"},{"location":"apm462/ufdoq.html#example-7","title":"Example 7","text":"<p>Question</p> <p>\\(f:\\mathbb R^2\\rightarrow \\mathbb R\\) is non-negative convex function, \\(F:\\mathcal A\\rightarrow \\mathbb R, F(\\mu) = \\int_0^1f(\\mu(x), \\mu'(x))dx\\) where \\(\\mathcal A = \\{\\mu\\in C^1: [0,1]\\rightarrow\\mathbb R\\}\\). Prove \\(F\\) is convex on \\(\\mathcal A\\).</p> <p>Let \\(a\\in (0, 1), u_1, u_2\\in \\mathcal A\\). </p> \\[F(au_1 + (1-a)u_2) = \\int_0^1f(au_1(x) + (1-a)u_2(x), (au_1 + (1-a)u_2)'(x)))dx\\] <p>Using chain rule</p> \\[= \\int_0^1f(au_1(x) + (1-a)u_2(x), au_1'(x) + (1-a)u_2'(x))dx\\] <p>Note that for any \\(x\\in [0, 1]\\), by convexity of \\(f\\)</p> \\[f(au_1(x) + (1-a)u_2(x), au_1'(x) + (1-a)u_2'(x)) \\leq af(u_1(x), u_1'(x)) + (1-a) f(u_2(x), u_2'(x))\\] <p>Because \\(f\\) is non-negative</p> \\[\\begin{align*} &amp;\\quad\\int_0^1 f(au_1(x) + (1-a)u_2(x), au_1'(x) + (1-a)u_2'(x))dx\\\\ &amp;\\leq \\int_0^1 af(u_1(x), u_1'(x)) + (1-a) f(u_2(x), u_2'(x))dx\\\\ &amp;= a\\int_0^1 f(u_1(x), u_1'(x))dx + (1-a)\\int_0^1 f(u_2(x), u_2'(x))dx\\\\ &amp;= aF(u) + (1-a)F(u) \\end{align*}\\]"},{"location":"apm462/ufdoq.html#example-8","title":"Example 8","text":"<p>Question</p> <p>If \\(f:\\Omega\\rightarrow \\mathbb R\\) is covex on \\(\\Omega=(a,b)\\), then \\(f\\) is also continuous. </p> <p>lemma If \\(f:\\Omega\\rightarrow\\mathbb R\\) is convex, then \\(\\forall x_1, x, x_2 \\in \\Omega, x_1\\leq x\\leq x_2. \\frac{f(x) - f(x_1)}{x-x_1} \\leq \\frac{f(x_2) - f(x_1)}{x_2-x_1} \\leq \\frac{f(x_2) - f(x)}{x_2-x}\\).  </p> <p>proof. Let \\(x_1, x, x_2\\in \\Omega. x_1 &lt; x &lt; x_2\\), note that \\(\\frac{x_2 - x}{x_2 - x_1} \\in [0, 1]\\) Since \\(f\\) is convex, </p> \\[f(x) = f(\\frac{x_2 - x}{x_2 - x_1}x_1 + \\frac{x-x_1}{x_2 - x_1} x_2) \\leq \\frac{x_2 - x}{x_2 - x_1}f(x_1) + \\frac{x- x_1}{x_2 - x_1}f(x_2)\\] <p>Then, the inequalities can be easily derived as </p> \\[\\begin{align*} \\frac{f(x) - f(x_1)}{x-x_1} &amp;\\leq \\frac{1}{x-x_1}\\big[\\frac{x_2 - x}{x_2 - x_1}f(x_1) + \\frac{x- x_1}{x_2 - x_1}f(x_2) - f(x_1)\\big]\\\\ &amp;= \\frac1{x-x_1}\\frac{x-x_1}{x_2-x_1}(f(x_2)-f(x_1))\\\\ &amp;= \\frac{f(x_2) - f(x_1)}{x_2-x_1} \\end{align*}\\] <p>Similar derivation holds for </p> \\[\\frac{f(x_2) - f(x)}{x_2 - x} \\geq \\frac{f(x_2) - f(x_1)}{x_2-x_1}\\] <p>Claim If \\(f\\) is convex, then \\(\\forall x_0\\in (a, b), \\lim_{x\\rightarrow x_0} f(x) = f(x_0)\\) (\\(f\\) is continuous using the limit definition). </p> <p>proof. Let \\(c, d \\in (a, b), a&lt;c&lt;x_0 &lt; d&lt;b\\). Take functions \\(l_1(x) = \\frac{f(x_0) - f(c)}{x_0 - c}(x-x_0) + f(x_0), l_2(x) = \\frac{f(d) - f(x_0)}{d - x_0}(x-x_0) + f(x_0)\\), where \\(l_1\\) is the line pass through \\((c, x_0)\\) and \\(l_2\\) is the line pass through \\((x_0, d)\\). Then, for any \\(x\\in (x_0, d)\\), since \\(f\\) is convex and use our lemma above, we have </p> \\[\\frac{f(x - x_0)}{x-x_0} \\leq \\frac{f(d) - f(x_0)}{d - x_0}\\] \\[\\frac{f(x) - f(c)}{x-c} \\geq \\frac{f(x_0) - f(c)}{x_0 - c}\\] <p>so that </p> \\[f(x) = \\frac{f(x - x_0)}{x-x_0}(x-x_0) + f(x_0) \\leq \\frac{f(d - x_0)}{d-x_0}(x-x_0) + f(x_0) = l_2(x)\\] \\[f(x) = \\frac{f(x) - f(c)}{x-c}(x-x_0) + f(x_0) \\geq \\frac{f(x_0-c)}{x_0-c}(x-x_0) + f(x_0) = l_1(x)\\] <p>Since \\(\\forall x\\in (x_0, d), l_1(x)\\leq f(x) \\leq l_2(x)\\) and \\(\\lim_{x\\rightarrow x_0+}l_1(x) = l_1(x_0) = f(x_0) = l_2(x_0) = \\lim_{x\\rightarrow x_0+}l_2(x)\\), by squeeze theorem</p> \\[\\lim_{x\\rightarrow x_0+}f(x) = f(x_0)\\] <p>With the similar arguments, we can show that \\(\\forall x\\in (c, x_0), l_2(x) \\leq f(x) \\leq l_1(x)\\), and use squeeze theorem, </p> \\[\\lim_{x\\rightarrow x_0-}f(x) = f(x_0)\\] <p>Finally, the two limits from both sides conclude that </p> \\[\\lim_{x\\rightarrow x_0}f(x) = f(x_0)\\] <p>Therefore, we have shown that \\(\\forall x\\in (a, b), \\lim_{x\\rightarrow x_0}f(x) = f(x_0)\\), which means \\(f\\) is continuous on \\((a,b)\\)</p>"},{"location":"apm462/ufdoq.html#example-9","title":"Example 9","text":"<p>Question</p> <p>If \\(f:\\Omega\\rightarrow \\mathbb R\\) is continuous and convex and exists some maximum on the interior of \\(\\Omega\\), then \\(f\\) is a constant function.</p> <p>proof. Let \\(x_0 \\in \\Omega_{int}\\) where \\(f(x_0)\\) is the maximum. Assume \\(f\\) is not constant. Take \\(x_1 \\in \\Omega\\) s.t. \\(f(x_1) &lt; f(x_0)\\).  Since \\(x_0\\) is an interior point, take some \\(t\\in(0, 1)\\) s.t. \\(x_2 = x_0 - t(x_1 - x_0), x_2 \\in B(x_0, \\epsilon)\\subset \\Omega\\) for some \\(\\epsilon &gt; 0\\). Then, \\(x_2, x_0, x_1\\) forms a line and \\(x_0 = \\frac{t}{1+t}x_1 + \\frac{1}{1+t}x_2\\).  By our assumption, \\(f(x_1) &lt; f(x_0), f(x_2) \\leq f(x_0)\\), hence exists \\(c = \\frac{1}{t} \\in (0, 1)\\)</p> \\[\\frac{1}{1+t}f(x_1) + \\frac{t}{1+t}f(x_2) &lt; \\frac{1}{1+t}f(x_0) + \\frac{t}{1+t}f(x_0) = f(x_0)\\] <p>This contradicts with the fact that \\(f\\) is convex, by contradiction, \\(f\\) must be a constant function. </p>"},{"location":"apm462/ufdoq.html#example-10","title":"Example 10","text":"<p>Question</p> <p>Let \\(f: \\Omega\\rightarrow \\mathbb R, f(x): a\\cdot x + b\\) where \\(\\Omega\\) is compact and convex subset of \\(\\mathbb R^n\\).</p> <p>Part (a)</p> <p>If \\(a\\neq 0\\), then any minimizer of \\(f\\) must be on \\(\\partial \\Omega\\).</p> <p>proof. Suppose exists some minimizer \\(x_0 \\in \\Omega_{int}\\), then take some \\(t &gt; 0\\) s.t. \\(x = x_0 - ta \\in B(x_0, \\epsilon) \\subset \\Omega\\) </p> \\[f(x) = a\\cdot (x_0 - ta) + b = a\\cdot x_0 - t\\|a\\|^2 + b = f(x_0) - t\\|a\\|^2\\] <p>If \\(a\\neq 0\\), then \\(t\\|a\\|^2 &gt; 0, f(x) &lt; f(x_0)\\), \\(x_0\\) is not a minimizer. By contradiction, the minimizer must be on \\(\\partial \\Omega\\). </p> <p>Part (b)</p> <p>Suppose \\(g(x) = \\|x\\|^2 + f(x)\\), under what condition of \\(a\\) can you guarantee that the minimizers do not occur in the interior of the set \\(\\Omega\\)?</p> <p>Note that \\(\\nabla g(x) = 2x + a\\). Note that a point \\(x_*\\) is not minimizer means that exists a feasible direction \\(d \\in \\mathbb R^n\\) s.t. \\(\\nabla g(x_*) &lt; 0\\). Because \\(\\|x\\|^2 + f(x)\\) is continuous on \\(\\mathbb R^n\\), for some interior point \\(x_*\\), all directions are feasible, therefore \\(x_*\\) is not a minimizer implies that \\(\\nabla g(x_*) =2x_* + a \\neq 0\\). Therefore, to guarantee that any interior point is not a minimizer, we want \\(a\\) to satisfy that \\(\\forall x\\in\\Omega_{int}, 2x +a \\neq 0\\)</p>"},{"location":"apm462/ufdoq.html#example-11","title":"Example 11","text":"<p>Question</p> <p>If \\(f(x):\\mathbb R^n\\rightarrow\\mathbb R\\) is convex, then \\(g(x, z) : \\mathbb R^n \\times \\mathbb R \\rightarrow \\mathbb R, g(x, z) := f(x) + \\|x+z\\|^2\\) is also convex.</p> <p>proof.  Let \\(x_1, z_1, x_2, z_2\\in \\mathbb R^n, c\\in [0, 1]\\), consider </p> \\[\\begin{align*} g(c(x_1, z_1) + (1-c)(x_2, z_2)) &amp;= f(cx_1 + (1-c)x_2) + \\|cx_1  + (1-c)x_2 + cz_1 + (1-c)z_2\\|^2\\\\ &amp;\\leq cf(x_1) + (1-c)f(x_2) &amp;f\\text{ is convex}\\\\ &amp;\\quad + \\|cx_1 + cz_1\\|^2 + \\|(1-c)x_2 + (1-c)z_2\\|^2 &amp;\\text{triangular inequality}\\\\ &amp;= cf(x_1) + c\\|x_1+z_1\\|^2  + (1-c)f(x_2) + (1-c)\\|x_2 + z_2\\|^2\\\\ &amp;= cf(x_1, z_1) + (1-c)f(x_2, z_2) \\end{align*}\\] <p>By definition of convexity, \\(g\\) is convex</p>"},{"location":"apm462/ufdoq.html#example-12","title":"Example 12","text":"<p>Question</p> <p>For \\(f: \\mathbb R^n \\rightarrow \\mathbb R\\) be \\(C^1\\) function, define \\(M = \\{(x, f(x))\\in \\mathbb R^{n+1}\\}\\), given \\(p = (x_0, f(x_0)) \\in M\\), find the tangent space \\(T_pM\\).</p> <p>Define \\(g(x, z) = f(x) - z\\), note that \\(\\nabla g(x, z) = [\\nabla f(x), -1]\\in\\mathbb R^{n}\\times \\mathbb R\\). Then, note that \\(g(p) = 0\\) and the equation of the tangent plane where \\(p\\) is on the plane is given as </p> \\[\\begin{align*} \\nabla g(x_0, f(x_0))\\cdot ((x, z) - (x_0, f(x_0))) &amp;= 0\\\\ \\nabla f(p)\\cdot(x-x_0) + (-1)(z-f(x_0))&amp;= 0\\\\ \\nabla f(p) \\cdot(x-x_0) + f(x_0)&amp;=z \\end{align*}\\] <p>Therefore, the tangent space is given as </p> \\[T_pM = \\{(x, \\nabla f(p)\\cdot(x-x_0) + f(x_0): x\\in\\mathbb R^n\\}\\]"},{"location":"apm462/uo_functional.html","title":"Unconstrainted Optimization on Calulus of Variations","text":""},{"location":"apm462/uo_functional.html#introducing-the-calculus-of-variations","title":"Introducing The Calculus of Variations","text":"<p>Consider the problem</p> \\[\\begin{align*} \\text{minimize } &amp;F[u] \\\\ &amp;u \\in \\mathcal{A}, \\end{align*}\\] <p>where \\(\\mathcal{A}\\) is a set of functions. Here, \\(F\\) is a function of functions, often called a Functional. This is the general unconstrained calculus of variations problem.</p> <p>For example, consider</p> \\[\\mathcal{A} = \\{ u \\in C^1([0, 1], \\mathbb R) : u(0) = u(1) = 1  \\}\\] <p>Define \\(F : \\mathcal{A} \\to \\mathbb R\\) by</p> \\[F[u(\\cdot)] := \\frac{1}{2} \\int_0^1 (u(x)^2 + u'(x)^2) \\, dx\\] <p>To solve the minimization problem</p> \\[\\begin{align*} \\text{minimize } &amp;F[u] \\\\ &amp;u \\in \\mathcal{A} \\end{align*}\\] <p>is to find a \\(u^* \\in \\mathcal{A}\\) such that \\(F[u^*] \\leq F[u]\\) for all \\(u \\in \\mathcal{A}\\).</p>"},{"location":"apm462/uo_functional.html#thrm-fundamental-lemma-of-calculus-of-variations","title":"Thrm. Fundamental Lemma of Calculus of Variations","text":"<p>Claim. Suppose \\(g \\in C^0([a,b])\\). If \\(\\int_a^b g(x)v(x) \\, dx = 0\\) for all test functions \\(v\\) on \\([a,b]\\), then \\(g \\equiv 0\\) on \\([a,b]\\).</p> <p>proof. Suppose \\(g\\not\\equiv 0\\). Then there is an \\(x_0 \\in (a,b)\\) such that \\(g(x_0) \\neq 0\\). (We can ensure that \\(x_0\\) is in the interior of the interval because of continuity.) Assume without loss of generality that \\(g(x_0) &gt; 0\\). There exists an open neighbourhood \\((c,d)\\) of \\(x_0\\) inside \\((a,b)\\) on which \\(g\\) is positive. Let \\(v\\) be a \\(C^1\\) function on \\([a,b]\\) such that \\(v &gt; 0\\) on \\((c,d)\\) and \\(v = 0\\) otherwise. Then \\(v\\) is a test function on \\([a,b]\\), so by the hypotheses, \\(0 = \\int_a^b g(x)v(x) \\, dx = \\int_c^d g(x)v(x) \\, dx &gt; 0\\) a contradiction.</p> <p>Intuitions. Fix \\(v \\in C^1([0,1],\\mathbb R)\\) with \\(v(0) = v(1) = 0\\). Suppose \\(u^*\\) is a minimizer of \\(F\\) on \\(\\mathcal{A}\\). Clearly \\(u^* + sv \\in \\mathcal{A}\\) for all \\(s \\in \\mathbb R\\). Define \\(f : \\mathbb R \\to \\mathbb R\\) by \\(f(s) := F[u^* + sv]\\). Then \\(f(s) \\geq f(0)\\) for all \\(s\\), since \\(u^*\\) is a minimizer of \\(F\\). Then \\(0\\) is a minimizer of \\(f\\), implying \\(f'(0) = 0\\). How do we actually compute \\(f'(0)\\)? Since</p> \\[\\begin{align*} f(s) &amp;= \\frac{1}{2} \\int_0^1 (u^*(x) + sv(x))^2 + (u^{*'}(x) + sv'(x))^2 \\, dx \\\\ &amp;= \\frac{1}{2} \\int_0^1 (u^*(x)^2 + u^{*'}(x)^2) \\, dx + s\\int_0^1 ( u^*(x)v(x) + u^{*'}(x)v'(x) ) \\, dx + \\frac{s^2}{2} \\int_0^1 (v(x)^2 + v'(x)^2) \\, dx, \\end{align*}\\] <p>implying that</p> \\[f'(s) = \\int_0^1 ( u^*(x)v(x) + u^{*'}(x)v'(x) ) \\, dx + s\\int_0^1 (v(x)^2 + v'(x)^2) \\, dx\\] <p>or</p> \\[0 = \\int_0^1 ( u^*(x)v(x) + u^{*'}(x)v'(x) ) \\, dx \\text{ for all } v \\in C^1([0,1], \\mathbb R) \\text{ such that } v(0)=v(1)=0 \\:(*)\\] <p>The above equality holds for all \\(v \\in C^1([0,1], \\mathbb R)\\) such that \\(v(0)=v(1)=0\\). This is a primitive form of the first order necessary conditions.</p> <p>Let us call the functions \\(v\\) described in \\((*)\\) the test functions on \\([0,1]\\). We would like to write \\((*)\\) in a more useful way. Let us make the simplifying assumption that \\(u^*\\) is \\(C^2\\). Integration by parts gives</p> \\[\\int_0^1 u^{*'}(x)v'(x) \\, dx = \\underset{=0}{\\left. u^{*'}(x)v(x) \\right|_0^1} - \\int_0^1 u^{*''}(x) v(x) \\, dx = \\int_0^1 u^{*''}(x) v(x) \\, dx\\] <p>By substituting this into \\((*)\\) we obtain</p> \\[\\int_0^1 (u^*(x)v(x) - u^{*''}(x)v(x))\\, dx = 0\\] <p>Factor the common \\(v\\) out to get</p> \\[\\int_0^1 (u^*(x) - u^{*''}(x))v(x) \\, dx = 0 \\text{ for all test functions } v \\text{ on } [0,1]\\] <p>So we have a continuous function \\(u^*(x) - u^{*''}(x)\\) that is zero whenever \"integrated against test functions\". We claim that any function satisfying this condition must be zero. This result or its variations is called the fundamental lemma of the calculus of variations. We shall show that \\(u^* = u^{*''}\\) on \\([0,1]\\); this gives us the first order necessary conditions we wanted in the first place.</p>"},{"location":"apm462/uo_functional.html#defn-variational-derivative","title":"Defn. Variational Derivative","text":"<p>Given \\(u \\in \\mathcal{A}\\), suppose there is a function \\(g : [a,b] \\to \\mathbb R\\) such that </p> \\[\\left. \\frac{d}{ds} \\right|_{s = 0} F[u + sv] = \\int_a^b g(x) v(x) \\, dx\\] <p>for all test functions \\(v\\) on \\([a,b]\\). Then \\(g\\) is called the variational derivative of \\(F\\) at \\(u\\). We denote the function \\(g\\) by \\(\\frac{\\delta F}{\\delta u}(u)\\).</p> <p>We can think of \\(\\frac{\\delta F}{\\delta u}(u)\\) as an analogue of the gradient. We have</p> \\[\\left. \\frac{d}{ds} \\right|_{s = 0} F[u + sv] = \\int_a^b \\frac{\\delta F}{\\delta u}(u)(x) v(x) \\, dx\\] <p>for all test functions \\(v\\) on \\([a,b]\\). Compare this with the finite-dimensional formula</p> \\[\\left. \\frac{d}{ds} \\right|_{s=0} f(u + sv) = \\nabla f(u) \\cdot v = \\sum_{i=1}^n \\nabla f(u)_i v_i\\] <p>if one thinks of the integral as an \"infinite sum of infinitesimally small pieces\", then we can understand how the functional derivative might be an \"infinite-dimensional\" version of the gradient.</p>"},{"location":"apm462/uo_functional.html#euler-lagrange-equation-in-1-dim","title":"Euler Lagrange Equation in 1-Dim","text":"<p>Lemma (A corollary of Fundamental Lemma) Suppose \\(u_* \\in \\mathcal{A}\\) satisfies \\(u_* + v \\in \\mathcal{A}\\) for all test functions \\(v\\) on \\([a,b]\\). Then if \\(u_*\\) minimizes \\(F\\) on \\(\\mathcal{A}\\) and if \\(\\frac{\\delta F}{\\delta u}(u_*)\\) exists and is continuous, then \\(\\frac{\\delta F}{\\delta u}(u_*) \\equiv 0\\).</p>"},{"location":"apm462/uo_functional.html#thrm-euler-lagrange-equation","title":"Thrm. Euler-Lagrange equation","text":"<p>Claim. Suppose \\(L,u\\) are \\(C^2\\) functions. Then \\(\\frac{\\delta F}{\\delta u}(u)\\) exists, is continuous, and \\(\\frac{\\delta F}{\\delta u}(u)(x) = -\\frac{d}{dx} L_p(x,u(x), u'(x)) + L_z(x,u(x),u'(x))\\)</p> <p>proof. Let \\(v\\) be a test function on \\([a,b]\\). Then</p> \\[\\begin{align*} \\left. \\frac{d}{ds} \\right|_{s=0} F[u + sv] &amp;= \\left. \\frac{d}{ds} \\right|_{s=0} \\int_a^b L(x, u(x) + sv(x), u'(x) + sv'(x)) \\, dx \\\\ &amp;=  \\int_a^b \\left. \\frac{d}{ds} \\right|_{s=0} L(x, u(x) + sv(x), u'(x) + sv'(x)) \\, dx \\\\ &amp;= \\int_a^b \\left( L_x(\\cdots) \\frac{dx}{ds} + L_z(\\cdots)\\frac{d}{ds}(u(x) + sv(x)) + L_p(\\cdots)\\frac{d}{ds}(u'(x) + sv'(x)) \\right) \\, dx \\\\ &amp;= \\int_a^b \\left( L_z(x,u(x),u'(x))v(x) + L_p(x,u(x),u'(x))v'(x) \\right)\\, dx \\\\ &amp;= \\int_a^b L_z(x,u(x),u'(x))v(x) \\, dx + \\int_a^b L_p(x,u(x),u'(x))v'(x) \\, dx \\\\ &amp;= \\int_a^b \\left( -\\frac{d}{dx} L_p(x,u(x),u'(x)) + L_z(x,u(x),u'(x)) \\right) v(x) \\, dx \\qquad \\text{integration by parts.} \\end{align*}\\] <p>Since \\(u\\) and \\(L\\) are \\(C^2\\), the function in the integrand is continuous. By the definition of the variational derivative we have the desired result.</p>"},{"location":"apm462/uo_functional.html#thrm-dubois-raymond-lemma","title":"Thrm. DuBois-Raymond Lemma","text":"<p>The lemma allows us to relax the restrictions for the Euler-Lagrange equation to hold from twice continuously differentiable to only once continuously differentiable.</p> <p>Claim. Suppose \\(\\alpha, \\beta\\) are continuous functions on \\([a,b]\\) such that \\(\\int_a^b ( \\alpha(x)v(x) + \\beta(x)v'(x) ) \\, dx = 0\\) for all test functions \\(v\\) on \\([a,b]\\). Then \\(\\beta\\) is \\(C^1\\), and \\(\\beta' = \\alpha\\) on \\([a,b]\\).</p> <p>proof. Let \\(A(x) = \\int_a^x \\alpha(t) \\, dt\\) be an antiderivative of \\(\\alpha\\). Since \\(\\alpha\\) is continuous, \\(A\\) is \\(C^1\\). Then</p> \\[\\int_a^b \\alpha(x) v(x) \\, dx = \\int_a^b A'(x)v(x) \\, dx = -\\int_a^b A(x)v'(x) \\, dx\\] <p>By the original assumption,</p> \\[0 = \\int_a^b (\\alpha(x)v(x) + \\beta(x)v'(x)) \\, dx = \\int_a^b(-A(x) + \\beta(x))v'(x) \\, dx\\] <p>We are done if we are able to show that \\(-A(x) + \\beta(x)\\) is constant on \\([a,b]\\). Let \\(\\gamma = -A + \\beta\\). Define \\(C\\) to be the constant</p> \\[C := \\frac{1}{b-a}\\int_a^b \\gamma(t) \\, dt\\] <p>so that \\(\\int_a^b (\\gamma(t) - C) \\, dt = 0\\). Define \\(v(x) := \\int_a^x (\\gamma(t) - C) \\, dt\\). The function \\(v\\) is \\(C^1\\) since \\(\\gamma(t) - C\\) is continuous, and \\(v(a) = v(b) = 0\\); so \\(v\\) is a test function on \\([a,b]\\). By some algebra,</p> \\[\\int_a^b (\\gamma(x) - C)^2 \\, dx = \\int_a^b (\\gamma(x) - C)v'(x) \\, dx = 0\\] <p>Since \\((\\gamma(x) - C)^2 \\geq 0\\) on \\([a,b]\\), we must have \\(\\gamma(x) = C\\). Therefore \\(\\gamma\\) is constant, which proves the lemma.</p>"},{"location":"apm462/uo_functional.html#example","title":"Example","text":"<p>Consider two points \\((a,A), (b,B)\\) in \\(\\mathbb R^2\\) with \\(a &lt; b\\). We seek a function \\(u\\) on \\([a,b]\\) with \\(u(a) = A\\), \\(u(b) = B\\), and with</p> \\[F[u] := \\int_a^b \\sqrt{1 + u'(x)^2} \\, dx\\] <p>minimized. Denote by \\(\\mathcal{A}\\) the set of \\(C^1\\) functions \\(u\\) with \\(u(a) = A\\) and \\(u(b) = B\\). Suppose that \\(u_*\\) is a minimizer. Then, by the previous lemma applied to the last result of the previous lecture, \\(u_*\\) satisfies the Euler-Lagrange equation. Let \\(L(x,z,p) := \\sqrt{1 + p^2}\\). Then \\(L_z = 0\\), and</p> \\[L_p = \\frac{p}{\\sqrt{1 + p^2}}\\] <p>The Euler-Lagrange equation is, in this case,</p> \\[0 = -\\frac{d}{dx} L_p + L_z = -\\frac{d}{dx} \\frac{u_*'(x)}{\\sqrt{1 + u_*'(x))^2}} \\:(*)\\] <p>This implies that </p> \\[u_*'(x) = C\\sqrt{1 + u_*'(x)^2}\\] <p>for some constant \\(C\\), implying</p> \\[u_*'(x)^2 = C(1 + u_*'(x)^2) = C + C u_*'(x)^2\\] \\[\\implies u_*'(x)^2(1 - C) = C\\] <p>hence \\(u_*'\\) is constant, or \\(u_*(x) = \\alpha x + \\beta\\) for some constants \\(\\alpha, \\beta\\). As expected, the minimizer is a line. This answer is expected, since the shortest path joining two points is the line joining them.</p>"},{"location":"apm462/uo_functional.html#example-area-of-revolution","title":"Example: Area of Revolution","text":"<p>Suppose \\(u\\) is a \\(C^1\\) function on an interval \\([a,b]\\). Consider the surface of revolution obtained by rotating the graph of \\(u\\) on \\([a,b]\\) about the \\(x\\)-axis. Consider the functional</p> \\[F[u] := \\text{area of the surface of revolution obtained by rotating $\\Gamma_u$ about the $x$-axis}\\] <p>which is, by some calculus,</p> \\[F[u] = \\int_a^b 2\\pi u(x) \\sqrt{1 + u'(x)^2} \\, dx\\] <p>With the set \\(\\mathcal{A}\\) of functions defined as in the previous example, we seek to find a function \\(u_* \\in \\mathcal{A}\\) minimizing \\(F\\) on \\(\\mathcal{A}\\).</p> <p>In this example, the Lagrangian is \\(L(x,z,p) = 2\\pi z \\sqrt{1 + p^2}\\), which gives</p> \\[L_z = 2\\pi \\sqrt{1 + p^2}, L_p = \\frac{2\\pi z p}{\\sqrt{1 + p^2}}\\] <p>The Euler-Lagrange equation is, in this case,</p> \\[0 = -\\frac{d}{dx} L_p + L_z = -\\frac{d}{dx} \\left[ \\frac{2\\pi u(x)u'(x)}{\\sqrt{1 + u'(x)^2}} + 2\\pi \\sqrt{1 + u'(x)^2} \\right]\\] <p>Cancel the \\(2\\pi\\)'s to get the ODE</p> \\[\\left[ \\frac{u(x)u'(x)}{\\sqrt{1 + u'(x)^2}} + \\sqrt{1 + u'(x)^2} \\right] = 0 \\:(**)\\] <p>By magic, the general solution to this differential equation has the form</p> \\[u(x) = \\beta \\cosh\\left(\\frac{x - \\alpha}{\\beta} \\right)\\] <p>for some constants \\(\\alpha, \\beta\\). We won't argue why we got this solution, but we can differentiate it and check that it solves the ODE; uniqueness theorems give us what we want.</p> \\[u'(x) = \\beta \\sinh \\left( \\frac{x - \\alpha}{\\beta} \\right) \\frac{1}{\\beta} = \\sinh \\left( \\frac{x - \\alpha}{\\beta} \\right)\\] \\[\\implies \\sqrt{1 + u'(x)^2} = \\cosh \\left( \\frac{x - \\alpha}{\\beta} \\right)\\] <p>It is now obvious that \\(u\\) solves \\((**)\\). Therefore a minimizer \\(u_*\\) must be of the form</p> \\[u_*(x) = \\beta \\cosh \\left( \\frac{x - \\alpha}{\\beta} \\right)\\] <p>We may use the boundary conditions to find \\(\\alpha, \\beta\\).</p> <p>Consider the special case \\((a,A) = (0, 1)\\) and \\((b, B) = (1,0)\\). The boundary conditions on \\(u_*\\) give us the system</p> \\[\\begin{align*} \\beta \\cosh \\left( \\frac{x - \\alpha}{\\beta} \\right) &amp;= 1 \\\\ \\beta \\cosh\\left( \\frac{1 - \\alpha}{\\beta} \\right) &amp;= 0. \\end{align*}\\] <p>Since \\(\\cosh\\) is strictly positive, the second equation gives us \\(\\beta = 0\\), a contradiction. We conclude that there is no \\(C^1\\) minimizer in this special case.</p>"},{"location":"apm462/uo_functional.html#euler-lagrange-equation-in-n-dim","title":"Euler-Lagrange Equation in N-Dim","text":"<p>We consider a functional</p> \\[F[u] := \\int_a^b L(x, u(x), u'(x)) \\, dx\\] <p>where \\(u : [a,.b] \\to \\mathbb R^n\\). In this case,</p> \\[L : [a, b] \\times \\mathbb R^n \\times \\mathbb R^n \\to \\mathbb R\\] <p>Our general space of functions will be denoted by </p> \\[\\mathcal{A} := \\{ u : [a,b] \\to \\mathbb R^n : u \\in C^1, u(a) = A, u(b) = B \\}.\\] <p>The Euler-Lagrange equation from the real-valued case generalizes to </p> \\[\\begin{align*} -\\frac{d}{dx} \\nabla_p L(x, u_*(x), u_*'(x)) + \\nabla_z L(x, u_*(x), u_*'(x)) = 0. \\end{align*}\\] <p>The proof is a straightforward generalization of the proof given when \\(n = 1\\).</p>"},{"location":"apm462/uo_functional.html#example-newtons-second-law","title":"Example: Newton's Second Law","text":"<p>Let us consider an example from classical mechanics. We consider the physical situation of a point mass moving in a potential field. Denote by \\(V(x)\\) the potential energy at a point \\(x\\). The kinetic energy of a point of mass \\(m\\) with velocity \\(v\\) is \\(\\frac{1}{2} m |v|^2\\). Define the Lagrangian</p> \\[L(t,x,v) = \\frac{1}{2} m |v|^2 - V(x)\\] <p>as the difference between the kinetic and potential energies. Suppose our particle is moving along a path \\(x = x(t)\\) parametrized by time in \\(\\R^n\\). Our functional is</p> \\[F[x] := \\int_{t_1}^{t_2} \\left( \\frac{1}{2} m |\\dot{x}(t)|^2 - V(x(t)) \\right) \\, dt\\] <p>This represents the difference between the kinetic energy and the potential energy \\emph{along the entire path}. We can think of it as the net change in energy from the kinetic energy to potential energy along the path.</p> <p>In this case, the Euler-Lagrange equations for a a minimizing path \\(x(t)\\) are</p> \\[0 = -\\frac{d}{dt} \\nabla_v L(t, x(t), \\dot{x}(t)) + \\nabla_x L(t, x(t), \\dot{x}(t))\\] <p>One computes that </p> \\[\\begin{align*} \\nabla_v L(t,x,v) &amp;= mv \\\\ \\nabla_x L(t,x,v) &amp;= -\\nabla V(x). \\end{align*}\\] <p>Then the Euler-Lagrange equations are</p> \\[{m \\ddot{x}(t) = -\\nabla V(x).}\\]"},{"location":"apm462/uo_functional_q.html","title":"Examples: Unconstrainted Optimization on Calulus of Variations","text":""},{"location":"apm462/uo_functional_q.html#example-1","title":"Example 1","text":"<p>Question</p> <p>Consider \\(Q+scc^T\\) where \\(Q\\) is positive definite symmetric, prove that the largest eigenvalue \\(\\mu_n(s)\\) of \\(Q+scc^T\\) is bounded by \\(a_2 + \\beta s \\leq \\mu_n(s) \\leq a_1 + \\beta s\\).</p> <p>proof. </p> \\[\\begin{align*} \\mu_n(s) &amp;= \\max_{\\|x\\|=1}\\big\\{x^T(Q+scc^T)x\\big\\}\\\\ &amp;= \\max_{\\|x\\|=1} \\big\\{x^TQx + x^Tscc^Tx\\big\\} \\end{align*}\\] <p>Because \\(Q\\) is positive definite, \\(x^TQx \\geq 0\\), </p> \\[\\begin{align*} \\mu_n(s) &amp;\\geq \\max_{\\|x\\|=1} \\big\\{x^Tscc^Tx\\big\\}\\\\ &amp;= \\max_{\\|x\\|=1} \\big\\{x^Tcc^Tx\\big\\}s &amp;s\\text{ is a scalar} \\end{align*}\\] <p>Also, we have </p> \\[\\begin{align*} \\mu_n(s) &amp;\\leq \\max_{\\|x\\|=1}\\big\\{x^TQx\\big\\} + \\max_{\\|x\\|=1} \\big\\{x^Tscc^Tx\\big\\}\\\\ &amp;= \\max_{\\|x\\|=1}\\big\\{x^TQx\\big\\} + \\max_{\\|x\\|=1} \\big\\{x^Tcc^Tx\\big\\}s \\end{align*}\\] <p>Let \\(\\alpha_2 = 0, \\alpha_1 = \\max_{\\|x\\|=1}\\big\\{x^TQx\\big\\}, \\beta = \\max_{\\|x\\|=1} \\big\\{x^Tcc^Tx\\big\\}\\), since \\(c\\neq 0, \\beta &gt; 0\\). Therefore, </p> \\[a_2 + \\beta s \\leq \\mu_n(s) \\leq a_1 + \\beta s\\]"},{"location":"apm462/uo_functional_q.html#example-2","title":"Example 2","text":"<p>Question</p> <p>Derive the formula \\(p(t):\\mathbb R\\rightarrow \\mathbb R^2 := (x(t), y(t))\\) where \\(p(t)\\) is the position of point on the rim of a circle of radius \\(R\\), the circle is rolling along the \\(x\\)-axis at a constant speed \\(a\\). Assume \\(c(t) = (at, R)\\) is the position of the center of the circle at \\(t\\) and \\(p(0) = (0, 0)\\).</p> <p>Let \\(f(\\theta):\\mathbb R\\rightarrow \\mathbb R^2 := (R\\cos\\theta, R\\sin\\theta)\\) be the parametric equation of the circle.</p> <p>Consider the point \\(q(t) = (at, 0)\\) on the rim of the circle at time \\(t\\), let </p> \\[q_c(t) = q(t) - c(t) = (at, 0) - (at, R) = (0, -R)\\] <p>be its position relative to the center of the circle, note that </p> \\[q_c(t) = f(-\\pi/2) = (\\cos(-\\frac{\\pi}{2}), \\sin(-\\frac{\\pi}{2}))\\] <p>Then, let \\(L\\) be the arc of the circle between \\(q_c\\) and \\(p_c\\) and \\(\\theta(t)\\) be the angle subtended by arc \\(L\\) from the center of the circle. Since the arc length is \\(at\\), we have</p> \\[\\begin{align*} \\theta(0) - \\theta(t) &amp;= \\frac{at}{R}\\\\ \\theta(t) &amp;= \\theta(0) - \\frac{at}{R}\\\\ \\theta(t) &amp;= -\\frac{\\pi}{2} - \\frac{at}{R} \\end{align*}\\] <p>Therefore, let \\(q_c(t)\\) be \\(p\\)'s position relative to the center of the circle, </p> \\[q_c(t) = f(\\theta(t)) = \\big(R\\cos(-\\frac{\\pi}{2} - \\frac{at}{R}), R\\sin(-\\frac{\\pi}{2} - \\frac{at}{R})\\big)\\] <p>so that </p> \\[p(t) = p_c(t) + c(t) = \\big(at + R\\cos(-\\frac{\\pi}{2} - \\frac{at}{R}), R\\sin(-\\frac{\\pi}{2} - \\frac{at}{R})\\big)\\]"},{"location":"apm462/uo_functional_q.html#example-3","title":"Example 3","text":"<p>Question</p> \\[F[\\mu[\\cdot] = \\int_a^b L(x, \\mu(x), \\mu'(x))dx, x(a)=A, x(b)=B\\] <p>\\(L, \\mu \\in C^2\\), partition \\([a,b]\\) into \\(x_0, ..., x_{n+1}\\) where \\(x_i = a + \\frac{b-a}{n+1}i\\) so that we can approximate the functional by</p> \\[F:\\mathbb R^n\\rightarrow \\mathbb R, F(\\mu_1, ..., \\mu_n) :=\\sum_{i=1}^{n+1}L(x_i, \\mu_i, \\frac{\\mu_i - \\mu_{i-1}}{h})h\\] <p>Part (a)</p> <p>Calculate \\(\\frac{dF}{d\\mu_i}(\\mu)\\).</p> \\[\\begin{align*} \\frac{dF}{d\\mu_i}(\\mu) &amp;= \\sum_{j=1}^{n+1}\\frac{d}{d\\mu_i}L(x_j, \\mu_j, \\frac{\\mu_j - \\mu_{j-1}}{h})h\\\\ &amp;= \\frac{d}{d\\mu_i}L(x_{i}, \\mu_{i}, \\frac{\\mu_i - \\mu_{i-1}}{h})h + \\frac{d}{d\\mu_i}L(x_{i+1}, \\mu_{i+1}, \\frac{\\mu_{i+1} - \\mu_{i}}{h})h\\\\ &amp;= L_z(x_{i}, \\mu_{i}, \\frac{\\mu_i - \\mu_{i-1}}{h})h\\frac{d}{d\\mu_i}\\mu_i + L_p(x_{i}, \\mu_{i}, \\frac{\\mu_i - \\mu_{i-1}}{h})h\\frac{d}{d\\mu_i}\\frac{\\mu_i - \\mu_{i-1}}{h}\\\\ &amp;\\quad+L_p(x_{i+1}, \\mu_{i+1}, \\frac{\\mu_{i+1} - \\mu_{i}}{h})h\\frac{d}{d\\mu_i}\\frac{\\mu_{i+1} - \\mu_{i}}{h}\\\\ &amp;= hL_z(x_{i}, \\mu_{i}, \\frac{\\mu_i - \\mu_{i-1}}{h}) - (L_p(x_{i+1}, \\mu_{i+1}, \\frac{\\mu_{i+1} - \\mu_{i}}{h})- L_p(x_{i}, \\mu_{i}, \\frac{\\mu_i - \\mu_{i-1}}{h})) \\end{align*}\\] <p>Part (b)</p> <p>Take \\(h\\rightarrow 0\\), show that \\(\\frac{dF}{d\\mu_i}(\\mu) = 0\\Rightarrow L_z(x, \\mu(x), \\mu'(x) - \\frac{d}{dx}L_p(x, \\mu(x), \\mu'(x))) = 0\\).</p> <p>proof. Since \\(\\frac{dF}{d\\mu_i}(\\mu) = 0\\)</p> \\[\\begin{align*} \\lim_{h\\rightarrow 0} \\frac{1}{h}\\frac{dF}{d\\mu_i}(\\mu) &amp;= \\lim_{h\\rightarrow 0}L_z(x_i, \\mu_i, \\frac{\\mu_i-\\mu_{i-1}}h) \\\\ &amp;\\quad- \\lim_{h\\rightarrow 0}\\frac{L_p(x_{i+1}, \\mu_{i+1}, \\frac{\\mu_{i+1} - \\mu_{i}}{h})- L_p(x_{i}, \\mu_{i}, \\frac{\\mu_i - \\mu_{i-1}}{h})}{h}\\\\ &amp;= L_z(x_i, \\mu(x_i), \\mu(x_i)')\\\\ &amp;\\quad - \\lim_{h\\rightarrow 0}\\frac{L_p(x_i+h, \\mu(x_i+h), \\mu'(x_i+h))- L_p(x_{i}, \\mu(x_i), \\mu'(x_i))}{h}\\\\ &amp;\\text{by definition of derivative}\\\\ &amp;= L_z(x_i, \\mu(x_i), \\mu(x_i)') - \\frac{d}{dx}L_p(x_i, \\mu(x_i), \\mu(x_i)') \\end{align*}\\] <p>Therefore, for some \\(x\\in[a,b]\\), since \\(h\\rightarrow 0\\), we can partition \\([a,b]\\) s.t. \\(x\\in \\text{partition}(a, b)\\), so that we have </p> \\[L_z(x_i, \\mu(x_i), \\mu(x_i)') - \\frac{d}{dx}L_p(x_i, \\mu(x_i), \\mu(x_i)') = \\lim_{h\\rightarrow 0} \\frac{1}{h}\\frac{dF}{d\\mu_i}(\\mu) = 0\\]"},{"location":"apm462/uo_functional_q.html#example-4","title":"Example 4","text":"<p>Question</p> <p>Find FONC of</p> \\[\\begin{align*} \\text{minimize}\\quad &amp;F[\\mu(\\cdot)] := \\int_a^b L(x, u(x), u'(x))dx\\\\ \\text{subject to}\\quad &amp;\\mu\\in \\mathcal A:= \\{\\mu:[a,b]\\rightarrow\\mathbb R\\mid \\mu\\in C^2, \\mu(a) = A\\} \\end{align*}\\] <p>Let \\(\\mu_*\\) be the minimizer of the problem. Let \\(\\mathcal T:= \\{v:[a, b]\\rightarrow\\mathbb R\\mid v\\in C^2, v(a) = 0\\}\\) be the set of test functions so that \\(\\forall v\\in \\mathcal T. \\forall s\\in\\mathbb R. u_*+sv \\in \\mathcal A\\). Then as what have been done in the class till the integrations by parts </p> \\[\\begin{align*} \\frac{d}{ds}F[u_*+sv] &amp;=\\int_a^b \\big[L_z(...) - \\frac{d}{dx}L_p(...)\\big]v(x)dx + \\big[L_p(...)\\mid_a^b\\big] \\end{align*}\\] <p>Since \\(v(a) = 0\\), note that </p> \\[\\begin{align*} \\big[L_p(x, u_*(x), u_*'(x))v(x)\\mid_a^b\\big] &amp;= L_p(b, u_*(b), u_*'(b))v(b) - L_p(a, u_*(a), u_*'(a))v(a)\\\\ &amp;= L_p(b, u_*(b), u_*'(b))v(b) \\end{align*}\\] <p>Consider \\(v_1\\in\\mathcal T\\) s.t. \\(v(b) = 0\\), hence</p> \\[\\big[L_p(x, u_*(x), u_*'(x))v(x)\\mid_a^b\\big] = 0\\] <p>Given \\(u_*\\) is the minimizer,  </p> \\[0 = \\frac{d}{ds}F[u_*+sv] =\\int_a^b \\big[L_z(...) - \\frac{d}{dx}L_p(...)\\big]v(x)dx\\] <p>follows the original Euler-Lagrange equation, i.e. </p> \\[L_z(x, u_*(x), u_*'(x)) - \\frac{d}{dx}L_p(x, u_*(x), u_*'(x)) = 0\\] <p>Consider \\(v_2\\in\\mathcal T, v_2(b)\\neq 0\\), since we have to fulfill the necessary conditions for cases like \\(v_1\\), we have </p> \\[L_z(x, u(x), u'(x)) - \\frac{d}{dx}L_p(x, u(x), u'(x)) = 0\\] <p>so that </p> \\[0 = \\frac{d}{ds}F[u_*+sv] = 0 + L_p(b, u(b), u'(b))v(b)\\] <p>Therefore, there are two FONC as</p> \\[L_z(x, u_*(x), u_*'(x)) - \\frac{d}{dx}L_p(x, u_*(x), u_*'(x)) = 0\\] \\[L_p(b, u(b), u'(b))v(b) = 0\\]"},{"location":"cs267/communication.html","title":"Communication Optimal Matmul","text":"<p>Communication is equivalent to moving data, which is often the most expensive operation. The goal is to find the minimal communication required for nested loops accessing arrays. </p>"},{"location":"cs267/communication.html#general-communication-lower-bound","title":"General communication lower bound","text":"<p>Informally, we can prove that regular grid problems has a lower bound of communications, and more importantly, we can always attain the lower bound by constructing an optimal tiling. </p>"},{"location":"cs267/communication.html#n-body-problems","title":"N-body problems","text":"<p>For an N-body problem, we have a dense array of particles (as <code>struct</code>) containing position and force. What we want is to compute the updated positions for each particle under the influence of all other particles. Which means</p> <pre><code>for (int i = 0; i &lt; N; i++) {\n    for (int j = 0; j &lt; N; j++) {\n        /* update points[i] from points[j] */\n    }\n}\n</code></pre> <p>If we know that our cache can fit at most \\(M\\) points, we can do at most \\((M/2)^2 = M^2 / 4\\) loop iterations. We have \\(N^2\\) iterations, which means we need to fill the cache at least \\(n^2 / (M^2 / 4) = 4n^2/M^2\\) times. Each time when we fill the cache, we need \\(M\\) reads/writes so that the total communication is \\(4n^2 / M^2 \\times M = 4n^2/M\\). Therefore, the lower bound for communication is \\(\\Omega(n^2/M)\\). </p> <p>We can achieve this lower bound in practice by </p> <pre><code>for (int i = 0; i &lt; N; i += M / 2) {\n    for (int j = 0; j &lt; N; j += M / 2) {\n        for (int ii = 0; ii &lt; M / 2; ii++ ) {\n            for (int jj = 0; jj &lt; M / 2; jj++) {\n                /* update points[i + ii] from points[j + jj] */\n            }\n        }\n    }\n}\n</code></pre>"},{"location":"cs267/communication.html#matrix-multiplication","title":"Matrix multiplication","text":"We know that tiled / blocked matrix multiplication is an optimized way for `matmul` for a similar reason as the N-body problem. <p>For \\(C_{M\\times N} = A_{M\\times K}\\cdot B_{K\\times N}\\), we have \\(M\\times N\\times K\\) computations as </p> <pre><code>for (int i = 0; i &lt; M; i++) {\n    for (int j = 0; j &lt; N; j++) {\n        for (int k = 0; k &lt; K; k++) {\n            C[i, j] += A[i, k] + B[k, j];\n        }\n    }\n}\n</code></pre> <p>Suppose that the cache can fit <code>C[i:i+x, j:j+y]</code>, <code>A[i:i+x, k:k+z]</code>, <code>B[k:k+z, j+j+y]</code> so that we can do \\(xyz\\) operations, which is </p> \\[xyz = \\sqrt{xy\\cdot xz \\cdot zy} = \\sqrt{area(C) area(A) area(B)}\\] <p>Thus, the problem is equivalent to maximize the box volume under the constraint of total surface area \\(2M\\). We know that the volume is maximized when the box is a cube with side length \\(\\sqrt{\\frac{M}{3}}\\). Therefore, each cache can do \\((\\frac{M}{3})^{3/2}\\) operations, and the lower bound for communication is </p> \\[M \\times N^3 / (\\frac{M}{3})^{3/2} \\in \\Omega(N^3 / M^{1/2})\\] <p>For parallel computers, if both the work and data are perfectly balanced, then each processor should get \\(N^3 / P\\) operations and \\(3N^2 / P\\) data. Therefore the lower bound is </p> \\[(3N^2 / P) \\times (N^3 / P) / (3N^2 / P)^{3/2} = \\Omega(N^2 / P^{1/2})\\]"},{"location":"cs267/communication.html#convolutions","title":"Convolutions","text":"<p>For a 2D convolution (omitting stride), the input is a \\((B, H, W, C_{in})\\) image, it is convolve with \\((S, R, C_{in}, C_{out})\\) kernel to produce \\((B, H, W, C_{out})\\) output. We will use lower letter for entry index, and the computation is defined by a 7 nested loop, note that the loop variables are inter-changeable.</p> <pre><code>for cout = 1:C_out     % output channel\n for h = 1:H           % image height\n  for w = 1:W          % image width\n   for s = 1:S         % kernel height\n    for r = 1:R        % kernel width\n     for cin = 1:C_in  % input channel\n      for b = 1:B      % batch\n        out[b, h, w, cout] += in[b, h+s, w+r, cin] * f[s, r, cin, cout]\n</code></pre> <p>For 1D case, suppose the image is size \\((H, C_{in})\\) and filter is size \\((S, C_{in}, C_{out})\\) this can be implemented as a moving window. For each input channel, construct the image matrix of dimension \\(H \\times F\\) (omitting padding) where the ith row is \\(x_i, x_{i+1}, \\cdots, x_{i+S}\\). The filter matrix is \\(F \\times C_{out}\\), and we can do matmul on them. </p> <p>For 2D CNN, the case is very similar, we first convert 2D image to a matrix where each row is \\(S\\times R\\) size, and we flatten 2D filter into 1D. Therefore, the 2D convolution is reduced to 1D. </p>"},{"location":"cs267/dataparallel.html","title":"Data Parallel Algorithms","text":"<p>Data parallel algorithms often apply to an array and the same operation is performed onto each (or each group) or elements at the same time. </p>"},{"location":"cs267/dataparallel.html#common-operators","title":"Common operators","text":"<ul> <li>Unary operators: \\(B = f(A)\\) such as <code>square</code>. </li> <li>Binary operators: \\(C = f(A, B)\\) such as <code>add</code>.</li> <li>Broadcast: \\(C = f(a, B)\\), such as <code>fill</code> (map a scalar to the array) or <code>saxpy</code> (\\(aX + Y\\), scalar multiplication and add)</li> <li> <p>Memory operations: array indexing include (stride) scatter/gather. Commonly seen in <code>numpy</code>. For example</p> <pre><code>A = B[::2] # copy with stride 2\n\nindices = [3, 0, 4, 2, 1]\nA = B[indices] # gather indexed values from one array\nA[indices] = B # scatter index values from one array\n</code></pre> </li> <li> <p>Masked operations: A mask is an array of <code>true / false</code> or <code>0 / 1</code> and operations only performed when mask element is 1. </p> <p><pre><code>mask = A % 2 == 1\nA[mask] += 1 # add 1 to all odd numbers in A \n</code></pre> - Reduction accumulate an array to one value with any associative op. Note that associativity is very important by allowing up to change operation orders (so that optimize and parallelize). Examples <code>sum(A), prod(A), dot(A, B)</code>.</p> </li> <li> <p>Scan fill an array with partial reductions of any associative op. Examples <code>cumsum = scan(add)</code></p> <ul> <li>Inclusive scan includes input \\(x_i\\) for output \\(y_i\\), exclusive scan does not. </li> </ul> </li> </ul>"},{"location":"cs267/dataparallel.html#ideal-cost","title":"Ideal cost","text":"<p>Suppose that we have infinite number of processors, free control, and free communication. Then, the cost on the parallelism will be the depth of the spanned data dependency tree. </p> <ul> <li>For unary and binary operations, the output only depends on corresponding elements, thus the cost is \\(O(1)\\). </li> <li>For broadcast, data get doubled at each time, thus the cost is \\(O(\\log n)\\). </li> <li>For reduction, associativity allows up to do operation group-wise, thus the cost is \\(\\Theta(\\log_b n)\\), where \\(b\\) is the number of elements in a group, in general, the operations are binary, \\(b=2\\). Note that this is also a lower bound.</li> </ul>"},{"location":"cs267/dataparallel.html#matrix-multiplications","title":"Matrix multiplications","text":"<p>For a matrix multiplication \\(C = A \\cdot B\\), wlog. assume that the matrics are all \\(n\\times n\\). We note that for each entry </p> \\[c_{i,j} = \\sum_{k=1}^N a_{i, k} b_{k, j} = \\sum_{k=1}^N d_{i,j,k}\\] <p>Which is </p> <ol> <li>A element-wise multiplication between \\(A_{i,:}\\) and \\(B_{:, j}\\). Costs \\(O(1)\\) </li> <li>A reduction sum on \\(D_{i,j}\\). Costs \\(O(\\log n)\\)</li> </ol> <p>Thus, the total cost is \\(\\in O(\\log n)\\). </p>"},{"location":"cs267/dataparallel.html#scan-via-parallel-prefix","title":"Scan via parallel prefix","text":"<p>A na\u00efve parallel scanning can broadcast the \\(i\\)th value to an array of length \\(i\\), and perform reduction on it to get the output for \\(i\\)th output. Then, the cost if \\(O(\\log n)\\), but the total work is \\(1+2+\\cdots + n \\in O(n^2)\\). </p> <p>An idea was that we can divide 1 scan into two parts. Wlog assume that the operation is add. Observe that </p> \\[y_i = (\\sum^{i-1} x_{j}) + x_i = (\\sum^{i-2} x_{j}) + (x_{i-1} + x_i)\\] <p>we have a recursive way to reduce the problem</p> <ol> <li>Pairwise add two numbers so that even entries become \\(x_{i-1} + x_i\\).</li> <li>Compute scan on the even entries, so that even entries will have its scanned output</li> <li>For each odd number, add its previous number, which is the even entry that has the correct scanned output.</li> </ol> recursive prefix<pre><code>def recursive_prefix_sum(arr):\n    print(arr)\n    n = len(arr)\n    if n &lt;= 2:\n        if n == 2:\n            arr[1] += arr[0]\n        return\n    arr[1::2] = arr[::2] + arr[1::2] if n % 2 == 0 else arr[:n-1:2] + arr[1::2]\n    print(arr)\n    recursive_prefix_sum(arr[1::2])\n    arr[2::2] = arr[1:n-1:2] + arr[2::2] if n % 2 == 0 else arr[1::2] + arr[2::2]\n    print(arr)\n\nrecursive_prefix_sum(np.arange(16))\n# [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15]\n# [ 0  1  2  5  4  9  6 13  8 17 10 21 12 25 14 29]\n# [ 1  5  9 13 17 21 25 29]\n# [ 1  6  9 22 17 38 25 54]\n# [ 6 22 38 54]\n# [ 6 28 38 92]\n# [28 92]\n# [  6  28  66 120]\n# [  1   6  15  28  45  66  91 120]\n# [  0   1   3   6  10  15  21  28  36  45  55  66  78  91 105 120]\n</code></pre>"},{"location":"cs267/dataparallel.html#non-trivial-scan-applications","title":"Non-trivial Scan applications","text":"<p>Some applications that can benefit from <code>scan</code></p> <ul> <li>n-bits Integer adding (scan for the carryover bits) in \\(O(\\log n)\\) time</li> <li>Inverting a \\(n\\times n\\) dense matrices / triangular matrices in \\(O(\\log^2 n)\\) time</li> <li>Evaluating expressions with \\(n\\) tokens in \\(O(\\log n)\\) time</li> <li>Evaluating linear recurrences (e.x. Fibonacci series) of size \\(n\\) in O(\\log n)$ time</li> <li>Sparse matrix-vertex multiplication <code>SpMV</code> using segmented scan</li> </ul>"},{"location":"cs267/dataparallel.html#stream-compression","title":"Stream Compression","text":"<p>Given a mask \\(m\\) and array \\(A\\), output \\(A[m]\\) as a dense array. This is different from masked operation, where the operations happens in-place.</p> <p>Solution Compute cumulative sum on \\(m\\), and then scatter values into result. </p> stream compression<pre><code># n:    8\n# arr:  [3, 2, 4, 1, 5, 3, 3, 1]\n# mask: [1, 0, 1, 1, 0, 0, 1, 1]\nidx  = scan(mask, add)\n# idx:  [1, 1, 2, 3, 3, 3, 4, 5]\nresult = empty(idx[n - 1]) # empty arr of size 5\nresult[idx[mask]] = arr[mask]\n# result: [3, 4, 1, 3, 1]\n</code></pre> <p>Stream compression is the same as remove elements satisfying a condition. In this case, we let <code>mask = condition(arr)</code>. </p>"},{"location":"cs267/dataparallel.html#radix-sort","title":"Radix sort","text":"<p>Radix sort works for sorting array with fixed range, and its binary representation is ordered. For example, sorting an <code>int32</code> array. </p> serial radix sort<pre><code>// Radix sort for 32 bit unsigned integer arr of size n\nvoid radix_sort(uint32_t *arr, size_t n) {\n    uint32_t bit = 1;\n    uint32_t buffer[n];\n    uint32_t temp[n];\n    int start0, start1;\n    for (int b = 0; b &lt; 32; b++) {\n        for (int i = 0; i &lt; n; i++) {\n            temp[i] = arr[i];\n            buffer[i] = arr[i] &amp; bit;\n            if (buffer[i]) start1++;\n        }\n        for (int i = 0; i &lt; n; i++) {\n            if (buffer[i]) \n                arr[start1++] = temp[i];\n            else \n                arr[start0++] = temp[i];\n        }\n        bit &lt;&lt;= 1;\n    }\n\n}\n</code></pre> <p>To parallelize, observe that we can mask all entries end with 1 from <code>buffer[i] = arr[i] &amp; bit</code> and entries end with 0 from the complement. Then, the problem becomes a stream compression for 0-ending entries and a stream compression for 1-ending entries where the starting position is count of 0-ending entries (scalar add via broadcast). </p>"},{"location":"cs267/dataparallel.html#linked-list-length","title":"Linked list length","text":"<p>For a linked list, finding the size of a linked list takes \\(O(n)\\) time for serial algorithms. For parallel cases, we can see the problem as a reduction problem or a scan problem (if we want to assign the distance to <code>tail</code> for every node). </p> <p>In general, the algorithm works exactly the same as recursive prefix scan, but at the same time we prefix the pointers as well. </p>"},{"location":"cs267/dataparallel.html#fibonacci-series","title":"Fibonacci Series","text":"<p>Note that Fibonacci series is a linear occurrence problem \\(F_{n+1} = F_{n} + F_{n-1}\\) and depends on previous two states. However, we can expand this by </p> \\[\\begin{bmatrix}F_{n+1}\\\\F_n\\end{bmatrix} = \\begin{bmatrix}1&amp;1\\\\1&amp;0\\end{bmatrix}\\begin{bmatrix}F_{n}\\\\F_{n-1}\\end{bmatrix}\\] <p>so that it only depends on previous one state. </p> <p>Also, matmul is associative so that it go backs to a scan problem. </p>"},{"location":"cs267/dataparallel.html#adding-n-bit-integers","title":"Adding n-bit integers","text":"<p>A serial n-bit bitwise add is implemented as </p> <pre><code>c[-1] = 0 % carryover c_in\nfor i = 0:n-1\n    s[i] = (a[i] xor b[i]) xor c[i-1] % 1 or 3 one's\n    c[i] = ((a[i] xor b[i]) and c[i-1]) or (a[i] and b[i]) % has carryover\n</code></pre> <p>Now, we see that <code>s[i]</code> and <code>c[i]</code> both depends on <code>c[i-1], a[i], b[i]</code>. We can first precompute things related to <code>a[i], b[i]</code> in \\(O(1)\\) time. </p> <pre><code>s[i] = p[i] xor c[i-1] % p = a xor b for bit propagation\nc[i] = (p[i] and c[i-1]) or g[i] % g = a and b for bit generation\n</code></pre> <p>Mathematically, we could represent \\(c\\) as a boolean matmul</p> \\[\\begin{bmatrix}c_{i}\\\\1\\end{bmatrix} = \\begin{bmatrix}p_i&amp;g_i\\\\0&amp;1\\end{bmatrix}\\begin{bmatrix}c_{i-1}\\\\1\\end{bmatrix}\\]"},{"location":"cs267/dataparallel.html#inverting-matrices","title":"Inverting matrices","text":"<p>Wlog suppose that \\(T\\) is \\(n\\times n\\) lower triangular matrix. Then, we can break \\(M\\) into \\(\\frac{n}{2}\\times \\frac{n}{2}\\) matrices \\(T = \\begin{bmatrix}A &amp;\\mathbf{0}\\\\C&amp;B\\end{bmatrix}\\) where \\(A\\) and \\(B\\) are both lower triangular and \\(C\\) is dense. In addition, we could prove the lemma</p> \\[T^{-1} = \\begin{bmatrix}A &amp;\\mathbf{0}\\\\C&amp;B\\end{bmatrix}^{-1} = \\begin{bmatrix}A^{-1} &amp;\\mathbf{0}\\\\-B^{-1}CA^{-1}&amp;B^{-1}\\end{bmatrix}\\] <p>proof. </p> \\[\\begin{align*} T^{-1}T &amp;=  \\begin{bmatrix}A^{-1} &amp;\\mathbf{0}\\\\-B^{-1}CA^{-1}&amp;B^{-1}\\end{bmatrix} \\begin{bmatrix}A &amp;\\mathbf{0}\\\\C&amp;B\\end{bmatrix}\\\\ &amp;= \\begin{bmatrix}A^{-1}A + (-B^{-1}CA^{-1})\\mathbf{0} &amp;A^{-1}\\mathbf{0} + \\mathbf 0 B\\\\(-B^{-1}CA^{-1})A + B^{-1}C&amp;(-B^{-1}CA^{-1})\\mathbf 0 + B^{-1}B\\end{bmatrix}\\\\ &amp;= \\begin{bmatrix}I&amp;\\mathbf0\\\\\\mathbf0&amp;I\\end{bmatrix} \\\\ &amp;= I \\end{align*}\\] <p>Therefore, we can construct the recursive triangular inverting algorithm. </p> inverse triangular matrix<pre><code>def tri_inv(T, n):\n    if n == 1:\n        return 1. / T\n    A = T[:n//2, :n//2]\n    C = T[n//2:, :n//2]\n    B = T[n//2:, n//2:]\n    Ainv = tri_inv(A, n//2) # Ainv, Binc in parallel\n    Binv = tri_inv(B, n//2) \n    T[:n//2, :n//2] = Ainv\n    T[n//2:, n//2:] = Binv\n    T[n//2:, :n//2] = - Binv @ C @ Ainv # log(n) matmul\n</code></pre> <p>The total cost is \\(O(\\log^2 n)\\) by masters theorem</p>"},{"location":"cs267/distributed.html","title":"Advanced MPI","text":"<p>Basics of MPI and MPI communication APIs</p>"},{"location":"cs267/distributed.html#latency-and-bandwidth-model","title":"Latency and bandwidth model","text":"<p>For each communication of length \\(n\\), is time cost is roughly </p> \\[t = a + n\\beta\\] <p>where \\(t\\) is the time, \\(a\\) is the latency and \\(\\beta\\) is the reciprocal of bandwidth. In practice, \\(\\alpha &gt;&gt; \\beta &gt;&gt; \\text{FLOP}\\), which means we want one large communication rather than many small ones. In addition, we need to perform at much arithmetic operations as possible before a communication to happen. </p>"},{"location":"cs267/distributed.html#mpi-collectives-cost","title":"MPI Collectives Cost","text":"<p>Consider \\(P\\) procs and \\(n\\) words of data, assume that \\(n\\geq P\\). Let \\(\\alpha\\) be the latency, \\(\\beta\\) be the reciprocal bandwidth, \\(\\gamma\\) be the speed for reduction operations. The per-proc communication cost of efficiently implemented collective operations are </p> collective description cost <code>scatter</code> <code>root</code> scatters \\(n\\) words, each proc receives \\(n/P\\) words \\(\\lg P \\alpha + n \\beta\\) <code>gather</code> each proc sends \\(n/P\\) words to <code>root</code> \\(\\lg P \\alpha + n \\beta\\) <code>allgather</code> each proc sends \\(n/P\\) words, gathered by all procs \\(\\lg P\\alpha + n\\beta\\) <code>broadcast</code> <code>root</code> broadcast \\(n\\) words to all proc \\(2\\lg P\\alpha + 2n\\beta\\) <code>all_to_all</code> each proc sends different \\(n/P\\) words to every other proc \\(\\lg P\\alpha + \\frac{n}{2}\\lg P\\beta\\) <code>reduce-scatter</code> reduction on \\(n\\) words from each proc, and then scatter the result on all procs \\(\\lg P\\alpha + n\\beta + n\\gamma\\) <code>reduce</code> reduction on \\(n\\) words from each proc, result returned on root \\(2\\lg P\\alpha + 2n\\beta + n\\gamma\\) <code>allreduce</code> reduction on \\(n\\) words from each proc, and then returned the result on all procs \\(2\\lg P\\alpha + 2n\\beta + n\\gamma\\)"},{"location":"cs267/distributed.html#implementations-for-collectives","title":"Implementations for Collectives","text":"<p>Using <code>AllGather</code> as an example, other collective are implemented in a similar manner. </p>"},{"location":"cs267/distributed.html#ring-algorithm","title":"Ring Algorithm","text":"<p>At time \\(0\\), send original data to right; at time \\(t\\), send the data received at time \\(t-1\\) to right, and receive new data from left.  Optimal in bandwidth (\\(\\frac{n(p-1)}{P}\\) data), high latency (\\(\\alpha(P-1)\\)). Total time </p> \\[t_{ring} = \\alpha(P-1) + \\beta \\frac{n(P-1)}{P}\\]"},{"location":"cs267/distributed.html#recursive-doubling-algorithm","title":"Recursive doubling algorithm","text":"<p>At time t, proc \\(i\\) exchanges (send/receive) all its current data with process proc \\(i+2^t\\). Data exchanged at time \\(t\\) will be \\(2^{\\lg P - 1}\\frac{n}{p}\\). However, for non-power-of-two nodes, it's trickier to implement. Total time</p> \\[T_{recur} = \\alpha \\lg P + \\beta \\frac{n(P-1)}{P}\\] <p></p>"},{"location":"cs267/distributed.html#bruck-algorithm","title":"Bruck algorithm","text":"<p>An extension of recursive doubling. At time t, proc \\(i\\) receives all current data from \\(i+2^t \\mod P\\) and sends all of its current data to \\(i-2^t \\mod P\\). After \\(\\lfloor \\lg P\\rfloor\\) steps, send the top \\(P - 2^{\\lg P}\\) entries and do a local shift to get data in correct order. Total time</p> \\[t_{Bruck} = \\alpha \\lceil \\lg(P) \\rceil + \\beta \\frac{n(P-1)}{P}\\]"},{"location":"cs267/distributed.html#hybrid-programming","title":"Hybrid Programming","text":"<p>Note that MPI describes parallelism between procs (separate memory address spaces), if procs are referring to physical cores, the core can also have multiple threads. Therefore, a hybrid programming model provides two levels of parallelism. </p> <p>In general, all threads share all MPI objects (world, communicators, requests). MPI provides \\(r\\) levels of thread safety</p> <ul> <li><code>MPI_THREAD_SINGLE</code> only one thread exists in the application (multithread is not allowed)<ul> <li>no threads in the system</li> </ul> </li> <li><code>MPI_THREAD_FUNNELED</code> multithreaded, but only main thread (who makes <code>MPI_Init</code>) makes MPI calls <ul> <li>MPI calls are outside <code>omp parallel</code> or inside <code>omp master</code></li> </ul> </li> <li><code>MPI_THREAD_SERIALIZED</code> multithreaded, but only one thread at a time makes MPI calls<ul> <li>MPI calls inside <code>critical</code></li> </ul> </li> <li><code>MPI_THREAD_MULTIPLE</code> any thread can make MPI calls at any time, with some restrictions<ul> <li>MPI calls any time (with restrictions)</li> </ul> </li> </ul> <p>Any MPI application that hybrids with thread, should initialize with <code>MPI_Init-thread</code>. </p>"},{"location":"cs267/distributed.html#restrictions-for-mpi_thread_multiple","title":"Restrictions for <code>MPI_THREAD_MULTIPLE</code>","text":"<p>Ordering: When multiple threads make MPI calls concurrently, the outcome will be as if the calls executed sequentially in some (any) order. </p> <p>Use has to ensure that collectives on the same communicator, window, or file handle are correctly ordered among threads. For example, cannot call a <code>Bcast</code> on one thread and <code>reduce</code> on another on the same communicator. The user needs to handle race conditions post conflicting MPI calls. </p> <p>MPI's blocking will only impact the calling thread, instead of all threads. </p>"},{"location":"cs267/distributed.html#one-sided-communication","title":"One-sided communication","text":"<p>To decouple data movement with process synchronizations. Each proc exposes a part of its memory to public, the memory segment works as a \"shared\" memory and all other processes can read and write to this memory. </p>"},{"location":"cs267/distributed.html#window-creation","title":"Window creation","text":"<p>In MPI, this public memory space is called a window. Once the memory is allocated locally, make an explicit MPI call to declare a memory region by </p> <pre><code>/* Each process specifies a window of existing memory that it exposes to RMA \n   accesses by the processes in the group of comm. \n   The window consists of size bytes, starting at address base.\n*/ \nint MPI_Win_create(void *base, MPI_Aint size, int disp_unit, MPI_Info info, \n                  MPI_Comm comm, MPI_Win *win);\n\n/* On each process, it allocates memory of at least size bytes, \n   outputs a pointer baseptr to it, \n   the rest is just the same as Win_create\n*/\nint MPI_Win_allocate(MPI_Aint size, int disp_unit, MPI_Info info,\n                     MPI_Comm comm, void *baseptr, MPI_Win * win);\n\n/* Creates a window object, but no memory attached yet. \n   dynamically add/remove buffers to/from the window later\n   using attach/detach\n*/\nint MPI_Win_create_dynamic(MPI_Info info, MPI_Comm comm, MPI_Win * win);\n\n/*\nmultiple processes on the same node will share the same buffer space\n*/\nint MPI_Win_allocate_shared(MPI_Aint size, int disp_unit, MPI_Info info, MPI_Comm comm,\n                             void *baseptr, MPI_Win *win);\n\n\n/*\nfree the window\n*/\nMPI_Win_free(MPI_Win *win);\n</code></pre> <p>Examples for <code>allocate</code> and <code>create_dynamic</code></p> <pre><code>int main(int argc, char ** argv) {\n    int *a; \n    MPI_Win win;\n    MPI_Init(&amp;argc, &amp;argv);\n    // collectively create remote accessible memory in a window\n    MPI_Win_allocate(1000*sizeof(int), sizeof(int), MPI_INFO_NULL,\n    MPI_COMM_WORLD, &amp;a, &amp;win);\n    // Array \u2018a\u2019 is now accessible from all processes in MPI_COMM_WORLD\n    MPI_Win_free(&amp;win);\n    MPI_Finalize(); \n    return 0;\n}\n\nint main(int argc, char ** argv) {\n    int *a; \n    MPI_Win win;\n    MPI_Init(&amp;argc, &amp;argv);\n    MPI_Win_create_dynamic(MPI_INFO_NULL, MPI_COMM_WORLD, &amp;win);\n    // create private memory\n    a = (int *) malloc(1000 * sizeof(int));\n    // use private memory like you normally would\n    a[0] = 1; a[1] = 2;\n    // locally declare memory as remotely accessible\n    MPI_Win_attach(win, a, 1000*sizeof(int));\n    // Array 'a' is now accessible from all processes\n    // undeclare remotely accessible memory\n    MPI_Win_detach(win, a); \n    free(a);\n    MPI_Win_free(&amp;win);\n    MPI_Finalize(); \n    return 0;\n}\n</code></pre>"},{"location":"cs267/distributed.html#data-movement","title":"Data movement","text":"<p>Data movement is similar to RESTful </p> <pre><code>int MPI_Put(\n    const void *origin_addr, int origin_count, MPI_Datatype origin_datatype, \n    int target_rank, \n    MPI_Aint target_disp, int target_count, MPI_Datatype target_datatype, \n    MPI_Win win);\n\nint MPI_Get(\n    void *origin_addr, int origin_count, MPI_Datatype origin_datatype, \n    int target_rank, \n    MPI_Aint target_disp, int target_count, MPI_Datatype target_datatype, \n    MPI_Win win);\n\n/* Accumulate data into the target process from reduction op */\nint MPI_Accumulate(\n    const void *origin_addr, int origin_count, MPI_Datatype origin_datatype, \n    int target_rank, \n    MPI_Aint target_disp, int target_count, MPI_Datatype target_datatype, \n    MPI_Op op, MPI_Win win);\n</code></pre>"},{"location":"cs267/distributed.html#orderings","title":"Orderings","text":"<p>There is no guaranteed ordering for <code>put/get</code> operations. The results of concurrent <code>put</code> to the same location is UNDEFINED, the result of <code>get</code> while concurrently <code>put/accumulate</code> is UNDEFINED. </p> <p>Note that concurrent accumulation to the same location are defined according to the order in which they occurred. </p>"},{"location":"cs267/fft.html","title":"Fast Fourier Transform","text":""},{"location":"cs267/fft.html#discrete-fourier-transform-dft","title":"Discrete Fourier Transform (DFT)","text":"<p>Given an 1D vector \\(v\\in\\mathbb R^m\\), the 1D DFT is \\(\\mathcal F\\cdot v\\) where \\(\\mathcal F\\in\\mathbb R^{m\\times m}\\) is defined as </p> \\[F_{j,k} = \\omega^{j\\times k}, 0\\leq j,k\\leq m - 1\\] <p>where \\(\\omega\\) is a complex number whose \\(m\\)'s power \\(\\omega^m = 1\\)</p> \\[\\omega = e^{2\\pi\\frac{i}{m}} = \\cos(\\frac{2\\pi}{m}) + i\\sin(\\frac{2\\pi}{m})\\] <p>2D DFT of an \\(m\\times m\\) matrix \\(V\\) is \\(\\mathcal F V \\mathcal F\\), which is equivalent to do 1D DFT on all columns independently, and then all the rows. </p>"},{"location":"cs267/fft.html#applications","title":"Applications","text":"<p>FFT is often used in frequency filtering (convolution theorem), compressions (same idea), and interestingly, solving Poisson's equations. </p> <p>The Poisson's problems can be written as solving \\(L_1X + XL_1 = B\\) and note that 2D FFT works similar to Eigen-decomposition, actually we have that \\(L_1 = F \\Lambda F^{-1}\\) being a eigen decomposition where </p> \\[F_{j,k} = \\sqrt{\\frac{2}{m+1}} \\sin(\\pi\\frac{jk}{m+1}), \\Lambda_j = 2(1 - \\cos(\\pi \\frac{j}{m+1}))\\] <p>we can substitute \\(L_1 = F\\Lambda F^T\\) and transform the RHS \\(B\\) by \\(B = FB'F^T\\), and then we can solve the whole thing as </p> \\[\\begin{align*} F \\Lambda F^{-1} X + X F D F^T &amp;= FB'F^T\\\\ F(\\Lambda (F^TXF) + (F^TXF)D)F^T &amp;= FB'F^T\\\\ DX' + X'D &amp;= B' &amp;X'_{jk} = \\frac{B'_{jk}}{\\Lambda_j + \\Lambda_k}\\\\ X &amp;= FX'F^T \\end{align*}\\] <p>Thus, the total cost is 2 2D FFT , \\(m^2\\) adds, and \\(m^2\\) divisions. all of which in \\(O(m^2\\log m)\\)</p>"},{"location":"cs267/fft.html#serial-algorithms","title":"Serial Algorithms","text":"<p>For each entry of FFT \\(F\\cdot v\\),  </p> \\[(F\\cdot v)_j = \\sum_{k=0}^{m-1} F_{j,k} v_k = V(\\omega^j), V(x) = \\sum_{k=0}^{m-1} x^k v_k\\] <p>Therefore, FFT is the same as evaluating a degree \\(m-1\\) polynomial \\(V(x)\\) at \\(m\\) different points. </p>"},{"location":"cs267/fft.html#divide-and-conquer-fft","title":"Divide and Conquer FFT","text":"<p>Note that we can decompose \\(V\\) into even and odd components</p> \\[\\begin{align*} V(x) &amp;= \\sum_{k=0}^{m-1} x^k v_k\\\\ &amp;= (v_0 + x^2 v_2 + x^4 v_4 + \\cdots) + x(v_1 + x^2 v_3 + x^4 v_5 + \\cdots)\\\\ &amp;= V_{even}(x^2) + V_{odd}(x^2) \\end{align*}\\] <p>Each \\(V_{even}\\) and \\(V_{odd}\\) has degree \\(\\frac{m}{2}-1\\), and we note that \\((\\omega^{j+\\frac{m}{2}})^2 = \\omega^{2j}\\omega^m = (\\omega^j)^2\\), hence we are only evaluating \\(m/2\\) different points. </p> <p>Therefore, we have the algorithm</p> FFT_DnC<pre><code>\"\"\"\nAssume that m is power of 2\n\"\"\"\ndef FFT(v, omega, m):\n    if m == 1:\n        return [v[0]]\n    v_even = FFT(v[0::2], omega ** 2, m / 2)\n    v_odd = FFT(v[1::2], omega ** 2, m / 2)\n    omega_precomp = [omega ** i for i in range(m/2)]\n    return concat(\n        v_even + omega_precomp * v_odd,\n        v_even - omega_precomp * v_odd,\n    )\n</code></pre> <p>The time complexity, by Master's theorem, is \\(T(m) = 2T(m/2) + O(m) \\in O(m\\log m)\\). </p>"},{"location":"cs267/fft.html#parallel-fft","title":"Parallel FFT","text":"<p>Consider the stack tree of the recursive calls, it has \\(m\\) leaves and \\(\\log m\\) depth. Therefore, instead of doing top-down recursion, we can do bottom-up computations, which gives a natural way for parallelizing the algorithm. </p> <p>Then, consider the recursive call, note that the divide is done by odd/even, or the last digit of the binary. Now, consider \\(m=2^4\\), the stack tree looks like</p> <pre><code>XXX\nXX0             XX1\nX00     X10     X01     X11\n000 100 010 110 001 101 011 111 \n</code></pre> <p>Note that from left to right, the leaves are ordered by reversed bit order. Therefore, what we can do is to compute FFT at each level, rewrite the elements \\(v_i = (F\\cdot v)_{\\text{bitreverse(i)}}\\).</p> <p>Then, for \\(p\\) processors, the computation takes \\(2m\\log m / p\\), for communications, we need \\(\\log(p)\\) stages and \\(m\\log(p)/p\\) words. </p>"},{"location":"cs267/grid.html","title":"Structured Grid for Poisson's Equations","text":"<p>Consider the Poisson's equation, which arises in many physics applications \\(\\nabla^2 \\Phi = \\sigma\\).</p> <p>As a concrete example, given a unit length bar of uniform material, at time 0, the temperature at one side is \\(0\\) and the other side is 1. Let \\(u(x,t):[0,1]\\times[0,1]\\rightarrow [0,1]\\) be the temperature at position \\(x\\) at time \\(t\\). We know that heat travels from \\(x-h\\) to \\(x+h\\) at rate </p> \\[\\frac{\\partial}{\\partial t} u(x,t) = \\frac{C}{h^2}(u(x-h,t) - u(x,t) - (u(x,t) - u(x+h, t)))\\] <p>As \\(h\\rightarrow 0\\), we have the heat equation as a Poisson's equation </p> \\[\\frac{\\partial}{\\partial t} u(x,t) = C \\frac{\\partial^2}{\\partial x^2}u(x,t)\\]"},{"location":"cs267/grid.html#table-for-different-methods","title":"Table for different methods","text":"<p>Time and space complexity for \\(2D (3D)\\) Poisson equations with \\(N = n^2(n^3)\\) grid cells.</p> algorithm serial time space complexity idealized parallel time #procs required Dense LU \\(N^3\\) \\(N^2\\) \\(N\\) \\(N^2\\) Band LU \\(N^2(N^{7/3})\\) \\(N^{3/2}(N^{5/2})\\) \\(N\\) \\(N(N^{4/3})\\) Jacobi \\(N^2(N^{5/3})\\) \\(N\\) \\(N\\) \\(N(N^{2/3})\\) Conj. Gradients \\(N^{3/2}(N^{4/3})\\) \\(N\\) \\(N^{1/2}\\log N (N^{1/3}\\log N)\\) \\(N\\) R/B SOR \\(N^{3/2}(N^{4/3})\\) \\(N\\) \\(N^{1/2} (N^{1/3})\\) \\(N\\) Multigrid \\(N\\) \\(N\\) \\(\\log^2 N\\) \\(N\\) lower bound \\(N\\) \\(N\\) \\(\\log N\\)"},{"location":"cs267/grid.html#stencils-discretization","title":"Stencils (Discretization)","text":"<p>We can discretize the equation onto a \\(M\\times N\\) regular grid with step size \\(\\Delta x = 1/M, \\Delta t = 1/N\\). At each grid location, we have measured the temperature at position \\(i/M\\) at time \\(j/N\\) as \\(\\mathbf u_{ij} = u(i/M, j/N)\\).</p> <p>Using implicit Euler we can approximate time derivative \\(\\partial_t u(x,t) = \\frac{u(x,t+\\Delta t) - u(x,t)}{\\Delta t}\\) so that we have the heat equation estimated as </p> \\[\\begin{align*} \\frac{u(x,t+\\Delta t) - u(x,t)}{\\Delta t} &amp;= \\frac{C}{(\\Delta x)^2}(u(x-\\Delta x, t+\\Delta t) - 2u(x, t+\\Delta t) + u(x+\\Delta x, t+\\Delta t))\\\\ u(x, t) &amp;= u(x, t+\\Delta t) - \\frac{C\\Delta t}{(\\Delta x)^2}(u(x-\\Delta x, t+\\Delta t) - 2u(x, t+\\Delta t) + u(x+\\Delta x, t+\\Delta t))\\\\ \\mathbf u_{i,j} &amp;= \\mathbf u_{i,j+1} - z(\\mathbf u_{i-1,j+1} - 2\\mathbf u_{i,j+1} + \\mathbf u_{i+1,j+1}) \\end{align*}\\] <p>we note that \\(z = \\frac{C\\Delta t}{(\\Delta x)^2}\\) is constant, and we can represent the equations as a sparse matrix vector multiplication</p> \\[(I + zL)\\cdot \\mathbf u_{:, i+1} = \\mathbf u_{:i}\\] <p>where \\(I\\) is the identity matrix, \\(L\\) is a \\(N\\times N\\) sparse matrix, where \\(L_{ii} = 2, L_{i-1,i} = L_{i+1, i} = -1\\) and the rest entries are all \\(0\\). </p> <p>By Trapezoidal rule, another formulization will yield better numerical properties</p> \\[(I + \\frac{z}{2}L)\\cdot \\mathbf u_{:, i+1} = (I - \\frac{z}{2}L)\\mathbf u_{:i}\\] <p>This results in the stencil for 1D Poisson's equation. </p> <p>For 2D (for example, heat transfer one a sheet of metal), the story is similar, the matrix \\(L\\) is still a nearly diagonal matrix. </p> <p></p> <p>Therefore, we need to solve a system of equations. The most na\u00efve method is to use dense LU, but we can observe that \\(L\\) has special structure (almost diagonal with structured off-diagonal entries), thus we can solve it via a specialized band LU. </p>"},{"location":"cs267/grid.html#iterative-methods-relaxation","title":"Iterative Methods (Relaxation)","text":"<p>Even band LU requires to store the \\(n\\times n\\) matrix and requires large memory. Iterative methods starts with some initial guess at the solution, and more step by step towards the solution. </p>"},{"location":"cs267/grid.html#jacobis-method","title":"Jacobi's Method","text":"<p>Note that the equation can be written as </p> \\[\\begin{align*} \\sigma(i\\Delta x, j\\Delta t)\\Delta x\\Delta t &amp;\\approx \\mathbf u_{i+1, j} + \\mathbf u_{i-1, j} + \\mathbf u_{i, j+1} + \\mathbf u_{i, j-1} - 4\\mathbf u_{i, j}\\\\ \\mathbf u_{i, j}&amp;\\approx \\frac{1}{4}(\\mathbf u_{i+1, j} + \\mathbf u_{i-1, j} + \\mathbf u_{i, j+1} + \\mathbf u_{i, j-1} - \\sigma(i\\Delta x, j\\Delta t)\\Delta x\\Delta t) \\end{align*}\\] <p>we pre-compute and rewrite \\(\\mathbf b_{ij} := -\\sigma(i\\Delta x, j\\Delta t)\\Delta x\\Delta t\\) as a grid and derive the Jacobi's method. </p> Jacobi's method<pre><code>def step(U, B):\n    max_error = 0\n    U_new = empty_like(U) \n    for i in range(n):\n        for j in range(n):\n            U_new[i, j] = (\n                U[i+1, j] + U[i-1, j] + U[i, j+1] + U[i, j-1] + B[i, j]\n            ) / 4.\n            error = abs(U_new[i, j] - U[i, j])\n            max_error = max(max_error, error)\n    return U_new, max_error\n\n# B = precompute matrix with all \n# sigma values on the grid\nU = initial_guess\nboundary_conditions(U)\nU, max_error = step(U, B)\nwhile max_error &gt; error_tolerance:\n    U, max_error = step(U, B)\n</code></pre> <p>Parallelizing Jacobi's method is very straight forward. For each step, each entries of \\(U\\) only requires the 4 neighboring entries. Therefore, the matrices \\(U, B\\) can be evenly divided onto \\(p\\) procs. At each step, the proc updates its partition of \\(U\\), and communicate (<code>sendrecv</code>) the boundary (\\(n/\\sqrt{p}\\)) to each neighboring procs. </p> <p>We can further optimize the communication by doing some extra computations per-proc. The halo area has side length \\(1\\), if we expand it to \\(k\\), then we can do \\(k\\) steps before one communication with \\(kn/\\sqrt{p}\\) data.</p> <p></p>"},{"location":"cs267/grid.html#red-black-gauss-seidel","title":"Red-black Gauss-Seidel","text":"<p>Note that Jacobi's method can be also async, meaning that we can do the value updates in-place. This will give better convergence speed than Jacobi's, since the values are closer to the true value. </p> Gauss-Seidel<pre><code>def step(U, B):\n    max_error = 0\n    U_new = empty_like(U) \n    for i in range(n):\n        for j in range(n):\n            old_value = U[i, j]\n            U[i, j] = (\n                U[i+1, j] + U[i-1, j] + U[i, j+1] + U[i, j-1] + B[i, j]\n            ) / 4.\n            error = abs(U[i, j] - old_value)\n            max_error = max(max_error, error)\n    return max_error\n</code></pre> <p>However, Gauss-Seidel is not parallelizable. Since each \\(\\mathbf u_{i,j}\\) will depend on \\(\\mathbf u_{i-1,j}, \\mathbf u_{i,j-1}\\). Therefore, we could take something in the middle. For each step, we do Jacobi's in a red-black manner. </p> Red-black GS<pre><code>def step(U, B):\n    max_error = 0\n    U_new = empty_like(U) \n    # red step\n    for i in range(0, n, 2):\n        for j in range(0, n, 2):\n            old_value = U[i, j]\n            U[i, j] = (U[i+1, j] + U[i-1, j] + U[i, j+1] + U[i, j-1] + B[i, j]) / 4.\n            error = abs(U[i, j] - old_value)\n            max_error = max(max_error, error)\n\n    # black step\n    for i in range(1, n, 2):\n        for j in range(1, n, 2):\n            old_value = U[i, j]\n            U[i, j] = (U[i+1, j] + U[i-1, j] + U[i, j+1] + U[i, j-1] + B[i, j]) / 4.\n            error = abs(U[i, j] - old_value)\n            max_error = max(max_error, error)\n\n    return max_error\n</code></pre> <p>R/B GS converges twice as fast as Jacobi. For higher dimensions and general graphs, if the graph stencils is 2 colorable, then we can use R/B GS. </p> <p>The parallelization is exactly the same as Jacobi's. </p>"},{"location":"cs267/grid.html#successive-overrelaxation-sor","title":"Successive overrelaxation (SOR)","text":"<p>Note that for each step, we are updating \\(\\mathbf u_{i,j}\\) through its finite difference, or an estimated gradient. Therefore, if we write the update step as </p> \\[\\mathbf u_{i,j}^{k+1} = \\mathbf u_{i,j}^k + c^k_{i,j}\\] <p>We could take more aggressive steps if \\(c\\) is a good direction to move. Therefore, we can take some \\(w&gt;1\\) and move further at each step as </p> \\[\\mathbf u_{i,j}^{k+1} = \\mathbf u_{i,j}^k + w c^k_{i,j}\\]"},{"location":"cs267/grid.html#conjugate-gradient","title":"Conjugate Gradient","text":"<p>More notes on deriving CG</p> <p>For system of equations \\(Ax = b\\), if \\(A\\) is symmetric and positive definite, then we can solve for \\(x\\) using conjugate gradient. </p> <p>The algorithm and possible parallelization is </p> Conjugate gradient<pre><code>def step(rem, old_rho):\n    rho = dot(rem, rem) # dot\n    if old_rho is None:\n        p = rem\n    else:\n        p = rem + (rho / old_rho) @ p # saxpy\n    q = A @ p # SpMV or stenceil\n    alpha = rho / dot(p, q) # dot\n    x += alpha * p # saxpy\n    r -= alpha * q # saxpy\n    return rho\n\nx = initial_guess(n)\nrem = b - A @ x\nold_rho = None\nwhile rho &gt; tolerance:\n    old_rho = step(rem, old_rho)\n</code></pre> <p>If \\(A\\) is a stencil, then this question can be reorganized to use matrix powers kernel to avoid communication. </p>"},{"location":"cs267/grid.html#multigrid","title":"Multigrid","text":"<p>Instead of solve a fixed size grid, adapting a coarse to fine approach. Starts with a coarse grid and updates till some error tolerance, then use it as a starting guess for a finer grid. Moreover, we can have multiple levels and solve the problem recursively. </p> <p>Let \\(A^i\\) be the problem at level \\(i\\). For example, if we have a 2D grid, we can have \\(A^1\\) be \\(2\\times 2\\), \\(A^2\\) be \\(4\\times 4\\), and the actual estimation wanted is \\(A:=A^k\\) be \\(2^k \\times 2^k\\). Let \\(\\mathbf b^i\\) be the known values at level \\(i\\) and \\(\\mathbf x^i\\) be the estimated solution at level \\(i\\). </p> <p>In addition, define the following operator</p> <ul> <li>Restriction <code>restrict</code> \\(R^i\\) s.t. \\(\\mathbf b^{i-1} = R^i(\\mathbf b^i)\\). Restricts known values at level \\(i-1\\) from \\(i\\) so that the problem won't diverge. </li> <li>Interpolation <code>lerp</code> \\(lerp^i\\) s.t. \\(\\mathbf x^i = lerp^i(\\mathbf x_*^{i-1})\\). Interpolate solutions of \\(x\\) at level \\(i-1\\) to make initial guesses at level \\(i\\) (often average of nearest neighbors). </li> <li>Solution <code>solve</code> \\(S^i\\) s.t. \\(\\mathbf x_*^i = S^i(\\mathbf b_i, \\mathbf x_i)\\) to be the solution of \\(A\\mathbf b= \\mathbf x\\) at level \\(i\\), \\(S\\) can be Jacobi, SOR, or any method mentioned above.</li> </ul> <p>Therefore, the overall algorithm is defined as </p> multigrid V-Cycle<pre><code>def MGV(b, x, level):\n    x = solve(b, x)\n    if level == 1:\n        return x # the coarsest level\n    r = A @ x - b # error on current level\n    b_res = restrict(r) # restricted error at level - 1\n    d = lerp(MGV(b_res, 0, level - 1)) # solve Ad = r recursively\n    x -= d # correction on grid solution\n    x = solve(b, x) # refine again\n    return x\n</code></pre> <p>Suppose that at each level, the grid half its side length, then the total computation is a geometric series</p> \\[\\sum_{i=1}^k O(2^i\\times 2^i) = O(4^k) = O(N)\\] <p>Paralleling multigrid is eventually similar to other method. At each level, divide the grid into squares for each proc, synchronization and communication is required at each level among neighbors. Note that when the dimension of procs is larger than the dimension of grid, then some procs have to be idle. Therefore, we don't have to go as coarse as level 1. </p>"},{"location":"cs267/la.html","title":"Parallel Dense Linear Algebra","text":"<p>For dense linear algebra, we need to solve problems such as matrix-multiplication, solve \\(x\\) for linear systems \\(Ax=b\\), and the special forms of linear systems including eigen decomposition, singular value decomposition, and accelerated algorithms on special properties of matrices. </p> <p>For parallelization, since the communication cost is critical to the performance, we need to consider that how should we partition the data, and hence what's the data layout, topology of machine (which proc to send to / receive from), and scheduling of communication. </p>"},{"location":"cs267/la.html#matrix-multiplications","title":"Matrix multiplications","text":"<p>For distributed dense linear algebra, the core consideration is still to minimize communication cost. From Communication Optimal Matmul we found that the communication lower bound is \\(\\Omega(N^2 / P^{1/2})\\) and is attainable. </p> <p>Consider the <code>dgemm</code> problem \\(C = A\\cdot B\\), wlog assume \\(A,B,C\\) are \\(n\\times n\\) where \\(n\\) is divisible by the number of procs \\(p\\). The computation is defined as</p> \\[c_{ij} = a_{i:}b_{:j} = \\sum_{k=1}^n a_{ik}b_{kj}\\] <p>and a review from blocked matmul, we have that each block of </p> \\[C[i_1:i_2, j_1:j_2] = A[i_1:i_2, :] \\cdot B[:, j_1:j_2]\\]"},{"location":"cs267/la.html#1d-column-layout","title":"1D Column Layout","text":"<p>Suppose that \\(A,B,C\\) are all stored in dense column major such that \\(a_{ij} = A[i + j\\times n]\\). Each proc aims to compute \\(n/p\\) columns, denote \\(C_{i,j} = C[\\frac{ni}{p}:\\frac{n(i+1)}{p}, \\frac{nj}{p}:\\frac{n(j+1)}{p}]\\) be the \\(\\frac{n}{p}\\times\\frac{n}{p}\\) block, \\(C_{i,:} = [\\frac{ni}{p}:\\frac{n(i+1)}{p}, :]\\) be the \\(\\frac{n}{p}\\) columns. Then proc \\(i\\) will compute </p> \\[C_{:, i} = A_{:,:}\\cdot B_{:,i} = \\sum_{j=0}^{p-1} A_{:, j} \\cdot B_{j,i}\\] <p>Suppose that we cannot broadcast \\(A\\), a ring-based algorithm will be </p> 1D Column major<pre><code>/*  A_mat: the full A matrix stored in rank 0\n    n = matrix dimension, p = num procs\n*/\n\n// Scatter(src_ptr, size, dest_ptr, size);\n// SendRecv(send_ptr, send_size, dest_rank, recv_ptr, recv_size, send_rank);\nScatter(C_mat, n * n, C, n * n / p); // C[:, rank]\nScatter(A_mat, n * n, A, n * n / p); // A[:, rank]\nScatter(B_mat, n * n, B, n * n / p); // B[:, rank]\n\ndgemm(C, A, B); // Compute O(n * (n / p) * (n / p))\nfor (int i = 0; i &lt; p; i++) {\n    double *A_old = copy(A, n * n / p);\n    // send A to next proc and receive A from previous proc\n    SendRecv(A_old, n*n/p, (rank + 1) % p,\n             A, n*n/p, (rank - 1) % p); // Communicate 2 * n^2 / p\n    dgemm(C, A, B); // Compute O(n * (n / p) * (n / p))\n    sync();\n}\n</code></pre> <p>For the inner loop, the communication cost is \\(2(\\alpha + \\beta n^2 / p)\\) and computation cost is \\(2n(n/p)^2 = 2n^3/p^2\\). Therefore the total time is </p> \\[p(2(\\alpha + \\beta n^2 / p) + 2n^3/p^2) = 2n^3/p + 2p\\alpha + 2\\beta n^2\\] <p>and efficiency is </p> \\[\\frac{2n^3}{2n^3/p + 2p\\alpha + 2\\beta n^2} = \\frac{1}{1+O(p/n)}\\]"},{"location":"cs267/la.html#summa-scalable-universal-matrix-multiply","title":"SUMMA: Scalable Universal Matrix Multiply","text":"<p>A better implementation for lower communication cost is SUMMA. Instead of partition by the columns of \\(C\\), each proc computes a square block \\(C_{i,j}\\) of dimension \\(n/\\sqrt{p}\\) in a scalable way. However, we can broadcast the columns of \\(A\\) and rows of \\(B\\) to each proc \\((i,j)\\) all at once since the memory cannot hold such large matrices. </p> <p>Instead, let each proc store \\(A_{i,k}\\) and \\(B_{k,j}\\) and we have the algorithm</p> SUMMA<pre><code>/* Suppose that p can be perfectly squared\n   rank is 2D tuple \n   A_old stores A[rank[0], rank[1]] block\n   B_old stores B[rank[0], rank[1]] block\n*/\nfor (int k = 0; k &lt; sqrt(p); k++) {\n    for (int i = 0; i &lt; sqrt(p); i++) {\n        if (rank == (i, k))\n            // Bcast A_old to rank (i, *)\n    }\n    for (int j = 0; j &lt; sqrt(p); j++) {\n        if (rank == (k, j))\n            // Bcast B_old to rank (*, j)\n    }\n    // recv the broadcasted content \n    Recv(A); \n    Recv(B); \n    dgemm(C, A, B);\n}\n</code></pre> <p>By using broadcast, the communication time is reduced to \\(\\log(p)\\).</p>"},{"location":"cs267/la.html#other-optimizations","title":"Other optimizations","text":"<p>2.5D matmul Note that up to now (1D and SUMMA), each proc only stores \\(3n^2/p\\) words of data. However, when we have more procs, we also have more memory and cache (L1 cache was built per-proc). Thus, what if each proc stores more data so that the total amount of data is \\(3cn^2, c&gt;1\\). In this case, the communication lower bound should be smaller, since we don't need to exchange data that already redundant. </p> <p>Strassen's matmul A \\(2\\times 2\\) matrix can be computed in 7 multiplications and 18 adds instead of 8 multiplications and 4 adds.</p> \\[\\begin{align*} p_1 &amp;= (a_{12} - a_{22})(b_{21}+b_{22})\\\\ p_2 &amp;= (a_{11} + a_{22})(b_{11}+b_{22})\\\\ p_3 &amp;= (a_{11} - a_{21})(b_{11}+b_{12})\\\\ p_4 &amp;= (a_{11} + a_{12})b_{22}\\\\ p_5 &amp;= a_{11} (b_{12} - b_{22})\\\\ p_6 &amp;= a_{22} (b_{21} - b_{11})\\\\ p_7 &amp;= (a_{21} + a_{22}) b_{11}\\\\ C   &amp;= \\begin{bmatrix} p_1 + p_2 - p_4 + p_6 &amp; p_4 + p_6\\\\ p6 + p_7 &amp; p_2 - p_3 + p_5 - p_7 \\end{bmatrix} \\end{align*}\\]"},{"location":"cs267/la.html#gaussian-elimination","title":"Gaussian Elimination","text":"<p>GE and LU Decomposition and Pivoting in serial implementation. </p> <p>In general, a Gaussian elimination with partial pivoting (GEPP) as</p> GEPP<pre><code>pivots = []\nfor i in range(1, n - 1):\n    k = argmax(abs(A[i:, i])) + i # kth row has the largest abs value at A[k,i]\n    if abs(A[k, i]) == 0:\n        raise ValueError(\"A is nearly singular\")\n    elif k != i:\n        A[i, :], A[k, :] = A[k, :], A[i, :]\n    A[i+1:, i] /= A[i, i]\n    A[i+1:, i+1:] -= A[i+1:, i] * A[i, i+1:]\n</code></pre> <p>All the computations here uses vector-vector or matrix-vector operations, which can be vectorized through blocking. The idea is delayed updates by saving updates to \"trailing matrix\" from several consecutive update and apply one mat-mat multiplication at once. </p> Blocked GEPP<pre><code>for bs in range(1, n - 1, b):\n    be = bs + b - 1\n    pivots, L, U = GEPP(A[bs:, bs:be]) # 1 \n    LL = identity(be - bs) + strict_lower_triangle(A[bs:be, bs:be])\n    A[bs:be:, be+1:] = inv(LL) @ A[bs:be, be+1:] # 2\n    A[be+1:, be+1:] -= A[be+1:, bs:be] @ A[bs:be:, be+1:] # 3\n</code></pre> <p></p>"},{"location":"cs267/ml.html","title":"Parallel Machine Learning","text":""},{"location":"cs267/ml.html#multi-gpu-stochastic-gradient-descent","title":"Multi-GPU Stochastic Gradient Descent","text":"<p>For mini-batch SGD, the weights are updated as </p> \\[W^{t+1} \\leftarrow W^t - \\alpha \\frac{1}{B}\\sum_{i=k+1}^{k+b}\\nabla_W f_i(W^t, x)\\] <p>where \\(\\alpha\\) is the learning rate and \\(B\\) is the mini-batch size.</p>"},{"location":"cs267/ml.html#batch-parallelism","title":"Batch parallelism","text":"<p>For batch parallelism, distribute input samples (training data) to different processors, and each processor get a replica of model weights. There are several ways to update the weights.</p> <ul> <li>Using a parameter server (master process), all other processes will do one iteration, send the gradient to the server, wait to receive the new weights. <ul> <li>The fetching and updating of gradients can be either synchronously (wait for all processes finish one iteration, and have all gradients processed by the parameter server) or asynchronously (the parameter server sends partially updated weights whenever receives weights update).</li> <li>Synchronous updates suffers heavily from communication and synchronization.</li> <li>Asynchronous updates is not-reproducible and might hurt convergence. </li> </ul> </li> <li>Synchronous method, but use <code>all-reduce</code> routine over the network to update gradients. Can avoid communication bottleneck on the parameter server. </li> <li>Asynchronous method, peer-to-peer gossiping, or send and receive gradients to all other processes. The communication overhead will be higher but the convergence performance is better. </li> </ul> <p>In either model, we need to replicated \\(W\\) onto each process at each iteration, thus the communication overhead for each iteration is </p> \\[T_{comm} = 2 \\alpha \\log(P) + \\beta \\frac{P-1}{P}|W_i|\\] <p>Since data is not communicated among processors, the overhead is independent of the batch size. However, if we have more servers, the batch size will be smaller due to the small data partition, and the per-processor utilization goes down significantly. </p>"},{"location":"cs267/ml.html#model-parallelism","title":"Model parallelism","text":"<p>Consider the data dependency graph of a neural network. The forward pass is often a sequence of matrix multiplication and per-entry activation functions. Therefore, it's possible to distribute the matrix multiplications and corresponding activations to different processors, and then gather at the output \\(Y\\) and enable back-propagation (essentially more matmuls) partitioned on different processors. </p> <p>However, model parallelism is hard, since the many dimensions and new per-layer operators. Many models requires manually define the distribution methods. </p>"},{"location":"cs267/ml.html#domain-parallel-for-cnn","title":"Domain parallel for CNN","text":"<p>For CNN, note that the convolution is performed on each pixel, the bottleneck is on the activations (\\(H\\times W\\times \\times C_{out}\\)) instead of the filter weights (\\(F\\times F\\times C_{in}\\times C_{out}, F&lt;&lt;H/W\\)). Therefore, we can partition images into smaller patches over \\((H, W)\\) and distribute over different devices. Note that this is beneficial for for early layers since \\(H,W\\) gets smaller after pooling. </p> <p>Assuming that each device gets a square patch and the boundary if of the size of \\(F//2\\), then for each layer we need to broadcast the weights (filters) and send/recv the halo boundary to 2D neighboring device. </p> \\[\\begin{align*} T_{input} &amp;= \\sum_{i=0}^L \\alpha + \\beta B W^i_{in} C_{in}^i F^i/2\\\\ T_{activation} &amp;= \\sum_{i=0}^L \\alpha + \\beta B W^i_{out} C_{out}^i F^i/2\\\\ T_{weights} &amp;= \\alpha\\log(P) + \\beta \\frac{P-1}{P}|F^iF^i| \\end{align*}\\]"},{"location":"cs267/ml.html#pipeline-parallelism","title":"Pipeline Parallelism","text":"<p>Divides input mini-batch into smaller micro-batches, different GPUs work different micro-batches simultaneously. Gradients are sync at the end. </p> <p>Pipeline bubbles start of the forward propagation on a minibatch, requires the backprop of the previous minibatch to complete. </p>"},{"location":"cs267/ml.html#support-vector-machine","title":"Support Vector Machine","text":"<p>A linear support vector machine aims to find a linear separator to classify data, such that the separation bandwidth is maximized. </p> \\[\\arg\\max_{\\mathbf w, b} \\frac{1}{\\|w\\|}\\:s.t.\\: \\forall i \\in\\mathcal D. \\mathbf y_i(\\mathbf w^T \\mathbf x_i + b)\\geq 1\\] <p>For non-linear separators, we transform all input \\(x\\) into features \\(\\phi(x)\\) and find a linear separator in the feature space. In practice, the feature space will have higher dimension than the input space (otherwise we can find a linear separator in the input space directly).</p> <p>Therefore, a full SVM  takes a feature dataset (after applying \\(\\phi\\) to  each of the input) of size \\(N\\times D\\), where \\(N\\) is the number of data points, and \\(D\\) is the feature dimension. For each pair of the data point \\(\\mathbf x, \\mathbf y\\), compute the similarity w.r.t. the current \\(w, b\\), the resulting similarity matrix (kernel matrix) is \\(N\\times N\\) where \\(ij\\)th entry is the similarity between \\(x_i, x_j\\). </p>"},{"location":"cs267/ml.html#kernel-methods","title":"Kernel Methods","text":"<p>Replace the dot-product similarity with an arbitrary similarity (symmetric, positive semi-definite) kernel \\(K(x,y): \\mathcal X \\times \\mathcal X \\rightarrow \\mathbb R\\). For example, the Gaussian kernel \\(K(\\mathbf x, \\mathbf y) = \\exp(\\|-\\gamma \\| \\mathbf x -\\mathbf y\\|^2)\\).</p> <p>Another full example that to compute a 2D circular decision boundary.  Define the feature transformation as \\(\\phi(x,y) = (x^2, y^2, \\sqrt{2} x y)\\) and the kernel function \\(K(\\mathbf x_1, \\mathbf x_2) = (\\mathbf x_1\\cdot\\mathbf x_2)^2\\). The magic is that </p> \\[K(\\phi(x_1,y_1), \\phi(x_2, y_2)) = x_1^2y_1^2 + x_2^2 y_2^2 + 2\\sqrt{2}x_1x_2y_1y_2 = (x_1y_1 + x_2y_2)^2\\]"},{"location":"cs267/ml.html#sequential-minimal-optimization","title":"Sequential minimal optimization","text":"<p>Note that at each step of optimization, na\u00efve SVM generates \\(O(N^2)\\) kernel matrix in \\(O(DN^2)\\) time. SMO avoids computing the large kernel matrix all at once by taking only two rows of the kernel matrix at each iteration. For binary classification task, SMO can be expressed in dual form as </p> \\[\\begin{align*} \\arg\\max_{\\Alpha} &amp;\\sum_{i=1}^N a_i - \\frac{1}{2} \\sum_{i=1}^N\\sum_{j=1}^N a_ia_j y_iy_j K(\\mathbf x_i, \\mathbf x_j)\\\\ s.t. &amp; \\forall a_i \\in A. 0 \\leq a_i \\leq C. \\sum_{i=1}^N a_iy_i = 0 \\end{align*}\\] <p>where \\(C\\) is the a defined hyperparameter, \\(y_i\\) is the label of \\(\\mathbf x_i\\), and \\(a\\)'s are the Lagrange multipliers. </p> <p>The algorithm select some pair \\(a_i, a_j\\) (with heuristics) and update the total cost by only re-computing the \\(i,j\\) rows. </p>"},{"location":"cs267/ml.html#possible-parallelism","title":"Possible parallelism","text":"<ul> <li>Cascade SVM: evenly partition the data and create SVM for each data partition, then for each layer, remove the non-support vectors in a tree structure.</li> <li>Divide-and-Conquer SVM: partition the data by k-means, and for each layer, all data are used to train the layer-wise SVM (instead of removing SV). </li> <li>Cluster partition SVM: partition the data by k-means, then instead of training a tree of SVM, just train k SVs and choose the best one on the test dataset.  </li> </ul>"},{"location":"cs267/ml.html#parallel-graph-based-machine-learning","title":"Parallel Graph based Machine Learning","text":""},{"location":"cs267/ml.html#clustering","title":"Clustering","text":"<p>Given a set of data, clustering aims to exploit compactness (data points that are close to each other) or connectivity (data points that falls into the same underlying kernel). </p> <p>A clustering can be understood as a graph partition problem. The vertices are data points and the edge between each pair of data points, weighted by the similarity. Then, given the aimed number of clusters \\(k\\), we want to find a normalized minimum cut to divide the graph into \\(k\\) disconnected components, where each partition has balanced number of vertices (data points) and the total similarity is maximized. </p>"},{"location":"cs267/ml.html#spectral-clustering","title":"Spectral Clustering","text":"<p>The idea is to do the partition recursively. Partition the data into two sets of similar size by finding the minimum cut (cut s.t. the total weights/similarity on the edges is minimized), and then recursively partition each set until enough clusters are identified. </p> <p>First, represent the graph using the adjacency matrix. Let \\(D\\) be the degree matrix where \\(D\\) is a diagonal matrix with \\(d_{ii} = deg(\\mathbf v_i)\\). Let \\(W\\) be the adjacency weight matrix where \\(w_{ij} =\\) the weight/similarity of edge \\((v_i, v_j)\\). Then, the unnormalized graph Laplacian is \\(L = D-W\\) and two forms of normalized graph Laplacians</p> \\[L_{sym} = D^{-1/2} L D^{-1/2} = I - D^{-1/2}WD^{-1/2}\\] \\[L_{rw} = D^{-1}L = I - D^{-1}W\\] <p>Then, the spectral clustering algorithm is defined as </p> spectral clustering<pre><code>D, W = construct_graph(data)\nL_sym = normalized_graph_laplacian(D, W)\nvals, vecs = eig(L_sym) # eigen decomposition\nU = vecs[-k:] # k smallest eigen vectors\nU = normalize(U, axis=0) # row-wise normalization\n# regard each row of U as a data point and run k-means\nk_means(U, axis=0)\n</code></pre> <p>Note that the penalization is possible for almost all steps :graph construction by parallelizing the similarity computation, sparse matrix vector (SpMV) for \\(L_sym\\), eigen decomposition, and normalization. </p>"},{"location":"cs267/ml.html#markov-cluster-mcl","title":"Markov Cluster (MCL)","text":"<p>For a graph based clustering problem, we make the assumption that the total weights for any pair of vertices within the same cluster will be greater than vertices in two different clusters. Given such assumptions, perform a random walk on the graph (probability by edge weights) will be more likely to remain within a cluster. Therefore, the algorithm computes the probability of random walks through the graph, and remove lower probability terms. </p> <p>For each step, compute</p> <ol> <li>expansion compute the higher length random walk by squaring the edge weight matrix, remove random walks with lower probability.</li> <li>inflation elementwise powers to boost the probabilities so that walks inside clusters gets strong weights.</li> </ol> <p>Possible prallelism involves a sparse matrix squaring (SpGEMM) and pruning all entries belong a threshold (can do it inplace, along with SpGEMM). </p>"},{"location":"cs267/ml.html#dimension-reduction","title":"Dimension Reduction","text":"<p>For a dataset of \\(N\\times D\\), dimension reduction aims to find a smaller feature set \\(N\\times K\\) where \\(K &lt;&lt; D\\) while the information loss is minimized. </p>"},{"location":"cs267/ml.html#principal-component-analysis","title":"Principal Component Analysis","text":"<p>PCA is the most classical algorithm for dimension reduction, and it is equivalent to a singular value decomposition (proof of equivalence). </p>"},{"location":"cs267/ml.html#non-negative-matrix-factorization-nmf","title":"Non-negative matrix factorization (NMF)","text":"<p>NMF aims to solve the optimization problem</p> \\[\\arg\\min_{W\\geq 0, H\\geq 0} \\frac{1}{2}\\|A - WH\\|_F\\] <p>where \\(A \\in\\mathbb R^{m\\times n}, W\\in\\mathbb R^{m\\times k} , H \\in \\mathbb R^{k\\times n}\\) are all non-negative, \\(k &lt;&lt; m, n\\). For this class of problem, one common framework is the alternating iterative method, at each step, fixing one matrix and optimize the energy by updating the other matrix. </p> <p>The possible parallelism is the large-scale matrix multiplications. Also, depends on the application, \\(A\\) is often much larger than \\(W,H\\) and also sparse (correlation matrix, count matrix, etc.). Therefore, avoiding communications on \\(A\\).  </p>"},{"location":"cs267/ml.html#graph-neural-networks","title":"Graph Neural Networks","text":"<p>For GNN, a graph \\((V, E)\\) where each node \\(v\\) describes a data point and edge/adjacency \\(e\\) describes its inter-data relationships. Taking one node \\(v\\) and its adjacent relationship (edges) \\(e_0, e_1, ..., e_{deg(v)}\\), we encode the data into its latent representation \\(\\mathbf h\\) (often a vector embedding) via a GNN. Then, we are interested in various results by integrating the latent code only, latent code convolving with all adjacency information, and querying inter-nodes relationship given two latent nodes and their connectivity. </p> <p></p> <p>One special case for GNN is CNN, if we consider each pixel as a node, and add edges to all neighboring pixels with distance smaller than half kernel size, and each edge is weighted by the kernel weights. </p> <p>In general, a graph is represented by a sparse matrix (adjacency matrix) or tensor (\\(|V| \\times |V| \\times |C|\\)), and the operations are very similar to CNN, instead of doing 2D convolutions, we are doing graph convolutions, which is a more unstructured convolution with larger radius. </p> <p>The challenges for parallelism includes the partitioning of sparse matrices and the distribution of data (since conventionally we represents a sparse matrix as <code>(row, col, value)</code> or pointer-chasing). Also, GNN often consumes larger amount of memory, and hence distributed learning (multiple GPU support) is often required. </p>"},{"location":"cs267/random.html","title":"Randomized Algorithms","text":""},{"location":"cs267/random.html#dimension-reduce-with-random-sketching","title":"Dimension reduce with Random sketching","text":"<p>Given a high dimensional dataset, wants to find an embedding to reduce into a lower dimensional one, while preserving some geometry, with high probability. </p> <p>More formally, An embedding for a set \\(S\\subseteq \\mathbb R^n\\) with distortion \\(\\epsilon\\) is an \\(l\\times m\\) matrix \\(\\Omega\\) s.t. </p> \\[\\forall x \\in S. (1 - \\epsilon) \\|x\\|^2 \\leq \\|\\Omega x\\|^2 \\leq (1 + \\epsilon)\\|x\\|^2\\] <p>A subspace embedding is an embedding for a set \\(S\\), where \\(S\\) is a \\(m\\)-dimensional linear subspace. </p>"},{"location":"cs267/random.html#obvious-subspace-embedding","title":"Obvious subspace embedding","text":"<p>Intuitively, an obvious subspace embedding is a subspace embedding at most times. Formally, a random matrix \\(\\Omega \\in \\mathbb R^{l\\times m}\\) is an obvious subspace embedding with parameters \\(\\text{OSE}(n, \\epsilon, \\delta)\\) if with probability \\(\\geq 1 - \\delta\\), \\(\\Omega\\) is a subspace embedding for \\(S\\) with distortion \\(\\epsilon\\). </p> <p>A dense Gaussian matrix \\(\\Omega \\in \\mathbb R^{l\\times m}\\) is a matrix whose entries are i.i.d. standard Gaussian random variables \\(\\omega_{ij} \\sim N(0, 1)\\), multiplied by \\(l^{-1/2}\\) where </p> \\[l = O(\\epsilon^{-2}(n+ \\log(\\frac{1}{\\epsilon})))\\] <p>Then, such \\(\\Omega\\) is \\(\\text{OSE}(n, \\epsilon, \\delta)\\). Which means that we can sample some matrix \\(\\Omega\\), compute \\(I\\) from OSE parameters, and do the embedding. </p> <p>The embedding is just a matrix matrix multiplication, thus the total parallelized cost is </p> \\[O(\\gamma(2mnl / p) + \\alpha(\\lg p)) + \\beta(ln \\lg p)\\]"},{"location":"cs267/random.html#fast-johnson-lindenstrauss-transform","title":"Fast Johnson-Lindenstrauss Transform","text":"<p>The goal is to find a sparse or structured \\(\\Omega\\), we compute a subsampled random Hadamard transform (SRHT).</p> <p>Given \\(m=2^q, l &lt; m\\), the SRHT ensemble embedding \\(\\mathbb R^m\\) into \\(\\mathbb R^l\\) is defined as </p> \\[\\Omega = \\sqrt{\\frac{m}{l}} P H D\\] <ul> <li>\\(D\\in \\mathbb R^{m\\times m}\\) is a diagonal matrix whose diagonals are uniformly random signs \\(\\pm 1\\). </li> <li>\\(H\\in\\mathbb{R}^{m\\times m}\\) is the normalized Walsh-Hadamard transform. </li> <li>\\(P\\in \\mathbb{R}^{l\\times m}\\) is the random choose matrix to draw \\(l\\) rows from \\(HD\\). </li> </ul> <p>The normalized Walsh-Hadamard transform is defined that for given \\(m=2^q\\), define the (unnormalized) WH transform as</p> \\[H_2 = \\begin{bmatrix}1&amp;1\\\\1&amp;-1\\end{bmatrix}, H_m = \\begin{bmatrix}H_{m/2}&amp;H_{m/2}\\\\H_{m/2}&amp;-H_{m/2}\\end{bmatrix}\\] <p>and the normalization is \\(H = m^{-1/2}H_m\\). </p> <p>SRHT is \\(\\text{OSE}(n,\\epsilon, \\delta)\\) with \\(l = O(\\epsilon^{-2}(n+\\lg \\frac{m}{\\delta}) ln \\frac{n}{\\delta})\\). </p> <p>Note that because of the special structure of the matrix, the actual computation time is much reduced, the parallelized cost is </p> \\[O(\\gamma(2mn\\lg m / p) + \\alpha(\\lg p)) + \\beta(mn \\lg p)\\]"},{"location":"cs267/random.html#problems-using-random-sketching","title":"Problems using random sketching","text":""},{"location":"cs267/random.html#least-squares-problems","title":"Least squares problems","text":"<p>Given \\(A\\in\\mathbb R^{m\\times n}\\) and \\(b\\in\\mathbb R^n\\) with \\(n &lt;&lt; m\\)$, aims to solve </p> \\[y:= \\arg\\min_{x\\in\\mathbb R^n} \\|Ax - b\\|_2\\] <p>One (deterministic) approach for this problem is to do QR factorization on \\(A\\), or we can solve by random sketching with \\(\\Omega \\in \\mathbb R^{l\\times m}\\) </p> \\[y^* := \\arg\\min_{x\\in\\mathbb R^n}\\|\\Omega(Ax - b)\\|_2\\] <p>Can be shown that with probability \\(\\delta\\), we have </p> \\[\\|Ay^* - b\\|_2^2 \\leq (1+O(\\epsilon))\\|Ay-b\\|_2^2\\]"},{"location":"cs267/random.html#low-rank-matrix-approximation","title":"Low rank matrix approximation","text":"<p>For a matrix \\(A\\in\\mathbb R^{m\\times n}\\), decompose \\(A\\) into lower rank approximations \\(Z\\cdot W^T\\) where \\(Z\\in \\mathbb R^{m\\times k}, W^T \\in \\mathbb R^{k\\times n}\\). The low rank approximation is commonly used in many problems, as the number of flops is significantly reduced (\\(2mn\\) vs. \\(2(m+n)k\\)). </p>"},{"location":"cs267/random.html#singular-value-decomposition","title":"Singular value decomposition","text":"<p>One of the most common way for matrix approximation \\(A = U\\Lambda V^T\\), a rank-\\(k\\) approximation of \\(A\\) takes the first \\(k\\) singular vectors \\(A_k = U_k \\Sigma_k V_k^T\\) where \\(U_k\\in\\mathbb R^{m\\times k}, V_k^T \\in\\mathbb R^{k\\times n}\\) is the first \\(k\\) columns of \\(U, V\\) respectively, \\(\\Sigma_k\\in\\mathbb R^{k\\times k}\\) is the first \\(k\\) singular values.</p>"},{"location":"cs267/random.html#rank-revealing-qr-factorization","title":"Rank Revealing QR factorization","text":"<p>Given \\(A \\in\\mathbb R^{m\\times n}\\), consider the QR decomposition</p> \\[A\\Pi_c = QR = \\begin{bmatrix}Q_1&amp;Q_2\\end{bmatrix} \\begin{bmatrix}R_{11}&amp;R_{12}\\\\&amp;R_{22}\\end{bmatrix}\\] <p>where we choose \\(k\\) and a column permutation \\(\\Pi_c\\) so that</p> <ul> <li>\\(R_{11} \\in\\mathbb R^{k\\times k}\\) is well-conditioned</li> <li>\\(\\|R_{22}\\|\\) is small</li> </ul> <p>Then, we can have a rank-\\(k\\) approximation of \\(A_{qr}\\) as </p> \\[A_{qr} = Q_1 \\begin{bmatrix}R_{11}&amp;R_{12}\\end{bmatrix} \\Pi_c^T = Q_1Q_1^T A =: \\mathcal P^\\circ A\\] <p>and we have the error bound \\(\\|A - A_{qr}\\|_2 = \\|R_{22}\\|\\), which is small by construction. </p>"},{"location":"cs267/random.html#qr-with-column-pivoting-qrcp","title":"QR with column pivoting (QRCP)","text":"<p>Based on the desired construction, the algorithm is</p> QR with column pivoting<pre><code>def QRCP(A):\n    m, n = A.shape\n    Q = identity()\n    colnorm = A.norm(axis=1) # column-wise norm of size n\n    for j in range(n):\n        p = argmax(colnorm) # the column with largest norm\n        if colnorm[p] &gt; eps:\n            # pivoting: swap columns of A and colnorm\n            A[:, j], A[:, p] = A[:, p], A[:, j]\n            colnorm[j], colnorm[p] = colnorm[p], colnorm[j]\n            # reduction: determine Householder matrix Hj\n            Hj = Householder(A) # Hj @ A[j:, j] = +/- norm(A[j:, j]) @ e1\n            # update\n            A[j:, j+1:] = Hj @ A[j:, j+1:]\n            colnorm[j+1:] = colnorm[j+1:] - A[j, j+1:] ** 2\n            Q = Q @ Hj\n        else:\n            break\n    return (Q, A) # Q = Hj * ... H1, R = A\n</code></pre>"},{"location":"cs267/sort.html","title":"Parallel Searching and Sorting","text":""},{"location":"cs267/sort.html#master-theorem-for-recursions","title":"Master Theorem for Recursions","text":"<p>A quick recap for the time complexity of recurrence algorithms. Let \\(T(n)\\) be the time complexity of a problem of size \\(n\\), \\(b\\) be the number of sub problems in the recursive step, and \\(f(n)\\) be the time for the non-recursive part. Then a recursive algorithm's running time can be expressed as </p> \\[T(n) = a T(n/b) + f(n)\\] <p>Then, we have </p> <ul> <li>\\(f(n) = O(n^{\\log_b a - \\epsilon}), \\epsilon &gt; 0 \\implies T(n) \\in \\Theta(n^{\\log_b a})\\). </li> <li>\\(f(n) = O(n^{\\log_b a \\lg^k n}), k \\geq 0 \\implies T(n) \\in \\Theta(n^{\\log_b a} \\lg^{k+1}n)\\).</li> <li>\\(f(n) = O(n^{\\log_b a + \\epsilon}), \\epsilon &gt; 0 \\implies T(n) \\in \\Theta(f(n))\\) (with regularity condition).  </li> </ul> <p>Example cases: binary divide and conquer problem such as merge sort, binary search is </p> \\[T(n) = 2T(n/2) + \\Theta(n), f(n) = \\Theta(n^{\\log_2^2} \\lg^0 n), T(n) = \\Theta(n^{\\log_2^2} \\lg^1 n) = \\Theta(n\\lg n)\\]"},{"location":"cs267/sort.html#k-th-largest-element","title":"K-th Largest Element","text":"<p>Task: find the kth largest element in a list of length \\(N\\), often used for finding median or quartiles. An easy (and relatively efficient) algorithm is <code>quickselect</code>, basically similar to <code>quicksort</code>, pick a pivot and subdivide the problem to larger than and smaller than. Then, depends on the position of \\(k\\), go to one of the subproblem. The average time is \\(O(n)\\) and the worst case is \\(O(n^2)\\), each time picked an extremum within the current array. </p>"},{"location":"cs267/sort.html#median-of-the-medians","title":"Median of the Medians","text":"<p>Based on <code>quickselect</code>, but guarantees worst case linear time. </p> <pre><code>def select(arr, k):\n    \"\"\" find the kth smallest element in arr\n    \"\"\"\n\n    # divide arr into chunks of 5 elements, and sort each chunk\n    # O(n) time, each sort on O(1) elements with O(n/5) arrs\n    chunks = [\n        sorted(arr[i: i+5]) \n        for i in range(0, len(arr), 5)\n    ] \n    medians = [chunk[len(chunk) // 2] for chunk in chunks]\n\n    # O(n/5) size recursive call\n    if len(medians) &lt;= 5:\n        pivot = sorted(medians)[len(medians) // 2]\n    else:\n        pivot = select(medians, len(medians) // 2)\n\n    # O(n) time\n    p = partition(arr, pivot)\n\n    # recursion depends on the position of pivot\n    if k == p:\n        return pivot\n    if k &lt; p:\n        # O(7n/10) recursive\n        return select(arr[0:p], k)\n    else:\n        # O(7n/10) recursive\n        return select(arr[p+1:len(arr)], k - p - 1)\n\n\ndef partition(arr, pivot):\n    \"\"\" Two pointers implementation of \n    partition. partition the arr such that\n    arr[i] &lt; pivot for all i &lt; p\n    arr[i] &gt;= pivot for all i &gt; p\n    \"\"\"\n    left, right = 0, len(arr) - 1\n    i = 0\n\n    while i &lt;= right:\n        if arr[i] == pivot:\n            i += 1\n        elif arr[i] &lt; pivot:\n            arr[left], arr[i] = arr[i], arr[left]\n            left += 1\n            i += 1\n        else:\n            arr[right], arr[i] = arr[i], arr[right]\n            right -= 1\n    return left\n</code></pre> <p></p> <p>Note that \\(O(7n/10)\\) comes from excluding at least \\(O(3n/10)\\) elements. Therefore, by Master's Theorem, the total time is</p> \\[T(n) = T(n/5) + T(7n/10) + O(n) = T(\\frac{9}{10}n) + O(n)\\in O(n)\\]"},{"location":"cs267/sort.html#weighted-mom-for-parallelism","title":"Weighted MoM for Parallelism","text":"<p>Based on the algorithm, a simple idea is to let each processor compute a local median, gather and find the global median of medians, then discard elements and recurve. However, if the array is not evenly distributed, then the global median of medians is not held. Therefore, after each recursion stack, we need to redistribute arrays to make it even, which takes large amount of communication. </p> <p>Define the weighted median of the medians by given \\(p\\) distinct elements \\(m_1,...,m_p\\) with corresponding positive weights \\(w_1,...,w_p\\) s.t. \\(\\sum_{i=1}^p w_i = 1\\), the weighted median is the element \\(m_j\\) s.t. \\(\\sum_{i:m_i &lt; m_j} w_i\\leq 1/2 \\land \\sum_{i:m_i &gt; m_j} w_i\\leq 1/2\\). </p> <pre><code># distribute the global array to local proc\narr = scatter(arr_global)\n\ndef parallel_select(arr, k):\n    median_l = select(arr, k)\n    # exchange local medians\n    medians_g = Allgather(median_l)\n\n    # find the weighted median \n    wmm = weighted_median(medians_g)\n\n    p_l = partition(arr, wmm)\n\n    # total number of elements smaller than wmm\n    p_g = Allreduce(p_l, SUM)\n\n    if k == p:\n        return wmm\n    if k &lt; p:\n        # O(3n/4) recursive\n        return parallel_select(arr[0:p_l], k)\n    else:\n        # O(3n/4) recursive\n        return parallel_select(arr[p_l+1:], k - p_l - 1)\n</code></pre> <p>The recursive step is \\(O(3n/4)\\) because the weighted median guarantees that we are discarding half of the elements that's smaller than respective local median, and by median of medians, that's \\(1/4\\) in total. </p>"},{"location":"cs267/sort.html#merge-sort-for-shared-memory-model","title":"Merge Sort for Shared Memory Model","text":"<p><pre><code>def P_mergesort(arr):\n    n = len(arr)\n    if n == 1:\n        return arr\n    arr_l = P_mergesort(arr[:n//2])\n    arr_r = P_mergesort(arr[n//2:])\n    return merge(arr_l, arr_r)\n</code></pre> For merge sort, it's natural to parallelize over the recursive calls. Consider the spanned stack tree, the span (width) is \\(O(n)\\) (the base case where each array has only 1 element), and the depth is \\(O(\\lg n)\\). Therefore, the ideal running time for parallelized merge sort should be \\(O(\\lg n)\\). However, we know that <code>merge</code> takes \\(O(n)\\) time, thus we need to parallelize merge as well. </p> <pre><code>def P_merge(arr1, arr2):\n    \"\"\" arr1 and arr2 are both sorted\n    return sorted arr1 + arr2\n    \"\"\"\n    if len(arr1) &lt; len(arr2):\n        arr1, arr2 = arr2, arr1\n    if len(arr2) == 0:\n        return arr1\n    mid1 = len(arr1) // 2\n    # binary search for the index s.t. \n    # arr2[:mid2] are smaller than arr1[mid]\n    mid2 = bisect(arr2, arr1[mid1])\n    return P_merge(arr1[:mid1], arr2[:mid2]) + \\\n           P_merge(arr1[mid1:], arr2[mid2:])\n</code></pre>"},{"location":"cs267/sort.html#sorting-for-distributed-memory-model","title":"Sorting for Distributed Memory Model","text":""},{"location":"cs267/sort.html#bucket-sort","title":"Bucket Sort","text":"<p>Assume that we have the range of the data, then we can partition the range into \\(p\\) intervals. Each interval will corresponds to one proc, or one \"bucket\" that holds all elements fall within the interval. Each processor runs through its local list and assigns each of its elements to the appropriate \"bucket\" (communication required). Then, each processor can sort its own bucket and finally gather all elements. </p> <p>The main issue with bucket sort is load imbalance, it's hard to pre-decide the interval partitions. </p>"},{"location":"cs267/sort.html#sample-sort","title":"Sample Sort","text":"<p>Sample sort aims to solve the load imbalance issue with bucket sort.  1. scatter the global array of size \\(n\\) into \\(p\\) procs, each with size \\(n/p\\).  2. Each proc sort the local array 3. Select \\(p-1\\) evenly spaced samples from local array. 4. Gather the \\(p(p-1)\\) samples, sort the pivots array, and select \\(p\\) evenly spaced samples from the pivot list. 5. Use the \\(p\\) samples as interval to define the buckets. </p> <p></p> <p>We can guarantee that each bucket has at most \\(2n/p\\) elements. </p>"},{"location":"cs267/sparse_matrix.html","title":"Sparse Matrices","text":""},{"location":"cs267/sparse_matrix.html#representations-of-sparse-matrices","title":"Representations of Sparse Matrices","text":"<p>A sparse matrix is a matrix where \\(\\geq 90%\\) entries are 0. Consider a \\(M\\times N\\) sparse matrix with \\(n\\) non-zero entries. If we store and do operations on sparse matrices just like a dense matrix, we will waste lots of space and computational time. For example, a diagonal matrix and diagonal matrix vector multiplication takes \\(O(N)\\) instead of \\(O(N^2)\\). </p> <p>The main goal for sparse matrix representations is to store the non-zeros in a dense format. For any arbitrary sparse matrix, the simplest idea is to store it as coordinate (COO) as a dense array <code>(row, col, value)</code> or a hash table <code>(row, col) -&gt; value</code>, which takes \\(3n\\) space and it's easy to build and modify. </p>"},{"location":"cs267/sparse_matrix.html#compressed-sparse-rowcolumn-csrcsc","title":"Compressed sparse row/column (CSR/CSC)","text":"<p>Taking one step further, if we have multiple entries in the same row, we could simply store it as <code>row -&gt; [(col_1, value_1), (col_2, value_2), ..., (col_i, value_i)]</code>. Therefore, we store two arrays, An index array and an adjacency array. The \\(i\\)th entry of the index array is a pointer or index to the starting point of all non-zero entries in row \\(i\\); the adjacency array is a dense array of all <code>(col, value)</code> tuples ordered from top left to the bottom right of the matrix. The total space is \\(M+2n\\). CSC is same idea, but indexing columns instead of rows. </p> <pre><code>// dense matrix\n12,  0, 26,  0,\n 0,  0, 19, 14,\n26, 19,  0,  0,\n 0, 14,  0,  7\n\n// COO\n(0, 0, 12), (0, 2, 26),\n(1, 2, 19), (1, 3, 14),\n(2, 0, 26), (2, 1, 19),\n(3, 1, 14), (3, 3,  7)\n\n// CSR\n0, 2, 2, 3, 5\n(0, 12), (2, 26), (2, 19), (3, 14), (0, 26), (1, 19), (1, 14), (3, 7)\n</code></pre> <p>In general, sparse matrices are stored in CSR since linear algebra on CSR is easier to implement. </p>"},{"location":"cs267/sparse_matrix.html#sparse-matrix-vector-multiplication-spmv","title":"Sparse matrix vector multiplication (SpMV)","text":"<p>For SpMV, we simply iterate through row's indexing array, and then for each non-zero entry in the row. </p> <pre><code>/*  M * N sparse matrix A with n non-zeros stored as\n    int ptr[M + 1]  row pointers\n    int ind[n]      column index array\n    int val[n]      value array\n*/\nfor (int i = 0; i &lt; M; i++) {\n    for (int j = A.ptr[i]; j &lt; A.ptr[i+1]; j++) {\n        y[i] += A.val[j] * x[A.ind[j]];\n    }\n}\n</code></pre> <p>One problem with sparse matrix is the pointer chasing problem, where <code>ind, val</code> is dense, but we need to access <code>x[ind[j]]</code>, which is not contiguous, and such pointer chasing takes multiple CPU cycles. </p>"},{"location":"cs267/sparse_matrix.html#parallel-spmv","title":"Parallel SpMV","text":"<p>For the loop, it's easier to see that SpMV can be parallelized by partitioning the rows. Of course, such partitioning would have workload imbalance since the nonzero entries in each row is different. </p> <p>Another way is to think SpMV as a segmented suffix scan problem. For each row, we scan for mapped <code>x</code> and finally gather them together.</p> <pre><code>\"\"\"\n    x = [1, 2, 1, 2, 1, 2, 1]\n    A.ptr = [0, 2, 5, 7, 9]\n    A.ind = [\n        1, 2, \n        0, 3, 4, \n        1, 2, \n        5, 6, \n        0, 1, 4\n    ]\n    A.val = [\n        1, 1, \n        1, 2, 2, \n        2, 1, \n        2, 2, \n        1, 3, 1\n    ]\n\"\"\"\n\nflag = zeros(len(A.val)) + ones[A.ptr]\n# [1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0]\n\nprod = val * x[ind]\n# [\n#   2, 1, \n#   1, 4, 2,\n#   4, 1,\n#   4, 2,\n#   1, 6, 1\n# ]\n\n# start reduction at each flag and ends at the next\ny = scan(prod, segment=flag, op=sum)\n# [3, 7, 5, 6, 8]\n</code></pre> <p>For diagonal banded matrix multiplication, the matrix can be represented as a \\(M\\times d\\) matrix where \\(d\\) is the number of entries per row, and a <code>column_offset</code> array to record the offset from the diagonal. </p> banded matrix multiplication<pre><code>\"\"\"\nA = [\n    11, 12,  0,  0,  0,  0,  0,\n    21, 22, 23,  0,  0,  0,  0,\n     0, 32, 33, 34,  0,  0,  0,\n     0,  0, 43, 44, 45,  0,  0,\n     0,  0,  0, 54, 55, 56,  0,\n     0,  0,  0,  0, 65, 66, 67,\n     0,  0,  0,  0,  0, 76, 77,\n]\n\nval = [\n    12, 23, 34, 45, 56, 67,  0,\n    11, 22, 33, 44, 55, 66, 77,\n     0, 21, 32, 43, 54, 65, 76\n]\n\ncol_offset = [-1, 0, 1]\n\"\"\"\n\nfor k in range(d):\n    # parallel on rows\n    for i in range(M):\n        col = i + col_offset[k]\n        if col &gt;= 0 and col &lt; n:\n            y[i] += val[k][col] * x[col]\n</code></pre>"},{"location":"cs267/sparse_matrix.html#tuning","title":"Tuning","text":"<p>The main problem with SpMV is load imbalance and pointer chasing, if we know the general distribution of the sparse matrix, we can plan for the partitioning and register blocking</p>"},{"location":"cs267/sparse_matrix.html#reordering","title":"Reordering","text":"<p>The most ideal structure for parallelized SpMV is block diagonal. In this case, we can partition \\(y=Ax\\) into \\(p\\) processors and we requires no communication except for a final <code>gather</code>. </p> <p></p> <p>Of course, this is only a very small class of sparse matrix. However, we can reorder the rows or columns to get close to such format. Then, we can use it for computation and permute things back using a permutation matrix. </p>"},{"location":"cs267/sparse_matrix.html#register-blocking","title":"Register blocking","text":"<p>If we know that most entries of the sparse matrix are clustered in small blocks, then instead of storing each nonzero entry with <code>ind</code>, storing a \\(r\\times c\\) block as a dense matrix indexed by the top-left coordinate. Although we have some zeros in this case, we can use vector instructions to quickly fetch data. Note that \\(r\\times c\\) and ways to partition the matrix is up for tuning.</p> <p>Register blocking with various size: we can have \\(r,c\\) stored in the top-left coordinate so that each block has its own size. </p>"},{"location":"cs267/upcxx.html","title":"UPC++ Introduction","text":"<p>UPC++ User guide</p> <p>UPC++ is an asynchronous RMA/RPC library for distributed C++ applications. Compared to MPI, which the distribution happens on processors, UPC communicates by reading/writing memory using a partitioned global address space. </p> <p>The distributed computing model are great for structured/regular data, since we need to cache the remote data and synchronize after each timestep. However, MPI is hard to use for irregular data structures such as meshes, sparse matrices, hash tables; and irregular/asynchronous data movement/updates. </p>"},{"location":"cs267/upcxx.html#remote-direct-memory-access-rdma","title":"Remote Direct Memory Access (RDMA)","text":"<p>Recent advancement in hardware allows for RDMA and changes the communication pattern for distributed systems. </p> <p>Considering the communication model, MPI uses a two-sided message model, one proc send a data package to another, and the other one need to receive them in (possibly virtual) buffer and put them into correct locations. A shared-memory model often use one-sided communication, each thread does not check the memory address but send/put data directly into the address. </p> <p>The process model for UPC++ is similar to MPI in many ways, but focuses on one-sided communication model (no need to match sends to receives, no unexpected messages, no need to guarantee message ordering). The metadata is shared among processors rather than split between send and receiver. </p>"},{"location":"cs267/upcxx.html#introduction","title":"Introduction","text":"<p>Compiling a UPC++ program, </p> <pre><code>upcxx -g hello-world.cpp -o hello-world.exe\n</code></pre> <p>Launching the program with UPC++ context </p> <pre><code>upcxx-run -n 4 ./hello-world.exe # -n num_procs\n</code></pre> hello-world.cpp<pre><code>#include &lt;iostream&gt;\n#include &lt;upcxx/upcxx.hpp&gt;\nusing namespace std;\nusing namespace upcxx;\nint main() {\n    init(); \n    cout &lt;&lt; \"process: \"&lt;&lt; rank_me() &lt;&lt; endl;\n    cout &lt;&lt; \"number processes: \"&lt;&lt; rank_n() &lt;&lt; endl;\n    finalize();\n    return 0;\n}\n</code></pre>"},{"location":"cs267/upcxx.html#global-memory","title":"Global Memory","text":"<p>Allocate global memory in shared segments, accessible by all processors. The actual data will exist on the calling process's shared segment. </p> <pre><code>// the data is on a global shared segment\nnew_&lt;T&gt;(...); // analogy to new , calls the class constructor and allocating memory\ndelete_(global_ptr&lt;T&gt;); // analogy to delete ptr\n\n// Example: store the rank of the proc\nglobal_ptr&lt;int&gt; gptr = new_&lt;int&gt;(rank_me());\n\nnew_array&lt;T&gt;(size); // allocating 1-d array\ndelete_array(gptr); // delete array\n\n// Example: create an int array of size 10\nglobal_ptr&lt;int&gt; gptr_arr = new_array&lt;int&gt;(10);\ndelete_array(gptr_arr); \n\n// allocate and deallocate without calling constructors\nallocate&lt;T&gt;(size); // analogy to malloc\ndeallocate(gptr); // analogy to free\n</code></pre>"},{"location":"cs267/upcxx.html#downcasting-with-local","title":"Downcasting with <code>local</code>","text":"<p>If the shared memory is located to a local process, then can obtain a local address.  <pre><code>// a precondition of global_ptr&lt;T&gt;::local()\nUPCXX_ASSERT(x_arr.is_local());   \nint *local_ptr = x_arr.local();\nlocal_ptr[i] = ... // work with local ptr\n</code></pre></p>"},{"location":"cs267/upcxx.html#distributed-objects","title":"Distributed Objects","text":"<p>Each process cannot automatically get global pointers defined by other processes, we need a space to get each other's global pointers (or values). UPC++ provides distributed objects. Each proc will have the same variable name, but different values. </p> <p><pre><code>// create a distributed variable with value\ndist_object&lt;T&gt; var(T value);\n\n// Example: get each other's value\ndist_object&lt;double&gt; x(0);\n*x = random(); // assign a random number to the dist as of a pointer\n// get value from the next proc\ndouble next_x = x.fetch((rank_me() + 1) % rank_n()).wait(); \n</code></pre> This is essentially for sharing global pointers <pre><code>dist_object&lt;global_ptr&lt;double&gt;&gt; x_global(new_array&lt;double&gt;(10));\n// get local pointer\ndouble *x = x_global-&gt;local();\n// get pointer to the next proc\nglobal_ptr&lt;double&gt; next_x = x_global.fetch((rank_me() + 1) % rank_n()).wait(); \n</code></pre></p>"},{"location":"cs267/upcxx.html#asynchronous-communication-rma","title":"Asynchronous communication (RMA)","text":"<p>The data communication in the shared segment is in general asynchronous. We need to initiate the operation and wait for its completion. </p> <pre><code>// remotely get data from the pointer\nfuture&lt;T&gt; rget(global_ptr&lt;T&gt; gptr);\n// remotely put the data value \nrput(T local_value, global_ptr&lt;T&gt; gptr);\n\n// Example: rget\nglobal_ptr&lt;int&gt; gptr; // assume that gptr points to some remote data\nfuture&lt;int&gt; f1 = rget(gptr);\n/* unrelated work, rget is non-blocking */\nint t1 = f1.wait(); // wait for completion, blocking\n</code></pre>"},{"location":"cs267/upcxx.html#synchronization","title":"Synchronization","text":"<p>Barriers that is the same as of other libraries. </p> <pre><code>barrier(); // block until all other threads arrive\n\n// barrier in the upcxx manner\nfuture&lt;&gt; f = barrier_async(); // thread is ready for barrier\nf.wait();\n</code></pre>"},{"location":"cs267/upcxx.html#future","title":"Future","text":"<p>Similar to <code>promise</code> in javascript, future is the returning value of an async operation and holds the returning values plus a state. </p> <pre><code>future&lt;T&gt; f = ... // any async operation defined by UPC++\nbool ready = f.ready(); // return true if finishes\nf.wait(); // blocking until f is ready\n</code></pre>"},{"location":"cs267/upcxx.html#callbacks","title":"Callbacks","text":"<p>Similar to the callback chaining in javascript. <code>then</code> can be attached to a future, and invoked after the future is ready. Note that the return type of <code>then</code> is a <code>future</code>. </p> <pre><code>// remotely get numbers and add 1 to it\nfuture&lt;int&gt; f1 = rget(gptr);\nfuture&lt;int&gt; f2 = f1.then([](int x1) { return x1 + 1; });\nint gptrp1 = f2.wait();\n\n// chaining to simplify the syntax\nint gptrp1 = rget(gptr).then([](int x) { return x + 1; }).wait();\n</code></pre>"},{"location":"cs267/upcxx.html#conjoining-futures","title":"Conjoining futures","text":"<p>Conjoin multiple <code>future</code>s into one, and wait for all of them are ready. </p> <pre><code>future&lt;T1, T2&gt; when_all(future&lt;T1&gt; f1, future&lt;T2&gt; f2);\n\n// add two numbers from two remote pointers and return the sum\nint sum = when_all(rget(gptr1), rget(gptr2)).then([](int x1, int x2) { return x1 + x2; }).wait();\n</code></pre>"},{"location":"cs267/upcxx.html#remote-procedure-calls-rpc","title":"Remote procedure calls (RPC)","text":"<p>An RPC enables the calling process to invoke a function at a remote process, using parameters sent to the remote process via the RPC. </p> <pre><code>future&lt;RType&gt; rpc(intrank_t rank, Func function, Arg arg1, Arg arg2, ...);\n\n// Example: invoke a multiplication on rank r\nfuture&lt;int&gt; fut_result = rpc(r, [](int a, int b) { return a * b; }, a, b);\n// the execution is done on r, but the result is returned back to local rank\nint result = fut_result.wait();\n\n// Example: summing local on rank 0\nint sum = 0; // global variable\nrpc(0, [](int x) { sum += x;}, local).wait();\n// now sum in rank 0 will have the sum of all local\n</code></pre>"},{"location":"cs267/upcxx.html#atomics","title":"Atomics","text":"<p>Atomic operations use an \"atomic domain\" to specify the operations needed and the data type. </p> <pre><code>atomic_domain&lt;Dtype&gt; ad({ atomic_op1, atomic_op2, ... });\n\n// Example: summing numbers\n\nglobal_ptr&lt;int&gt; sum = ...; // global ptr points to sum\nint local = ...; // local number to be added\n\n// specify the supported ops and create the atomic domain\natomic_domain&lt;int&gt; ad({ atomic_op::load, atomic_op::fetch_add }); \n// perform the atomic add\nad.fetch_add(local, sum, memory_order_relaxed).wait();\n</code></pre>"},{"location":"cs267/upcxx.html#collectives","title":"Collectives","text":"<p>Collectives are similar to other libraries, mainly broadcast and reduction</p>"},{"location":"cs267/upcxx.html#broadcast","title":"Broadcast","text":"<pre><code>future&lt;T&gt; broadcast(T value, intrank_t sender);\nfuture&lt;&gt; broadcast(T *buffer, size_t count, intrank_t sender);\n\n// broacast rank number from rank 0 to all procs\n// root_rank on all procs will be 0\nint root_rank = broadcast(rank_me(), 0).wait();\n\n\nint arr[10];\nif (rank_me == 0) {\n  for (int i = 0; i &lt; 10; i++)\n    arr[i] = i;\n}\nbroadcast(arr, 10, 0).wait();\n// all procs now will have arr = [0, 1, 2, ..., 9]\n</code></pre>"},{"location":"cs267/upcxx.html#reduction","title":"Reduction","text":"<p>Reduction supports operations includes <code>op_fast_add, op_fast_mul, op_fast_min, op_fast_max, op_fast_bit_and, op_fast_bit_or, and op_fast_bit_xor</code></p> <pre><code>future&lt;T&gt; reduce_all(T value, BinaryOp op);\nfuture&lt;T&gt; reduce_all(T* src, T* dest, size_t count, BinaryOp op);\nfuture&lt;T&gt; reduce_one(T value, BinaryOp intrank_t receiver);\nfuture&lt;T&gt; reduce_one(T* src, T* dest, size_t count, BinaryOp intrank_t receiver);\n\n// summing all local across procs\nint sum = reduce_all(local, op_fast_add).wait();\n\n// elementwise summing arrays to root\nint sum_buffer[4];\n\n/* suppose that there are 4 procs with local_buffer\nproc 0:  1  2  3  4\nproc 1:  2  2  3  4\nproc 2:  0  3  1  3\nproc 3:  9  5  3  1\nsum   : 12 12 10 12\n*/\nreduce_one(local_buffer, sum_buffer, 4, op_fast_add, 0);\n// now sum_buffer on rank 0 will haev 12 12 10 12\n</code></pre>"},{"location":"cs267/upcxx.html#example-distributed-hash-table","title":"Example: Distributed Hash Table","text":"<pre><code>#include &lt;map&gt;\n#include &lt;upcxx/upcxx.hpp&gt;\n\nusing namespace upcxx;\nusing namespace std;\n\nclass DistrMap {\nprivate:\n  // store the local unordered map in a distributed object to access from RPCs\n  using dobj_map_t = dist_object&lt;unordered_map&lt;string, string&gt;&gt;;\n  dobj_map_t local_map;\n  // map the key to a target process\n  int get_target_rank(const string &amp;key) {\n    return hash&lt;string&gt;{}(key) % rank_n();\n  }\npublic:\n  // initialize the local map\n  DistrMap() : local_map({}) {}\n  // insert a key-value pair into the hash table\n  future&lt;&gt; insert(const string &amp;key, const string &amp;val) {\n    // the RPC returns an empty future by default\n    return rpc(\n      get_target_rank(key),\n      // lambda to insert the key-value pair\n      [](dobj_map_t &amp;lmap, const string &amp;key, const string &amp;val) {\n        // insert into the local map at the target\n        lmap-&gt;insert({key, val});\n      },\n      local_map, key, val\n    );\n  }\n  // find a key and return associated value in a future\n  future&lt;string&gt; find(const string &amp;key) {\n    return rpc(\n      get_target_rank(key),\n      // lambda to find the key in the local map\n      [](dobj_map_t &amp;lmap, const string &amp;key) -&gt; string {\n        auto elem = lmap-&gt;find(key);\n        if (elem == lmap-&gt;end()) return string(); // not found\n        else return elem-&gt;second; // key found: return value\n      }, \n      local_map, key\n    );\n  }\n};\n</code></pre>"},{"location":"cs284/camera.html","title":"Cameras","text":""},{"location":"cs284/camera.html#field-of-views-fov","title":"Field of views (FOV)","text":"<p>Consider a camera - \\(h\\) be its sensor size, conventionally \\(h\\) is the height of the sensor and the aspect ratio of the sensor is \\(3:2\\).  - \\(f\\) be its focal length, physically speaking, focal length is the distance between lens and the focus. For photography, focal length often refers to the distance between the lens and the sensor when the object is in focus.  - \\(\\text{FOV} = 2 \\tan^{-1}(\\frac{h}{2f})\\) field of view is the degree (conventionally vertical degree) of the viewing angles. </p> <p></p> <p>Conventionally, most manufactures refers FOV as the diagonal FOV by a full-frame sensor (also called 35mm sensor, but the actual size is \\(36\\times 24\\)mm). For example, \\(17\\)mm focal length wide angle lens is \\(2\\tan^{-1}(\\frac{\\sqrt{36^2+24^2}}{2\\times 17}) \\approx 104\\degree\\).</p>"},{"location":"cs284/camera.html#exposure","title":"Exposure","text":"<p>Exposure is the amount of photons received by each sensor pixel during a given duration. amplified by a coefficient. In general it can be expressed as an integral over area and time. Therefore, the exposure can be controlled by </p> <ul> <li>Aperture lens f-stop, open/close the physical aperture to control the area of lens that receive light</li> <li>Shutter speed the duration that the sensor pixels are integrating light</li> <li>ISO (Gain) change the system amplification. </li> </ul>"},{"location":"cs284/camera.html#iso-gain","title":"ISO (Gain)","text":"<p>Abstracting the lens reflections, the sensor value can be thought as the number of photos hit on the pixel area, modelled by a Poisson process. Therefore, fewer examples multiplied by a constant will give a more noisy estimation (variance) of the true model, with the mean being unbiased. </p> <p>ISO is just a constant multiplied onto each sensor pixel value, hence it is linear. </p>"},{"location":"cs284/camera.html#f-stop-aperture","title":"F-stop (Aperture)","text":"<p> Apetures defined as f-stop</p> <p>The f-number/f-stop of a lens is defined as \\(f/d\\) where \\(f\\) is the focal length and \\(d\\) is the diameter of the aperture. One stop doubles the exposure. </p> <p>Note that f-stop \\(x\\) is often written as f/x where \\(x = f/d\\).</p> <p>Of course, all aperture can fully close, so that a len's F-Number is the maximum aperture (focal length / max diameter). For example, a 50mm F/1.4 lens has \\(50/1.4\\approx 36\\)mm diameter. </p>"},{"location":"cs284/camera.html#shutter","title":"Shutter","text":"<p>A shutter can be physical, which is a machine curtain to block light, or electronic shutter, which the sensor read-out data and \"ends\" exposure and reset to start exposure by fills with photoelectrons. </p> <p>Motion blur with longer exposure time, the motion in the scene will be integrated over time with changing positions. Therefore, given the same motion, longer exposure time will have more motion blur. </p> <p>Rolling shutter effect the sensor pixels are not read-out at the same time. In practice, it's a sequential pipeline. The exposure time for each pixel is the same, but starting and ending time is different (each pixel starts with \\(s_i\\) and ends with \\(e_i+\\Delta t\\) with \\(\\Delta_t\\) is the exposure time, independent of pixels). The difference between the first and last pixel can take \\(1/30\\) sec. The resulting picture will have sheared objects. </p>"},{"location":"cs284/camera.html#gauss-ray-diagram","title":"Gauss Ray Diagram","text":"<p> Gauss Ray Diagram</p> <p>By similar triangles we have equations </p> \\[\\frac{h_o}{z_o - f} = \\frac{h_i}{f}\\quad \\frac{h_o}{f} = \\frac{h_i}{z_i - f}\\] <p>rearranges to </p> \\[\\frac{h_o}{h_i} = \\frac{z_o - f}{f} = \\frac{f}{z_i - f}\\] <p>Which gives the Newtonian Thin Lens Equation</p> \\[\\frac{z_o - f}{f} = \\frac{f}{z_i - f}\\implies (z_o - f)(z_i - f) = f^2\\] <p>Expand LHS and rearrange into Gaussian Thin Lens Equation</p> \\[\\begin{align*} z_oz_i -(z_o + z_i)f + f^2 &amp;= f^2\\\\ \\frac{1}{f} &amp;= \\frac{z_o + z_i}{z_oz_i} \\\\ \\frac{1}{f} &amp;= \\frac{1}{z_o} + \\frac{1}{z_i} \\\\ \\end{align*}\\] <p>and magnification \\(m = \\frac{h_i}{h_o} = \\frac{z_i}{z_o}\\)</p>"},{"location":"cs284/camera.html#defocus-blur-circle-of-confusion","title":"Defocus Blur (Circle of Confusion)","text":"<p>Let \\(A\\) be the diameter of the aperture and \\(C\\) be the diameter circle of confusion. \\(z_s\\) be the sensor plane and \\(z_i\\) be the image's focus. Then </p> \\[\\frac{C}{A} = \\frac{|z_s - z_i|}{z_i}\\] <p>Note that the camera can changes its focal point at \\(z_c\\) by changing the \\(z_s\\), using the Gauss' thin lens equation as \\(z_s = (f^{-1} - z_c^{-1})^{-1}\\), and given an object that's \\(z_o\\) away, its image is at \\(z_i = (f^{-1} - z_o^{-1})^{-1}\\).</p> <p>Also, note that the \\(C = A\\frac{|z_s - z_i|}{z_i}\\) is proportional to the aperture diameter and we know that \\(A = \\frac{f}{\\text{F-number}}\\) so that \\(C\\) is inverse proportional to F-number. </p>"},{"location":"cs284/camera.html#depth-of-field","title":"Depth of Field","text":"<p>DOF is where the circle of confusion is smaller than the sensor's pixel size. Suppose that the camera's object focus is \\(z_S\\) and image focus in \\(d_S\\) (done by adjusting the distance between lens and sensor). Then, we need to find far and near s.t. \\(C_F = A\\frac{d_F - d_S}{d_F}, C_N = A\\frac{d_N - d_S}{d_N}\\) is below the wanted threshold \\(C_0\\). Using Gauss' Len equation to change \\(d\\)'s to \\(z\\)'s, we have that </p> \\[\\frac{(f^{-1}-d_F^{-1})^{-1} - (f^{-1} - d_S^{-1})^{-1}}{f^{-1} - d_F^{-1}} = \\frac{C_0}{A}\\] <p>rearrange the equation and get </p> \\[z_F = \\frac{z_S f^2}{f^2 - \\frac{C_0f}{A}(z_s - f)}, z_N =  \\frac{z_S f^2}{f^2 + \\frac{C_0f}{A}(z_s - f)}\\] <p>Then note that F-number \\(N = f/A\\) so that </p> \\[z_F = \\frac{z_S f^2}{f^2 - C_0N(z_s - f)}, z_N =  \\frac{z_S f^2}{f^2 + C_0 N(z_s - f)}\\] <p>Therefore, the larger aperture (smaller f-number) gives small depth of field</p>"},{"location":"cs284/camera.html#depth-of-field-vs-motion-blur","title":"Depth of Field vs Motion Blur","text":"<ul> <li>Depth of field is impacted by the aperture (F-number), large aperture (small f-number) gives small depth of field. </li> <li>Motion blur is impacted by the shutter speed, longer shutter time gives more motion blur. </li> <li>The same amount of light can be controlled by aperture, shutter time, ISO. Higher ISO will create random noise, which is not desired. Therefore, we need to tradeoff in between motion blur and defocus blur. </li> </ul>"},{"location":"cs284/camera.html#ray-tracing-with-defocus-blur","title":"Ray Tracing with defocus blur","text":"<p>The idea is to put a \"thin lens\" in front of the camera's image plane so that the viewing rays are refracted. </p> <ol> <li>For pixel \\(\\mathbf x\\) on the image plane, randomly select \\(\\mathbf x'\\) on the lens plane</li> <li>trace the ray pass through conjugate point \\(\\mathbf x''\\) on the focus plane and estimate radiance by multiple samples. </li> </ol>"},{"location":"cs284/color.html","title":"Color Sciences","text":""},{"location":"cs284/color.html#spectral-power-distribution-spd","title":"Spectral Power Distribution (SPD)","text":"<p>SPD describes the amount of light present at each wavelength, often measured in radiometric units (energy per unit area) divided by wavelength unit, for example watt / nanometer. For a real-world scene, we often use \"relative units\" scaled to max wavelength. The final color can be seen as the integrating SPD over the wavelength. The relative SPD can be expressed as a function </p> \\[s:\\mathbb{R}\\rightarrow[0, 1]. s(\\lambda) = p\\] <p> </p> Examples of SPD <p>Note that SPD are linear, i.e. for two light sources \\(s_1, s_2\\), the resulted SPD  \\(s(\\lambda) = s_1(\\lambda) + s_2(\\lambda)\\)</p>"},{"location":"cs284/color.html#light-detector","title":"Light Detector","text":"<p>Consider a simple light detector model:</p> <ul> <li>light as photons (particles), and SPD \\(n(\\lambda)\\) describes the number of photons at different wavelength. </li> <li>detector is a surface that record the number of photon hits, but the detector does not know the what wavelength each photon has. The output is just the total number hits over the unit area. However, the detector has different detection efficiency (probability of detecting photons at specific wavelength), which the efficiency \\([0,1]\\) is described as a probability function \\(p(\\lambda)\\), or sensitivity \\(r(\\lambda)\\). </li> </ul> <p>The total number hits \\(X_N\\) is observed, or by changing the unit from number hits to measured signal \\(X\\), over SPD and detector sensitivity</p> \\[X_N = \\int n(\\lambda)p(\\lambda)d\\lambda\\text{, or } X = \\int s(\\lambda) r(\\lambda)d\\lambda\\] <p>Given the integral, we can easily discretize it by measuring \\(s\\), \\(r\\) at \\(n\\) points \\(\\lambda_1, \\lambda_2,\\cdots,\\lambda_n\\) as \\(\\mathbf s = (s_1 := s(\\lambda_1), s_2,\\cdots, s_n), \\mathbf r = (r_1 := r(\\lambda_1), r_2,\\cdots, r_n)\\) and the estimated \\(X = \\mathbf s\\cdot\\mathbf r\\). </p>"},{"location":"cs284/color.html#human-color-perception","title":"Human Color Perception","text":"<p>Human has 3 types of cones (light detector), we call it \\(S,M,L\\) (short, medium, long) and each of them has a distinctive sensitivity function \\(r_S, r_M, r_L\\). Therefore, each cone cell will give a response (understood as a scalar) dependent on its type. The final precepted light is understood as a 3D vector</p> \\[\\begin{bmatrix}S\\\\M\\\\L\\end{bmatrix}=\\int \\mathbf r(\\lambda)s(\\lambda)d\\lambda=\\begin{bmatrix}r_{S,1}&amp;r_{S,2}&amp;\\cdots&amp;r_{S,n}\\\\r_{M,1}&amp;r_{M,2}&amp;\\cdots&amp;r_{M,n}\\\\r_{L,1}&amp;r_{L,2}&amp;\\cdots&amp;r_{L,n}\\end{bmatrix}\\begin{bmatrix}s_1\\\\s_2\\\\\\vdots\\\\s_n\\end{bmatrix}\\]"},{"location":"cs284/color.html#metamers-and-displays","title":"Metamers and Displays","text":"<p>Note that we are mapping from spectrum (\\(\\infty\\)-dim) to cone responses (3-dim). Obviously such mapping cannot be 1-to-1, hence metameters are spectra that project to the same response (which means the save color to human). </p> <p>Therefore, we only need a 3-dim light emitter to match all the possible responses, call them \\(s_R, s_G, s_B\\) and for each real spectrum \\(s(\\lambda)\\), we output its matched display spectrum </p> \\[s(\\lambda)\\approx s_{\\text{display}}(\\lambda) =Rs_R(\\lambda) + Gs_G(\\lambda) + Bs_B(\\lambda)\\] <p>Given that \\(s_R, s_G, s_B\\) are fixed dependent on the display material, we can describe color as \\((R,G,B)\\). </p>"},{"location":"cs284/color.html#color-reproduction","title":"Color reproduction","text":"<p>We aim to reproduce the perception of real spectrum \\([S, M, L]_{\\text{real}}\\) by the perception of displayed spectrum \\([S, M, L]_{\\text{disp}}\\), i.e. </p> \\[\\begin{align*} \\begin{bmatrix}\\mathbf r_S\\\\\\mathbf r_M\\\\\\mathbf r_L\\end{bmatrix}\\mathbf s &amp;= \\begin{bmatrix}\\mathbf r_S\\\\\\mathbf r_M\\\\\\mathbf r_L\\end{bmatrix} \\begin{bmatrix}\\mathbf s_R&amp;\\mathbf s_G&amp;\\mathbf s_B\\end{bmatrix} \\begin{bmatrix} R\\\\ G\\\\ B\\end{bmatrix}\\\\ \\begin{bmatrix} R\\\\ G\\\\ B\\end{bmatrix} &amp;= (\\begin{bmatrix}\\mathbf r_S\\\\\\mathbf r_M\\\\\\mathbf r_L\\end{bmatrix}\\mathbf s)^{-1} \\begin{bmatrix}\\mathbf r_S\\\\\\mathbf r_M\\\\\\mathbf r_L\\end{bmatrix} \\begin{bmatrix}\\mathbf s_R&amp;\\mathbf s_G&amp;\\mathbf s_B\\end{bmatrix}\\\\ RGB &amp;= (M_{SML}M_{RGB})^{-1}M_{SML}\\mathbf s \\end{align*}\\] <p>The color matching functions \\((M_{SML}M_{RGB})^{-1}M_{SML}\\) is a \\(3\\times N\\) fixed w.r.t. display material and human perception. Therefore, for each measured spectrum \\(\\mathbf s\\), we need to evaluate \\(RGB\\). If any of \\(R,G,B\\) is negative, then the display can't physically reproduce the desired color. </p>"},{"location":"cs284/color.html#lms-response","title":"LMS Response","text":"<p>Note that SPDs are additive, which means that if we can color match two light spectrum with \\(RGB_1, RGB_2\\), then we can match their sum \\(RGB_1 + RGB_2\\), since all of them are positive. Therefore, if we can color match for since response for each wavelength (\\(c\\) at a specific wavelength, and \\(0\\) elsewhere \\(SPD(\\lambda) = c\\mathbb I(\\lambda = \\lambda_0)\\)), then we can color match any spectrum by a linear combination of infinitely many response values. </p> <p>Taking the inverse, we have human color perception \\((S,M,L)\\) at each wavelength. In other words, the \"spectral locus\" of human cone cells' response to monochromatic light (light with energy in a single wavelength). The spectral locus, which is a subspace in 3D SML space, represents all possible responses.</p>"},{"location":"cs284/color.html#chromaticity-diagram","title":"Chromaticity Diagram","text":"<p>We can map the LMS response to 2D by perspective projection looking diagonally down at origin from \\((1,1,1)\\), the result is the Chromaticity diagram. One of the mapping (CIE 1931 xy) is defined as </p> \\[\\begin{bmatrix}X\\\\Y\\\\Z\\end{bmatrix}= \\begin{bmatrix}1.19121&amp;-1.1121&amp;0.2019\\\\0.3709&amp;0.6291&amp;0\\\\0&amp;0&amp;1\\end{bmatrix} \\begin{bmatrix}L\\\\M\\\\S\\end{bmatrix},  \\begin{bmatrix}x\\\\y\\end{bmatrix} = \\frac{1}{|X| + |Y|+|Z|}\\begin{bmatrix}X\\\\Y\\end{bmatrix}\\] <p>The resulted projection is a 2D color gamut, </p> <p> </p> Color gamut"},{"location":"cs284/color.html#color-spaces","title":"Color Spaces","text":"<p>We know that color is represented in 3D space, but there are many different coordinate systems to represent the same color space. The common one is RGB. However, RGB is device dependent, the same RGB value can look differently on different displays. </p>"},{"location":"cs284/color.html#standard-rgb-srgb","title":"Standard RGB (sRGB)","text":"<p>Like other industries, the simplest way it to setup a particular standard for RGB. Color devices will simulate the particular RGB standard by calibration. </p> <p>Historically, one \"Standard\" color space is CIE XYZ. It's designed s.t. XYZ spans all observable color, the matching functions are \\(&gt;0\\), and \\(Y\\) corresponds to luminance. Note that the actual standard is imaginary because it can only be realized with primaries that are negative at some wavelengths. </p> <p>The luminance is defined by </p> \\[Y =\\int \\Phi(\\lambda)V(\\lambda)d\\lambda\\] <p>where \\(V\\) is the visual luminous efficiency, which measures how bright a light at a given wavelength is perceived by a human. \\(\\Phi\\) is th incoming radiance. </p>"},{"location":"cs284/color.html#hue-saturation-value-hsv","title":"Hue-Saturation-Value (HSV)","text":"<p>The perceptual dimension of color</p> Hue Saturation Value (lightness) description \"kind\" of color, regardless of attributes colorfulness overall amount of light colorimetric meaning dominant wavelength purity luminance artist's meaning pigment color fraction of paint from the colored tube tints / shades"},{"location":"cs284/fluids.html","title":"Fluid Simulation","text":"<p>Generally, there are two approaches for fluid simulation: </p> <ol> <li>Grid approach: discretize the 3D field as a grid and save quantities on voxels, the fluids will move through grid and updates quantities accordingly. Easy to scale, but hard to track the surface.</li> <li>Particle approach: each particle represents a blob of volume and stores quantities. The fluid behavior is achieved through inter-particle forces.  Hard to scale, but easy to track surface. </li> </ol>"},{"location":"cs284/fluids.html#grid-based-fluid-simulation","title":"Grid based Fluid Simulation","text":"<p>For simplicity, assume a 2D simulation, and 3D simulation behaves exactly the same. </p> <p>For grid-based simulation, we store velocity, pressure, density on a regular grid. Note that pressure and density are scalar fields \\(p: \\mathbb R^2 \\rightarrow \\mathbb R\\) while velocity is 3D fluid \\(\\mathbf v: \\mathbb R^2\\rightarrow\\mathbb R^2, \\mathbf v(x,y) = (u,v)\\). </p>"},{"location":"cs284/fluids.html#staggered-grid","title":"Staggered Grid","text":"<p>Intuitively, we can store all quantities on discretized grid locations, and linearly interpolate the values at any location (lerp from \\(2^D\\) points), i.e. </p> \\[\\begin{align*} p(x,y) &amp;= lerp(p_{i,j}, p_{i+1,j}, p_{i,j+1}, p_{i+1,j+1})\\\\ \\mathbf v(x,y) &amp;= lerp(\\mathbf v_{i,j}, \\mathbf v_{i+1,j}, \\mathbf v_{i,j+1}, \\mathbf v_{i+1,j+1})\\\\ \\end{align*}\\] <p>where \\(i = \\lfloor x / dx \\rfloor, j = \\lfloor y / dy \\rfloor\\). However, to prevent unstable checkerboard, we use a staggered grid instead. The pressure grid is unchanged, while the velocity is staggered by half, which means that </p> \\[\\begin{align*} i_p &amp;= \\lfloor x / dx \\rfloor, j_p = \\lfloor y / dy \\rfloor\\\\ i_{u} &amp;= \\lfloor x / dx + 0.5\\rfloor, j_{u} = \\lfloor y / dy \\rfloor\\\\ i_{v} &amp;= \\lfloor x / dx \\rfloor, j_{v}= \\lfloor y / dy + 0.5\\rfloor \\end{align*}\\] <p> </p> Staggered grid, shift velocity grid by half unit w.r.t position"},{"location":"cs284/fluids.html#vector-field","title":"Vector Field","text":"<p>Define several operators on the vector field. </p> <ul> <li>gradient for some scalar field \\(p\\), \\(\\nabla p = (\\partial_x p, \\partial_y p)\\) is the direction of greatest change in \\(p\\)</li> <li>divergence for some field \\(\\mathbf v:(x,y)\\rightarrow(u,v)\\), \\(\\nabla \\cdot \\mathbf v = \\partial_x u + \\partial_y v\\) is the net flow of the region near \\((x,y)\\).</li> <li>curl \\(\\nabla\\times \\mathbf v = \\partial_xv - \\partial_yu\\) is the circulation around \\((x,y)\\).</li> <li>Laplacian \\(\\nabla^2 p= \\nabla\\cdot\\nabla p= \\frac{\\partial^2p}{\\partial x^2} + \\frac{\\partial^2p}{\\partial y^2}\\) is the difference from the neighborhood average.</li> <li>directional derivative \\(\\nabla_{\\mathbf v} \\mathbf v = \\mathbf v \\cdot \\nabla \\mathbf v\\) measures how a quantity changes as point of observation moves</li> </ul>"},{"location":"cs284/fluids.html#navier-stokes-equations","title":"Navier-Stokes Equations","text":"<p>The change in fluid velocity is described as </p> \\[\\frac{\\partial\\mathbf v}{\\partial t} = -\\nabla_{\\mathbf v}\\mathbf v - \\frac{\\nabla p}{\\rho} + \\frac{\\nu}{\\rho}\\nabla^2 \\mathbf u + \\frac{\\mathbf f}{\\rho}\\] <ul> <li>\\(\\nabla_{\\mathbf v}\\mathbf v\\) is the advection</li> <li>\\(\\frac{\\nabla p}{\\rho}\\) is pressure, \\(\\rho\\) is density</li> <li>\\(\\frac{\\nu}{\\rho}\\nabla^2 \\mathbf u\\) is viscosity, \\(\\nu\\) is the viscosity coefficient</li> <li>\\(\\frac{\\mathbf f}{\\rho}\\) is the external forces on the field</li> </ul> <p>This leads to a forward solver</p> \\[d\\mathbf v = dt(-\\nabla_{\\mathbf v}\\mathbf v - \\frac{\\nabla p}{\\rho} + \\frac{\\nu}{\\rho}\\nabla^2 \\mathbf u + \\frac{\\mathbf f}{\\rho})\\] <p>where we update pressure \\(p\\) from density \\(rho\\), then update velocities by N-SE, and update densities from \\(\\dot{\\rho} \\propto -(\\nabla_{\\mathbf v}\\rho + \\nabla\\cdot (\\rho\\mathbf v))\\). </p> <p>However, iteratively solving the equation (explicit method) will quickly go unstable because pressure waves move fast.  </p>"},{"location":"cs284/fluids.html#stable-fluids","title":"Stable Fluids","text":"<p>Stable Fluids [Stam 1999]<sup>1</sup> is an early work that provide a more stable solver for N-SE by making the incompressible assumption. </p>"},{"location":"cs284/fluids.html#incompressible-fluids","title":"Incompressible Fluids","text":"<p>We add an additional assumption that the fluid has constant density / volume, which implies that the divergence of flow velocity is \\(0\\), i.e. \\(\\nabla\\cdot \\mathbf v = 0\\). Then we separate the problem by taking out advection \\(d\\mathbf v_a = -dt\\nabla_{\\mathbf v}\\mathbf v\\) and pressure projection \\(d\\mathbf v_p = - dt\\frac{\\nabla p}{\\rho}\\). Then, the equation becomes</p> \\[d\\mathbf v = d\\mathbf v_a + d\\mathbf v_p +dt( \\frac{\\nu}{\\rho}\\nabla^2 \\mathbf u + \\frac{\\mathbf f}{\\rho})\\] <p>We separate problem term \\(\\mathbf v^*\\), the unprojected and unadvected velocities, from the rest</p> \\[\\mathbf v^* = \\mathbf v^t + dt(\\frac{\\nu}{\\rho}\\nabla^2 \\mathbf u + \\frac{\\mathbf f}{\\rho})\\] <p>The incompressible fluid assumption gives that </p> \\[\\begin{align*} \\nabla\\cdot\\mathbf v &amp;= 0\\\\ \\nabla \\cdot(\\mathbf v^* + d\\mathbf v_p) &amp;= 0\\\\ \\nabla\\cdot \\mathbf v^* + \\nabla\\cdot(-dt \\frac{\\nabla p}{\\rho}) &amp;= 0\\\\ \\nabla\\cdot\\mathbf v^* &amp;= dt \\nabla\\cdot\\frac{\\nabla p}{\\rho}\\\\ \\nabla\\cdot\\mathbf v^* &amp;= \\frac{dt}{\\rho}\\nabla^2 p \\end{align*}\\] <p>we can now solve \\(p\\) and unproject pressure back to as \\(\\mathbf v^+ = \\mathbf v^* - \\frac{dt}{\\rho}\\nabla^2 p\\). </p> <p>Finally, we can update advection simply by track backward each velocity \\(\\mathbf v_{i,j}\\) for \\(dt\\), interpolate the values and copy to new location. </p> <ol> <li> <p>Stam, J. 1999. Stable fluids. Proceedings of the 26th annual conference on computer graphics and interactive techniques, 121\u2013128.\u00a0\u21a9</p> </li> </ol>"},{"location":"cs284/kinematics.html","title":"Animation through Kinematics","text":"<p>Kinematics with a concrete example</p>"},{"location":"cs284/kinematics.html#data-driven-models-pca","title":"Data driven models (PCA)","text":"<p>Given a dataset of \\(n\\) people scanning (point cloud) in the exact same pose </p> \\[\\mathcal V = \\{V_1, V_2,\\cdots, V_n\\}, V_i\\in\\mathbb R^{3\\times |V_i|}\\] <p>Reconstruct meshes based on the scanning so that they have the same topology and vertex semantics (for example, via remeshing). Then, the geometry (point positions) have the same cardinality, and all models are in \\(\\mathbb R^{3\\times |V|}\\) space. Which means that we have a \\(n\\times 3|V|\\) matrix. We are interested in parameterizing human shapes via \\(m\\) parameters, i.e. each sample \\(V_i\\) can be represented by some basis function</p> \\[\\mathcal V \\approx \\bar{V} + \\sum_{j=1}^m \\beta_{j}\\mathbf B_j\\] <p>In other words, a mean \\(\\bar V\\) and variance components \\(\\mathbf B_j\\). Using principal component analysis, we can find a set of eigenvectors \\(\\mathbf B_1, \\mathbf B_2, ...\\) and corresponding eigenvalues \\(\\beta_1, \\beta_2, ...\\) s.t. the variables successively inherit the maximum possible variance from \\(\\mathcal V\\), or equivalently minimize residual errors (Proof of equivalence). </p>"},{"location":"cs284/kinematics.html#forward-kinematics-skeleton","title":"Forward Kinematics (Skeleton)","text":"<p>Skeleton is a hierarchical structure, often represented as a tree. Each bone is a line segment of length \\(l\\) (or other shapes) with a joint and topology (connectivity). The joint defines the constraints of the local rotation and translation and the topology is represented by the edges in the skeleton tree. Joint is often of type ball (2 DoF rotation), pin (1 DoF rotation), or prismatic (translation). </p> <p>The transformation of each bone is the composition of the transformation of all its ancestors, i.e.</p> forward kinematics<pre><code>def transform(bone, skeleton):\n    if bone is skeleton.root:\n        return bone.Rt\n    else:\n        return bone.Rt @ transform(bone.parent, skeleton)\n# note that we can precache the global transformation at each bone\n# to prevent from repeating the computation\n</code></pre>"},{"location":"cs284/kinematics.html#inverse-kinematics","title":"Inverse Kinematics","text":"<p>Given a skeleton, a root transformation, a rest pose (initial configuration),  and some constraints on the leaf bone tips, or handles,  aim to find the interior parameter settings for all bones. </p> <p>Note that in most cases, the solution is not unique, or a solution does not exist. Therefore, the problem is often configured as an energy minimization problem. We want to write the current position as a function of joint parameters \\(\\Theta\\) and minimize the squared distance between handle positions and the bone positions, say \\(E(\\Theta) = \\|\\mathbf p(\\Theta) - \\mathbf p^* \\|^2\\) under the constraints (on \\(\\Theta\\) and other terms). </p> <p>Note that there are multiple ways to formulate the objective function to achieve different constraints. For example</p> <ul> <li> <p>Multiple handles with different weights/importance </p> \\[w_1\\|\\mathbf p_1(\\Theta) - \\mathbf p_1^*\\|^2 + w_1\\|\\mathbf p_2(\\Theta) - \\mathbf p_2^*\\|^2\\] </li> <li> <p>Pointing at given a direction \\(\\mathbf d\\), asking the tip bone points at the direction</p> \\[E_{\\mathbf d}(\\Theta) = \\|1 - \\frac{\\mathbf p_1(\\Theta) - \\mathbf p_1^*}{\\|\\mathbf p_1(\\Theta) - \\mathbf p_1^*\\|}\\cdot \\mathbf d\\|^2\\] </li> <li> <p>Balancing given the mass of each bone \\(m_i\\) and a supporting point \\(\\mathbf s\\) (either the center of the support polygon or a fixed joint on some bone), need to keep balance w.r.t. gravity. </p> \\[E_{\\text{balance}}(\\Theta) = \\|\\frac{\\sum_{i=1}^n m_i \\mathbf c_i(\\Theta)}{\\sum_{i=1}^n m_i} - \\mathbf s\\|_{xy}^2\\] </li> <li> <p>Keyframing given handle positions at some keyframes \\(t_1, t_2, ...\\), want to find \\(\\Theta(t)\\) as a smooth function over time. To ensure smoothness, we can treat the keyframes as control points and interpolate in-between using curves, such as Bezier curve. </p> </li> </ul> \\[\\Theta(t) = \\sum_j c_{ij}B_j(t)\\]"},{"location":"cs284/mcintegral.html","title":"Monte Carlo Integration","text":"<p>The idea behind Monte Carlo integration is to estimate integral based on random sampling of function. </p> <p>Notes on MC integration and importance sampling</p> <p>Here we provide some common examples for the applications on MC integration in graphics</p>"},{"location":"cs284/mcintegral.html#basic-monte-carlo-estimator","title":"Basic Monte Carlo Estimator","text":"<p>A basic Monte Carlo estimator is a special case where \\(X\\sim \\text{Uniform}(\\Omega)\\) where \\(\\Omega\\) is the area that we want to integrate on. In the above case, we have that \\(X\\sim \\text{Uniform}(a, b)\\) so that \\(F_N = \\frac{b-a}{N} \\sum_{i=1}^N f(X_i)\\).</p> <p>Extending to 3D cube, we can have </p> \\[F_N = (x_1-x_0)(y_1-y_0)(z_1-z_0)N^{-1} \\sum_{i=1}^N f(X_i)\\]"},{"location":"cs284/mcintegral.html#sampling-light-source-area","title":"Sampling light source area","text":"<p>Consider an environment where we have one area light source, and we want to find the incoming irradiance on a surface at location \\(\\mathbf x\\). </p> \\[E(\\mathbf x) = \\int_{H^2} L_i(\\mathbf x, \\omega) \\cos\\theta d\\omega\\] <p>Omitted any reflections, we know that almost all of the incoming irradiance comes from the light source. If we uniformly sample from the hemisphere, the integral will give much noise. Instead, note that the integral is equivalent to all the outgoing light at each position \\(\\mathbf x'\\) of the light source, to the direction \\(\\omega' = \\mathbf x - \\mathbf x'\\), multiplying the Lambert effect </p> \\[\\mathbf l\\cdot \\mathbf n = \\frac{\\omega}{\\|\\omega\\|}\\cdot \\frac{\\omega'}{\\|\\omega'\\|} = \\frac{\\cos\\theta \\cos\\theta'}{\\|\\mathbf x - \\mathbf x'\\|^2}\\] <p>Thus, the final integral is </p> \\[E(\\mathbf x) = \\int_A L_o(\\mathbf x', \\omega') V(\\mathbf x,\\mathbf x') \\frac{\\cos\\theta \\cos\\theta'}{\\|\\mathbf x - \\mathbf x'\\|^2}dA'\\] <p>where \\(V\\) is the visibility map s.t. the surface point \\(\\mathbf x\\) is visible from light position \\(\\mathbf x'\\). </p> <p>Thus, we can use MC integration by randomly sample \\(N\\) light beams with position \\(\\mathbf x_i'\\) and direction \\(\\omega_i'\\). </p>"},{"location":"cs284/mcintegral.html#inversion-method","title":"Inversion Method","text":"<p>Inversion method aims to draw samples from some PDF given its CDF (so that the PDF is unknown). For some CDF \\(F:\\mathcal X\\rightarrow [0, 1], F(x) := Pr(X &lt; x) = u\\), if we know the inverse \\(F^{-1}: [0, 1]\\rightarrow \\mathcal X\\), then we can draw samples from \\(U\\sim \\text{Uniform}(0, 1)\\) and draw examples from \\(X = F^{-1}(U)\\). </p>"},{"location":"cs284/mcintegral.html#uniform-sampling-from-circles","title":"Uniform sampling from circles","text":"<p>WLOG Consider a unit circle \\((\\cos\\theta, sin\\theta)\\). If we independently draw \\(u \\in [0, 1], v\\in[0,2\\pi)\\), then \\((u\\cos\\theta, u\\sin\\theta)\\) is not uniform over the area of the circle. </p> <p>Note that the area of a unit sphere is 1, so that we have the CDF</p> \\[F(r, \\theta) = \\frac{1}{2\\pi}\\int_0^r \\int_0^{\\theta} dv du = \\frac{\\theta}{2\\pi} r^2\\] <p>The marginal CDFs are </p> \\[F_\\theta(r) = r^2, F_r(\\theta) = 2\\pi\\theta\\] <p>so that the inversion method gives \\(r = u^{1/2}, \\theta = 2\\pi v\\) where \\(U,V\\sim \\text{Uniform}(0, 1)\\).</p>"},{"location":"cs284/mcintegral.html#uniform-direction-sampling-from-hemispheres","title":"Uniform direction sampling from Hemispheres","text":"<p>The idea is very similar, we are sampling from the sphere of a unit hemisphere. The CDF is </p> \\[F(\\theta, \\phi) = \\frac{1}{2\\pi}\\int_0^\\theta \\int_0^\\phi \\sin\\phi d\\phi d\\theta = \\frac{(1-\\cos\\phi)\\theta}{2\\pi}\\] <p>Marginalizing to \\(F(\\theta) = \\theta/ 2\\pi, F(\\phi) = 1 - \\cos\\phi\\) and inverse \\(\\theta = 2\\pi u, \\phi = \\cos^{-1}(1-v) = \\cos^{-1}(v')\\) since \\(1-V \\sim \\text{Uniform}(0,1)\\) as well.</p> <p>Turning into Cartesian coordinates, we have </p> \\[(u,v)\\rightarrow(\\sqrt{1-v^2}\\cos(2\\pi u), \\sqrt{1-v^2}\\sin(2\\pi u), v)\\]"},{"location":"cs284/mesh.html","title":"Mesh Representations","text":"<p>In general, a mesh is set of oriented triangles. Then, note that for each vertex, it can be shared among many triangles (degree \\(\\geq 1\\)). Leading to a graph-like representation \\((V, F)\\) where \\(V = \\{v_0, v_1, ..., v_n\\}\\) is the set of all vertices and \\(F = \\{(a_0, b_0, c_0), (a_1, b_1, c_1), ...\\}\\) is the set of triangles or faces, represented by the index of the point.  </p>"},{"location":"cs284/mesh.html#topology","title":"Topology","text":"<p>In general, \\(V\\) represents the geometry, or the position in the space.  \\(T\\) represents the topology, including connectivity, orientation, etc. </p>"},{"location":"cs284/mesh.html#2d-manifold","title":"2D Manifold","text":"<p>A 2D manifold is a surface that when cut with a small sphere always yields a disk. If a mesh is a manifold, then</p> <ul> <li>Each edge connects exactly two faces</li> <li>Each edge connects exactly two vertices</li> <li>Euler's polyhedron formula holds: \\(|F| - |E| + |V| = 2\\)</li> </ul> <p>In general, a 2D manifold can be locally expanded to a 2D sheet. </p>"},{"location":"cs284/mesh.html#orientation","title":"Orientation","text":"<p>In convention, the orientation of the face is represented by the order of vertex indexes. A face is oriented outward if the vertices are counterclockwise listed. The object has consistent orientations is every pair of faces that share the same edge has the same orientation. A mesh is non-orientable if it is equivalent to a Mobius ring (note that it's consistent but not orientable). </p>"},{"location":"cs284/mesh.html#triangle-neighbor","title":"Triangle-neighbor","text":"<p>We are interested in meshes with good topological properties, or manifolds in general. To support for traversing in a topological order (for manifold) and finding neighbors, we need additional cache.  </p> <pre><code>struct T {\n    V *vertices[3]; // 3 vertices\n    T *neighbors[3]; // 3 triangles that share edge (v0, v2), (v1, v0), (v2, v1)\n};\n\nstruct V {\n    VectorXD position; // 2D or 3D location of the vertex\n    T *t; // any one of the triangle that v belongs\n}\n\n// find next triangle c.c.w. around vertex v \n// from triangle t\nT *tccwvt(V *v, T *t) {\n    if (v == t-&gt;v[0]) return t-&gt;neighbors[0];\n    if (v == t-&gt;v[1]) return t-&gt;neighbors[1];\n    if (v == t-&gt;v[2]) return t-&gt;neighbors[2];\n}\n\n// traverse over all triangles around vertex v\nvoid *tccwv(V *v) {\n    T *t = v-&gt;t;\n    do {\n        /* operations on t*/\n        t = t-&gt;neighbors[0];\n    } while (t != v-&gt;t);\n}\n</code></pre>"},{"location":"cs284/mesh.html#half-edge","title":"Half Edge","text":"<p>A more popular data structure is half edge, where the key idea is to use two half-edges act as the glue between mesh elements. </p> <pre><code>struct H {\n    H *twin; // the opposite halfedge that share the same edge\n    H *next; // the next halfedge within the face\n    V *vertex; // the starting vertex of the half edge\n    E *edge; // the edge that is shared between this and twin\n    F *face; // the triangle face that this belongs to\n}\n\nstruct V {\n    VectorXd position;\n    H *h; // any one of the halfedge that starts from this vertex\n}\n\nstruct E {\n    H *h; // any one of the halfedge that corresponds to this edge\n}\n\nstruct F {\n    H *h; // any one of the halfedge that is in this face\n}\n</code></pre>"},{"location":"cs284/mesh.html#traversal","title":"Traversal","text":"<p>Note that the half edge is the orientation of the triangle face, and we can traverse all edges / vertices around the face.</p> travese all edges or vertices in a face<pre><code>H *entry_h = ... // vertex-&gt;h, face-&gt;h, edge-&gt;h, or any given ;\nH *curr_h = entry_h-&gt;next;\ndo {\n    /* do things on curr_h, curr_h-&gt;edge, curr_h-&gt;vertex */\n    curr_h = curr_h-&gt;next;\n} while (curr_h != entry_h);\n</code></pre> <p>Note that any sphere around a manifold will cover a disk, which means if we traverse the faces around the vertex, we should be able to come back to the original face. For half-edge, this is achieved by following the half edges until we come back.</p> travese all faces around a vertex<pre><code>H *h = v-&gt;h;\ndo {\n    /* do thing on h-&gt;face or \n       traverse in the face \n    */\n    h = h-&gt;next-&gt;twin;\n} while (h != v-&gt;h);\n</code></pre> <p></p>"},{"location":"cs284/mesh.html#local-operations","title":"Local operations","text":"<p>Some local operations that we'd like to support:</p> <ul> <li><code>flip</code>: for two adjacent faces \\(abc\\) and \\(bad\\), make them to \\(acd\\) and \\(cdb\\).</li> <li><code>split</code>: split two adjacent faces into 4 faces by breaking one edge</li> <li><code>collapse</code>: remove one edge, make the two vertices into a new vertex and connect to all previous edges</li> </ul> <p>Note that half edge is implemented as arrays of struct pointers. Thus, the local operations are just a bunch of pointer re-assignment. </p> <p></p>"},{"location":"cs284/mesh.html#loop-subdivision","title":"Loop Subdivision","text":"<p>Subdivision aims to smooth the mesh by adding more vertices and faces, and update vertices via local average. </p> <p>Loop subdivision is an approximation (old vertices will move) and make \\(C^2\\) smooth faces. The method works by taking the midpoint of each edge, and then use the new midpoint vertices to divide each triangle into 4 triangles. Finally, update the positions of vertices using its adjacent vertices. The implementation is the following</p> Loop subdivision<pre><code>Vector3D old_vertex_position(V *v) {\n    H *h = v-&gt;h;\n    int degree = 0;\n    Vector3D position_new(0, 0, 0);\n    do {\n        position_new += h-&gt;next-&gt;vertex-&gt;position;\n        h = h-&gt;next-&gt;twin;\n        degree++;\n    } while (h != v-&gt;h);\n    double u = (degree == 3) ? 3. / 16. : 3. / (8. * degree);\n    return u * position_new + (1. - degree * u) * v-&gt;position;\n}\n\nVector3D new_vertex_position(E *e) {\n    return\n        0.375 * e-&gt;h-&gt;vertex-&gt;position + \n        0.375 * e-&gt;h-&gt;twin-&gt;vertex-&gt;position + \n        0.125 * e-&gt;h-&gt;next-&gt;next-&gt;vertex-&gt;position + \n        0.125 * e-&gt;h-&gt;twin-&gt;next-&gt;next-&gt;vertex-&gt;position; \n}\nvoid loop_subdivision(Mesh m) {\n    Mesh old_m = m.copy(); // make a deep copy for reference \n\n    // update the existing vertices positions \n    for (int i = 0; i &lt; old_m.vertices.size(); i++) {\n        m.vertices[i]-&gt;position = old_vertex_position(old_m.vertices[i]);\n    }\n    // create new vertices and compute their positions\n    for (int i = 0; i &lt; old_m.edges.size(); i++) {\n        V *v = new V();\n        v-&gt;position = new_vertex_position(old_m.edges[i]);\n        v-&gt;isNew = true;\n        m.vertices.push_back(v);\n    }\n\n    // split each edge\n    for (int i = 0; i &lt; old_m.edges.size(); i++) {\n        // split will add newly created edges to the end of m.edges\n        split(m.edges[i], m);\n    }\n\n    // flip edges that connect one old vertex and one new vertex\n    for (int i = old_m.edges.size(); i &lt; m.edges.size(); i++) {\n        E *e = m.edges[i];\n        // one endpoint is new and the other is old\n        if (e-&gt;h-&gt;vertex-&gt;isNew != e-&gt;h-&gt;twin-&gt;vertex-&gt;isNew)\n            flip(e);\n    }\n}\n</code></pre>"},{"location":"cs284/mesh.html#semi-regular-meshes","title":"Semi-regular meshes","text":"<p>A mesh is regular if \\(\\forall v\\in V. degree(v) = 6\\). Loop subdivision creates semi-regular meshes, which means only a few extraordinary points (where degree is not 6). </p> <p>Note that if a triangular mesh is topologically equivalent to sphere (a manifold without any boundary), then it will not be regular. </p>"},{"location":"cs284/mesh.html#cutmull-clark-subdivision","title":"Cutmull-Clark Subdivision","text":"<p>Cutmull Clark subdivision is a popular subdivision algorithm for general meshes (triangular, quad, or even a mix). </p> <p>First, consider a regular quad mesh, where the topology is equivalent to a structured grid (each vertex has exactly degree of 4 unless a boundary). The idea is very simple, breaking each grid into 4 grid. In order to do so, we need </p> <ul> <li>For each face, add a face point as the center of the face, i.e. the average of the 4 vertices </li> </ul> \\[\\mathbf x_f = \\frac{1}{4}(\\mathbf x_1 +\\mathbf x_2 + \\mathbf x_3+\\mathbf x_4)\\] <ul> <li>For each edge, add an edge point as the average of two endpoints and two face points from the faces associated with the edge </li> </ul> \\[\\mathbf x_e = \\frac{1}{4}(\\mathbf x_1 + \\mathbf x_1 + \\mathbf x_{f1} + \\mathbf x_{f2})\\] <ul> <li>For each existing point, update its position with its 4 face points, 4 mid points (the average of two endpoints of an edge), and its own position</li> </ul> \\[\\mathbf x' = \\frac{1}{16}(\\mathbf x_{f1} + \\mathbf x_{f2}+\\mathbf x_{f3} + \\mathbf x_{f4} + 2(\\mathbf x_{m1} + \\mathbf x_{m2} + \\mathbf x_{m3} + \\mathbf x_{m4}) + 4\\mathbf x)\\] <p>For non-regular meshes (not necessarily quad), the process is similar</p> <ul> <li>Each face point is the average of surrounding vertices</li> </ul> \\[\\mathbf x_f = \\frac{1}{n}\\sum_{i=1}^n \\mathbf x_i\\] <ul> <li> <p>Each edge point is the average of two endpoints and two face points from the faces associated with the edge (exactly the same as before)</p> </li> <li> <p>Each existing point by a weighted average of adjacent face points, midpoints, and vertices. </p> </li> </ul> \\[\\mathbf x' = \\frac{1}{n}(\\bar{\\mathbf x_f} + 2\\bar{\\mathbf x_m} + (n-3) \\mathbf x)\\] <p>where \\(n\\) is degree of vertex \\(\\mathbf x\\), \\(\\bar{\\mathbf x_f}, \\bar{\\mathbf x_m}\\) is the average of face points and midpoints. </p>"},{"location":"cs284/mesh.html#mesh-simplification","title":"Mesh Simplification","text":"<p>Want to simplify the mesh but make the geometry/surface suffers a minimal change. Locally, we wan collapse one edge to simplify a model. Note that each edge collapse reduce a constant number of edges, vertices, and faces. However, the error caused by removing different edges are very different. </p> <p>The idea is that we want to collapse the edge s.t. the distance from the actual surface to the mesh has the minimal change, and we repeat this greedily. Then, what is the distance? When we remove one edge, the change is made locally, thus we only need to measure the distance changes as sum of (squared)distances from the new vertex to planes containing the impacted triangles. </p> <p>Note that the squared distance for a plane \\(\\mathbf n =(n_x, n_y, n_z), \\mathbf p = (p_x, p_y, p_z)\\) to vertex \\(\\mathbf v = (v_x, v_y, v_z)\\) is </p> \\[\\begin{align*} d(\\mathbf v) &amp;= (\\mathbf n \\cdot \\mathbf p - \\mathbf n \\cdot \\mathbf v)^T (\\mathbf n \\cdot \\mathbf p - \\mathbf n \\cdot \\mathbf v) \\\\ &amp;=  \\mathbf p^T \\cdot \\mathbf n^T \\cdot \\mathbf n \\cdot \\mathbf p - 2\\mathbf v^T \\cdot \\mathbf n^T \\cdot \\mathbf n \\cdot \\mathbf p + \\mathbf v^T \\cdot \\mathbf n^T \\cdot \\mathbf n \\cdot \\mathbf v\\\\ &amp;= \\mathbf p^T \\mathbf N \\mathbf p - 2\\mathbf v^T \\mathbf N \\mathbf p + \\mathbf v^T \\mathbf N \\mathbf v\\\\ &amp;= \\tilde{\\mathbf v}^T Q \\tilde{\\mathbf v}\\\\ \\end{align*}\\] <p>where \\(\\tilde{\\mathbf v} = (v_x, v_y, v_z, 1)\\) is the homogeneous form of \\(\\mathbf p\\) and \\(Q\\) is the quadratic error matrix</p> \\[d = -\\mathbf n \\cdot \\mathbf p, Q = \\begin{bmatrix} n_x^2&amp;n_xn_y&amp;n_xn_z&amp;n_x d\\\\ n_yn_x&amp;n_y^2&amp;n_yn_z&amp;n_y d\\\\ n_zn_x&amp;n_zn_y&amp;n_z^2&amp;n_z d\\\\ n_x d &amp;n_y d &amp;n_z d &amp; d^2 \\end{bmatrix}\\] <p>Note that \\(Q\\) only depends on the plane, i.e. per-triangle, and the sum of squared distance is </p> \\[\\sum d_i(\\mathbf v) = \\sum_i \\tilde{\\mathbf v}^T Q_i \\tilde{\\mathbf v} = \\tilde{\\mathbf v}^T \\sum_i Q_i \\tilde{\\mathbf v}\\] <p>Finally, the algorithm as</p> Quadric cost mesh simplification<pre><code>void mesh_simplify(Mesh m, int target) {\n    for (F&amp; face: m.faces) {\n        face.Q = compute_cost(face);\n    }\n    for (V &amp;vertex: m.vertices) {\n        vertex.Q = 0;\n        for (F f: vertex.get_neighbor_faces())\n            vertex.Q += f.Q;\n    }\n    for (E &amp;edge: m.edges) {\n        edge.Q = edge.endpoint[0].Q + edge.endpoint[1].Q;\n    }\n\n    while (m.faces.size() &gt; target) {\n        /* try to collapse each edge and find its cost */\n        /* apply the collapse on the minimum cost edge */\n        /* update vertex Q, and then edge Q */\n    }\n\n}\n</code></pre>"},{"location":"cs284/mesh.html#mesh-regularization","title":"Mesh regularization","text":"<p>To measure the regularity of a mesh, one simple metric is that all faces have roughly the same shape. Another way of thinking is that each vertex has 6 degrees (for triangles) or 4 degrees (for quads). In this case, all triangles are close to equilateral triangles (since each angle is close to \\(360/6 =60\\) degrees) and quads are close to rectangles (\\(360/4 = 90\\) degrees). </p> <p>One way for improving regularity is to flip edges s.t. total deviation from degree 6 get smaller. </p>"},{"location":"cs284/pathtracing.html","title":"Global Illumination and Path Tracing","text":""},{"location":"cs284/pathtracing.html#radiometry-and-photometry","title":"Radiometry and Photometry","text":"<p>Radiometry Measurement system and units for illumination, measure the spatial properties of light.  Photometry Measure the radiometric quantities accounts for response of human visual system. </p> Physics Radiometry Photometry Description energy \\(Q\\) radiant energy (Joules \\(J\\)) luminous energy (\\(\\text{lm}\\cdot\\text{sec}\\)) energy of electromagnetic radiation Flux (power) \\(\\Theta = d_tQ\\) radiant Power (Watt \\(W\\)) luminous power (lumen \\(\\text{lm}\\)) energy emitted, reflected, transmitted or received, per unit time angular flux density \\(I\\) radiant intensity (\\(W/sr\\)) luminous intensity (candela \\(\\text{cd} = \\text{lm}/sr\\)) power per unit solid angle emitted by a point light source spatial flux density \\(E\\) irradiance (\\(W/m^2\\)) illuminance (\\(\\text{lux} = \\text{lm}/m^2\\)) power per unit area incident on a surface point spatio-angular flux density \\(L\\) radiance (\\(W/m^2/sr\\)) luminance (\\(\\text{nit} = \\text{cd}/m^2\\)) power emitted, reflected, transmitted or received by a surface, per unit solid angle, per unit projected area."},{"location":"cs284/pathtracing.html#solid-angles","title":"Solid angles","text":"<p>A solid angle is similar to the radian on a circle, a solid angle is the ratio of subtended area on sphere to radius squared. \\(\\Omega = A / r^2\\), a sphere has \\(4\\pi\\) steradians (\\(sr\\)). </p> <p>The surface area of the unit sphere \\(S^2\\) is</p> \\[\\Omega = \\int_{S^2} d\\omega = \\int_0^{2\\pi}\\int_0^\\pi \\sin\\theta d\\theta d\\phi = 4\\pi\\] <p>where \\(\\omega = (\\phi, \\theta)\\) is the unit length direction. Thus, the angular flux intensity is derived from the total emitted power as </p> \\[\\Phi = \\int_{S^2} Id\\omega = 4\\pi I, I = \\Phi / 4\\pi\\]"},{"location":"cs284/pathtracing.html#irradiance","title":"Irradiance","text":"<p>Lambert's Cosine Law Irradiance at surface is proportional to cosine of angle between light direction and surface normal. </p> \\[E = \\frac{\\Phi}{A} \\cos\\theta = \\frac{\\Phi}{A} \\mathbf l \\cdot \\mathbf n\\] <p>where \\(\\mathbf l\\) is the light direction and \\(\\mathbf n\\) is the surface normal. </p> <p>Falloff Assume that light is emitting flux \\(\\Phi\\) in a uniform angular distribution, then the intensity falloffs by squared radius. </p> \\[E_1 = \\frac{\\Phi}{4\\pi}, E_r = \\frac{\\Phi}{4\\pi r^2}, E_r = \\frac{E_1}{r^2}\\]"},{"location":"cs284/pathtracing.html#radiance","title":"Radiance","text":"<p>radiance / luminance is the power emitted, reflected, transmitted or received by a surface, per unit solid angle, per unit projected area. </p> \\[L(\\mathbf x, \\omega) = \\frac{\\partial^2 \\Phi}{\\partial\\omega \\partial A \\cos\\theta} = \\frac{\\partial E}{\\partial \\omega \\cos\\theta} = \\frac{\\partial I}{\\partial A \\cos\\theta}\\] <p>Consider a opaque object surface, light will absorb, reflection, and refract on the surface. Thus, the incident radiance and exitant radiance are different. </p>"},{"location":"cs284/pathtracing.html#environment-irradiance","title":"Environment irradiance","text":"<p>For a given surface point \\(\\mathbf x\\), the environment irradiance is the amount of flux received is the integral over incoming light from all directions.</p> \\[E(\\mathbf x) = \\int_{H^2} L_i(\\mathbf x, \\omega) \\cos\\theta d\\omega\\] <p>where \\(\\theta = \\omega_i \\cdot \\mathbf n_x\\) is the angle between the incoming light and the surface normal at \\(\\mathbf x\\), note that \\(H^2\\) is the hemisphere because \\(\\cos\\theta &lt; 0\\) for the over half. </p> <p>For a uniform area light source, the light irradiance on the surface is proportional to the projected area on the unit sphere, then projected onto surface tangent plane.</p> \\[E(\\mathbf x) = \\int_{H^2} L_i(\\mathbf x, \\omega)\\cos\\theta d\\omega = L\\int_{\\Omega}\\cos\\theta d\\omega = L\\text{proj}(\\Omega)\\] <p>Thus, for a disk area light of radius \\(r\\) and \\(h\\) units above the surface. The projection on the unit sphere is a circle of radius \\(\\sin(\\alpha) = \\sin(\\tan^{-1}(r/h))\\). Thus \\(\\text{proj}(\\Omega) = \\pi r^2 = \\pi\\sin^2(\\alpha)\\)</p>"},{"location":"cs284/pathtracing.html#material-reflectance-brdf","title":"Material reflectance (BRDF)","text":"<p>For a material surface point \\(\\mathbf x\\), the incoming irradiance from angle \\(\\omega_i\\) is a differential of the total irradiance.</p> \\[\\partial_{\\omega}E(\\omega_i) = L_i(\\mathbf x, \\omega_i)\\cos\\theta_i d\\omega_i\\] <p>and outgoing radiance, or the reflectance, at direction \\(\\omega_r\\), is proportional to the incoming irradiance, and we define the bidirectional reflectance distribution function (BRDF) as</p> \\[f_{\\mathbf x}(\\omega_i, \\omega_r) = \\frac{\\partial_{\\omega} L_r (\\omega_r)}{\\partial_{\\omega} E_i (\\omega_i)} = \\frac{\\partial_{\\omega} L_r (\\omega_r)}{L_i(\\mathbf x, \\omega_i)\\cos\\theta_i d\\omega_i}\\] <p>so that the outgoing reflectance at position \\(\\mathbf x\\) and viewing direction \\(\\omega_r\\) is</p> \\[L_r(\\mathbf x, \\omega_r) = \\int_{H^2} f_{\\mathbf x}(\\omega_i, \\omega_r) L_i(\\mathbf x, \\omega_i) \\cos\\theta_i d\\omega_i\\]"},{"location":"cs284/pathtracing.html#global-illumination-and-path-tracing_1","title":"Global illumination and path tracing","text":"<p>Note that the outgoing irradiance \\(L_r\\) can intersect other surfaces and become \\(L_i\\) on other surfaces, thus the integral is recursive (and probably infinitely recursive if the light is bouncing within a closed environment). Which means</p> \\[L_i(\\mathbf x, \\omega_i) = L_o(T(\\mathbf x, \\omega_i), -\\omega_i)\\] <p>where \\(T(\\mathbf x, \\omega_i)\\) is the first intersection in the scene along ray \\(\\mathbf x, \\omega_i\\).</p> <p>Therefore, we rewrite the reflection equation at surface position \\(\\mathbf x\\) and outgoing angle \\(\\omega_o\\) as</p> \\[L_o(\\mathbf x, \\omega_o) = L_e(\\mathbf x, \\omega_o) + \\int_{H^2} f_{\\mathbf x}(\\omega_i, \\omega_o) L_o(T(\\mathbf x, \\omega_i), -\\omega_i)\\cos\\theta_i d\\omega_i\\] <p>We simplify the notation by using higher order operator as </p> <ul> <li>\\(R(L_i) = L_o\\) the reflection operator, the outgoing radiance resulting from the incoming radiance</li> <li>\\(T(L_o) = L_i\\) the transport operator, the incoming radiance that is the light transport of the other outgoing irradiance. </li> </ul> <p>Together, we simplify the notation as </p> \\[L_o = L_e + (R\\circ T)(L_o) := L_e + K(L_o)\\] <p>the outgoing light is the sum of self-emitting light, plus all the lights bouncing around the scene and reflected on the surface. </p> <p>Intuitively, the amount of the light on the surface will converge since we lose some light at each bounce. In fact, we can make sure that \\(L_o\\) converges if the BRDF is correctly defined. Therefore, we have that </p> \\[L = \\sum_{i=0}^{\\infty} K^i(L_e)\\] <p>where \\(L_e\\) is the emitted light and \\(K^0(L_e) = L_e\\). In other words, the total amount of light of a surface is the sum of its own emitted light \\(K^0\\), all emitted light that reflected by the surface \\(K^1\\), all emitted light that bounced once and reflected by the surface \\(K^2\\), all emitted light bounced twice, and so on. </p>"},{"location":"cs284/pathtracing.html#monte-carlo-estimate","title":"Monte Carlo estimate","text":"<p>Given a distribution \\(p(\\omega_i)\\) on the incoming light direction, the Monte Carlo estimate of the outgoing reflectance is </p> \\[\\tilde{L_r}(\\mathbf x, \\omega_r) = \\frac{1}{N}\\sum_{j=1}^N \\frac{1}{p(\\omega_j)} f_{\\mathbf x}(\\omega_j, \\omega_r)L_i(\\mathbf x, \\omega_j)\\cos \\theta_j\\] <p>some choice of \\(p\\) can be the uniform distribution of hemisphere directions with \\(p(\\omega_i) = \\frac{1}{2\\pi}\\), importance sampling of the BRDF function, or importance sampling of the lights (uniformly sample positions on the light)</p> <p>We can implement the operator as a recursion, i.e.</p> <pre><code># eye: the viewing position\n# dir: the viewing angle\ndef Lout(eye, dir, recursion_depth=0):\n    x = scene.intersect(eye, dir) # the object surface\n    w_out = -dir # the outgoing direction of x\n    L = x.emission(w_out) # emitted light\n\n    # avoid infinite recursion\n    if recursion_depth == MAX_RECURSION_DEPTH:\n        return L\n\n    # importance sampling a incoming direction based on brdf\n    w_in, pdf = x.brdf.sample(w_out) \n    L += Lout(x, w_in, recursion_depth + 1) * x.brdf.f(w_in, w_out) * dot(x.normal, w_in) / pdf\n    return L\n</code></pre> <p>Although higher <code>MAX_RECURSION_DEPTH</code> will give better estimation, since light contributions from higher bounces decrease exponentially and will eventually become small enough. The above implementation is biased, especially when <code>MAX_RECURSION_DEPTH</code> is small, since we can never observe any bounced lights for <code>MAX_RECURSION_DEPTH + 1</code> bounces. </p>"},{"location":"cs284/pathtracing.html#russian-roulette","title":"Russian Roulette","text":"<p>Russian Roulette is a Monte Carlo estimate of infinite integrals. For some random variable \\(X\\), let \\(Y \\sim \\text{Uniform}(0, 1)\\) and define </p> \\[X_{rr} = \\frac{X}{p_{rr}} \\mathbb I(Y &lt; p_{rr})\\] <p>so that the expected value for \\(X_{rr}\\) is </p> \\[E(X_{rr}) = p_{rr}E(\\frac{X}{p_{rr}}) + (1-p_{rr})0 = E(X)\\] <p>which means that we stop the recursion if we fail a Bernoulli trial with probability \\(p_{rr}\\). </p>"},{"location":"cs284/pathtracing.html#path-tracing","title":"Path tracing","text":"<p>We further decompose <code>Lout</code> into two parts. The direct illumination given by light sources (\\(K^1(L_o)\\)) and indirect illumination given by reflected light from other surfaces (\\(K^{&gt;1}(L_o)\\)). Summarize all things together, we have</p> <pre><code>def L0(x, w_out):\n    return x.emission(w_out)\n\ndef L1(x, w_out):\n    L = 0\n    for _ in raneg(NUM_SAMPLES):\n        # randomly choose light i from multiple lights in the scene\n        # with probability pi\n        light, pi = scene.sample_light_source()\n        # importance sampling of light position with probability pl\n        light_pos, pl = light.sample_dir()\n        w_in = normalized(light_pos - x)\n        # if the light arrives the object, not blocked by others\n        if not scene.intersect(x, w_in):\n            l = light.emission(light_pos, -w_in)\n            L += l * x.brdf(w_in, w_out) * dot(w_in, x.normal) / pi / pl\n    return L\n\n\n\ndef L1_and_more(x, w_out, recursion_depth=0):\n    L = L1(x, w_out)\n    w_in, pdf = x.brdf.sample(w_out) \n    cpdf = continue_probability(x.brdf, w_in, w_out) \\\n        if recursion_depth &gt; MAX_RECURSION_DEPTH \\\n        else 1\n    # start Russian Roulette when we already enough bounces\n    if random() &gt; cpdf:\n        return L\n    # importance sampling a incoming direction based on brdf\n    x_reflected = scene.intersect(x, w_out)\n    w_in_reflected = -w_out\n    L += L1_and_more(x_reflected, w_in_reflected, recursion_depth + 1) \\ \n        * x.brdf.f(w_in, w_out) \\ \n        * dot(x.normal, w_in) \\ \n        / pdf / cpdf\n    return L\n\n# eye: the viewing position\n# dir: the viewing angle\ndef Lout(eye, dir):\n    x = scene.intersect(eye, dir) # the object surface\n    w_out = -dir # the outgoing direction of x\n    return L0(x, w_out) + L1_and_more(x, w_out)\n</code></pre>"},{"location":"cs284/raytracing.html","title":"Ray Tracing and Acceleration Data Structure","text":""},{"location":"cs284/raytracing.html#viewing-rays","title":"Viewing Rays","text":"<p>The 3D parametric line from the eye \\(\\mathbf o\\in\\mathbb R^3\\) to a point \\(\\mathbf p\\in\\mathbb R^3\\) is </p> \\[p(t) = \\mathbf o + t(\\mathbf p-\\mathbf o) = \\mathbf o + t \\mathbf d\\] <p>We advance from \\(\\mathbf o\\) along the vector \\(\\mathbf d := \\mathbf p-\\mathbf o\\) a fractional distance \\(t\\) to find the point \\(\\mathbf p\\). \\(\\mathbf e\\) is the ray's origin and \\(\\mathbf d\\) is the ray's direction. </p>"},{"location":"cs284/raytracing.html#basic-setup","title":"Basic Setup","text":"<p>Given a camera frame \\(\\mathbf o\\), i.e. the eye point or view point, and \\(\\mathbf u, \\mathbf v, \\mathbf w\\) for the basis. We specify that \\(-\\mathbf w = view\\), \\(\\mathbf u\\) points rightward from the \\(view\\) and \\(\\mathbf v\\) points upward from the \\(view\\). </p> <p>The viewing rays should start on the plane defined by the point \\(\\mathbf o\\) and the vectors \\(\\mathbf u\\) and \\(\\mathbf v\\); the only remaining information required is where on the plane the image is supposed to be. We'll define the image dimensions with four numbers, for the four sides of the image: \\(l\\) and \\(r\\) are the positions of the left and right edges of the image, as measured from e along the \\(\\mathbf u\\) direction; and \\(b\\) and \\(t\\) are the positions of the bottom and top edges of the image, as measured from e along the \\(\\mathbf v\\) direction. Usually \\(l &lt; 0 &lt; r\\) and \\(b &lt; 0 &lt; t\\).</p> <p>Therefore, the pixel at \\((i,j)\\) in the raster image has the position </p> \\[u = l + (r-l)(i+0.5) / n_x; v = b + (t-b)(j + 0.5) / n_y\\]"},{"location":"cs284/raytracing.html#ray-casting","title":"Ray casting","text":"<p>For an orthographic view, the ray's origin is generated from the image and are parallel with each other. Therefore, given \\(u,v\\)</p> \\[\\mathbf d(u, v) = -\\mathbf w, \\mathbf o(u, v) = \\mathbf o + u \\mathbf u + v\\mathbf v\\] <p>For an perspective view, the ray's origin is the camera origin and rays have different directions. Therefore, given \\(u,v\\)</p> \\[\\mathbf d(u, v)= -\\mathbf w + u\\mathbf u + v\\mathbf v, \\mathbf o(u, v) = \\mathbf o\\]"},{"location":"cs284/raytracing.html#ray-intersection","title":"Ray Intersection","text":"<p>In general, we have the each ray as a line </p> \\[\\mathbf r(t) = \\mathbf o + t\\mathbf d, t\\in [0, \\infty)\\] <p>For some implicit surface represented by 0-level set \\(\\{\\mathbf p \\in\\mathbb R^3: f(\\mathbf p) = 0\\}\\), we can substitute \\(\\mathbf p = \\mathbf r(t)\\) and solves for real-positive roots for \\(t\\). </p>"},{"location":"cs284/raytracing.html#sphere","title":"Sphere","text":"<p>A sphere is represented by a center \\(\\mathbf c= (x_c, y_c, z_c)\\) and radius \\(r\\), where </p> \\[f(\\mathbf p) := \\|\\mathbf p-\\mathbf c\\|^2-r^2\\] <p>so we can plug in \\(\\mathbf p=\\mathbf o+t\\mathbf d\\) and obtain the equation </p> \\[\\|\\mathbf o+t\\mathbf d-\\mathbf c\\|^2-r^2 = 0\\] <p>Note that this is a quadratic function about \\(t\\), i.e. </p> \\[\\mathbf d^T\\mathbf d t^2 + 2\\mathbf d^T(\\mathbf o-\\mathbf c)t + (\\mathbf o-\\mathbf c)^T(\\mathbf o-\\mathbf{c}) - r^2 = 0\\] <p>let \\(A = \\mathbf d^T\\mathbf d, B = 2\\mathbf d^T(\\mathbf o-\\mathbf c), C = (\\mathbf o-\\mathbf c)^T(\\mathbf o-\\mathbf{c}) - r^2\\),  Note that a ray must have two points intersect with a sphere, one point going inside and one going outside. Therefore, we need \\(B^2 - 4AC &gt; 0\\) and </p> \\[t = \\frac{-B \\pm \\sqrt{B^2 - 4AC}}{2A}\\] <p>And the normal vector and unit normal at \\(p\\) is </p> \\[\\mathbf n = \\nabla f(\\mathbf p) = 2(\\mathbf p-\\mathbf c), \\hat{\\mathbf n} = \\frac{\\mathbf n}{2r}\\]"},{"location":"cs284/raytracing.html#plane","title":"Plane","text":"<p>A plane can be represented by an arbitrary point \\(\\mathbf p_0\\) and its normal \\(\\mathbf n\\) as </p> \\[f(\\mathbf p) = (\\mathbf p-\\mathbf p_0)\\cdot \\mathbf n\\] <p>since any vector lies on the plane should be perpendicular to the plane's normal. Therefore, we want to solve </p> \\[\\begin{align*} (\\mathbf o+t\\mathbf d - \\mathbf p_0)\\cdot \\mathbf n &amp;= 0\\\\ t &amp;= \\frac{(\\mathbf p_0 - \\mathbf o)^T \\mathbf n}{\\mathbf d^T\\mathbf n} \\end{align*}\\] <p>and the normal is just \\(\\hat{\\mathbf n} = \\mathbf n/\\|\\mathbf n\\|\\)</p>"},{"location":"cs284/raytracing.html#triangle","title":"Triangle","text":"<p>Triangle can be represented with 3 vertices \\(\\mathbf a, \\mathbf b, \\mathbf c\\), or the 3 corners. One way of implementing triangle intersection is to find the intersection point \\(\\mathbf p=\\mathbf o+t\\mathbf d\\) with the plane that the triangle lines on and then decide whether the point is within the triangle. However, we can also use barycentric coordinates where we solves </p> \\[\\mathbf o + t\\mathbf d = \\mathbf a + \\beta(\\mathbf b-\\mathbf a) + \\gamma(\\mathbf c-\\mathbf a)\\] \\[\\begin{bmatrix} x_a - x_b&amp;x_a-x_c &amp;x_d\\\\ y_a - y_b&amp;y_a-y_c &amp;y_d\\\\ z_a - z_b&amp;z_a-z_c &amp;z_d \\end{bmatrix}\\begin{bmatrix}\\beta\\\\\\gamma\\\\t\\end{bmatrix} = \\begin{bmatrix}x_a-x_e\\\\y_a-y_e\\\\z_a-z_e\\end{bmatrix}\\] <p>if exists such \\(t, \\beta, \\gamma &gt;0, \\beta + \\gamma &lt; 1\\), then there is an intersection. Then, the easiest way to solve such \\(3\\times 3\\) matrix is to use Cramer's rule.  For \\(Ax = b\\) where \\(A\\) is \\(n\\times n\\) matrix, denote \\(A_i = A\\) with the \\(i\\)th column being replaced by \\(b\\), so that \\(x_i = \\det(A_i) / \\det(A)\\). </p> <p>The normal is the plane's normal, i.e. can be obtained by any two vector's cross product. </p>"},{"location":"cs284/raytracing.html#bvh-for-ray-intersections","title":"BVH for ray intersections","text":"<p>BVH implementations - AABB Tree</p>"},{"location":"cs284/raytracing.html#partition-heuristics","title":"Partition Heuristics","text":"<p>A good partition aims to minimize the average cost of tracing a ray, which is proportional to the number of objects in the leaf node. Therefore, the average cost for ray tracing a BVH is the average cost of all triangles, weighted by the probability of hitting the object. </p> <p>If we assume uniform ray distribution with no occlusions, then the probability should be proportional to the surface area of the object. </p>"},{"location":"cs284/sampling.html","title":"Sampling Theory","text":""},{"location":"cs284/sampling.html#fourier-transform-theory","title":"Fourier Transform Theory","text":"<p>Given function \\(f:\\mathbb R\\rightarrow \\mathbb R\\) in the spatial domain, the function in the frequency domain \\(F\\) is </p> \\[F(w) = \\int_{-\\infty}^\\infty f(x)e^{-iwx}\\] <p>Often is defined instead with spatial</p> \\[F(f) = \\int_{-\\infty}^\\infty f(x)e^{-i2\\pi x} dx\\] <p>The inverse Fourier transform is</p> \\[f(x) = \\int_{-\\infty}^\\infty F(w)e^{iwx}dw\\] <p>Note that \\(e^{ix} = \\cos(x) + i\\sin(x)\\).</p> <p>A Fourier transformation is linear \\(F(a f_1 + bf_2) = aF(f_1) + bF(f_2)\\).</p>"},{"location":"cs284/sampling.html#delta-distribution","title":"Delta Distribution","text":"<p>(Informally) Delta function can be though as \\(\\delta(x): \\mathbb{R}\\rightarrow\\mathbb{R} = \\begin{cases}\\infty &amp;x=0\\\\0&amp;x\\neq 0\\end{cases}\\) and \\(\\int_{-\\infty}^\\infty \\delta(x)dx = 1\\). Delta function can be used to represent the impulse of Fourier components in the frequency domain. </p> \\[\\begin{align*} F(u) = \\delta(u)) &amp;= \\int_{-\\infty}^\\infty \\delta(x) e^{-i2\\pi x}dx = 1\\\\ F(u) = \\delta(u-t_0)) &amp;\\iff f(x) = e^{i2\\pi t_0 x}\\\\ F(u) = \\frac{1}{2}(\\delta(u-t_0) + \\delta(u+t_0)) &amp;\\iff f(x) = \\cos(2\\pi t_0x)\\\\ F(u) = \\frac{1}{2i}(\\delta(u-t_0) + \\delta(u+t_0)) &amp;\\iff f(x) = \\sin(2\\pi t_0x) \\end{align*}\\]"},{"location":"cs284/sampling.html#2d-fourier-transformations","title":"2D Fourier Transformations","text":"<p>The 2D Fourier transform is given as</p> \\[F(u,v) = \\iint_{-\\infty}^\\infty f(x, y) e^{-2\\pi(ux+vy)i}dxdy\\] \\[f(x,y) = \\iint_{-\\infty}^\\infty F(u,v)e^{2\\pi (ux+vy)i}dudv\\] <p>In general, the Fourier transform can be represented as the magnitude spectrum \\(A = |F|\\) and the phase spectrum \\(\\phi = \\text{arc}\\tan(\\frac{Im(F)}{Re(F)})\\). Where \\(A\\) represents the amplitude of each Fourier component, and \\(\\phi\\) represents the phase/shift of the Fourier component.  </p> <p>2D FT has many useful properties</p> <ul> <li>Linearity (same as 1D)</li> <li>Symmetry \\(|F(u,v)| = |F(-u,-v)|\\)</li> <li>Convolution theory \\(f*g \\iff F\\cdot G\\)</li> <li>Separable \\(f(x,y) = f_x(x)f_y(y)\\iff F(u,v)=F_x(u)F_y(v), F_x(u) = F_x(f_x), F_y(v) = F_y(f_y)\\)</li> </ul> <p>Thus, we have some common examples</p> <ul> <li>constant function will have pulse at \\((0,0)\\) \\(F(u,v) = \\delta(u,v)\\iff f(x,y) = c\\). </li> <li>cosine waves \\(f(x,y) = \\cos(2\\pi t_0 x + 2\\pi t_1 y)\\) will have pulse at \\(\\pm (t_0, t_1)\\)</li> <li>sum of waves \\(f(x,y) = \\sin(2\\pi t_0x) + \\cos(2\\pi t_1y)\\) will have pulse at \\(\\pm(t_0, 0), \\pm (0, t_1)\\)</li> <li>product of waves \\(f(x,y) = \\sin(2\\pi t_0x)\\cos(2\\pi t_1)y\\) will have pulse at \\((\\pm t_0, \\pm t_1)\\)</li> <li>2D Gaussian is still a Gaussian in magnitude spectrum. </li> </ul>"},{"location":"cs284/sampling.html#image-frequency-space","title":"Image Frequency Space","text":"<p>Note that an image is (mathematically) a continuous 2D function \\(I(x, y)\\), which can be transformed into frequency domain. </p> <p>In practice, digital images is discrete and we can transform it into frequency domain using discrete Fourier transform.</p> <p> </p> 2D functions in spatial space (left) and Fourier space (right) <p>Aliasing happens when two signals are indistinguishable at the same sampling rate. </p>"},{"location":"cs284/sampling.html#filtering-convolution-theorem","title":"Filtering / Convolution Theorem","text":"<p>Convolution in the spatial domain is equivalent to the multiplication in Fourier domain. </p> <p>For example, Gaussian filter is a low-pass filter and is equivalent to multiplication in Fourier domain. </p> <p> </p> Image in spatial space (top) and Fourier space (bottom) <p>As shown in the Gaussian example, larger convolution kernel is equivalent to lower frequencies filter and verse versa. Thus, very large convolution can be implemented as FFT and inverse FFT. </p> <p>(running time for convolution: \\(O(F^2 N^2)\\) where \\(F\\) is the kernel size and \\(N\\) is image size, running time for FFT filtering \\(O(N^2 \\log(N^2))\\)).</p>"},{"location":"cs284/sampling.html#aliasing","title":"Aliasing","text":"<p>Intuitively, Fourier transform represents the sin and cos waves that can add into a function in real domain. </p> <p>If high frequency signal is insufficiently sampled, then samples will erroneously appear to be from a low frequency sample. </p>"},{"location":"cs284/sampling.html#nyquist-theorem","title":"Nyquist Theorem","text":"<p>We get no aliasing from frequencies in the signal that are less than the Nyquist frequency (which is defined as \\(1/2\\) sampling frequency). </p> <p>Intuitively, consider a wave of frequency \\(t_0\\), then we need at least two points for each period to capture the frequency. </p>"},{"location":"cs284/sampling.html#antialiasing","title":"Antialiasing","text":"<p>On 2D images, the same theorem applies. Thus there are two solutions, increase the sampling rate (super-sampling, which means more pixels), or lower the image frequency (low-pass filter or blurring before sampling).</p>"},{"location":"cs284/sampling.html#super-sampling","title":"Super-sampling","text":"<p>Instead of outputting whether inside of outside a triangle, supersampling works by sample \\(N\\times N\\) samples per pixel, and then average over all values for the pixel. </p> <pre><code># simple sampling, naive implementation\nfor triangle in triangles:\n    for i, j in bounding_box(triangle):\n        frame_buffer[i, j] = inside(triangle, i + 0.5, j + 0.5)\n\n# super-sampling, N samples per pixel \nstride = 1 / (N ** 0.5)\nfor triangle in triangles:\n    for i, j in bounding_box(triangle):\n        idx = 0\n        for x in range(i + stride / 2, i + 1, stride):\n            for y in range(j + stride / 2, j + 1, stride):\n                sample_buffer[i, j, idx] = inside(triangle, x, y)\n                idx += 1\n        frame_buffer[i, j] = sample_buffer.sum(axis=2) / N\n</code></pre> <p>Super-sampling is equivalent to apply a box filter on larger sampled frame. </p>"},{"location":"cs284/simulation.html","title":"Physics based Animation","text":""},{"location":"cs284/simulation.html#mass-spring-system","title":"Mass spring system","text":"<p>Given a set of mass points (vertices) and springs (edges). For each spring connecting two mass points \\((\\mathbf a, \\mathbf b)\\), the stretch  force from \\(\\mathbf a\\) to \\(\\mathbf b\\) as \\(\\mathbf f_{a\\rightarrow b} = k_s(\\mathbf b - \\mathbf a)\\) where \\(k_s\\) is the stiffness of the spring. The reverse force \\(\\mathbf f_{b\\rightarrow a} = -\\mathbf f_{a\\rightarrow b} = k_s(\\mathbf a - \\mathbf b)\\).</p> <p>For non-zero length spring, let \\(l_0\\) be its length as rest pose, we can expand the equation to </p>"},{"location":"cs284/simulation.html#potential-force","title":"Potential force","text":"\\[\\mathbf f_{a\\rightarrow b} = k_s \\frac{\\mathbf b -\\mathbf a}{\\|\\mathbf b -\\mathbf a\\|}(\\|\\mathbf b -\\mathbf a\\| - l_0)\\] <p>The potential energy of the spring is the integral over the displacement length \\(x = (\\|\\mathbf b -\\mathbf a\\| - l_0)\\) and the potential energy is </p> \\[V = k_s (\\|\\mathbf b - \\mathbf a\\| - l_0)^2\\] <p>Note that \\(k_s\\) defines the \"stiffness\" of a spring, while the value of \\(k_s\\) has different effect on springs of different resolution. Therefore, we need to normalize each \\(k_s\\) by spring length, so that the stiffness is defined on the change in length as a fraction of the original length.</p>"},{"location":"cs284/simulation.html#damping-force","title":"Damping force","text":"<p>Behaves like viscous drag on motion, slow down motion in the direction of motion. However, we should only drag on change in spring length so that the damping force is only on internal, spring-driven motion instead of the whole system. </p> \\[\\mathbf f_a = -k_d \\frac{\\mathbf b - \\mathbf a}{\\|\\mathbf b - \\mathbf a\\|}(\\dot{\\mathbf b} - \\dot{\\mathbf a}) \\cdot \\frac{\\mathbf b - \\mathbf a}{\\|\\mathbf b - \\mathbf a\\|}\\]"},{"location":"cs284/simulation.html#standard-form","title":"Standard Form","text":"<p>The standard form of motion describes the external forces as </p> \\[\\begin{align*} &amp;\\mathbf f = &amp;\\mathcal K(\\mathbf x) &amp;+\\mathcal D(\\mathbf x, \\dot{\\mathbf x}) &amp;+\\mathcal M(\\mathbf x, \\dot{\\mathbf x}, \\ddot{\\mathbf x})\\\\ &amp;\\text{external force} = &amp;\\text{internal elasticity} &amp;+ \\text{damping} &amp;+\\text{momentum} \\end{align*}\\] <p>where \\(\\mathcal{K}, \\mathcal{D}, \\mathcal{M}\\) can be linearized as matrices. Also, zero-length springs can result in constant \\(K, D\\) and typically we use a pre-computed constant \\(M\\) and keep it diagonal by lumping. </p>"},{"location":"cs284/simulation.html#eulers-method-forward-euler","title":"Euler's Method (Forward Euler)","text":"\\[\\begin{align*} \\mathbf x^{t+dt} &amp;= \\mathbf x^t + dt \\dot{\\mathbf x}^t\\\\ \\dot{\\mathbf x}^{t+dt} &amp;= \\dot{\\mathbf x}^t + dt \\ddot{\\mathbf x}^t \\end{align*}\\] <p>Forward Euler is simple but only first order accurate, and often goes unstable. In a mass spring system, the velocity is always estimated from the last timestamp. Thus, energy is always increasing exponentially. </p>"},{"location":"cs284/simulation.html#modified-euler","title":"Modified Euler","text":"<p>To prevent the error growing too quick, average velocity at start and end of step. </p> \\[\\begin{align*} \\dot{\\mathbf x}^{t+dt} &amp;= \\dot{\\mathbf x}^t + dt \\ddot{\\mathbf x}^t\\\\ \\mathbf x^{t+dt} &amp;= \\mathbf x^t + \\frac{dt}{2} (\\dot{\\mathbf x}^t + \\dot{\\mathbf x}^{t+dt})\\\\ &amp;= \\mathbf x^t  + dt \\dot{\\mathbf x}^t + \\frac{(dt)^2}{2}\\ddot{\\mathbf x}^t \\end{align*}\\] <p>This accumulates less error and is OK for stiff systems, while still instable when \\(k_s\\) is large. </p>"},{"location":"cs284/simulation.html#implicit-euler-backward-euler","title":"Implicit Euler (Backward Euler)","text":"<p>Instead of computing the force by </p> \\[\\mathbf f^t = \\mathcal K(\\mathbf x^t) +\\mathcal D(\\mathbf x^t, \\dot{\\mathbf x}^t) +M\\ddot{\\mathbf x}\\] <p>using the future derivatives for the current step as </p> \\[\\mathbf f^t = \\mathcal K(\\mathbf x^{t+dt}) +\\mathcal D(\\mathbf x^{t+dt}, \\dot{\\mathbf x}^{t+dt}) +M\\ddot{\\mathbf x}\\] <p>To be able to solve the unknown, we the system is linear so that </p> \\[\\begin{align*} \\mathbf f^t &amp;=  K(\\mathbf x^t + \\Delta \\mathbf x) + D(\\dot{\\mathbf x}^t + \\Delta \\dot{\\mathbf x}) +M\\ddot{\\mathbf x}\\\\             &amp;=  K(\\mathbf x^t +  dt\\dot{\\mathbf x}^{t+dt}) + D(\\dot{\\mathbf x}^t + dt \\ddot{\\mathbf x}) +M\\ddot{\\mathbf x} \\end{align*}\\] <p>Implicit Euler can be made unconditionally stable, while we need to solve a nonlinear problem for the unknown acceleration, which often involves optimization based solver, such as Newton's method. Therefore, implicit Euler takes longer than explicit method. </p>"},{"location":"cs284/simulation.html#position-based-verlet-integration","title":"Position-based / Verlet Integration","text":"<p>Note that forward Euler is unstable because error is growing exponentially. Masses will move increasingly faster and eventually diverge. Therefore, the idea is to constrain mass positions after each forward step so that it cannot diverge. The constraints will dissipate the increased energy hence stabilize the system. </p> <p>Generally, the constraints are expressed as \"stiffness\" of the springs. For example, limit the spring max lengths by </p> \\[\\forall \\mathbf a,\\mathbf b \\in V. \\|\\mathbf a - \\mathbf b\\| \\leq l_{\\max}\\] <p>At each update, do the forward step as normally, then enforce the stiff constraints by adjusting the velocities. </p>"},{"location":"cs284/splines.html","title":"Splines","text":"<p>Given a set of control points, we are interested in finding a \"smooth\" interpolation (note that this can be \\(C^k\\) smooth, not necessarily mathematically smooth \\(C^\\infty\\)) over the control points. In general, we look at splines, or piecewise polynomials functions because they are easy to construct and evaluate. </p>"},{"location":"cs284/splines.html#cubic-hermite-interpolation","title":"Cubic Hermite Interpolation","text":"<p>Given the endpoints \\(P(0), P(1)\\) and its derivative \\(P'(0), P'(1)\\), we can interpolate a spline over the interval \\([0, 1]\\) using a cubic polynomial. </p>"},{"location":"cs284/splines.html#polynomial-basis-functions","title":"Polynomial basis functions","text":"<p>A polynomial function is defined as </p> \\[x(u) = \\sum_{i=0}^d c_i u^i = C\\cdot \\mathcal P^d\\] <p>where \\(C = [c_0, c_1, c_2, \\cdots c_d], \\mathcal P^d(u) = [1, u, u^2, \\cdots, u^d]\\) and the elements of \\(P^d\\) are linearly independent \\(\\forall k. u^k \\neq \\sum_{i\\neq k} c_i u^i\\).</p>"},{"location":"cs284/splines.html#polynomial-fitting","title":"Polynomial Fitting","text":"<p>Construct a cubic polynomial and its derivative</p> \\[x(u) = c_0 + c_1u + c_2 u^2 + c_3 u^3, x'(u) = c_1 + 2c_2 u + 3c_3 u^2\\] <p>Then, given the endpoints and derivative at end points</p> \\[\\begin{align*} P(0) &amp;= c_0\\\\ P(1) &amp;= c_0 + c_1 + c_2 + c_3\\\\ P'(0) &amp;= c_1\\\\ P'(1) &amp;= c_1 + 2c_2 + 3 c_3 \\end{align*}\\] <p>This system of equations can be written as \\(\\begin{bmatrix}1&amp;0&amp;0&amp;0\\\\1&amp;1&amp;1&amp;1\\\\0&amp;1&amp;0&amp;0\\\\0&amp;1&amp;2&amp;3\\end{bmatrix} \\begin{bmatrix}c_0\\\\c_1\\\\c_2\\\\c_3\\end{bmatrix} = \\begin{bmatrix}P(0)\\\\P(1)\\\\P'(0)\\\\P'(1)\\end{bmatrix}\\), or conveniently through a multiplication of the inverse matrix </p> \\[C = \\begin{bmatrix}c_0\\\\c_1\\\\c_2\\\\c_3\\end{bmatrix} = \\begin{bmatrix}1&amp;0&amp;0&amp;0\\\\0&amp;0&amp;1&amp;0\\\\-3&amp;3&amp;-2&amp;-1\\\\2&amp;-2&amp;1&amp;1\\end{bmatrix} \\begin{bmatrix}P(0)\\\\P(1)\\\\P'(0)\\\\P'(1)\\end{bmatrix} = \\beta_{H} \\cdot \\mathbf h\\] <p>\\(\\beta_H\\) is called the Hermite basis matrix. </p>"},{"location":"cs284/splines.html#catmull-rom-interpolation","title":"Catmull-Rom Interpolation","text":"<p>Now, consider multiple control points (without knowing the derivative). Note that each points introduces one equations and we could use high-order polynomials to fit. However, high-order polynomials require more computations and is not stable. Instead, for each control point, we aim to match slop between its previous and next value. </p> <p>Consider 4 points \\((u_0, x(u_0)), (u_1, x(u_1)), (u_2, x(u_2)), (u_3, x(u_3))\\), set the slope at \\(x(u_i)\\) as \\(d_ux|_{u_i} = \\frac{1}{2}{x(u_{i+1}) - x(u_{i-1})}\\) and we can use cubic Hermite interpolation to compute the curve between each pair of neighboring points. In addition, the overall curve is \\(C^1\\) continuous. For each point, its derivative is defined and it's continuous from both left and right. </p>"},{"location":"cs284/splines.html#matrix-form","title":"Matrix Form","text":"<p>Note that for every parameterized curve \\((u, \\mathbf x(u))\\), whether \\(\\mathbf x\\) is 1D, 2D, or 3D, we have the general schema </p> \\[\\mathbf x(u) = \\mathcal P(u)^T \\beta_H \\cdot \\mathbf h\\] <p>where \\(\\mathcal P(u)\\) is the polynomial basis, \\(\\beta_H\\) is the Hermite basis matrix, and \\(\\mathbf h\\) are the control points values. In this case, </p> \\[\\mathbf h = \\begin{bmatrix}\\mathbf x_i\\\\\\mathbf x_{i+1}\\\\\\frac{1}{2}(\\mathbf x_{i+1} - \\mathbf x_{i-1})\\\\\\frac{1}{2}(\\mathbf x_{i+2} - \\mathbf x_{i})\\end{bmatrix} = \\begin{bmatrix} 0&amp;1&amp;0&amp;0\\\\0&amp;0&amp;1&amp;0\\\\-\\frac{1}{2}&amp;0&amp;\\frac{1}{2}&amp;0\\\\0&amp;-\\frac{1}{2}&amp;0&amp;\\frac{1}{2}\\end{bmatrix}\\begin{bmatrix}\\mathbf x_{i-1}\\\\\\mathbf x_{i}\\\\\\mathbf x_{i+1}\\\\\\mathbf x_{i+2}\\end{bmatrix} = M_{CR\\rightarrow H} \\cdot\\mathbf p\\] <p>so that we can write everything in full</p> \\[\\mathbf x(u) = \\mathcal P(u)^T \\beta_H\\cdot M_{CR\\rightarrow H}\\cdot\\mathbf p\\] <p>Then, note that \\(\\beta_H\\cdot M_{CR\\rightarrow H}\\) are both pre-defined and we can combine them into one Catmull-Rom basis matrix as</p> \\[\\beta_{CR} = \\beta_H\\cdot M_{CR\\rightarrow H} = \\begin{bmatrix}0&amp;1&amp;0&amp;0\\\\-\\frac{1}{2}&amp;0&amp;\\frac{1}{2}&amp;0\\\\1&amp;-\\frac{5}{2}&amp;2&amp;-\\frac{1}{2}\\\\-\\frac{1}{2}&amp;\\frac{3}{2}&amp;-\\frac{3}{2}&amp;\\frac{1}{2}\\end{bmatrix}\\]"},{"location":"cs284/splines.html#bezier-curves","title":"Bezier Curves","text":"<p>Instead of finding a curve passing through 4 points, now we let \\(\\mathbf x'(u_0) = 3(\\mathbf p_1 - \\mathbf p_0)\\) be the tangent for \\(\\mathbf p_0\\) and \\(\\mathbf x'(u_3) = 3(\\mathbf p_3 - \\mathbf p_2)\\) be the tangent of \\(\\mathbf p_3\\), and Bezier curve pass through \\(\\mathbf p_0\\) and \\(\\mathbf p_3\\) with slope specified before. Now, we can use cubic Hermite directly as </p> \\[\\mathbf x(u) = \\mathcal P(u)\\cdot\\beta_H \\cdot \\mathbf h\\] <p>and similarly we have </p> \\[\\mathbf h = \\begin{bmatrix}\\mathbf p_0\\\\\\mathbf p_3\\\\3(\\mathbf p_1 - \\mathbf p_2)\\\\3(\\mathbf p_3 - \\mathbf p_2)\\end{bmatrix} = \\begin{bmatrix} 1&amp;0&amp;0&amp;0\\\\1&amp;0&amp;0&amp;1\\\\-3&amp;3&amp;0&amp;0\\\\0&amp;0&amp;-3&amp;3\\end{bmatrix} \\cdot\\mathbf p = M_{Z\\rightarrow H}\\cdot \\mathbf p\\] <p>and the cubic Bezier matrix as </p> \\[\\beta_Z = \\begin{bmatrix}1&amp;0&amp;0&amp;0\\\\-3&amp;3&amp;0&amp;0\\\\3&amp;-6&amp;3&amp;0\\\\-1&amp;3&amp;-3&amp;1\\end{bmatrix}\\]"},{"location":"cs284/splines.html#convex-hull-property","title":"Convex hull property","text":"<p>Convex hull property means that all points on curve are inside the convex hull formed by control points. A spline has convex hull property if for any \\(u\\), the basis functions are all non-negative and sum to \\(1\\). </p> <p>For example, the Bezier basis functions are </p> \\[\\mathcal P(u)\\cdot \\beta_Z = \\begin{bmatrix} &amp;1 &amp;-3u  &amp;+3u^2  &amp;-u^3\\\\ &amp;  &amp;+3u  &amp;-6u^2  &amp;+3u^3\\\\ &amp;  &amp; &amp;+3u^2 &amp;-3u^3\\\\ &amp;&amp;&amp;&amp;+u^3 \\end{bmatrix}^T\\]"},{"location":"cs284/splines.html#implementing-splines","title":"Implementing splines","text":"<pre><code>basis_catmull_rom = np.array([\n    [0,      1,   0,   0],\n    [-0.5,   0, 0.5,   0],\n    [1,   -2.5,   2,-0.5],\n    [-0.5, 1.5,-1.5, 0.5]\n])\n\nbasis_cubic_bezier = np.array([\n    [1, 0, 0,  0],\n    [-3, 3, 0, 0],\n    [3, -6, 3, 0],\n    [-1, 3, -3, 1]\n])\n\ndef spline(p, basis, n=20):\n    u = np.linspace(0, 1, n)\n    P = np.vstack([\n        np.ones(n),\n        u,\n        u * u,\n        u * u * u\n    ])\n    return P.T @ basis @ p\n</code></pre>"},{"location":"cs284/splines.html#de-casteljau-algorithm","title":"de Casteljau Algorithm","text":"<p>Consider control points \\(\\mathbf p^0_0, \\mathbf p^0_1, \\mathbf p^0_2, \\mathbf p^0_3\\) and \\(u\\in[0, 1]\\). Define </p> \\[\\mathbf p_i^j = u \\mathbf p_i^{j-1} + (1-u)\\mathbf p_{i+1}^{j-1}\\] <p>We can recursively compute the equation 3 times until one point left and let \\(\\mathbf x(t) = \\mathbf p^3_0\\). Then, it is the value for Bezier curve. </p> \\[\\begin{align*} \\mathbf p_0^3 &amp;= u\\mathbf p_0^2 + (1-u)\\mathbf p_1^2\\\\ &amp;= u(u\\mathbf p_0^1 + (1-u)\\mathbf p_1^1) + (1-u)(u\\mathbf p_1^1 + (1-u)\\mathbf p_2^1)\\\\ &amp;= u^2 \\mathbf p_0^1 + 2u(1-u)\\mathbf p_1^1 + (1-u)^2 \\mathbf p_2^1\\\\ &amp;= u^2(u\\mathbf p_0^0 + (1-u)\\mathbf p_1^0) + 2u(1-u)(u\\mathbf p_1^0 + (1-u)\\mathbf p_2^0) + (1-u)^2(u\\mathbf p_2^0 + (1-u)\\mathbf p_3^0)\\\\ &amp;= u^3 \\mathbf p_0^0 + 3u^2(1-u)\\mathbf p_1^0 + 3u(1-u)^2 p_2^0 + u^3 p3^0 \\end{align*}\\] <p>Note that this can be extended to \\(n\\)-order polynomials as </p> \\[B_i^n(u) = {n\\choose i}u^i (1-u)^{n-i}\\]"},{"location":"cs284/splines.html#bezier-surfaces","title":"Bezier Surfaces","text":"<p>A surface is defined by some function \\(\\mathbf x(u, v)\\). Therefore, we need \\(4\\times 4\\) control points and then for each \\((u,v)\\) we can interpolate per-row Bezier curves in \\(u\\), and then per-column Bezier curve in \\(v\\). </p> <p>We can write the surface in matrix form as</p> \\[\\mathbf x(u,v) = \\mathcal P(u) \\cdot \\beta_Z \\cdot \\mathbf P \\cdot (\\mathcal P(v)\\cdot \\beta_Z)^T\\] <p>where \\(P\\) is the \\(4 \\times (4 \\times 3)\\) matrix of all control points. </p>"},{"location":"cs284/texture.html","title":"Texture Mapping","text":""},{"location":"cs284/texture.html#barycentric-coordinates","title":"Barycentric Coordinates","text":"<p>Barycentric coordinate defines a linear interpolation over a triangle. Given corners \\(A, B, C\\), the barycentric coordinates \\((a, \\beta, \\gamma)\\) defines any point within the triangle as </p> \\[aA + \\beta B + \\gamma C = (x,y), a,\\beta,\\gamma &gt; 0, a + \\beta + \\gamma = 1\\] <p>Geometrically, \\(\\alpha, \\beta, \\gamma\\) is the proportional projective distance of \\((x, y)\\) to the corners \\(A, B, C\\). </p> <p></p> <p>Consider the orthogonal project from \\(P = (x, y)\\) to line \\(\\overline{AB}\\), the distance of the projection is given by </p> \\[d_{AB}(P) = \\frac{(A-B)\\times (P - B)}{\\|B-A\\|} = \\frac{|(x_B - x_A)(y_A - y) - (x_A - x)(y_B - y_A)|}{\\sqrt{(x_A - x_B)^2 + (y_A - y_B)^2}}\\] <p>Then, note that this projected normal lines on the projection normal from \\(C\\) to \\(\\overline{AB}\\), and the proportional distance is </p> \\[\\gamma = \\frac{d_{AB}(P)}{d_{AB}(C)} = \\frac{(x_B - x_A)(y_A - y) - (x_A - x)(y_B - y_A)}{(x_B - x_A)(y_A - y_C) - (x_A - x_C)(y_B - y_A)}\\] <p>Similarly we can obtain \\(\\alpha, \\beta\\), and note that if we already know 2 coefs, we can get the third from \\(a+\\beta+\\gamma=1\\). </p> <p>Note that we can re-write the equation into homogeneous coordinates as </p> \\[\\begin{pmatrix}x\\\\y\\\\1\\end{pmatrix} = \\begin{bmatrix} x_A&amp;x_B&amp;x_C\\\\y_A&amp;y_B&amp;y_C\\\\1&amp;1&amp;1 \\end{bmatrix}\\begin{pmatrix}\\alpha\\\\\\beta\\\\\\gamma\\end{pmatrix} = M_{t}\\begin{pmatrix}\\alpha\\\\\\beta\\\\\\gamma\\end{pmatrix}\\] <p>where \\(x, y\\) is the coordinates in the texture space. At the same time, the same point in the triangle exists in the screen space and we have that \\([u, v, 1]^T = M_{s}[\\alpha, \\beta, \\gamma]^T\\). Therefore, we have </p> \\[\\begin{bmatrix}u\\\\v\\\\1\\end{bmatrix} = M_s M_t^{-1} \\begin{bmatrix}x\\\\y\\\\1\\end{bmatrix}\\] <p>\\(M_s M_t^{-1}\\) is the transformation from texture space to the screen space. </p>"},{"location":"cs284/texture.html#texture-mapping_1","title":"Texture Mapping","text":"<p>Note that texture is an image, when transfer from the screen space \\((x, y)\\) to the texture space \\((u,v)\\), \\((u,v)\\) doesn't necessarily fall into the pixel coordinate. Instead, consider the texture as a continuous function and we are sampling from it. </p> <p>The problem of texture mapping is that the perspective transformation brings depth distortion. Considering the vanishing line effect, one pixel away in the screen can be multiple pixels in the texture. Which means the sampling rate of the texture can be much higher than the screen's sampling rate</p> \\[\\Psi(x, y) = (u, v), \\Psi(x+1, y) = (u + \\partial_x u, v + \\partial_x v), \\Psi(x, y+1) = (u + \\partial_y u, v + \\partial_y v)\\]"},{"location":"cs284/texture.html#texture-magnification","title":"Texture magnification","text":"<p>When the texture sampling rate is lower than screen, we need to interpolate \\((u,v)\\) from neighboring pixels. This is the same as image resizing. The most commonly used is bilinear, i.e. a linear weighted average of the closest 4 pixels. It can be done in 3 1D linear interpolations. Let \\(u_0 = \\lfloor u \\rfloor, v_0 =  \\lfloor v \\rfloor, u_1 =  \\lceil u \\rceil, v_1 = \\lceil v \\rceil, s = u - u_0, t = v - v_0\\). Then we have </p> \\[\\begin{align*} a_0 &amp;= (1-s)I(u_0, v_0) + sI(u_1, v_0)\\\\ a_1 &amp;= (1-s)I(u_0, v_1) + sI(u_1, v_1)\\\\ I(u,v) &amp;\\approx (1-t)a_0 + ta_1 \\end{align*}\\] binear interpolation<pre><code>vec3 mix2(float u, float v, Texture tex)\n{\n\n  float u0 = floor(u); float u1 = ceil(u);\n  float v0 = floor(v); float v1 = ceil(v);\n  float s = u - u0;\n  float t = v - v0;\n  // mix(a, b, c) = (1 - a) * b + a * c\n  return mix(t,\n    mix(s, tex(u0, v0), tex(u1, v0)),\n    mix(s, tex(u0, v1), tex(u1, v1)),\n  );\n}\n</code></pre>"},{"location":"cs284/texture.html#mipmap-texture-minification","title":"Mipmap Texture minification","text":"<p>From sampling theory we know that per-blurring is a good antialiasing strategy. However, the texture sampling rate varies within the same screen space due to perspective views. Therefore, we need different sampling rate for textures mapped to different places. </p> <p>Mipmap's idea is to store different levels of filtered images, and we determine which level to use by a given UV rate. For each level, we downsample the texture to \\(H/2, W/2\\). Therefore, the total memory requirements is at most \\(\\sum_0^\\infty \\frac{1}{4^i} = 4/3\\). </p> <p>To compute which level of mipmap to use at \\((u,v)\\), note that we have </p> \\[\\Psi(x, y) = (u, v), \\Psi(x+1, y) = (u + \\partial_x u, v + \\partial_x v), \\Psi(x, y+1) = (u + \\partial_y u, v + \\partial_y v)\\] <p>Basically, we can estimate the footprint by estimating the Jacobian and compute the level \\(D\\) </p> \\[\\begin{align*} \\partial_x u &amp;= u(x+1, y) - u(x, y), \\partial_y u = u(x, y+1) - u(x, y)\\\\ \\partial_x v &amp;= v(x+1, y) - v(x, y), \\partial_y v = v(x, y+1) - v(x, y)\\\\ L &amp;= \\max(\\|\\partial_xu + \\partial_xv\\|, \\|\\partial_yu + \\partial_yv\\|)\\\\ D &amp;= \\log_2 L \\end{align*}\\] <p>Then, we can use bilinear interpolation to sample from level \\(\\text{round(D)}\\), or average from level \\(\\lfloor D\\rfloor, \\lceil D\\rceil\\) (Trilinear).</p>"},{"location":"cs284/texture.html#visibility","title":"Visibility","text":"<p>When we move from 2D to 3D, one issue to consider is the visibility, or depth. In the simplest case, we can sort each object (triangles essentially) from far to near, and then paint from back to front and at each time overwrite the frame buffer (painter's algorithm). The problem is that object-based depth order does not always exist. </p> <p> </p> Examples of painter's algorithm"},{"location":"cs284/texture.html#z-buffer","title":"Z-buffer","text":"<p>Store current minimum \\(z\\)-value for each sample position in an additional buffer (z-buffer), which is often a float/double array. </p> z-buffer<pre><code>for tri in triangles:\n  for x, y, z in tri.samples():\n    if z &lt; zbuffer[x, y]:\n      framebuffer[x, y] = tri.color(x, y, z)\n      zbuffer[x, y] = z\n</code></pre> <p>The running time for z-buffer based rasterization is linear to the number of triangles, and we can accelerate it by hierarchical data structure. Also, it can be well paralleled over samples. </p>"},{"location":"cs284/texture.html#alpha-buffer-for-transparency","title":"Alpha-buffer for transparency","text":"<p>Z-buffer cannot well handle transparency because alpha-blending requires correct ordering. </p> \\[aC_{\\text{front}} + (1-a)C_{\\text{back}} \\neq (1-a)C_{\\text{front}} + aC_{\\text{back}}\\] <p>One solution is to draw things in two passes, i.e. draw opaque things using z-buffer first, and then color the partially transparent objects. Another solution is to use \\(\\alpha\\)-buffer instead of z-buffer, i.e. a linked list of RGB-z-\\(\\alpha\\) at each pixel, and finally draw everything from front to end (stops at the first opaque pixel).</p>"},{"location":"cs284/transforms.html","title":"3D Transformations","text":""},{"location":"cs284/transforms.html#properties-of-transformations","title":"Properties of Transformations","text":"<p>Transformations such as translation, rotation, and scaling are represented as matrices using homogeneous coordinates. </p> <p>Applying transformation is equivalent to left multiply the matrix. </p> <p>Since matrix multiplication is a linear transformation. Applying a sequence of transformation is equivalent to left multiply them in the transformation order, hence transformations are associative and transformations can be composed. </p> \\[T_n T_{n-1} ... T_{1}x = (T_n T_{n-1} ... T_{1})x\\] <p>Multiply its inverse will reverse the transformation (since \\(T^{-1}Tx = (T^{-1}T)\\)). </p> <p>Transformation order matters, since matrix multiplications are not communitive. </p>"},{"location":"cs284/transforms.html#common-2d-transformations","title":"Common 2D Transformations","text":"<p>\\(T\\) translation, \\(R\\) rotation counterclockwise, \\(S\\) scale / reflection (when \\(s_x=-1\\) or \\(s_y = -1\\))</p> \\[T(x, y) = \\begin{bmatrix} 1&amp;0&amp;x\\\\0&amp;1&amp;y\\\\0&amp;0&amp;1 \\end{bmatrix}, R(\\theta) = \\begin{bmatrix} \\cos\\theta&amp;-\\sin\\theta&amp;0\\\\\\sin\\theta&amp;\\cos\\theta&amp;0\\\\0&amp;0&amp;1 \\end{bmatrix}, S(s_x, s_y) = \\begin{bmatrix} s_x&amp;0&amp;0\\\\0&amp;s_y&amp;0\\\\0&amp;0&amp;1 \\end{bmatrix}\\]"},{"location":"cs284/transforms.html#decomposing-transformations","title":"Decomposing Transformations","text":"<p>Since multiple transformations can be composed into one transformation, we can then decompose the matrix. </p> <ul> <li>Eigen decomposition: \\(A = V \\Lambda V^{-1}\\) where \\(\\Lambda\\) is diagonal. Used to decompose scaling (as \\(\\Lambda\\)).</li> <li>Singular value decomposition: \\(A = USV^T\\) where \\(S\\) is diagonal, \\(U, V\\) are orthonormal. Used to decompose rotation (as \\(UV^T\\)) and scaling (as \\(S\\)).</li> <li>Polar decomposition \\(A = PRSR^T, PR = Q\\) is a further decomposition of SVD. \\(P, R\\) are orthonormal. Used to decompose rotation (as \\(P\\) and scaling as \\(S\\)). </li> </ul>"},{"location":"cs284/transforms.html#3d-affine-transformations","title":"3D Affine Transformations","text":"<p>An 3D affine transformation is of the form </p> \\[A = \\begin{bmatrix}a&amp;b&amp;c&amp;t_x\\\\e&amp;f&amp;g&amp;t_y\\\\h&amp;i&amp;j&amp;t_z\\\\0&amp;0&amp;0&amp;1\\end{bmatrix}\\] <p>translation is of the form </p> \\[T = \\begin{bmatrix}1&amp;0&amp;0&amp;t_x\\\\0&amp;1&amp;0&amp;t_y\\\\0&amp;0&amp;1&amp;t_z\\\\0&amp;0&amp;0&amp;1\\end{bmatrix}\\] <p>rotation is of the form </p> \\[R =\\begin{bmatrix}\\mathbf R&amp;\\mathbf 0\\\\\\mathbf 0&amp;1\\end{bmatrix}, \\mathbf R\\in SO(3)\\] <p>scaling is of the form </p> \\[S = \\begin{bmatrix}s_x&amp;0&amp;0&amp;0\\\\0&amp;s_y&amp;0&amp;0\\\\0&amp;0&amp;s_z&amp;0\\\\0&amp;0&amp;0&amp;1\\end{bmatrix}\\] <p>3D points is represented as \\([x, y, z, 1]\\) and vectors as \\([x, y, z, 0]\\).</p> <p>For reverse transformation (inverse matrices). </p>"},{"location":"cs284/transforms.html#coordinate-systems","title":"Coordinate Systems","text":"<p>In general, a 3D coordinate system is defined by three normalized vectors \\(\\mathbf u, \\mathbf v, \\mathbf w\\) and one origin point \\(\\mathbf o\\). Any transformation can be understood as transform a point/vector relative to the coordinate system. Alternatively, a transformation can be seen as a reverse transformation of the coordinate system relative to the point. </p> <p>The frame to world transformation is written as \\(\\begin{bmatrix}\\mathbf u &amp; \\mathbf v &amp; \\mathbf w &amp; \\mathbf o\\\\0&amp;0&amp;0&amp;1\\end{bmatrix}\\) which transforms objects from the world coordinate system \\([\\mathbf{e}_1, \\mathbf{e}_2, \\mathbf{e}_3, \\mathbf 0]\\) to the defined coordinate system.</p>"},{"location":"cs284/transforms.html#representations-of-3d-rotations","title":"Representations of 3D Rotations","text":"<p>Geometrically, a 3D rotation rotates along a 3D-axis \\((x, y, z)\\) by \\(\\theta\\) degrees. </p>"},{"location":"cs284/transforms.html#euler-angle-rotation","title":"Euler Angle Rotation","text":"<p>Rotation around standard axes are </p> \\[R_x(a) = \\begin{bmatrix} 1&amp;0&amp;0&amp;0\\\\ 0&amp;\\cos a&amp;-\\sin a &amp;0\\\\ 0&amp;\\sin a &amp;\\cos a&amp;0\\\\ 0&amp;0&amp;0&amp;1 \\end{bmatrix}, R_y(a) = \\begin{bmatrix} \\cos a&amp;0&amp;\\sin a&amp;0\\\\ 0&amp;1&amp;0&amp;0\\\\ \\sin a&amp;0 &amp;\\cos a&amp;0\\\\ 0&amp;0&amp;0&amp;1 \\end{bmatrix}, R_z(a) = \\begin{bmatrix} \\cos a&amp;-\\sin a&amp;0&amp;0\\\\ \\sin a&amp;\\cos a&amp;0 &amp;0\\\\ 0&amp;0&amp;1&amp;0\\\\ 0&amp;0&amp;0&amp;1 \\end{bmatrix}\\] <p>Then, any rotation can be represented by \\(R = R_z(c)R_y(b)R_x(a)\\), which is called Euler angle. The rotation order matters. </p> <p>Euler angle is not a good representation because the same rotation cannot be uniquely defined. Also, there's gimbal lock problem, in which we lose degree of free when two rotation axis align. Also, the manifold is very ugly. </p>"},{"location":"cs284/transforms.html#rotation-around-an-axis","title":"Rotation Around an Axis","text":"<p>Suppose we are rotating around \\(\\mathbf w\\). The first problem is that how we define the degree of rotation \\(\\theta\\), or what is \\(R_{\\mathbf w}(0)\\). </p> <p>The idea is to have \\(\\mathbf w\\) aligned to \\(\\mathbf x\\) and we can do \\(R_x\\). Specifically, </p> <ol> <li>Rotate about \\(\\mathbf x\\)-axis to put \\(\\mathbf w\\) into xy plane. </li> <li>Rotate about \\(\\mathbf z\\) to align \\(\\mathbf w\\) with \\(\\mathbf x\\).</li> <li>Apply \\(R_x(\\theta)\\).</li> <li>Reverse step 2 and 1 by matrix inverse (both are rotation so transpose).</li> </ol>"},{"location":"cs284/transforms.html#rodrigues-rotation-formula","title":"Rodrigues' Rotation Formula","text":"<p>The rotation around axis \\(\\mathbf n\\) by angle \\(\\theta\\) can be written into \\(3\\times 3\\) rotation matrix as</p> \\[\\mathbf R(\\mathbf n, \\theta) = \\cos \\theta \\mathbf I + (1-\\cos\\theta)(\\mathbf n\\cdot \\mathbf n) + \\sin \\theta [\\mathbf n]_{\\times}\\] <p>\\([\\mathbf n]_{\\times}\\) is the cross product matrix. </p> <p>We can show that </p> <ul> <li>\\(\\mathbf R \\cdot \\mathbf n = \\mathbf n\\)</li> <li>\\(\\mathbf R e_1 = \\cos \\theta e_1 + \\sin\\theta e_2\\)</li> <li>\\(\\mathbf R e_2 = -\\sin\\theta e_1 + \\cos\\theta e_2\\)</li> </ul>"},{"location":"cs284/transforms.html#exponential-map","title":"Exponential Map","text":"<p>\"Exponential\" because of the matrix exponential form from Rodrigues' rotation formula. More on rotation time derivatives</p> <p>Note that we can combine \\(\\theta\\) into the rotation axis as the scale \\(|\\mathbf r\\| = \\theta\\) so that the rotation can be compactly written as a 3D vector. Thus, it can be a \"map\". </p>"},{"location":"cs284/transforms.html#quaternions","title":"Quaternions","text":"<p>A more popular way derived from Rodrigues' is the quaternion, represented in 4 numbers. </p> \\[\\mathbf q = (z_1, z_2, z_3, s) = iz_1 + jz_2 + kz_3 + s = (\\mathbf z, s)\\] <p>where \\(i,j,k\\) are complex variables \\(i^2 = j^2 = k^2 = -1\\) and \\(ij=k, jk=i, ki=j,ji=-k,kj=-i,ik=-j\\)</p> <p>Following regular complex arithmetic, we define </p> <ul> <li>dot product </li> </ul> \\[\\mathbf q\\cdot \\mathbf p = (\\mathbf z_q s_p + \\mathbf z_p s_q + \\mathbf z_p \\times \\mathbf z_q, s_ps_q - \\mathbf z_p \\cdot\\mathbf z_q)\\] <ul> <li>conjugate: \\(\\mathbf q^* = (-\\mathbf z, s)\\)</li> <li>magnitude: \\(\\|\\mathbf q\\|^2 = \\mathbf q\\cdot \\mathbf q^* = \\mathbf z\\cdot \\mathbf z + s^2\\)</li> <li>any real number vectors \\(\\mathbf v_q = (\\mathbf v, 0)\\)</li> <li>rotation as quaternions \\(\\mathbf r =(\\mathbf w\\sin\\frac{\\theta}{2}, \\cos\\frac{\\theta}{2})\\)</li> <li>rotating a vector \\(r^*\\cdot \\mathbf v_q\\cdot \\mathbf r^*\\)</li> <li>composing rotations \\(\\mathbf r = \\mathbf r_1\\cdot\\mathbf r_2\\)</li> </ul> <p>The reason behind quaternions is the great properties, it is \"double unique\" (\\((\\mathbf r, \\theta) = (-\\mathbf r, -\\theta)\\)), the surface is a 3-sphere in 4D and its nice for interpolation. </p> <p>Note that these good properties are shared by exponential map, but compositing quaternions is easier. </p>"},{"location":"cs284/transforms.html#camera-model","title":"Camera Model","text":"<p>We define camera as a coordinate system located at origin, the up direction is \\(+y\\)-axis, view direction is \\(-z\\) axis, and right is \\(+x\\)-axis. Note that this definition is just by convention, the camera coordinates are defined differently depending on implementations. </p>"},{"location":"cs284/transforms.html#look-at-transformation-c2w-and-w2c","title":"Look at Transformation (c2w and w2c)","text":"<p>Like any 3D object, a camera are positioned into the world space by a rigid transformation </p> \\[K = \\begin{bmatrix}\\mathbf R&amp;\\mathbf t\\\\\\mathbf 0&amp;1\\end{bmatrix} = \\begin{bmatrix} u_x&amp;r_x&amp;-v_x&amp;e_x\\\\ u_y&amp;r_y&amp;-v_y&amp;e_y\\\\ u_z&amp;r_z&amp;-v_z&amp;e_z\\\\ 0&amp;0&amp;0&amp;1 \\end{bmatrix}\\] <p>where \\(\\mathbf R = \\mathbf u, \\mathbf r, -\\mathbf v\\) rotates the camera coordinates right, up, view from \\((\\mathbf x, \\mathbf y, -\\mathbf z)\\) to \\((\\mathbf u, \\mathbf r, -\\mathbf v)\\) and \\(\\mathbf t = \\mathbf e\\) translates camera from origin to \\(\\mathbf e\\). </p> <p>\"look at\" matrix transforms objects in the world coordinates to the camera coordinates, thus is the inverse of \\(K\\), i.e. </p> \\[K^{-1} = \\mathbf R^T(-T) = \\begin{bmatrix}\\mathbf R^T&amp;\\mathbf -R^Tt\\\\\\mathbf 0&amp;1\\end{bmatrix}\\]"},{"location":"cs284/transforms.html#perspective-projection","title":"Perspective Projection","text":"<p>Consider the simple pinhole camera model, where camera is a point and we place the image plane in front of the camera with distance \\(d\\). Every 3D point \\((x,y,z)\\) is projected onto the image plane as \\((\\frac{d}{z}x, \\frac{d}{z}y, d)\\). Which can be written in homogenous transformation </p> \\[M = \\begin{bmatrix}1&amp;0&amp;0&amp;0\\\\0&amp;1&amp;0&amp;0\\\\0&amp;0&amp;1&amp;0\\\\0&amp;0&amp;d^{-1}&amp;0\\end{bmatrix}, M\\begin{pmatrix}x\\\\y\\\\z\\\\1\\end{pmatrix} = \\begin{pmatrix}x\\\\y\\\\z\\\\\\frac{z}{d}\\end{pmatrix}\\iff \\begin{pmatrix}\\frac{d}{z}x\\\\\\frac{d}{z}y\\\\d\\\\1\\end{pmatrix}\\] <p>Then, note that the image plane cannot be infinitely large. In practice, we fix the size of the image plane and change \\(d\\), or equivalently the focal length. When \\(d\\) is small, we get a larger viewing angle. Also, we can define near \\(n\\) and far \\(f\\), which are two planes to clip the 3D space, we only render objects that falls within the range of \\([n, f]\\). </p> <p>To summarize, we have additional parameters for camera (rather camera models in graphics pipeline instead of the real camera). </p> <ul> <li>\\(\\text{FOV}_y\\) the vertical angular field of view</li> <li>\\(w/h\\) aspect ratio as width and height of the image plane / field of view</li> <li>\\(n, f\\) the depth of near clipping and far clipping plane</li> </ul> <p>From this we can derive the corners of the near clipping plane as top-left corner \\((x_0, y_0, n)\\) and bottom-right corner \\((x_1, y_1, n)\\) where </p> \\[y_0 = n \\tan(\\text{FOV}_y), y_1 = -y_0, x_1 = \\frac{wy}{h}, x_0 = -x_1\\]"},{"location":"cs284/transforms.html#normalized-device-coordinates-ndc","title":"Normalized Device Coordinates (NDC)","text":"<p>Note that we can warp the volume in between near and far to a unit box, called normalized device coordinates (NDC). This transformation will preserve the depth order and is defined as </p> \\[P = \\begin{bmatrix} n / x_1&amp;0&amp;0&amp;0\\\\0&amp;n / y_00&amp;0&amp;0\\\\0&amp;0&amp;\\frac{f+n}{f-n}&amp;\\frac{2nf}{f-n}\\\\0&amp;0&amp;-1&amp;0 \\end{bmatrix}\\]"},{"location":"cs294173/3dgaussian.html","title":"Gaussian Splatting","text":"<p>Project Page  Paper  </p> <p>Instead of representing the scene as an implicit continuous field; 3DGS [Kerbl et al. 2023]<sup>1</sup> represent it as a unstructured, discrete set of points, augmented with features (either neural features or explicit things). Then, use the differentiable renderer to backprop/optimize over the set of images.  </p> <p>Combines several ideas of recent works. Including point-based volume rendering, Explicit anisotropic Gaussian splats for real-time rasterization. Efficient initialization from SfM points (sparse recon from COLMAP instead of lengthy dense recon)</p>"},{"location":"cs294173/3dgaussian.html#contributions","title":"Contributions","text":"<ul> <li>The introduction of anisotropic 3D Gaussians as a high-quality, unstructured representation of radiance fields. </li> <li>An optimization method of 3D Gaussian properties, interleaved with adaptive density control that creates high-quality representations for captured scenes. </li> <li>A fast, differentiable rendering approach for the GPU, which is visibility-aware, allows anisotropic splatting and fast backpropagation to achieve high-quality novel view synthesis.</li> </ul>"},{"location":"cs294173/3dgaussian.html#representation","title":"Representation","text":"<p>3D Gaussians is defined by </p> \\[\\mathbf G(\\mathbf x) = \\exp(-\\frac{1}{2}(\\mathbf x-\\mathbf p)^T \\Sigma^{-1}(\\mathbf x-\\mathbf p))\\] <p>where \\(\\mathbf p\\) is the position (mean) and \\(\\Sigma\\) is the covariance matrix, both in world space. Each Gaussian is multiplied by \\(\\alpha\\) (transparency) and augmented with spherical harmonics for directional appearance component. </p> <p>Each 3G Gaussian can be projected onto the 2D image space, the mean is simply a point projection, and the covariance \\(\\Sigma\\) is projected to a 2D covariance matrix \\(\\Sigma'\\) where</p> \\[\\Sigma' = JW\\Sigma W^T J^T\\] <p>where \\(W\\) is the viewing transformation matrix, \\(J\\) is the Jacobian of the affine approximation of the projective transformation. </p> <p>However, to enforce positive semi-definite property so that the matrix keeps the properties of a covariance matrix, we cannot directly optimize on the 3D matrix space. Instead, we decompose \\(\\Sigma\\) into a scaling matrix \\(S\\) and rotation matrix \\(R\\) as </p> \\[\\Sigma = RSS^TR^T\\] <p>which is the configuration of a 3D ellipsoid, analogous to a 3D Gaussian covariance. In practice, use axis scaling vector \\(\\mathbf s\\in\\mathbb R^3\\) for scaling and (normalized) quaternion \\(\\mathbf q\\in\\mathbb{R}^4\\) to represent rotation.</p>"},{"location":"cs294173/3dgaussian.html#optimizations","title":"Optimizations","text":"<p>Add sigmoid function on \\(\\alpha\\), exponential on \\(\\mathbf s\\) to constrain the range, normalize \\(\\mathbf q\\) so that it's a unit quaternion. </p> <p>Initialize the set of isotropic Gaussian with axes equal to the mean of the distance to the closest three points. Thanks to the rapid rendering, the method can be optimized on the whole image instead of randomly sampled rays, thus we can apply image based losses during training as well. For example, D-SSIM loss. </p> <p>To populate/prune Gaussians, after some iterations  </p> <ul> <li>remove Gaussians that are essentially transparent (\\(\\alpha &lt; \\epsilon_{\\alpha}\\))</li> <li>To encourage binarized alphas and reduce floaters, cut the alpha values every \\(N=3000\\) iterations. </li> <li>If the average magnitude of view-space position gradient is larger than some threshold \\(\\tau_{\\text{pos}}\\). <ul> <li>If the Gaussian is small (decided by scaling vector), then make a clone and move along the positional gradient. </li> <li>If the Gaussian is large, then shrink the scale by \\(\\phi = 1.6\\) and make a clone and move along the positional gradient. </li> </ul> </li> </ul>"},{"location":"cs294173/3dgaussian.html#rendering","title":"Rendering","text":"<p>Instead of doing pixel-wise ordering / ray-marching, the method pre-order the Gaussian splats for each view. First, split the screen into \\(16\\times 16\\) tiles, then only keep Gaussians that's \\(99\\%\\) within the view frustum (with a set-up near plane and far plane to avoid extreme cases). Then, simply do z-ordering on the Gaussians. </p> <ol> <li> <p>Kerbl, B., Kopanas, G., Leimk\u00fchler, T., and Drettakis, G. 2023. 3D gaussian splatting for real-time radiance field rendering. ACM Transactions on Graphics 42, 4.\u00a0\u21a9</p> </li> </ol>"},{"location":"cs294173/3dgsmath.html","title":"3DGS Implementation Details","text":"<p>Based on the 3DGS paper  [Kerbl et al. 2023]<sup>1</sup>, the gsplat doc , and the math supplement  [Ye and Kanazawa 2023]<sup>2</sup></p>"},{"location":"cs294173/3dgsmath.html#representation","title":"Representation","text":"<p>Each splat is represented as a 3D Gaussian blob, parameterized as mean (position) \\(\\mu \\in \\mathbb R^3\\), covariance (isotropic shape) \\(\\Sigma\\in\\mathbb R^{3\\times 3}\\), color \\(c\\in\\mathbb R^3\\), and opacity \\(o\\in\\mathbb R\\). </p> <p>The 3D covariance \\(\\Sigma\\) can be further parameterized as per-axis scale \\(s\\in\\mathbb R^3\\) and rotation in quaternion \\(q\\in\\mathbb R^4 = (x,y,z,w)\\). The quaternion is then converted to rotation matrix \\(R\\in SO(3)\\) as </p> \\[R = \\begin{bmatrix} 1 - 2(y^2+z^2) &amp; 2(xy-wz) &amp; 2(xz+wy)\\\\ 2(xy+wz) &amp; 1-2(x^2-z^2) &amp;2(yz-wx)\\\\ 2(xz-wy) &amp; 2(yz+wx) &amp;1-2(x^2+y^2) \\end{bmatrix}\\] <p>the 3D covariance \\(\\Sigma\\) is given by </p> \\[\\Sigma = RSS^TR^T, S=diag(s)\\] <p>The rendering camera is described by the extrinsic \\(R_{c2w}, t_{c2w}\\) in the world coordinate space and intrinsic \\(f_x, f_y\\) (focal length), \\(h, w\\) (height, width), \\(f, n\\) (far and near clipping distance).</p> <p>Following the rasterization pipeline, we transform each splat into the camera space through the world to camera transformation </p> \\[T_{w2c} = T^{-1}_{c2w} = \\begin{bmatrix} R_{c2w} &amp;t_{c2w}\\\\ \\mathbf{0} &amp; 1 \\end{bmatrix}^{-1}\\] <p>and then perspective projection </p> \\[P = \\begin{bmatrix} 2f_x/w &amp;0&amp;0&amp;0\\\\ 0&amp;2f_y/h&amp;0&amp;0\\\\ 0&amp;0&amp;(f+n)/(f-n) &amp;-2fn/(f-n)\\\\ 0&amp;0&amp;1&amp;0 \\end{bmatrix}\\] <p>to transform the camera space into normalized clip space (NDC space). </p>"},{"location":"cs294173/3dgsmath.html#gaussian-projection","title":"Gaussian projection","text":"<p>For the projection, the position \\(\\mu\\) is transformed into NDC space through the homogeneous transformation </p> \\[t =T_{w2c}\\begin{pmatrix}\\mu\\\\1\\end{pmatrix},  t' = Pt\\] <p>then into pixel coordinates </p> \\[\\mu' = (u,v) = \\begin{pmatrix}(wt_x/t_w+1)/2 + c_x\\\\(ht_y/t_w+1)/2+c_y\\end{pmatrix}\\] <p>the covariance \\(\\Sigma\\) is projected using the transformation from EWA Splatting paper [Zwicker et al. 2002]<sup>3</sup> </p> \\[J = \\begin{bmatrix} f_x/t_z &amp;0&amp; -f_xt_x/t^2_z\\\\ 0&amp;f_y/t_z &amp;-f_yt_y/y^2_z \\end{bmatrix}\\] <p>and the projection is </p> \\[\\Sigma' = JR_{c2w}\\sigma R_{c2w}^TJ^T\\]"},{"location":"cs294173/3dgsmath.html#sorting","title":"Sorting","text":"<p>A naive sorting will sort all splats by the view-space depth \\(t_z\\). Instead, notice that the size of splats is often few pixels and the frame is thus divided into \\(16\\times 16\\) pixels tiles. We find all overlapping tiles for each splat in the frame space (from 2D position \\(t'\\) and 2D covariance \\(\\Sigma'\\), we can find the bounding box that bounds \\(99\\%\\) confidence ellipse). Note that some splats may overlap more than one tile, thus we need to duplicate them. </p> <p>To utilize parallel sorting, assign each tiled splat a key, where first 32 bits encode the tile ID (<code>uint32</code>) and the lower 32 bits encode the depth (<code>float32</code>), and we do a parallel <code>radix_sort</code> on all splats. Then, finds the start and end indices of each tile by simply compare the tile ID with the neighbors. </p> <p>In general the pseudo-code is </p> pseudo code <pre><code>def create_splat_keys(g_frame: ProjectedGaussians, tiled_grid):\n    # the number of tiles each splat overlaps, the last index is \n    # for exclusive scan\n    n_tiles_overlapped = zeros_arr(len(g_frame) + 1)\n\n    @parallel\n    for idx, splat in enumerate(g_frame):\n        # integer bbox, for all tiles that overlaps the gaussian\n        bbox = find_bbox(splat, tiled_grid)\n        n_tiles_overlapped[idx] = bbox.area()\n\n    tiled_splat_key_start = parallel_exclusive_scan(n_tiles_overlapped)\n\n    tiled_splat_keys = empty_arr(tiled_splat_key_start[-1])\n\n    @parallel\n    for i, splat in enumerate(g_frame):\n        bbox = find_bbox(splat, tiled_grid)\n        for j, tile_id in enumerate(bbox):\n            key = get_key(tile_id, splat.depth)\n            tiled_splat_keys[tiled_splat_key_start[i] + j] = key\n    return tiled_splat_keys\n\ndef rasterize(cam: Camera, g: Gaussians):\n    # cull gaussians outside of the camera frustum\n    g_cull = cull_gaussian(g, cams) \n    # project gaussians into 2D frame space\n    g_frame = project_gaussians(g, cam)\n    # create tiles from the frame height and width\n    tiled_grid = create_tiles(cam.w, cam.h)\n\n    # create tiled splat keys\n    tiled_splat_keys = create_splat_keys(g_frame, tiled_grid)\n\n    # sort the keys\n    sorted_keys = parallel_radix_sort(tiled_splat_keys)\n    # find the start and end for each tile\n    start_indices, end_indices = parallel_find_edges(sorted_keys)\n\n    @parallel\n    for tile in tiled_grid:\n        start, end = start_indices[tile], end_indices[tile]\n        @parallel\n        for pixel in tile:\n            pixel = rasterize(sorted_keys[start:end+1])\n</code></pre>"},{"location":"cs294173/3dgsmath.html#composition","title":"Composition","text":"<p>Following the classic rendering equation, we need to blend the colors from front to back as </p> \\[C = \\sum_{n} c_n a_n T_n, T_n = \\prod_{m&lt;n} (1-a_m)\\] <p>where \\(a\\), or the conic opacity is computed from opacity parameter \\(o\\) multiples 2D Gaussian distribution</p> \\[a = o (p_{\\mu',\\Sigma'}(\\mathbf u)) = o \\cdot \\exp(-\\frac{1}{2}(\\mathbf u - \\mu')^T\\Sigma'^{-1}(\\mathbf u - \\mu'))\\] <p>where \\(\\mathbf u = (u,v)\\) is the pixel center. </p> <ol> <li> <p>Kerbl, B., Kopanas, G., Leimk\u00fchler, T., and Drettakis, G. 2023. 3D gaussian splatting for real-time radiance field rendering. ACM Transactions on Graphics 42, 4.\u00a0\u21a9</p> </li> <li> <p>Ye, V. and Kanazawa, A. 2023. Mathematical supplement for the <code>gsplat</code> library. https://arxiv.org/abs/2312.02121.\u00a0\u21a9</p> </li> <li> <p>Zwicker, M., Pfister, H., Baar, J. van, and Gross, M. 2002. EWA splatting. IEEE Transactions on Visualization and Computer Graphics 8, 3, 223\u2013238.\u00a0\u21a9</p> </li> </ol>"},{"location":"cs294173/intro.html","title":"Problem Formulations for 3D Vision","text":""},{"location":"cs294173/intro.html#fundamentals","title":"Fundamentals","text":""},{"location":"cs294173/intro.html#world-to-image-projection","title":"World to image projection","text":"<p>For 3D to 2D images, we go through </p> \\[[\\mathbf X_w = (x_w, y_w, z_w)]\\iff [\\mathbf X_c = (x_c, y_c, z_c)] \\implies [\\mathbf x_i (x_i, y_i)]\\] <p>where \\(\\mathbf X_w\\) is the world coordinates, \\(\\mathbf X_c\\) is the camera coordinates, and \\(\\mathbf x_i\\) is the image coordinates. The transformation from \\(\\mathbf X_w\\) to \\(\\mathbf X_c\\) is the world to camera (w2c) transformation, and the inverse if camera to world (extrinsic, camera poses) transformation. The transformation from \\(\\mathbf X_c\\) to \\(\\mathbf x_i\\) is the projection, which is not invertible (loss \\(z\\) in this process). Often time we use perspective projection, described by the camera intrinsic. </p>"},{"location":"cs294173/intro.html#representing-rotations","title":"Representing rotations","text":"<p>Representations of 3D Rotations notes</p> <p>For neural networks, we need continuous space so that we can \"learn\". One effective way is to predict some \\(3\\times 3\\) matrix \\(A\\) and use SVD to decompose \\(A\\) s.t. \\(A = USV^T\\), \\(R = UV^T\\). </p>"},{"location":"cs294173/intro.html#3-key-components","title":"3 Key Components","text":"<p>In a big picture, there are 3 key components for 3D from images. </p> <ul> <li>Camera the relative motion among camera poses</li> <li>Correspondences which pixel on the images correspond to the same point in 3D</li> <li>Structure the 3D points / objects</li> </ul>"},{"location":"cs294173/intro.html#photometric-stereo","title":"Photometric Stereo","text":"<p>Given fixed camera, fixed objects, a light source (point light of directional) s.t. the light source changes its position in each image. We want to recover the 3D shape.</p> <p>From BRDF, suppose that we are using directional light and the object has uniform material, then we have the specular light </p> \\[B_i = \\rho \\mathbf n_i \\mathbf l\\] <p>where \\(B_i\\) is the observed pixel value, \\(\\rho\\) is the coefficient for BRDF, light color, material, etc., \\(\\mathbf n_i\\) is the unknown surface model and \\(\\mathbf l\\) is the light direction. </p> <p>If we know the light direction in each image, then we can recover the normal from a system of equations, which facilitates high-fidelity 3D scans. </p>"},{"location":"cs294173/intro.html#camera-calibration-stereo-and-epipolar-geometry","title":"Camera calibration, stereo, and epipolar geometry","text":""},{"location":"cs294173/intro.html#camera-calibration","title":"Camera calibration","text":"<p>First, consider camera calibration from \\(s\\mathbf x = K \\cdot [R|\\mathbf t] \\cdot \\mathbf X\\) where \\(P:=K\\cdot [R|\\mathbf t]\\) is the \\(3\\times 4\\) projective matrix. Therefore, if we know the the position of some image pixels \\(\\mathbf x\\), 3D point \\(\\mathbf X\\), and the correspondence between them, we can solve a linear system of equations with 12 unknowns. In practice, since errors exist, we use linear least squares to optimize the system of equations.</p> <p>Note that we can recover intrinsic \\(K\\) and extrinc \\([R|\\mathbf t]\\) from \\(P\\) using QR decomposition, since \\(K\\) is a upper-triangular matrix and \\(R\\in SO3\\).  </p>"},{"location":"cs294173/intro.html#stereo","title":"Stereo","text":"<p>If two cameras are calibrated and we know their relative poses to each other. We then have a stereo camera. For simplicity, we assume that the cameras have parallel optical axis. We want to recover the relative depth from camera to the seen object. </p> <p>If we know pixels in two image planes \\((u_1, v_1), (u_2, v_2)\\) corresponds to the same pixel \\(\\mathbf X = (X,Y,Z)\\) in 3D. In addition, we know the extrinsic as eyes \\(\\mathbf O_1, \\mathbf O_2\\) and optical axis \\(\\mathbf d\\) (assuming parallel) for each camera, and the intrinsic \\(K\\) (assume identical intrinsic). </p> <p>We can compute  - 3D world space position \\(\\mathbf I_1 = (x_1, y_1, z_1), \\mathbf I_2 = (x_2, y_2, z_2)\\) for the pixels on the image plane.  - \\(B = \\mathbf O_1 - \\mathbf O_2\\), the difference between two camera positions - \\(d_1, d_2\\) the disparity of \\((u_1, v_1), (u_2, v_2)\\) to \\(\\mathbf d\\)</p> <p>Then, \\(\\mathbf X\\mathbf I_1\\mathbf I_2\\) forms a similar triangle to \\(\\mathbf X\\mathbf O_1\\mathbf O_2\\), and we have a angle-side-angle problem for the triangle \\(\\mathbf X\\mathbf O_1\\mathbf O_2\\).</p> <p>If we have two images taken from the same camera at different poses, we can also do stereo just as fine. However, translation is required for stereo. Consider the rotation case, where \\(B = \\mathbf O_1 - \\mathbf O_2 = 0\\), then we don't have the similar triangle and cannot perceive depth. </p>"},{"location":"cs294173/intro.html#epipolar-geometry","title":"Epipolar geometry","text":"<p>Consider the projection of the line \\(\\mathbf O_1\\mathbf X\\) onto the image plane of camera 2, the projection will form a line \\(\\mathbf l_2\\), and similarly on plane 1 we have \\(\\mathbf l_1\\) projected from \\(\\mathbf O_2\\mathbf{X}\\)</p> <p>Therefore, we have the epipolar constraint that vectors \\(\\mathbf x_1 = \\mathbf{I}_1 - \\mathbf{O}_1,\\mathbf x_2 = \\mathbf{I}_2 - \\mathbf{O}_2, \\mathbf t = \\mathbf{O}_2 - \\mathbf O_1\\) are colinear. </p> <p>If we know the camera poses between stereo cameras, and we have feature point on camera 1, then we only need to search for points on the epipolar line \\(\\mathbf l_2\\). </p>"},{"location":"cs294173/intro.html#multi-view-stereo-mvs","title":"Multi-view Stereo (MVS)","text":"<p>Now, expand stereo from 2 cameras to multiple camera. The input is calibrated images. In ideal case, MVS should give exactly the same depth as stereo. However, in practice they are different due to mismatching, errors in calibration, etc. The basic idea for MVS is that for each image, do stereo for each of its neighboring images and get depth from each pair. Finally, optimize all depth information so that the disagreement among images is minimalized. With more images, it gives a stronger match signal and more freedom for choosing a best subset of images to match. </p> <p>Depth fusion is a common problem for MVS, which given per image depth map, either dense depth map directly from RGBD, or sparse depth map from photometric stereo. The goal is to fuse the depth into a 3D model by maximizing the depth consistency. </p>"},{"location":"cs294173/intro.html#structure-from-motion-sfm-and-visual-slam","title":"Structure from Motion (SfM) and Visual SLAM","text":"<p>The all-in-one paper COLMAP: SfM Revisited</p> <p>The hardest type of problem in 3D vision is SfM, in which we only have unstructured images. What we can get from images is the correspondence (through feature extraction and patch match). </p> <p>The output is a sparse 3D location \\(\\mathbf x_i\\) for each matched pixel, and camera parameters (\\(R, \\mathbf t\\) and sometimes even \\(K\\) if the images are taken by different cameras) for each image. </p> <p>The objective is to minimizing re-projective error: given the set of cameras and the structure (sparse point cloud), how many pixels are the same as in the images. </p> <p>Common pipeline involves - Feature extraction and matching: SIFT feature extraction and its derivatives, matching involves RANSAC to estimate fundamental matrix (epipolar constraint) - Image connectivity graph: which clusters of images are neighbors - Correspondence estimation: linkup pairwise matches in the connectivity graph to form connected components of matches across more images - Camera parameter estimation and sparse cloud reconstruction</p>"},{"location":"cs294173/intro.html#bundle-adjustment","title":"Bundle adjustment","text":"<p>We could write the reprojection errors over \\(m\\) reconstructed points and \\(n\\) images as </p> \\[g(\\mathbf X, \\mathbf R, \\mathbf T) = \\sum_{i=1}^m \\sum_{j=1}^n w_{ij} \\|P(\\mathbf x_i, R_j, \\mathbf t_j) - (u_{ij}, v_{ij})\\|\\] <p>where \\(w_{ij}\\) is an indicator that point $i4 is visible in image \\(j\\), \\(\\mathbf x_i\\) and \\(P\\) is the projection from 3D space to the camera's image plane, and \\((u_{ij}, v_{ij})\\) is the pixel value corresponds to point \\(i\\) in image \\(j\\). </p> <p>Bundle adjustment aims to minimize the error. </p>"},{"location":"cs294173/intro.html#visual-slam","title":"Visual SLAM","text":"<p>Instead of giving a set of image, V-SLAM has continuous input stream from camera and additional sensors (IMU for motion acceleration, LIDAR for depth). Also, the localization often requires to be real-time (for robot exploration). </p> <p>In general, the algorithm is similar to SfM, but image connectivity and correspondence is given by nature (the robot cannot teleport). Bundle adjustment is often done periodically on a cluster of images collected. Also, V-SLAM introduces the problems such as loop closure so that more constraints are presented. </p>"},{"location":"cs294173/nerf.html","title":"Neural Radiance Fields","text":""},{"location":"cs294173/nerf.html#plenoptic-function","title":"Plenoptic Function","text":"<p>Consider some function to describe the set of all things that we can ever see (i.e. all the necessary inputs that will impact the seen color). The parameters are </p> <ul> <li>\\(\\theta, \\phi\\) the viewing directions</li> <li>\\(x_v, y_v, z_v\\) the eye position</li> <li>\\(\\lambda\\) wavelength</li> <li>\\(t\\) time</li> </ul> <p>The estimation of the function is the image based rendering / novel view synthesis problem. One classical approach for modeling plenoptic function is Lightfield (1996), which take pictures from many views. The problems are 3D consistency and also assume that the ray shouting out from a pixel is never occluded. Thus only the plenoptic surface.</p>"},{"location":"cs294173/nerf.html#nerf","title":"NeRF","text":"<p>NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis [Mildenhall et al. 2021]<sup>1</sup> github: https://github.com/bmild/nerf website: https://www.matthewtancik.com/nerf</p> <p></p>"},{"location":"cs294173/nerf.html#nerf-scene-representation","title":"NeRF Scene Representation","text":"<p>The representation can be seen as a funciton </p> \\[F_\\Theta(\\mathbf x, \\mathbf d) \\rightarrow (\\mathbf c,\\sigma)\\] <ul> <li>Input the 3D location \\(\\mathbf x = (x, y, z)\\) and 2D viewing direction \\((\\theta, \\phi)\\). In practice, we express the direction using \\(\\mathbf d\\), as a 3D Cartesian unit vector. </li> <li>Output  the color \\(\\mathbf c =(R,G,B)\\) and volume density \\(\\sigma\\). To help understand, the output can be imagined as <code>RGBA</code> value of a point \\(\\mathbf x\\) of a semi-transparent object, viewed from angle \\(\\mathbf d\\).</li> </ul>"},{"location":"cs294173/nerf.html#mlp-architecture","title":"MLP Architecture","text":"<p>\\(\\Theta\\) is the set of parameters for MLP. The arch of the MLP is very simple: 8 fully connected layers to process \\(\\mathbf x\\) and outputs \\(\\sigma\\) and a \\(256\\)-dim feature vector, and then concat with \\(\\mathbf d\\), to through another fully-connected layer to produce \\(\\mathbf c\\). In this case, the volume density \\(\\sigma\\) open depends on position, and is not dependent on viewing angles. </p>"},{"location":"cs294173/nerf.html#volume-rendering-with-radiance-fields","title":"Volume Rendering with Radiance Fields","text":"<p>For classical volume rendering, given a ray \\(\\mathbf r(t) = \\mathbf e + t\\mathbf d\\), the color of a ray is seen as an accumulation of colors near \\(t_n\\) and far \\(t_f\\) bounds, i.e. an integral </p> \\[C(\\mathbf r) = \\int_{t_n}^{t_f} T(t)\\sigma(\\mathbf r(t)) \\mathbf c(\\mathbf r(t), \\mathbf d)dt, T(t) = \\exp(-\\int_{t_n}^t \\sigma(\\mathbf r(s))ds)\\] <p>To compute this integral, numerically estimate it using quadrature. However, if we only estimate the value using discretized voxel grids (as previous approaches), the representation's resolution is limited, because the network won't learn our points. To account for the continuity, uniformally sampling each point from a evenly-spaced partition of \\([t_n, t_f]\\), i.e. </p> \\[t_i \\sim \\text{Unif}(t_n + \\frac{i-1}{N}(t_f-t_n), t_n + \\frac{i}{N}(t_f-t_n))\\] <p>Then by Gaussian quadrature, we have the estimation</p> \\[\\hat{C}(\\mathbf r) = \\sum_{i=1}^N T_i (1 - e^{-\\sigma_i (t_{i+1} - t_i)})\\mathbf c_i, T_i = \\exp(\\sum_{j=0}^i -\\sigma_i(t_{i+1}-t_i))\\]"},{"location":"cs294173/nerf.html#high-frequency-encoding","title":"High Frequency Encoding","text":"<p>This is consistent with recent work by Rahaman et al., which shows that deep networks are biased towards learning lower frequency functions. They additionally show that mapping the inputs to a higher dimensional space using high frequency functions before passing them to the network enables better fitting of data that contains high frequency variation. </p> <p>Instead of directly inputting \\((\\mathbf x, \\mathbf d)\\), apply the encoding function </p> \\[\\gamma: \\mathbb R\\rightarrow \\mathbb R^{2L}, \\gamma(p) = (\\sin(2^0\\pi p), \\cos(2^0\\pi p), ..., \\sin(2^{L-1}\\pi p), \\cos(2^{L-1}\\pi p))\\] <p>Specifically, for \\(\\mathbf x\\), choose \\(L = 10\\), for \\(\\mathbf d\\), choose \\(L=4\\)</p>"},{"location":"cs294173/nerf.html#hierarchical-volume-sampling","title":"Hierarchical Volume Sampling","text":"<p>In the volume rendering function, we don't bother to sample many points on free space and occulated regions (low \\(\\sigma\\)). Since the inaccuracy in those regions won't have much impact on the final color. Therefore, we can take an hierarchical approach. First, coarsely partition the ray into \\(N_c\\) pieces, and compute the coarse color</p> \\[\\hat{C}_c(\\mathbf r) = \\sum_{i=1}^{N_c} w_i \\mathbf c_i, w_i = T_i(1-\\exp(-\\sigma_i(t_{i+1}-t_i)))\\] <p>Then, using the weights \\(w_i\\)'s, we can get a piecewise-constant PDF by normalizing \\(w_i\\)'s, and sample finer positions \\(N_f\\) from the distribution using inverse tranform sampling, and compute \\(\\hat C_f\\) from the fine partition. </p> <p>In this way, we won't a very fine partition grid, but achieve similar results by focusing on important regions. </p>"},{"location":"cs294173/nerf.html#implementation-details","title":"Implementation details","text":"<p>The input training data will be a set of images and their camera poses (from intrinsic parameters), and scene bounds. </p> <p>Assuming the camera models are perspective models, we can sample viewing rays for each image (and their respective camera models). For training, we have the sampled rays \\(\\mathbf r\\) abd ground truth color \\(C(\\mathbf r)\\), and the loss is defined as </p> \\[\\mathcal L = \\sum_{\\mathbf r\\in \\mathbb R}\\big(\\|\\hat C_c(\\mathbf r) - C(\\mathbf r)\\|_2^2 + \\|\\hat C_f(\\mathbf r) - C(\\mathbf r)\\|_2^2\\big)\\] <p>For inference (rendering), given the viewing pose and image plane, we need to cast rays for each pixel, do coarse and then fine partitions, samplings, and coloring, and finally collect the color of the pixel. </p> <p>With the density \\(\\sigma\\), we can also reconstruct meshes using marching cude algorithm. </p>"},{"location":"cs294173/nerf.html#components-of-nerf","title":"Components of NeRF","text":"<p>A NeRF can be seen as 3 parts.  - Volumetric rendering equations - Fields (Embedders): how to represent the 3D space - Sampling function: how to sample points along the ray. </p>"},{"location":"cs294173/nerf.html#encodings","title":"Encodings","text":"<p>Intuitively, MLPs are differentiable and \"over-smooths\" high frequency features. Images typically have high frequency details and are not differentiable in local patches (edges). The idea is to transform the features into another space so that it can learn faster and preserves high frequency details. Existing approaches includes</p> <ul> <li>Fourier features, positional encoding The positional encoding used in original NeRF, also useful for other 2D/3D regression tasks. </li> <li>Hash encoding, instant-ngp A multi-res hash encoding. An image is \"continuous\" in non-edge spaces. If we hash the image pixels, then the image becomes continuous in the neighboring spaces. The hash value will be learned by an MLP at the same time. Also, \"multi-res\" means that we store the features at different resolution and do trilinear interpolations when retrieve them. This provides faster training and inference. </li> <li>Spherical harmonics RGB is not continuous, but SH coefficients are. The MLP can predict SH coefficients instead RGB. Also, we benefits from SH as the function is analytic, simple to compute, and simplify the learning. </li> </ul>"},{"location":"cs294173/nerf.html#fields-compression","title":"Fields Compression","text":"<p>In NeRF, the field is parameterized by a large MLP, which is expensive for inference. However, the space is not filled by object volume in most cases. Indeed, many space are often empty. The idea is to shrink the 3D space by different representations of fields. </p> <ul> <li>Sparse voxel grid, works such as NSVF and Plenoxels. The idea is to construct voxel grid on the space, the grid is \"sparse\" because we can skip empty voxels. </li> <li>Lower rank representations such as TensorRF. Decompose the 4D tensor volume into lower-rank matrices and vectors. </li> <li>A dictionary, or multi-res hash grid. instant-ngp. Hashing with the hash key learned by the neural network. Multi-res allows to store features in different frequency. </li> </ul>"},{"location":"cs294173/nerf.html#sampling","title":"Sampling","text":"<p>The considerations for sampling are </p> <ul> <li>Aliasing Do we have enough samplings in some space, do we \"skip\" the important surfaces. </li> <li>Speed Do we have redundant samplings in the empty space</li> </ul> <p>In the original NeRF, the idea is to do coarse-to-fine sampling. Starting with uniform sampling, and then do another fine sampling by importance (via volume density \\(\\sigma\\)). The problem is that it requires a very large number of point, hence very slow inference. </p> <p>Following up works start to use different field representations so that empty space are skipped (sparse voxels, occupancy grid checking in instant-ngp). </p> <p>Surface-based methods predict surface and only sample near the surfaces. </p> <p>For aliasing, MipNeRF samples from a cone instead of the original ray. Note that the sampling function is the same in MipNeRF, the idea is to use a integrated positional encoding so that features further away will have different sampling rate (mip as of mipmap). </p> <p>MipNeRF-360 further uses a proposal network (a tiny MLP) to learn (distilled model) the weights \\(w_i\\). </p>"},{"location":"cs294173/nerf.html#neural-sparse-voxel-fields","title":"Neural Sparse Voxel Fields","text":"<p>Neural Sparse Voxel Fields [Liu et al. 2020]<sup>2</sup>  github: https://github.com/facebookresearch/NSVF website: https://lingjie0206.github.io/papers/NSVF/</p> <p>The largest limitation for NeRF is that NeRF is not efficient. According to the author of NSVF</p> <p>Although NeRF performs importance sampling along the ray, due to allocating fixed computational budget for every ray, it cannot exploit this opportunity to improve rendering speed.</p> <p>The main idea behind NSVF is to using bounding volume hierachy and sparse voxel octree to model a scene in sparse hierarchical structure for acceleration. </p>"},{"location":"cs294173/nerf.html#voxel-bounded-implicit-fields","title":"Voxel-bounded Implicit Fields","text":"<p>For a scene, assume that the scene are contained within a set of sparse voxels \\(\\mathcal V = \\{V_1,...,V_K\\}\\). For each point \\(\\mathbf x\\) and its viewing direction \\(\\mathbf d\\), we first find \\(i\\) where \\(\\mathbf x\\in V_i\\), then </p> \\[F_\\Theta(\\mathbf x, \\mathbf d) = F_{\\theta}^i (g_i(\\mathbf x), \\mathbf d)\\] <p>where each \\(g_i\\) is defined as  </p> \\[g_i(\\mathbf x) = \\zeta\\circ \\chi \\circ \\tilde g_i\\circ p (\\mathbf x)\\] <ul> <li>\\(p(\\mathbf x) = (v_1, v_2, ..., v_8)\\) find the 8 vertices of \\(V_i\\)</li> <li>\\(g_i\\) are feature vectors stored at each vertex. </li> <li>\\(\\chi\\) is the triniliear interpolation to interpolate \\(\\mathbf x\\)</li> <li>\\(\\zeta\\) is the post-processing function, hence we use positional encoding as of NeRF.</li> </ul> <p>And each \\(F^i_\\theta\\) is a MLP with shared parameters \\(\\theta\\)</p>"},{"location":"cs294173/nerf.html#volume-rendering","title":"Volume Rendering","text":"<p>Assuming that we have obtained the voxels, do ray-voxel intersections using AABB tree and then for each ray  - if the ray hits some voxel, sample point only from the voxels by rejection sampling and accumalate color by volume rendering equation \\(\\hat{C}(\\mathbf r) = \\sum_{i=1}^N T_i (1 - e^{-\\sigma_i (t_{i+1} - t_i)})\\mathbf c_i\\)  where \\(\\mathbf c_i\\) is obtained from Voxel-bounded Implicit Fields - if the ray does not hit, then add an additional transparency term \\(\\alpha(\\mathbf r) = \\exp{\\sum_{i=1}^N -\\sigma_i (t_{i+1}-t_i)}\\) abd learnable RGB bg color \\(\\mathbf c_{bg} \\in\\mathbb R^3\\), and the color of non-hit is \\(\\alpha\\mathbf c_{bg}\\)</p>"},{"location":"cs294173/nerf.html#learning","title":"Learning","text":"<p>The loss function is defined as of NeRF, plus an additional transparency loss </p> \\[\\mathcal L = \\mathcal L_{\\mathbf c} + \\lambda \\Omega(\\alpha(\\mathbf r))\\] <p>where \\(\\Omega\\) is the beta-distribution regularization. </p> <p>For the voxel, initialize the bounding box that roughly encloses the scene, and then during the training, periodically prune \\(V_i\\) if </p> \\[\\min(\\exp(-\\sigma(\\mathbf x_j))) &gt; \\gamma, \\mathbf x_j \\in V_i\\] <p>where \\(\\mathbf x_j\\) is uniformaly sampled in \\(V_i\\) (\\(16^3\\) samples in the paper), and \\(\\gamma\\) is a threshold (\\(0.5\\) in the paper). </p> <p>After certain steps of training, reduce the ray-marching step size (partition length) and voxel side-length (by half in the paper). The voxel representations on the newly created vertices will be tri-linear interpolation. </p>"},{"location":"cs294173/nerf.html#nerv-neural-reflectance-and-visibility-fields-for-relighting-and-view-synthesis","title":"NeRV: Neural Reflectance and Visibility Fields for Relighting and View Synthesis","text":"<p>Page (https://pratulsrinivasan.github.io/nerv/) [Srinivasan et al. 2021]<sup>3</sup> paper(https://pratulsrinivasan.github.io/nerv/)</p> <p></p>"},{"location":"cs294173/nerf.html#light-model","title":"Light Model","text":"<p>In the original NeRF model, we only consider the radiance, or the light emitted at \\(x\\) from direction \\(d\\), denote it as \\(L_e(\\mathbf x, \\mathbf d_0)\\). In NeRV, we replace this term with the standard reflectance light \\(L_r(\\mathbf x, \\mathbf d_0)\\), which considers the light from all directions</p> \\[L_r(\\mathbf x, \\mathbf d_0) = \\int_S L_i(\\mathbf x, \\mathbf d_i) R(\\mathbf x, \\mathbf d_i, \\mathbf d_0)d\\mathbf d_i\\] <p>where \\(L_i\\) is the incoming radiance and \\(R\\) is the refleactance or transforming term. Practically, \\(L_i\\) is implemented as direction and indirect (one bounce) illunimnations and \\(R\\) is implemented using standard microfacet BRDF. </p>"},{"location":"cs294173/nerf.html#nerual-reflectance-fields-for-r","title":"Nerual Reflectance Fields for R","text":"<p>The idea is to use two networks, A shape MLP function \\(\\text{MLP}_s: \\mathbf x\\rightarrow \\sigma\\) to capture the surface, and then \\(\\nabla_\\mathbf x \\text{MLP}_s\\) will be the normal of the surface. Optionally, we can fine tune the normals with another MLP.    A reflactance MLP function \\(\\text{MLP}_r:\\mathbf x \\rightarrow (\\mathbf a, \\gamma)\\) to be albedo color and roughness, which are the parameters to microfacet BRDF. </p>"},{"location":"cs294173/nerf.html#light-transport-for-l","title":"Light Transport for L","text":"<p>\\(L_i = L_e + L_r\\) can be decomposed into the direct (light directly from environment) and indirect (ilght reflected from other surfaces/objects and the shadow). </p> <p>For direct light, we consider </p> \\[L_e(\\mathbf x, \\mathbf d_i) = V(\\mathbf x, \\mathbf d_i) E(\\mathbf x, -\\mathbf d_i)\\] <p>where \\(V\\) is the light visibility, which is approximated by a MLP (similar to \\(\\sigma\\) term in the original NeRF). and \\(E\\) is the environment light map, which is an spherical (hemi-spherical) light map. </p> <p>For indirect light, due to computation limit, we only consider the first bounce, and encode it as the indirect light map and shadow map. </p>"},{"location":"cs294173/nerf.html#limitations","title":"Limitations","text":"<p>Lots of parameters to be estimated by MLPs, much more intensive MLP computations and very expensive integrals (discretized to the light maps, a good approximation need at least 128 direction). </p> <p>Microfacet BRDF has very limited expressive power. Also, the approximation is not very good. The object loses plenty of the fidelity. </p>"},{"location":"cs294173/nerf.html#nerfactor","title":"NeRFactor","text":"<p>NeRFactor: Neural Factorization of Shape and Reflectance Under an Unknown Illumination Page (https://xiuming.info/projects/nerfactor/) Paper (https://arxiv.org/pdf/2106.01970.pdf)</p> <p>The idea and light modelling is similar to NeRV. but NeRFactor provides a more feasible training and evaluating pipeline. Also, replace the Microfacet BRDF with a data-driven MERL-BRDF model, adding much better expressive power. </p> <p></p> <p>Change plenoptic function from viewing centered to object centered (object emits light instead of eye receiving light) and simplify the problem to 5D (remove wavelength and time). Intuitively, a NeRF can be discretized as a 3D grid, and at each grid location, we have a sphere of colors.  </p> <p>Relations with some other works</p> <ul> <li>MVS: multi-view stereo aims to recover the surface mesh based on physics based rendering, hence struggle on thin, amorphous, and shiny objects. </li> <li>analysis by synthesis: A Theory of Shape by Space Carving (2000) by Kutulakos use calibrated images to \"carve\" the volume and form the final shape. However, the non-differentiable and handmade way are limited. </li> </ul> <ol> <li> <p>Mildenhall, B., Srinivasan, P.P., Tancik, M., Barron, J.T., Ramamoorthi, R., and Ng, R. 2021. NeRF: Representing scenes as neural radiance fields for view synthesis. Commun. ACM 65, 1, 99\u2013106.\u00a0\u21a9</p> </li> <li> <p>Liu, L., Gu, J., Zaw Lin, K., Chua, T.-S., and Theobalt, C. 2020. Neural sparse voxel fields. Advances in Neural Information Processing Systems 33, 15651\u201315663.\u00a0\u21a9</p> </li> <li> <p>Srinivasan, P.P., Deng, B., Zhang, X., Tancik, M., Mildenhall, B., and Barron, J.T. 2021. Nerv: Neural reflectance and visibility fields for relighting and view synthesis. Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 7495\u20137504.\u00a0\u21a9</p> </li> </ol>"},{"location":"cs294173/optical_flow.html","title":"Supervised Correspondence","text":"<p>Given a pair of image \\(I_1, I_2\\), optical flow is a dense displacement map \\(\\mathbf f\\) from \\(I_1\\) to \\(I_2\\) over time \\(\\Delta t\\). For each pixel \\((u_1, v_1)\\) in image 1 and its corresponding pixel \\((u_2, v_2)\\) in image 2. We have \\(I_1((u_1, v_1) + \\mathbf f(u_1, v_1)) = I_2(u_2, v_2)\\). Optical flow provides a strong perceptual cue for motion. </p> <p>Consider the optical flow map as a 2D field. We can treat it as an energy optimization problem. On one side, we have possible correspondence across two images, found by the similarities in their features. On the other side, the flow should be a continuous field on the pixel locations.  </p>"},{"location":"cs294173/optical_flow.html#direct-supervision","title":"Direct supervision","text":"<p>Early learned-based attempts (FlowNet series) uses CNN to predict the optical flow, the input is a pair of images and the supervision is the GT optical flow. FlowNet introduces the correlation between two patches centered at \\(\\mathbf x_1\\) in image 1 and centered at \\(\\mathbf x_2\\) in image 2 as </p> \\[corr(\\mathbf x_1, \\mathbf x_2) = \\sum_{\\mathbf d\\in [-k, k] \\times [-k, k]} \\mathbf g_1(\\mathbf x_1 + \\mathbf d) \\cdot  \\mathbf g_2(\\mathbf x_2 + \\mathbf d)\\] <p>where \\(k\\) is the half patch size and \\(\\mathbf g\\) is the feature extractor. If we compute correlation for all pair of pixels in image 1 and image 2, then the computation and backprop is intractable. Thus, the correlation is only computed for pixels that is within the a search radius \\(D\\).   </p> <p>Later works often adapting an iterative refinement using coarse-to-fine pyramids (PWC-Net series) to speedup and improve search radius. </p> <p>The issue with all existing approach is that the direct supervision is not effective, or hard for the network to learn. Another issue for optical flow prediction works is the data. Only synthetic data can have 100% accurate GT labels. This causes issue for synthetic to real generalization. </p>"},{"location":"cs294173/optical_flow.html#recurrent-all-pairs-field-transformation-raft","title":"Recurrent All-pairs Field Transformation (RAFT)","text":"<p>RAFT: Recurrent All-Pairs Field Transforms for Optical Flow</p> <p></p> <p>The input for RAFT is a pair of image, the output is optical flow from image 1 to image 2. </p> <p>RAFT combines all previous successes, and uses an iterative update strategy, and becomes the \"end paper\" for optical flow problems. </p>"},{"location":"cs294173/optical_flow.html#feature-extraction","title":"Feature Extraction","text":"<p>The first step is to extract image features \\(\\mathbf g_\\theta\\) from \\(I_1\\) and \\(I_2\\) using CNN, which is similar to other networks. In addition, RAFT use a context network \\(h\\) that only extract features from \\(I_1\\), \\(\\mathbf h_\\theta\\) has the same architecture as \\(\\mathbf g_\\theta\\)</p>"},{"location":"cs294173/optical_flow.html#4d-correlation-volumes","title":"4D Correlation Volumes","text":"<p>The correlation is a \\(H\\times W\\times H\\times W\\) 4D volume that describes the correlation for each pixel feature \\(\\mathbf g_1(u, v)\\) in image 1 and each \\(\\mathbf g_2(u,v)\\) in image 2. In addition, RAFT adapts the coarse-to-fine pyramids approach, and average pooling the last two dimensions of correlation volume so that there are 4 4D correlation volumes. </p> \\[corr_0: HWHW, corr_1: HW\\frac{H}{2}\\frac{W}{2}, corr_2: HW\\frac{H}{4}\\frac{W}{4}, corr_3: HW\\frac{H}{8}\\frac{W}{8}\\] <p>The first two dimension (image 1) preserves high res information, and pyramid (image 2) allows for a large search radius. </p> <p>This correlation volume will only be constructed once at the beginning. During the training, we query between the pixel in image 1 and all pixels in image 2 that is within a search radius to the estimated correspondence. Also, we query all the pyramids so that the search radius is larger. Because the correlation volume is not constructed during the run, it's efficient enough.</p>"},{"location":"cs294173/optical_flow.html#iterative-updates","title":"Iterative Updates","text":"<p>The core idea behind RAFT is the iterative updates. Instead of directly predicting the flow, RAFT estimates a sequence of flow \\(\\{\\mathbf f_1, ..., \\mathbf f_M\\}\\) from \\(\\mathbf f_0 = \\mathbf 0\\). For each iteration, the network predicts an update direction \\(\\Delta \\mathbf f\\) so that \\(\\mathbf f_{k+1} = \\Delta f + \\mathbf f_k\\). The supervision is added to the full sequence of predictions with exponentially increasing weights. </p> \\[\\mathcal L = \\sum_{i=1}^N \\gamma^{N-i}\\| \\mathbf f_{gt} - \\mathbf{f}_i\\|_1\\]"},{"location":"cs294173/optical_flow.html#generalizations","title":"Generalizations","text":"<p>In general, RAFT transfers well from synthetic datasets to real-world. </p> <ul> <li>The feature extractor is pre-trained on all other image tasks. Also the correlation lookup table is independent of the underlying RGB texture (already found by other existing works). </li> <li>Output the residual instead of predicting the final flow. Decompose one movement to multiple steps, so that the learning task is easier. </li> <li>\"Learning to update\", or similar to optimization algorithms, take small steps to approach the target. </li> </ul>"},{"location":"cs294173/optical_flow.html#droid-slam","title":"DROID-SLAM","text":"<p>DROID-SLAM: Deep Visual SLAM for Monocular, Stereo, and RGB-D Cameras</p> <p>Input a sequence of images, for each image \\(I_t\\) outputs camera pose \\(P_t\\) and disparity/depth \\(d_t\\). Applying the similar idea from optical flow to SLAM tasks. </p> <p>For SLAM tasks, we need to maintain a frame-graph where vertices are images \\(I_1, I_2, ...\\) and edges \\((i,j)\\) means that \\(I_i\\) and \\(I_j\\) have overlapping. For \\(I_i\\), we need to lookup the correlation for all images that \\(I_i\\) is connected with.</p> <p>The iterative updates happen on poses and depth, note that for camera poses, the rotation is through matrix exponentials (standard way for differentialize SE3). </p> <p>The training is based on GT camera poses and GT depth maps. The supervision includes a pose loss </p> \\[\\mathcal L_{pose} = \\sum_{k=1}^N\\sum_i \\|\\log_{SE3}(P_{GT}^{-1} P_{i, k})\\|_2\\] <p>and a correspondence loss similar to RAFT, which we induces optical flow from poses and depth. </p>"},{"location":"cs294173/pointcloud.html","title":"Deep Learning for 3D Point Clouds","text":"<p>PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation [Qi et al. 2017]<sup>1</sup> github (author): https://github.com/charlesq34/pointnet github (pytorch implementation): https://github.com/fxia22/pointnet.pytorch</p> <p></p>"},{"location":"cs294173/pointcloud.html#point-cloud","title":"Point cloud","text":"<p>A point cloud is a set of 3D points \\(P = \\{P_i:i=1, ..., n\\}\\), where each \\(P_i = (x_i, y_i, z_i)\\) and sometimes extra feature channels like color, normal, etc. The point cloud can often obtained from LIDAR or other 3D scannings. Compare to other object representations (volumetric), it's simple and \"raw\". </p>"},{"location":"cs294173/pointcloud.html#properties-of-point-sets","title":"Properties of Point Sets","text":"<ul> <li>Unordered (Permutation Invariance) The order of the set should not change the target output.  </li> <li>Interaction among points it is a \"cloud\", which means the points should not be isolated, and neighboring points should form a meaning subset. </li> <li>Invariance under transformation A point cloud is a geometric object, the learned representation of the point set should be invariant to certain transformations, for example, rigid transformation over the whole point cloud should not change the output. </li> </ul>"},{"location":"cs294173/pointcloud.html#pointnet","title":"PointNet","text":""},{"location":"cs294173/pointcloud.html#permutation-invariance-symmetry-functions","title":"Permutation Invariance: Symmetry Functions","text":"<p>A function is symmetric if the permutation of the input does not change the output. For example, \\(+, *, \\max\\). This is important for solving the undered property. </p> <p>The idea is to approximate some general function \\(f\\) defined on a point set by applying a symmetric function on transformed points in the set:</p> \\[\\begin{align*} f(\\{p_1,...,p_n\\})&amp;\\approx g(h(p_1),...h(x_n))\\\\ f&amp;:2^{\\mathbb R^N}\\rightarrow \\mathbb R, \\\\ h&amp;:\\mathbb R^N\\rightarrow \\mathbb R^K, \\\\ g&amp;: (\\mathbb R^k)^n\\rightarrow \\mathbb R \\text{ symmetric} \\end{align*}\\] <p>By experiment, \\(h\\) is approximated by a network and \\(g\\) by a composition of a single variable function and a max pooling function.</p>"},{"location":"cs294173/pointcloud.html#transformation-invariance-joint-alignment-network","title":"Transformation Invariance: Joint Alignment Network","text":"<p>The paper uses Spatial Transformer Networks[Jaderberg, Simonyan, Zisserman 2015] (T-net in PointNet paper). Compare to the original STN, which works on 2D images through sampling and interpolation, T-net predicts an affine transformation matrix, and we can use such matrix to transform point cloud coordinates.</p> <p>T-net is also extended to the alignment of feature space. However, feature space has higher dimensions (64 in paper), hence hard to optimize, we need to add a regularization term to trianing loss. </p> \\[L_{reg} = \\|I - AA^T\\|_F^2\\] <p>The T-net architecture is described in the paper's supplementary</p> <p>The first transformation network is a mini-PointNet that takes raw point cloud as input and regresses to a 3 \u00d7 3 matrix. It\u2019s composed of a shared MLP(64, 128, 1024) network (with layer output sizes 64, 128, 1024) on each point, a max pooling across points and two fully connected layers with output sizes 512, 256. The output matrix is initialized as an identity matrix. All layers, except the last one, include ReLU and batch normalization. The second transformation network has the same architecture as the first one except that the output is a 64 \u00d7 64 matrix. The matrix is also initialized as an identity. A regularization loss (with weight 0.001) is added to the softmax classification loss to make the matrix close to orthogonal.  </p> Code <pre><code>import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass TNet(nn.Module):\n\n    def __init__(self, input_dim, output_dim):\n        super(TNet, self).__init__()\n        self.output_dim = output_dim\n\n        self.conv_1 = nn.Conv1d(input_dim, 64, 1)\n        self.conv_2 = nn.Conv1d(64, 128, 1)\n        self.conv_3 = nn.Conv1d(128, 1024, 1)\n\n        self.bn_1 = nn.BatchNorm1d(64)\n        self.bn_2 = nn.BatchNorm1d(128)\n        self.bn_3 = nn.BatchNorm1d(1024)\n        self.bn_4 = nn.BatchNorm1d(512)\n        self.bn_5 = nn.BatchNorm1d(256)\n\n        self.fc_1 = nn.Linear(1024, 512)\n        self.fc_2 = nn.Linear(512, 256)\n        self.fc_3 = nn.Linear(256, self.output_dim*self.output_dim)\n\n    def forward(self, x: torch.Tensor):\n        n = x.shape[1]\n        x = x.transpose(2,1)\n\n        x = F.relu(self.bn_1(self.conv_1(x)))\n        x = F.relu(self.bn_2(self.conv_2(x)))\n        x = F.relu(self.bn_3(self.conv_3(x)))\n\n        x = nn.MaxPool1d(n)(x)\n        x = x.view(-1, 1024)\n\n        x = F.relu(self.bn_4(self.fc_1(x)))\n        x = F.relu(self.bn_5(self.fc_2(x)))\n        x = self.fc_3(x)\n\n        I = torch.eye(self.output_dim)\n        if torch.cuda.is_available():\n            I = I.cuda()\n        return x.view(-1, self.output_dim, self.output_dim) + I\n</code></pre> <pre><code>class FeatureExtractor(nn.Module):\n    def __init__(self, input_dim, local=False):\n        super(FeatureExtractor, self).__init__()\n        self.local = local\n        self.input_transform = TNet(input_dim=input_dim, output_dim=input_dim)\n        self.feature_transform = TNet(input_dim=64, output_dim=64)\n\n        self.conv_1 = nn.Conv1d(input_dim, 64, 1)\n        self.conv_2 = nn.Conv1d(64, 64, 1)\n        self.conv_3 = nn.Conv1d(64, 64, 1)\n        self.conv_4 = nn.Conv1d(64, 128, 1)\n        self.conv_5 = nn.Conv1d(128, 1024, 1)\n\n        self.bn_1 = nn.BatchNorm1d(64)\n        self.bn_2 = nn.BatchNorm1d(64)\n        self.bn_3 = nn.BatchNorm1d(64)\n        self.bn_4 = nn.BatchNorm1d(128)\n        self.bn_5 = nn.BatchNorm1d(1024)\n\n    def forward(self, x):\n        n = x.shape[1]\n        input_transform = self.input_transform(x)\n        x = torch.bmm(x, input_transform)\n        x = x.transpose(2, 1)\n        x = F.relu(self.bn_1(self.conv_1(x)))\n        x = F.relu(self.bn_2(self.conv_2(x)))\n        x = x.transpose(2, 1)\n\n        feature_transform = self.feature_transform(x)\n        x = torch.bmm(x, feature_transform)\n        local_point_features = x\n\n        x = x.transpose(2, 1)\n        x = F.relu(self.bn_3(self.conv_3(x)))\n        x = F.relu(self.bn_4(self.conv_4(x)))\n        x = F.relu(self.bn_5(self.conv_5(x)))\n        x = nn.MaxPool1d(n)(x)\n        x = x.view(-1, 1024)\n\n        if self.local:\n            x = x.view(-1, 1024, 1).repeat(1, 1, n)\n            return torch.cat([x.transpose(2, 1), local_point_features], 2), feature_transform\n        else:\n            return x, feature_transform\n</code></pre> <pre><code>class PointNet_Classification(nn.Module):\n    def __init__(self, num_classes, point_dim=3, dropout=.3):\n        super(PointNet_Classification, self).__init__()\n        self.feature_extract = FeatureExtractor(input_dim=point_dim, local=False)\n        self.fc_1 = nn.Linear(1024, 512)\n        self.fc_2 = nn.Linear(512, 256)\n        self.fc_3 = nn.Linear(256, num_classes)\n\n        self.bn_1 = nn.BatchNorm1d(512)\n        self.bn_2 = nn.BatchNorm1d(256)\n\n        self.dropout_1 = nn.Dropout(dropout)\n\n    def forward(self, x):\n        x, feature_transform = self.feature_extract(x)\n\n        x = F.relu(self.bn_1(self.fc_1(x)))\n        x = F.relu(self.bn_2(self.fc_2(x)))\n        x = self.dropout_1(x)\n\n        return F.log_softmax(self.fc_3(x), dim=1), feature_transform\n\nclass PointNet_Segmentation(nn.Module):\n\n    def __init__(self, num_classes, point_dim=3):\n        super(PointNet_Segmentation, self).__init__()\n\n        self.feature_extract = FeatureExtractor(input_dim=point_dim, local=True)\n\n        self.conv_1 = nn.Conv1d(1088, 512, 1)\n        self.conv_2 = nn.Conv1d(512, 256, 1)\n        self.conv_3 = nn.Conv1d(256, 128, 1)\n        self.conv_4 = nn.Conv1d(128, num_classes, 1)\n\n        self.bn_1 = nn.BatchNorm1d(512)\n        self.bn_2 = nn.BatchNorm1d(256)\n        self.bn_3 = nn.BatchNorm1d(128)\n\n    def forward(self, x):\n        x, feature_transform = self.feature_extract(x)\n\n        x = x.transpose(2, 1)\n        x = F.relu(self.bn_1(self.conv_1(x)))\n        x = F.relu(self.bn_2(self.conv_2(x)))\n        x = F.relu(self.bn_3(self.conv_3(x)))\n\n        x = self.conv_4(x)\n        x = x.transpose(2, 1)\n\n        return F.log_softmax(x, dim=-1), feature_transform\n</code></pre> <pre><code># num_classes = 5\nmodel_cls = PointNet_Classification(5).cuda()\nmodel_seg = PointNet_Segmentation(5).cuda()\n# input: x: B * n * c\nx = torch.rand(10, 60, 3).cuda()\n# output: y: B * num_classes, A: B * 64 * 64\nclass_result, A_class = model_cls(x)\n# output: y: B * n * num_classes, A: B * 64 * 64\nseg_result, A_seg = model_seg(x)\n</code></pre>"},{"location":"cs294173/pointcloud.html#dgcnn","title":"DGCNN","text":"<p>Dynamic Graph CNN for Learning on Point Clouds [Wang et al. 2019]<sup>2</sup> github (author): https://github.com/WangYueFt/dgcnn</p> <p></p> <p>PointNet-like models treat points independently to maintain permutation invariance (Conv1D parallel on each point). However, such operations cannot capture local geometric structure. </p> <p>DGCNN follows the point net approach and uses point cloud input. On top of PointNet systems, it designs a new EdgeConv module. EdgeConv helps to capture local geometric features while maintain permutation invariance. </p>"},{"location":"cs294173/pointcloud.html#edgeconv","title":"EdgeConv","text":"<p>The networks should input a point cloud \\(P = \\{p_1,...,p_n\\}\\) of \\(n\\) points, where each point \\(p_i\\in\\mathbb R^c\\), typically \\(c=3\\) representing the xyz coordinates. As in the pointnet system, each subsequenct layer computes some point-wise features in different dimensions, i.e. For , each layer \\(h_i\\) accepts point embeds in some feature dimension \\(X_{i-1} = \\{\\mathbf x_{i-1,1},...,\\mathbf x_{i-1,n}\\} \\subseteq \\mathbb R^{c_{i-1}}\\) and outputs in some new feature dimension \\(X_{i} = \\{\\mathbf x_{i,1},...,\\mathbf x_{i,n}\\} \\subseteq \\mathbb R^{c_{i}}\\).</p>"},{"location":"cs294173/pointcloud.html#edge-features","title":"Edge Features","text":"<p>To capture local point structure, the simplest case is to construct a directed kNN graph \\(G = (V, E)\\) so that each point can get some geometric information from its local neighbors. In order to get information from the point itself, the graph will include self-loop \\((\\mathbf x_i, \\mathbf x_i)\\in E\\). On such graph \\(G\\), we define edge feature \\(\\mathbf{e}_{ij} = h_\\Theta(\\mathbf x_i, \\mathbf x_j)\\) where \\(h_\\Theta: \\mathbb R^{c_0}\\times \\mathbb R^{c_0}\\rightarrow \\mathbb R^{c_1}\\) is a nonlinear function with learnable parameters \\(\\Theta\\). </p> <p>Then, we need to aggregate such edge features to each point \\(\\mathbf x_i\\) in order to pass to next layer. Therefore, we have the EdgeConv module as </p> \\[\\mathbf x_{i} = \\mathbf ?_{j:(i,j)\\in E} h_\\Theta(\\mathbf x_i, \\mathbf x_j)\\] <p>Note that this operation is similar to standard 2D convolution for images. For a \\(3\\times 3\\) 2D conv filter, it aggregates information from itself and 8 neighboring pixel points, i.e. a 9 near neighoring graph. </p>"},{"location":"cs294173/pointcloud.html#choices-of-functions","title":"Choices of functions","text":"<p>Note that if we choose \\(h_\\Theta(\\mathbf x_i, \\mathbf x_j) = h_\\Theta(\\mathbf x_i)\\), the graph actually constructs 1NN graph, and is exactly what PointNet did. </p> <p>Some of choices including Gaussian kernel convolution following the PCNN: [Atzmon, Maron, &amp; Lipman, 2018]</p> \\[x_{im} = \\sum_j h_\\theta(x_j)g(d_{x_i. x_j})\\] <p>Another choice is to only consider local information, i.e. \\(h_\\Theta(\\mathbf x_j - \\mathbf x_i)\\), while it loses global structure as for each point \\(h(\\mathbf x_i, \\mathbf x_i) = h(0)\\) and loses global structure. </p> <p>Therefore, the paper proposes EdgeConv as </p> \\[h_\\Theta (\\mathbf x_i, \\mathbf x_j) = \\tilde{h_\\theta}(\\mathbf x_i, \\mathbf x_j - \\mathbf x_i)\\] <p>more specifically, </p> \\[e_{ij_m} = \\text{ReLU}(\\theta_m \\cdot (\\mathbf x_j - \\mathbf x_i) + \\phi_m \\mathbf x_i)\\] <p>Finally, we need to aggregate them together, i.e. the choice of \\(\\mathbf ?\\), the paper uses channel-wise \\(\\max\\), i.e. </p> \\[\\mathbf x_{im} = \\max_{j:(i,j)\\in E} e_{ij_m}\\]"},{"location":"cs294173/pointcloud.html#properties-of-point-cloud","title":"Properties of Point Cloud","text":"<p>In EdgeConv, \\(\\max\\) used in kNN and aggregration is a symmetric operation, and the module is permutation invariant. </p> <p>The operation is partially translation invariant, as the \\(\\mathbf x_j - \\mathbf x_i\\) part is invariant to translation. </p> <p>For the \\(\\mathbf x_i\\) part, we can use a similar T-net structure in PointNet. (Problems here since the author's TF implementation is not the same as mentioned in the paper issue , and the module is missing in pytorch implementation issue)</p>"},{"location":"cs294173/pointcloud.html#dynamic-graph-update","title":"Dynamic Graph Update","text":"<p>Note that the kNN is applied on each feature space from each layer. As quoted by the authors, </p> <p>Our experiments suggest that it is beneficial to recompute the graph using nearest neighbors in the feature space produced by each layer. This is a crucial distinction of our method from graph CNNs working on a fixed input graph. Such a dynamic graph update is the reason for the name of our architecture, the Dynamic Graph CNN (DGCNN). With dynamic graph updates, the receptive field is as large as the diameter of the point cloud, while being sparse.</p> Code <pre><code>\"\"\"\nThe EdgeConv operation as implemented by the author\nhttps://github.com/WangYueFt/dgcnn/blob/master/pytorch/model.py\n\"\"\"\n\ndef knn(x, k):\n    inner = -2*torch.matmul(x.transpose(2, 1), x)\n    xx = torch.sum(x**2, dim=1, keepdim=True)\n    pairwise_distance = -xx - inner - xx.transpose(2, 1)\n\n    idx = pairwise_distance.topk(k=k, dim=-1)[1]   # (batch_size, num_points, k)\n    return idx\n\n\ndef get_graph_feature(x, k=20, idx=None):\n    batch_size = x.size(0)\n    num_points = x.size(2)\n    x = x.view(batch_size, -1, num_points)\n    if idx is None:\n        idx = knn(x, k=k)   # (batch_size, num_points, k)\n    device = torch.device('cuda')\n\n    idx_base = torch.arange(0, batch_size, device=device).view(-1, 1, 1)*num_points\n\n    idx = idx + idx_base\n\n    idx = idx.view(-1)\n\n    _, num_dims, _ = x.size()\n\n    x = x.transpose(2, 1).contiguous()   # (batch_size, num_points, num_dims)  -&gt; (batch_size*num_points, num_dims) #   batch_size * num_points * k + range(0, batch_size*num_points)\n    feature = x.view(batch_size*num_points, -1)[idx, :]\n    feature = feature.view(batch_size, num_points, k, num_dims) \n    x = x.view(batch_size, num_points, 1, num_dims).repeat(1, 1, k, 1)\n\n    feature = torch.cat((feature-x, x), dim=3).permute(0, 3, 1, 2).contiguous()\n\n    return feature\n</code></pre> <ol> <li> <p>Qi, C.R., Su, H., Mo, K., and Guibas, L.J. 2017. Pointnet: Deep learning on point sets for 3d classification and segmentation. Proceedings of the IEEE conference on computer vision and pattern recognition, 652\u2013660.\u00a0\u21a9</p> </li> <li> <p>Wang, Y., Sun, Y., Liu, Z., Sarma, S.E., Bronstein, M.M., and Solomon, J.M. 2019. Dynamic graph cnn for learning on point clouds. ACM Transactions on Graphics (tog) 38, 5, 1\u201312.\u00a0\u21a9</p> </li> </ol>"},{"location":"cs294173/rotations.html","title":"Pose Parameterization and Optimization","text":"<p>Note based on A tutorial on SE(3) transformation parameterizations and on-manifold optimization [Blanco-Claraco 2022]<sup>1</sup> and SE(3) Geometry and Kinematics</p> <p>In this note, unless specified, we are only interested in transformations in 3D. </p>"},{"location":"cs294173/rotations.html#parametrize-rotations","title":"Parametrize Rotations","text":"<p>The parameterization conventions are based on <code>three.js</code> implementations. </p> <p>A pose is just a rotation \\(\\mathbf R\\) and a translation \\(\\mathbf t = (x, y, z)\\), but the rotation can be parameterized in multiple ways with different pros and cons. </p>"},{"location":"cs294173/rotations.html#euler-angles","title":"Euler Angles","text":"<p>The rotation is parameterized by 3 Euler angles \\(\\mathbf r = (r_x, r_y, r_z)\\) and the rotation axis order (e.x. <code>xyz</code>, <code>roll-pitch-yaw</code>). Which means rotate around the first axis, and then the modified second axis, and finally the modified third axis. </p> <p>However, euler angle degenerates, a.k.a. one possible rotation can be represented by two different set of Euler angles. In practice, optimizations directly on Euler angle representation is not a good idea due to the rotation order and the degeneration problem. However, euler angle representation is easy to interpret, hence used as user input.</p>"},{"location":"cs294173/rotations.html#quaternions","title":"Quaternions","text":"<p>The rotation is parameterized by normalized 4D vector \\((q_x, q_y, q_z, q_r)\\). A geometric interpretation is that of a rotation of \\(\\theta\\) radians around the axis \\(\\mathbf v \\propto (q_x, q_y, q_z)\\), where the angle \\(\\theta\\) is \\(q_r = \\cos(\\frac{\\theta}{2})\\). </p> <p>Note that quaternions expects to be normalized, hence it also has only 3 degree of freedom despite of being 4D. </p>"},{"location":"cs294173/rotations.html#rotation-matrices","title":"Rotation Matrices","text":"<p>The speicial orthogonal group \\(SO(3)\\) is the set of all matrices \\(\\mathbf{R}\\) that is orthogonal (\\(\\mathbf{R}^T\\mathbf{R} = \\mathbf{RR}^T = I\\) or \\(\\mathbf{R}^{-1}=\\mathbf{R}\\), orthogonal also implies invertible) and \\(\\det(R)= 1\\). Matrices in \\(SO(3)\\) are rotation matrices, which are isometric (the transformation preserves distance).</p> <p>The special Euclidean group \\(SE(3)\\) is the set of all rigid transformation matrix in the form </p> \\[\\mathbf{T} = \\begin{pmatrix}&amp;&amp;&amp;t_x\\\\&amp;\\mathbf{R}&amp;&amp;t_y\\\\&amp;&amp;&amp;t_z\\\\0&amp;0&amp;0&amp;1\\end{pmatrix}\\] <p>which is the homogeneous transformation of \\(\\mathbf{R}\\mathbf x + \\mathbf t\\).</p>"},{"location":"cs294173/rotations.html#conversions","title":"Conversions","text":""},{"location":"cs294173/rotations.html#euler-angles-to-quaternions","title":"Euler Angles to Quaternions","text":"<p>Given Euler angle \\((r_x, r_y, r_z)\\) and the order <code>XYZ</code>, the quaternion is </p> \\[\\begin{align*} q_x &amp;= \\sin(r_x/2) \\sin(r_y/2) \\sin(r_z/2) + \\cos(r_x/2) \\sin(r_y/2) \\sin(r_z/2)\\\\ q_y &amp;= \\cos(r_x/2) \\sin(r_y/2) \\cos(r_z/2) - \\sin(r_x/2) \\cos(r_y/2) \\sin(r_z/2)\\\\ q_z &amp;= \\cos(r_x/2) \\cos(r_y/2) \\sin(r_z/2) + \\sin(r_x/2) \\sin(r_y/2) \\cos(r_z/2)\\\\ q_r &amp;= \\cos(r_x/2) \\cos(r_y/2) \\cos(r_z/2) - \\sin(r_x/2) \\sin(r_y/2) \\sin(r_z/2)\\\\ \\end{align*}\\] Quaternion.setFromEuler <pre><code>setFromEuler(euler) {\n  const cos = Math.cos;\n  const sin = Math.sin;\n\n  const c1 = cos( x / 2 );\n  const c2 = cos( y / 2 );\n  const c3 = cos( z / 2 );\n\n  const s1 = sin( x / 2 );\n  const s2 = sin( y / 2 );\n  const s3 = sin( z / 2 );\n\n  switch ( order ) {\n\n    case 'XYZ':\n      this._x = s1 * c2 * c3 + c1 * s2 * s3;\n      this._y = c1 * s2 * c3 - s1 * c2 * s3;\n      this._z = c1 * c2 * s3 + s1 * s2 * c3;\n      this._w = c1 * c2 * c3 - s1 * s2 * s3;\n      break;\n\n    // Other order see original code\n    ...\n  }\n  return this;\n}\n</code></pre>"},{"location":"cs294173/rotations.html#euler-angles-to-matrix","title":"Euler Angles to Matrix","text":"<p>Since applying a rotation transformation is equivalent to right-side multiplication of a rotation matrix, as of the Euler angle rotation definition, it can be composed of 3 matrices. </p> \\[ \\mathbf{R}_X(\\theta) = \\begin{bmatrix}1&amp;0&amp;0\\\\0&amp;\\cos\\theta&amp;-\\sin\\theta\\\\0&amp;\\sin\\theta&amp;\\cos\\theta\\end{bmatrix} \\mathbf{R}_Y(\\theta) = \\begin{bmatrix}\\cos\\theta&amp;0&amp;\\sin\\theta\\\\0&amp;1&amp;0\\\\-\\sin\\theta&amp;0&amp;\\cos\\theta\\end{bmatrix} \\mathbf{R}_Z(\\theta) = \\begin{bmatrix}\\cos\\theta&amp;-\\sin\\theta&amp;0\\\\\\sin\\theta&amp;\\cos\\theta&amp;0\\\\0&amp;0&amp;1\\end{bmatrix} \\] \\[\\mathbf R(r_x, r_y, r_z) = \\mathbf{R}_Z(r_z)\\mathbf{R}_Y(r_y)\\mathbf{R}_X(r_x)\\] Matrix4.makeRotationFromEuler <pre><code>makeRotationFromEuler(euler) {\n\n  const te = this.elements;\n\n  const x = euler.x, y = euler.y, z = euler.z;\n  const a = Math.cos(x), b = Math.sin(x);\n  const c = Math.cos(y), d = Math.sin(y);\n  const e = Math.cos(z), f = Math.sin(z);\n\n  switch ( order ) {\n\n    case 'XYZ':\n\n      const ae = a * e, af = a * f, be = b * e, bf = b * f;\n\n      te[0] = c * e;\n      te[4] = - c * f;\n      te[8] = d;\n\n      te[1] = af + be * d;\n      te[5] = ae - bf * d;\n      te[9] = - b * c;\n\n      te[2] = bf - ae * d;\n      te[6] = be + af * d;\n      te[10] = a * c;\n\n      break;\n\n    // Other order see original code\n    ...\n  } \n\n  return this;\n}\n</code></pre>"},{"location":"cs294173/rotations.html#quaternion-to-matrix","title":"Quaternion to Matrix","text":"\\[R(q_x, q_y, q_z, q_r) = \\begin{bmatrix} q_r^2 + q_x^2 - q_y^2 - q_z^2 &amp; 2(q_xqy - q_r q_z) &amp; 2(q_zq_x + q_rq_y)\\\\ 2(q_xq_y + q_rq_z) &amp; q_r^2 - q_x^2 + q_y^2 - q_z^2 &amp; 2(q_yq_z - q_rq_x)\\\\ 2(q_zq_x - q_rq_y) &amp; 2(q_yq_z + q_rq_x) &amp; q_r^2 - q_x^2 - q_y^2 + q_z^2 \\end{bmatrix}\\] Matrix4.makeRotationFromQuaternion <pre><code>makeRotationFromQuaternion(q) {\n  const te = this.elements;\n\n  const x = q.x, y = q.y, z = q.z, w = q.w;\n  const x2 = x + x, y2 = y + y, z2 = z + z;\n\n  // careful that xx = 2 * x * x instead of x * x\n  const xx = x * x2, xy = x * y2, xz = x * z2;\n  const yy = y * y2, yz = y * z2, zz = z * z2;\n  const wx = w * x2, wy = w * y2, wz = w * z2;\n\n  // q is normalized x*x+y*y+z*z+r*r = 1\n  te[ 0 ] = ( 1 - ( yy + zz ) );\n  te[ 1 ] = ( xy + wz );\n  te[ 2 ] = ( xz - wy );\n\n  te[ 4 ] = ( xy - wz );\n  te[ 5 ] = ( 1 - ( xx + zz ) );\n  te[ 6 ] = ( yz + wx );\n\n  te[ 8 ] = ( xz + wy );\n  te[ 9 ] = ( yz - wx );\n  te[ 10 ] = ( 1 - ( xx + yy ) );\n}\n</code></pre>"},{"location":"cs294173/rotations.html#matrix-to-quaternion","title":"Matrix to Quaternion","text":"<p>In general, the matrix can be converted into quaternion as </p> \\[\\begin{align*} q_x &amp;= \\frac{R_{32}-R_{23}}{4q_r}\\\\ q_y &amp;= \\frac{R_{13}-R_{31}}{4q_r}\\\\ q_z &amp;= \\frac{R_{21}-R_{12}}{4q_r}\\\\ q_r &amp;= \\frac{\\sqrt{1 + R_{11} +R_{22} + R_{33}}}{2} \\end{align*}\\] <p>Notice that if \\(tr(R) = R_{11} +R_{22} + R_{33} &lt; -1\\) then the formulas is invalid. Therefore, we need to identify which major diagonal element has the greatest value. </p> <p>Therefore, it's divided into 4 cases. </p> <ul> <li>The trace is good. Then we use the formula above</li> <li>\\(R_{11}\\) is the largest, then</li> </ul> \\[\\begin{align*}     q_x &amp;= \\frac{\\sqrt{1 + R_{11} - R_{22} - R_{33}}}{2} \\\\     q_y &amp;= \\frac{R_{12} + R_{21}}{4q_x}\\\\     q_z &amp;= \\frac{R_{13} + R_{31}}{4q_x}\\\\     q_r &amp;= \\frac{R_{32} - R_{23}}{4q_x} \\end{align*}\\] <ul> <li>\\(R_{22}\\) is the largest, then</li> </ul> \\[\\begin{align*}     q_x &amp;= \\frac{R_{12} + R_{21}}{4q_y} \\\\     q_y &amp;= \\frac{\\sqrt{1 + R_{11} - R_{22} - R_{33}}}{2} \\\\     q_z &amp;= \\frac{R_{23} + R_{32}}{4q_y}\\\\     q_r &amp;= \\frac{R_{13} - R_{31}}{4q_y} \\end{align*}\\] <ul> <li>\\(R_{33}\\) is the largest, then</li> </ul> \\[\\begin{align*}     q_x &amp;= \\frac{R_{13} + R_{31}}{4q_z}\\\\     q_y &amp;= \\frac{R_{23} + R_{32}}{4q_z}\\\\     q_z &amp;= \\frac{\\sqrt{1 + R_{11} - R_{22} - R_{33}}}{2} \\\\     q_r &amp;= \\frac{R_{21} - R_{12}}{4q_z} \\end{align*}\\] Quaternion.setFromRotationMatrix <pre><code>setFromRotationMatrix( m: Matrix4 ) {\n  // Assume m is 4x4 transformation matrix and\n  // the upper 3x3 of m is a pure rotation matrix (i.e, unscaled)\n\n  const te = m.elements,\n    m11 = te[ 0 ], m12 = te[ 4 ], m13 = te[ 8 ],\n    m21 = te[ 1 ], m22 = te[ 5 ], m23 = te[ 9 ],\n    m31 = te[ 2 ], m32 = te[ 6 ], m33 = te[ 10 ],\n    trace = m11 + m22 + m33;\n\n  if ( trace &gt; 0 ) {\n\n    const s = 0.5 / Math.sqrt( trace + 1.0 );\n\n    this._w = 0.25 / s;\n    this._x = ( m32 - m23 ) * s;\n    this._y = ( m13 - m31 ) * s;\n    this._z = ( m21 - m12 ) * s;\n\n  } else if ( m11 &gt; m22 &amp;&amp; m11 &gt; m33 ) {\n\n    const s = 2.0 * Math.sqrt( 1.0 + m11 - m22 - m33 );\n\n    this._w = ( m32 - m23 ) / s;\n    this._x = 0.25 * s;\n    this._y = ( m12 + m21 ) / s;\n    this._z = ( m13 + m31 ) / s;\n\n  } else if ( m22 &gt; m33 ) {\n\n    const s = 2.0 * Math.sqrt( 1.0 + m22 - m11 - m33 );\n\n    this._w = ( m13 - m31 ) / s;\n    this._x = ( m12 + m21 ) / s;\n    this._y = 0.25 * s;\n    this._z = ( m23 + m32 ) / s;\n\n  } else {\n\n    const s = 2.0 * Math.sqrt( 1.0 + m33 - m11 - m22 );\n\n    this._w = ( m21 - m12 ) / s;\n    this._x = ( m13 + m31 ) / s;\n    this._y = ( m23 + m32 ) / s;\n    this._z = 0.25 * s;\n\n  }\n\n  return this;\n\n}\n</code></pre>"},{"location":"cs294173/rotations.html#operations-on-rotations","title":"Operations on Rotations","text":"<p>For most operations, again we only consider quaternions and matrices. </p>"},{"location":"cs294173/rotations.html#pose-composition","title":"Pose Composition","text":"<p>For compositing two transformation matrices, it's simply right-side multiplication \\(T_2 T_1\\). </p> <p>For quaternions, applying a rotation on a 3D point can be effectively applied by first converting to a matrix, and then matrix multiplication. Therefore, the only useful operation is to composing two quaternions rotations. </p> <p>For quaternions \\(\\mathbf{q}_1, \\mathbf{q}_2\\), its multiplication is defined as </p> \\[\\mathbf{q}_1 \\cdot \\mathbf{q}_2 = \\begin{pmatrix} q_x = q_{r1}q_{x2} + q_{r2}q_{x1} + q_{y1}q_{z2} - q_{y2}q_{z1}\\\\ q_y = q_{r1}q_{y2} + q_{r2}q_{y1} + q_{z1}q_{x2} - q_{z2}q_{x1}\\\\ q_z = q_{r1}q_{z2} + q_{r2}q_{z1} + q_{x1}q_{y2} - q_{x2}q_{y1}\\\\ q_r = q_{r1}q_{r2} - q_{x1}q_{x2} - q_{y1}q_{y2} - q_{z1}q_{z2} \\end{pmatrix}\\] Quaternion.multiplyQuaternions <pre><code>multiplyQuaternions( a, b ) {\n\n  // from http://www.euclideanspace.com/maths/algebra/realNormedAlgebra/quaternions/code/index.htm\n\n  const qax = a._x, qay = a._y, qaz = a._z, qaw = a._w;\n  const qbx = b._x, qby = b._y, qbz = b._z, qbw = b._w;\n\n  this._x = qax * qbw + qaw * qbx + qay * qbz - qaz * qby;\n  this._y = qay * qbw + qaw * qby + qaz * qbx - qax * qbz;\n  this._z = qaz * qbw + qaw * qbz + qax * qby - qay * qbx;\n  this._w = qaw * qbw - qax * qbx - qay * qby - qaz * qbz;\n\n  return this;\n\n}\n</code></pre>"},{"location":"cs294173/rotations.html#inverse","title":"Inverse","text":"<p>For transformation matrix, the inverse is simply the inverse of the matrix, and we know that rotation matrices are orthogonal, hence the inverse is the transpose.</p> <p>For quaternions, thinks about the geometric interpretation. We can rotate the object along the opposite axis the same radians. Therefore, define the conjugate of the quaternion as \\(\\mathbf q^* = (-q_x, -q_y, -q_z, q_r)\\), and the conjugate is the inverse rotation. </p>"},{"location":"cs294173/rotations.html#pose-optimizations","title":"Pose Optimizations","text":"<p>The key problem for pose optimization is essentially find a smooth path s.t. the source camera pose \\(P_{s}\\) can be moved to the goal pose \\(P_d\\). </p> <p>Geometrically speaking, the path is a sequence of infinitely small delta transformation \\(T_0, T_1, T_2, ... \\in SE(3)\\), where each transformation \\(P_i = T_i T_{i-1}...T_0P_s\\) moves closer to and eventually become \\(P_d\\). </p>"},{"location":"cs294173/rotations.html#lie-groups","title":"Lie Groups","text":"<p>Let \\(G\\) be a set of elements and consider a binary operation (often understood as multiplication). For any two elements \\(A,B\\in G\\), denote the result of the multiplication as \\(AB\\). Then, \\(G\\) plus the operation is a group if the following properties are satisfied</p> <ul> <li>Closure \\(\\forall A, B\\in G. AB\\in G\\)</li> <li>Associativity \\(\\forall A,B,C\\in G. (AB)C=A(BC)\\)</li> <li>Identity \\(\\exists I\\in G. \\forall A\\in G. IA=AI=A\\)</li> <li>Inverse \\(\\forall A\\in G. \\exists A^{-1}\\in G. AA^{-1} = A^{-1} A = I\\)</li> </ul> <p>A manifold \\(M\\) is a topological space s.t. \\(\\forall p\\in M\\), its neighborhood is a homeomorphism. Intuitively, it means that for any point on the manifold, there's an infinitely small neighborhood that is \"flat\". </p> <p>Consider a paper, the paper is a 2D manifold embedded in 3D space. However, if we fold the paper to form a \"sharp\" edge, it's no longer a manifold. </p> <p>Denote the tangent space of manifold \\(M\\) at point \\(p\\) as \\(T_pM\\) can be intuitively defined as the vector space of the derivatives at \\(p\\) of all possible smooth curves on \\(M\\) that pass through \\(p\\). </p> <p>Finally, a Lie group \\(G\\subset \\mathbb R^n\\) is  - a group  - a manifold - the multiplication operation and inverse operation are both smooth. </p>"},{"location":"cs294173/rotations.html#se3-and-so3-as-lie-groups","title":"SE(3) and SO(3) as Lie Groups","text":"<p>For our purpose, we notice that SE(3) is a 6D manifold (3 degree of freedom for rotation and 3 for translation) since </p> <ul> <li>The product of 2 SE(3) matrices is still in SE(3). </li> <li>The products of SE(3), a.k.a. matrix multiplications, are associative</li> <li>Exists the identity matrix \\(I:=[\\mathbf I_3 \\mid \\mathbf 0_3]\\)</li> <li>Exists inverse transformation \\([\\mathbf R\\mid \\mathbf t]^{-1} = [\\mathbf R^T\\mid -\\mathbf R^T \\mathbf t]\\)</li> <li>The product and the inverse are continuous w.r.t. each of the element.</li> </ul> <p>Similarly, \\(SO(3)\\) is also a 3D manifold and another Lie group. </p> <p>Intuitively, we confirm that the pose optimization problem is an optimization over a smooth manifold. </p>"},{"location":"cs294173/rotations.html#lie-algebra","title":"Lie Algebra","text":"<p>A Lie algebra is a vector space \\(V\\) over some field \\(F\\) with a binary operation \\([\\cdot, \\cdot]\\) (called a Lie bracket) associated with every Lie group. The Lie bracket satisfies that \\(\\forall X, Y, Z \\in V, \\forall a, b\\in F\\)</p> <ul> <li>Closure \\([X, Y] \\in V\\)</li> <li>Bi-linearity \\([aX+bY, z] = a[X,Z] + b[Y,Z], [Z,aX+bY] = a[Z,X]+b[Z,Y]\\)</li> <li>Alternating \\([X, X] = 0\\)</li> <li>Jacobi identity \\([X,[Y,Z]] + [Y,[Z,X]] + [Z,[X,Y]] = 0\\)</li> </ul> <p>An exponential map maps elements from the Lie algebra to the Lie group (manifold) and determines the local structure of the manifold, and a logarithm map maps elements from the manifold to the algebra. </p>"},{"location":"cs294173/rotations.html#lie-algebra-of-so3","title":"Lie Algebra of SO(3)","text":"<p>Note that for rotations, local changes can be well-represented by the Euler angles. Therefore, let the Lie algebra of SO(3) be the space of </p> \\[so(3) := \\{\\mathbf{r}_{\\times} \\in\\mathbb R^{3\\times 3} | \\mathbf r\\in\\mathbb R^{3}\\}\\] <p>the exponential map is </p> \\[\\mathbf R = \\exp(\\mathbf r_{\\times}) = \\sum_{k=0}^\\infty \\frac{1}{k!}\\mathbf r_{\\times}^k = I + \\mathbf r_{\\times} + \\frac{1}{2!} \\mathbf r_{\\times}^2 +  \\frac{1}{3!}\\mathbf r_{\\times}^3 + \\cdots\\] <p>Rodrigues Formula gives the closed-form expression</p> \\[\\mathbf R = \\mathbf I_3 + \\frac{\\sin \\|\\mathbf r\\|}{\\|\\mathbf r\\|}\\mathbf r_{\\times} + \\frac{1 - \\cos \\|\\mathbf r\\|}{\\|\\mathbf r\\|^2}\\mathbf r_{\\times}^2\\] <p>Therefore, any local linear change to \\(\\mathbf r\\) can be mapped to rotation matrices \\(\\mathbf R\\). Practically, we can parameterize the delta transformation as 6D vector \\((\\mathbf r, \\mathbf t)\\) and optimize them separately.</p> exp_map_SO3 <pre><code>def skew_sym_mat(r):\n  \"\"\" take a 3D vector, return its skew symmetric matrix \n  \"\"\"\n  r_x, r_y, r_z = r\n  return torch.tensor([\n    [   0, -r_z,  r_y],\n    [ r_z,    0, -r_x],\n    [-r_y,  r_x,   0]\n  ])\n\ndef exp_map_SO3(r):\n  \"\"\" take a so(3) tangent vector r, return SO(3) rotation matrix R\n  \"\"\"\n\n  r_skew = skew_sym_mat(r)\n  r_norm = torch.norm(r)\n  r_norm_inv = 1.0 / r_norm\n\n  R = torch.eye(3)\n  R += r_norm_inv * r_norm.sin() * r_skew\n  R += r_norm_inv * r_norm_inv * (1.0 - r_norm.cos()) * (r_skew @ r_skew)\n  return R\n</code></pre>"},{"location":"cs294173/rotations.html#lie-algebra-of-se3","title":"Lie Algebra of SE(3)","text":"<p>Alternatively, we jointly optimize 6 parameters over the 6D manifold. </p> \\[se(3) := \\big\\{\\xi_{\\times} = \\begin{bmatrix}\\mathbf{r}_{\\times} &amp;\\mathbf t\\\\\\mathbf 0&amp;0\\end{bmatrix} \\in\\mathbb R^{4\\times 4} | \\xi = (\\mathbf r,\\mathbf t)\\in\\mathbb R^{6}\\big\\}\\] <p>The exponential map and Rodrigues formula is given as </p> \\[T = \\exp(\\xi_{\\times}) = \\sum_{k=0}^\\infty \\frac{1}{k!} \\xi_{\\times}^k = \\mathbf I_4 + \\xi_{\\times} + \\frac{1-\\cos(\\|\\mathbf r\\|)}{\\|\\mathbf r\\|^2}\\xi_{\\times}^2 + \\frac{\\|\\mathbf r\\|-\\sin(\\|\\mathbf r\\|)}{\\|\\mathbf r\\|^3}\\xi_{\\times}^3\\] <p>Note that the exponential map is consisted of the rotational exponential map and a translation</p> \\[\\begin{align*}\\exp(\\xi_{\\times}) &amp;= \\begin{bmatrix} \\exp(\\mathbf r_{\\times}) &amp; J_L(\\mathbf r) \\mathbf t \\\\\\mathbf 0 &amp; 1\\end{bmatrix}\\\\ J_L(\\mathbf r) &amp;= \\sum_{k=0}^\\infty \\frac{1}{(k+1)!}\\mathbf r_{\\times}^k = \\mathbf I_3 + \\frac{1 - \\cos(\\|\\mathbf r\\|)}{\\|\\mathbf r\\|^2}\\mathbf r_{\\times} + \\frac{\\|\\mathbf r\\| - \\sin(\\|\\mathbf r\\|)}{\\|\\mathbf r\\|^3} \\mathbf r_{\\times}^2 \\end{align*}\\] exp_map_SE3 <pre><code>def left_jacobian(r):\n  \"\"\" take a so(3) tangent vector r, return \n  the left jacobian matrix\n  \"\"\"\n  J = torch.eye(3)\n  r_norm = torch.norm(r)\n  r_norm_inv = 1.0 / r_norm\n  r_skew = skew_sym_mat(r)\n\n  J += r_norm_inv ** 2 * (1.0 - r_norm.cos()) * r_skew\n  J += r_norm_inv ** 3 * (r_norm - r_norm.sin()) * (r_skew @ r_skew)\n  return J\n\n\ndef exp_map_SE3(xi):\n  \"\"\" take a se(3) tangent vector xi = (r, t), return SE(3) transformation matrix T\n  \"\"\"\n  r = xi[:3]\n  t = xi[3:]\n  T = torch.eye(4)\n  T[:3, :3] = exp_map_SO3(r)\n  J = left_jacobian(r)\n  T[:3, 3] = J @ t\n  return T\n</code></pre> <ol> <li> <p>Blanco-Claraco, J.L. 2022. A tutorial on SE(3) transformation parameterizations and on-manifold optimization. https://arxiv.org/abs/2103.15980.\u00a0\u21a9</p> </li> </ol>"},{"location":"cs294173/sdf_nerf.html","title":"Surface based Volume Rendering","text":"<p>SDF Studio Github Repo is a unified framework for SDF-based volume rendering. </p>"},{"location":"cs294173/sdf_nerf.html#sdf-based-volume-rendering","title":"SDF based Volume Rendering","text":"<p>The works are aiming to solve the problem that given a set of images \\(\\{I_k\\}\\) of a 3D object, aim to output the surface \\(S\\) as a level-set of some field \\(f: \\mathbb R^3\\rightarrow \\mathbb R\\). \\(f\\) can be SDF field which maps position \\(\\mathbf x\\) to signed distance to the closest surface, or an occupancy field that maps \\(\\mathbf x\\) to a probability \\([0,1]\\) s.t. \\(\\mathbf x\\) is inside of outside of the volume. </p>"},{"location":"cs294173/sdf_nerf.html#neus","title":"NeuS","text":"<p>NeuS: Learning Neural Implicit Surfaces by Volume Rendering for Multi-view Reconstruction [Wang et al. 2021]<sup>1</sup></p> <p>Represents a scene as SDF field \\(f_\\theta(\\mathbf x)\\) and radiance field \\(c(\\mathbf x, \\mathbf d)\\). Both approximated by separate MLPs.</p> <p>Note that volume rendering is modelled by </p> \\[C(\\mathbf r) = \\int_{t_n}^{t_f} T(t)\\sigma(\\mathbf r(t)) \\mathbf c(\\mathbf r(t), \\mathbf d)dt\\] <p>where \\(T\\) is the accumulated transmittance and \\(\\sigma: \\mathbb R^3 \\rightarrow [0,1]\\) is the volume density (estimated from MLP in NeRF). Then, the issue is that how do we get \\(\\sigma\\) from SDF. The idea is to uses a logistic density distribution to map the signed distance \\(\\mathbb R\\rightarrow [0, 1]\\), i.e.</p> \\[\\sigma(\\mathbf x) = \\phi_s(f_\\theta(\\mathbf x)) = \\phi_s(x) = \\frac{se^{-sx}}{(1+e^{-sx})^2}\\] <p>However, in this case \\(T(\\mathbf x)\\sigma(\\mathbf x)\\) is biased to positive SDF value instead of the zero-level set, thus we should re-design the density \\(\\rho(t)\\) s.t. </p> \\[T(t) = \\exp(-\\int_0^t \\rho(u)du), T(t)\\rho(t)\\] <p>With some derivation (see the paper), we have that </p> \\[\\rho(t) = \\max(\\frac{-d_t\\Phi_s(f(\\mathbf r(t)))}{\\Phi_s(f(\\mathbf r(t)))}, 0)\\] <p>where \\(\\Phi_s\\) is the cdf of logistic distribution. </p>"},{"location":"cs294173/sdf_nerf.html#volsdf","title":"VolSDF","text":"<p>Volume Rendering of Neural Implicit Surfaces [Yariv et al. 2021]<sup>2</sup></p> <p>In VolSDF, the volume density is represented from SDF as </p> \\[\\sigma(\\mathbf x) = \\alpha \\Psi_\\beta(f(\\mathbf x)), \\Psi_\\beta(s) = \\begin{cases}\\frac{\\exp(s/\\beta)}{2}&amp;s\\leq 0\\\\1-\\frac{\\exp(-s/\\beta)}{2}&amp;s&gt;0\\end{cases}\\] <p>where \\(\\Psi_\\beta\\) is the CDF of Laplace distribution with \\(0\\) mean and \\(\\beta\\) scale. and \\(\\alpha &gt; 0, \\beta &gt; 0\\) are learnable parameters.</p> <p>VolSDF proposes a novel sampling method based on SDF, that requires significantly smaller sampling points for each ray. </p> <p>Given \\(\\beta\\) and a wanted error bound \\(\\epsilon\\) (typically \\(\\epsilon = 0.1\\)) on the opacity. The algorithm aims to get a set of sample from \\([0, M]\\) (near, far) such that the opacity approximation error is bounded by \\(\\epsilon\\). The detailed algorithm and proofs are provided in the paper. </p>"},{"location":"cs294173/sdf_nerf.html#unisurf","title":"UniSurf","text":"<p>UNISURF: Unifying Neural Implicit Surfaces and Radiance Fields for Multi-View Reconstruction [Oechsle et al. 2021]<sup>3</sup></p> <p>UniSurf uses occupancy field \\(o\\) s.t. \\(o(\\mathbf x)=0\\) when outside of the volume and \\(o(\\mathbf x)=1\\) when inside of the volume. Note that the surface normal \\(\\mathbf n(\\mathbf x) = \\nabla_{\\mathbf x} o / \\|\\nabla_{\\mathbf x} o\\|\\). Then, we replace volume density \\(\\sigma\\) with occupancy and estimate color \\(\\mathbf c\\) from \\(\\mathbf x, \\mathbf d, \\mathbf n(\\mathbf x)\\) and an additional feature vector \\(\\mathbf h(\\mathbf x)\\). </p> <p>UniSurf also gives a surface consistency regularization</p> \\[\\mathcal L_{reg} = \\sum_{\\mathbf x_x \\in\\mathcal S} = \\|\\mathbf n(\\mathbf x_s) - \\mathbf n(\\mathbf x_s + \\epsilon)\\|_2\\]"},{"location":"cs294173/sdf_nerf.html#additional-supervisions","title":"Additional Supervisions","text":"<p>With SDF, the level set is more concentrated onto a surface, which is beneficial for estimating normal and depth. Therefore, many methods have looked at additional supervision terms designated for various purposes. </p> <p>The most commonly used losses for SDF based NeRF are</p> <p>The L1 RGB reconstruction loss, directly supervised by the actual pixel color in the image</p> \\[\\mathcal L_{rgb} = \\sum_{\\mathbf r} \\|\\hat C(\\mathbf r) - C(\\mathbf r)\\|_1\\] <p>The Eikonal Loss for SDF values of near-surface sampled points</p> \\[\\mathcal L_{eik} = \\sum_{\\mathbf x\\in\\mathcal X}(\\|\\nabla f_\\theta(\\mathbf x)\\|_2 - 1)^2\\]"},{"location":"cs294173/sdf_nerf.html#monosdf","title":"MonoSDF","text":"<p>MonoSDF: Exploring Monocular Geometric Cues for Neural Implicit Surface Reconstruction [Yu et al. 2022]<sup>4</sup></p> <p>MonoSDF incorporates deep learning based 2D depth and normal estimation. 2D monocular depth/normal estimation provides dense depth map and is continuous within the image. However, any photometric approach suffers from inconsistency across images. In other words, multiple images of the same scene from different angles cannot be well aligned by projecting the depth. </p> <p>MonoSDF proposes two losses to solve this problem. </p> <p>The depth consistency loss</p> \\[\\mathcal L_d = \\sum_{\\mathbf r} \\|(w\\hat D(\\mathbf r) + q) - \\bar D(\\mathbf r)\\|^2\\] <p>where \\(\\hat D(\\mathbf r) = \\sum_{i} T_ia_i t_i\\) (similar to \\(\\hat C\\), replace sampled point radiance directly with ray depth). \\(\\bar D\\) is the 2D depth. and \\(w, q\\) are scale and translation term for alignment. \\(w,q\\) is estimated per image (ray batch) using least-squares. </p> <p>The normal consistency loss</p> \\[\\mathcal L_{\\mathbf n} = \\sum_{\\mathbf r} \\|\\hat N(\\mathbf r) - \\bar N(\\mathbf r)\\|_1 + \\|1-\\hat N(\\mathbf r)\\cdot \\bar N(\\mathbf r)\\|_1\\] <p>where \\(\\hat N(\\mathbf r) = \\sum_{i} T_ia_i \\mathbf n_i\\) (\\(\\mathbf n_i\\) is given by the gradient of SDF at \\(t_i\\)), \\(\\bar N\\) is the 2D normal. The later term is used to transform the 2D normal to the same coordinate system of the 3D construction. </p>"},{"location":"cs294173/sdf_nerf.html#manhattan-sdf","title":"Manhattan SDF","text":"<p>Neural 3D Scene Reconstruction with the Manhattan-world Assumption [Guo et al. 2022]<sup>5</sup></p> <p>Instead of using normal consistency loss, ManhattanSDF specifically targets at indoor scenes with the Manhattan world assumption. i.e. the floor will always have \\(\\mathbf n_f = (0,0,1)\\) and the room will always rectangular so that the walls are perpendicular to the floor, and parallel or perpendicular to other walls. </p> <p>Therefore, the loss can be written as </p> \\[\\mathcal L_{\\text{floor}}(t_i) = \\|1-\\mathbf n_i \\cdot \\mathbf n_f\\|_1, \\mathcal L_{\\text{wall}}(t_i) = \\min_{i\\in\\{-1, 0, 1\\}}\\|i-\\mathbf n_i \\mathbf n_w\\|_1\\] <p>However, not pixel in the scene is wall or floor, and we only want to apply the geometric loss to certain pixels. The first idea is to use a 2D segmentation network to segment out walls and floors, and use it as a mask for the loss. Then, the problem is that 2D segmentation can have errors. Therefore, ManhattanSDF uses another MLP to predict the semantic logits, similar to the radiance MLP, and the pixel logits is \\(\\hat S(\\mathbf r) = \\sum_i T_ia_i s_i\\) and softmax the logits to probabilities \\((\\hat p_{\\text{floor}}, \\hat p_{\\text{wall}}, \\hat p_{\\text{other}})\\). Then the joint geometric loss is defined as </p> \\[\\mathcal L_{joint} = \\sum_{\\mathbf r} \\hat p_{\\text{floor}} \\mathcal L_{\\text{floor}} + \\hat p_{\\text{wall}} \\mathcal L_{\\text{wall}}\\] <p>To make sure the probabilities not vanish, adding a cross entropy loss to supervise the semantic MLP</p> \\[\\mathcal L_{s} = \\sum_{\\mathbf r} \\bar p_k(\\mathbf r) \\log \\hat p_k(\\mathbf r)\\] <p>where \\(\\bar p_k\\) is the 2D segmentation logits.</p> <ol> <li> <p>Wang, P., Liu, L., Liu, Y., Theobalt, C., Komura, T., and Wang, W. 2021. Neus: Learning neural implicit surfaces by volume rendering for multi-view reconstruction. arXiv preprint arXiv:2106.10689.\u00a0\u21a9</p> </li> <li> <p>Yariv, L., Gu, J., Kasten, Y., and Lipman, Y. 2021. Volume rendering of neural implicit surfaces. Advances in Neural Information Processing Systems 34, 4805\u20134815.\u00a0\u21a9</p> </li> <li> <p>Oechsle, M., Peng, S., and Geiger, A. 2021. Unisurf: Unifying neural implicit surfaces and radiance fields for multi-view reconstruction. Proceedings of the IEEE/CVF international conference on computer vision, 5589\u20135599.\u00a0\u21a9</p> </li> <li> <p>Yu, Z., Peng, S., Niemeyer, M., Sattler, T., and Geiger, A. 2022. Monosdf: Exploring monocular geometric cues for neural implicit surface reconstruction. Advances in neural information processing systems 35, 25018\u201325032.\u00a0\u21a9</p> </li> <li> <p>Guo, H., Peng, S., Lin, H., et al. 2022. Neural 3d scene reconstruction with the manhattan-world assumption. Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 5511\u20135520.\u00a0\u21a9</p> </li> </ol>"},{"location":"cs294173/shl.html","title":"Spherical Harmonic Lighting","text":"<p>Spherical Harmonic Lighting: The Gritty Details  </p> <p>Spherical Harmonic Lighting A efficient method for capturing and displaying Global Illumination solutions across the surface of an object. </p>"},{"location":"cs294173/shl.html#background","title":"Background","text":""},{"location":"cs294173/shl.html#light-modeling","title":"Light Modeling","text":"<p>For a point \\(x\\) at the surface, the reflected light intensity from viewing direction $ \\mathbf d_0$ is modelled as </p> \\[\\begin{align*} L(x,  \\mathbf d_0) &amp;= L_e(x,  \\mathbf d_0) + \\int_S f_r(x,  \\mathbf d_i -  \\mathbf d_0) L_r(x',  \\mathbf d_i) G(x,x') \\mathbf I(x\\text{ see } x')d\\: \\mathbf d_i\\\\ \\end{align*}\\] <p>where \\(S\\) is the unit sphere centered at \\(x\\) and \\(L_e(x,  \\mathbf d_0)\\) is the direct emission light \\(f_r(x,  \\mathbf d_i -  \\mathbf d_0)\\) is the BRDF term, transforming incoming light $ \\mathbf d_i$ to \\(\\mathbf d_0\\) \\(L_r(x,x')\\) the the light reflected from \\(x'\\) to \\(x\\) \\(G(x, x')\\) is the geometric relationship between \\(x\\) and \\(x'\\) </p> <p>Of course, this computation is intractable and we do lots compromises (given up some terms, simplify the model). </p>"},{"location":"cs294173/shl.html#monte-carlo-integration","title":"Monte Carlo Integration","text":"<p>Known that the expectation is defined as</p> \\[E(f(x)) = \\int f(x)p(x) dx\\] <p>for any function \\(f\\), thus if we have that \\(f' = f/p\\), we have</p> \\[E(\\frac{f(x)}{p(x)}) = \\int \\frac{f(x)}{p(x)}p(x) dx = \\int f(x)dx \\approx \\hat E(\\frac{f(x)}{p(x)}) = \\frac{\\sum^N f(x_i)/{p(x_i)}}{N}\\] <p>More over, if we can uniformally sample \\(N\\) points on the sphere surface, then we have \\(p(x_i) = 1/4\\pi\\) is constant. so that the equation is reduced to summing samples.</p> <p>Note that uniform sampling from polar coordinate \\((\\theta\\in [0,2\\pi), \\phi\\in[-\\pi/2,\\pi/2))\\) is not a uniform sampling over the sphere, since we are sampling over the circles with different radius \\(\\sin\\phi\\). </p> <p>Instead, we transforms from unit square \\((u,v)\\) </p> \\[(2\\pi u, 2\\cos^{-1}(\\sqrt{1-v}))\\rightarrow (\\theta,\\phi)\\] <pre><code>function sphere_sample(N) {\n    const positions = new Float32Array(N * 3);\n    for (let i = 0; i &lt; N; i++) {\n        // uniform sample from [0, 1) * [0, 1)\n        const u = Math.random();\n        const v = Math.random();\n        // transform to polar\n        const theta = 2 * PI * u;\n        const phi = 2 * Math.acos(Math.sqrt(1 - v));\n        // transform to xyz\n        positions[3 * i] = Math.cos(theta) * Math.sin(phi);\n        positions[3 * i + 1] = Math.sin(theta) * Math.sin(phi);\n        positions[3 * i + 2] = Math.cos(phi);\n    }\n    return positions;\n}\n</code></pre>"},{"location":"cs294173/shl.html#orthogonal-basis-functions","title":"Orthogonal Basis Functions","text":"<p>Define two functions are orthogonal if </p> \\[\\langle f, g\\rangle = \\int f(x)g(x)dx = 0\\] <p>Using the inner product, we can project function \\(f\\) onto a basis function \\(g\\). For example, if we use a set of linear basis functions \\(B_i\\), then we can project each piece of \\(f\\) onto the piecewise basis, and approximate it by scaling each piece with the dot product. </p>"},{"location":"cs294173/shl.html#associated-legendre-polynomials","title":"Associated Legendre Polynomials","text":"<p>Orthogonal Polynomials are sets of polynomials s.t. </p> \\[\\int_{-1}^1 F_m(x)F_n(x)dx = c\\cdot \\mathbb I(n=m)\\] <p>furthermore, if \\(c=1\\), then the family of polynomials is called orthonormal basis functions. </p> <p>In particular, we are interested in Associated Legendre Polynomials. First, we have Legendre Polynomials, which is a particular case for orthonormal basis functions</p> \\[P_n(x) = \\frac{1}{2^n n!}\\frac{d^n}{dx^n}(x^2-1)^n\\] <p>then, Associated Legendre Polynomials is defined based on \\(P_n\\) as </p> \\[P_l^m(x)= (-1)^m (1-x^2)^{m/2}\\frac{d^m}{dx^m}P_l(x)\\] <p>where \\(l\\) is the degree.band indexm and \\(m\\) is the order. \\(l,m\\in\\mathbb N. m\\leq l. P_l^m:[-1,1]\\rightarrow\\mathbb R\\)</p> <p>There are some good property of ALP, which makes it easy to recurse.</p> \\[\\begin{align*} &amp;(1)&amp;(l-m)P_l^m &amp;= x(2l-1)P_{l-1}^m - (l+m-1)P^m_{l-2}\\\\ &amp;(2)&amp;P_m^m &amp;= (-1)^m (2m-1)!!(1-x^2)^{m/2}\\\\ &amp;(3)&amp;P_{m+1}^m &amp;= x(2m+1)P_m^m \\end{align*}\\] <p>where \\(n!! = n\\times (n-2)\\times (n-4) \\times ... \\times 1\\text{ or }0\\) where \\(1,0\\) depends on whether \\(n\\) is odd or even. </p> ALP<pre><code>function ALP(m, l, x) {\n    if (l == 0) \n        return 1.;\n    let pmm = 1.;\n    if (m &gt; 0) {\n        const somx2 = Math.sqrt((1. - x) * (1. + x));\n        let fact = 1;\n        for (let i = 0; i &lt; m + 1; i++) {\n            pmm *= -fact * somx2;\n            fact += 2;\n        }\n    }\n    if (l == m) \n        return pmm;\n    // p^m_{m+1} term by eq3\n    let pmmp1 = x * (2. * m + 1.) * pmm\n    if (l == m + 1)\n        return pmmp1\n    // start to raise l  by eq1\n    let pll = 0;\n    for (let ll = m + 2; ll &lt; l + 1; ll++) {\n        pll = ((2. * ll - 1.) * x * pmmp1 - (ll + m - 1.) * pmm) / (ll - m);\n        pmm = pmmp1;\n        pmmp1 = pll;\n    }\n    return pll;\n}\n</code></pre> <p></p>"},{"location":"cs294173/shl.html#spherical-harmonics","title":"Spherical Harmonics","text":"<p>Using the standard spherical coordinates </p> \\[(x,y,z) = (\\sin\\theta\\cos\\phi,\\sin\\theta\\sin\\phi,\\cos\\theta)\\] <p>the spherical harmonics is </p> \\[y_l^m(\\theta,\\phi) = \\begin{cases} \\sqrt 2 K_l^m \\cos (m\\phi) P_l^m (\\cos\\theta) &amp;m&gt;0\\\\ \\sqrt 2 K_l^m \\sin (-m\\phi) P_l^{-m} (\\cos\\theta) &amp;m&lt;0\\\\ K_l^0P_l^0(\\cos\\theta)&amp;m=0 \\end{cases}\\] <p>where \\(K\\) is the scaling factor to normalize the functions</p> \\[K_l^m = \\sqrt{\\frac{(2l+1)(l-|m|)!}{4\\pi(l+|m|)!}}\\] <p>Wiki for the visualizations and analytic formulas</p> SH<pre><code>function SH(m, l, theta, phi) {\n    const sqrt2 = 1.41421356237;\n    const mabs = m &gt; 0 ? m : -m;\n    let K = Math.sqrt(\n        (2. * l + 1) * factorial(l-mabs) / (4. * PI * factorial(l+mabs))\n    );\n    if (m == 0)\n        return K * ALP(0, l, Math.cos(theta));\n    else if (m &gt; 0)\n        return sqrt2 * K * Math.cos(m * phi) * ALP(m, l, Math.cos(theta));\n    else\n        return sqrt2 * K * Math.sin(-m * phi) * ALP(-m, l, Math.cos(theta));\n}\n</code></pre>"},{"location":"cs294173/shl.html#sh-projection","title":"SH Projection","text":"<p>The projection onto SH is hence </p> \\[c_l^m = \\int_S f(s) y_l^m(s)ds\\] <p>which can be evaluated using Monte-Carlo integration (since we have our unit sphere uniform sampling). </p> <p>and the reconstruction is </p> \\[\\tilde f(s) = \\sum_{l=0}^{n-l}\\sum_{m=-l}^l c_l^m y_l^m(s) = \\sum_{i=0}^{n^2} c_i y_i(s)\\] <p>where \\(i = l(l+1)+m\\) is used to flatten the nested indexes into 1D. </p> <p>Thus, we can connect everything together. </p>"},{"location":"cs294173/shl.html#properties","title":"Properties","text":""},{"location":"cs294173/shl.html#rotation-invariance","title":"Rotation Invariance","text":"<p>SH functions are orthonormal, thus suppose that \\(g:= f\\circ R\\) where \\(R\\in SO(3)\\), then \\(\\tilde g = \\tilde f \\circ R\\) is the same approximation of \\(g\\). Thus, if we do rotations of the light field (viewing from different angles, or in a dynamic lighting environment). </p>"},{"location":"cs294173/shl.html#integration-to-dot-product","title":"Integration to dot product","text":"<p>For the reflectance models, the light is an integral over a sampled area</p> \\[\\int_S L(s) t(s)ds\\] <p>where \\(L\\) is the incoming light from direction \\(s\\), and \\(t\\) is the transfer function (e.g. BRDF). However, if we use spherical harmonics to approximate \\(L\\) and \\(t\\), and take \\(L_i, t_i\\) be the SH coefs. Then we have that </p> \\[\\int_S \\tilde L(s) \\tilde t(s)ds = \\sum_{i=0}^{n^2} L_i t_i\\] <p>which is the dot product of two coef vectors. </p>"},{"location":"cs294173/supervised3d.html","title":"Single View 3D with Supervision","text":"<p>In general, 3D learning focus on 2D images input (along with other sensor data) and output a 3D representation that describe the scene. </p>"},{"location":"cs294173/supervised3d.html#depth-prediction-on-monocular-images","title":"Depth Prediction on Monocular Images","text":"<p>Given 2D RGB images, the goal is to give a depth map that describes the ray distance at each pixel position.</p> <p>A very initial idea is to directly predict the depth loss as the sum of distance between prediction and GT</p> \\[\\mathcal L_d(y, y^*) = \\sum_{i\\in I} \\|y_i - y_i^*\\|^2\\]"},{"location":"cs294173/supervised3d.html#scale-invariant-depth-loss","title":"Scale invariant depth loss","text":"<p>Depth Map Prediction from a Single Image using a Multi-Scale Deep Network </p> <p>However, the problem arises due to scale ambiguity. Smaller but closer object will appears the same size as larger but far object. Therefore, we only want to consider the relative depth of each set of values, which means \\(\\mathcal L(y, y^*) = \\mathcal L(ay, y^*)\\). This leads to the scale invariant depth loss</p> \\[\\mathcal L(y, y^*) = \\sum_{i\\in I} \\|\\log y_i - \\log y_i^* + \\frac{1}{n}\\sum_{i\\in I}(\\log y_i^* - \\log y_i)\\|^2\\] <p>The architecture adapts a coarse to fine structure to capture information from distant pixels. </p>"},{"location":"cs294173/supervised3d.html#scale-and-shift-invariant-disparity-loss","title":"Scale and shift invariant disparity loss","text":"<p>Towards Robust Monocular Depth Estimation: Mixing Datasets for Zero-shot Cross-dataset Transfer</p> <p>scale and shift invariant loss considers the shift of the camera and the scale ambiguity at the same time. Let \\(\\mathbf d\\) be the predicted disparity (inverse depth) and \\(\\mathbf d^*\\) be the ground truth. We first align the two disparity by computing predictors </p> \\[(s, t) = \\arg\\min \\sum_{i\\in I}(s\\mathbf d_i + t - \\mathbf d_I^*)^2\\] <p>so that we can align our predicted disparity with the ground truth as \\(\\hat{\\mathbf d} = s\\mathbf d + t\\). Note that \\(s,t\\) has a closed form solution </p> \\[t(\\mathbf d) = \\text{median}(\\mathbf d), s(\\mathbf d) = \\frac{1}{N} \\sum_{i\\in I} | \\mathbf d - t(\\mathbf d)|\\] <p>We can align both the prediction and the ground truth to have zero translation and unit scale</p> \\[\\hat{\\mathbf d} = \\frac{\\mathbf d - t(\\mathbf d)}{s(\\mathbf d)}, \\hat{\\mathbf d}^* = \\frac{\\mathbf d^* - t(\\mathbf d^*)}{s(\\mathbf d^*)}\\] <p>and the SSI loss is defined as </p> \\[\\mathcal L_{SSI}(\\hat{\\mathbf d}, \\hat{\\mathbf d}^*) = \\frac{1}{2N} \\sum_{i\\in I} \\|\\hat{\\mathbf d}_i - \\hat{\\mathbf d}_i^*\\|\\] <p>An additional regularization term (multiscale gradient matching introduced by MegaDepth) is also adapted.</p>"},{"location":"cs294173/supervised3d.html#depth-estimation-on-videos","title":"Depth Estimation on Videos","text":"<p>Robust Consistent Video Depth Estimation</p> <p>Compared to single images, video data makes the assumption that the adjacent frames have consistent camera poses (single image depth estimation fails) and the object in the scene have consistent movement (dynamic scenes, hence SfM fails). The task can be modelled by several components</p> <ul> <li>The objects in the scene, as a depth map</li> <li>The camera poses, as extrinsic and intrinsic \\(R, t, K\\)</li> <li>The deformation of the object</li> </ul> <p>Given an image sequence, RCVD computes the optical flow for frame \\(i\\) and \\(i+k\\) for every \\(k\\) frames, a mask for dynamic pixels, and per-image depth map (mapped to camera coordinates, so that projected as a 3D point cloud). If we know some other image's pose, then we can reproject the point cloud into the other image, and see if such point is the same as the projected pixel from optical flow. This is the reprojection loss from frame \\(i\\) to frame \\(j\\)</p> \\[\\mathcal L_{reproject}^{i,j}(x) = \\mathcal L_j(\\mathbf c_{i\\rightarrow j}(x), \\mathbf c_j(f_{i\\rightarrow j}(x)))\\] <p>where \\(x\\) is the 3D point in frame \\(i\\), \\(\\mathbf c_{i\\rightarrow j}\\) is the feature of the reprojection in frame \\(j\\), and \\(f\\) is the optical flow from \\(i\\) to \\(j\\). </p> <p>The reprojection mentioned above requires camera poses. However, direct optimization of the camera pose is hard for CNN. Instead, RCVD uses a depth deformation model to align the depth. Instead of given full 6D freedom, RCVD replaces the depth scale coefficient with a spatially varying bilinear spline so that the optimization can be smoothed. </p>"},{"location":"cs294173/supervised3d.html#3d-representations","title":"3D Representations","text":"<p>Similar to depth scale ambiguity issue, we need to consider which coordinate system to use for learning 3D. Camera coordinate system is not generally a good choice since the same object can be seen from infinitely many camera coordinates. Instead, we use a \"canonical\" coordinate system, centered at the object. Generally, we assume that for objects in the same category, they should have a similar shape structure, hence similar distribution of directions (canonical coordinates) and similar distribution of the volume (center of the object). </p> <p>Therefore, for the learning, the supervision will be 3D object representations in the canonical coordinate system. For image-based input, we need to recover the transformations (from camera space to the object space) and the object representation.</p>"},{"location":"cs294173/supervised3d.html#voxels","title":"Voxels","text":"<p>Voxel representation is a uniform 3D grid \\(V(x,y,z)\\in \\in \\{0, 1\\}\\). In general, it can be seen as a 3D mask hence we can seen the problem as a voxel-wise classification problem. The 3D supervision will be another voxel volume representation, and the supervision loss is simply cross entropy loss at each voxel location. </p> <p>Earlier attempts (Learning a Predictable and Generative Vector Representation for Objects, Pix2vox) uses an encoder-decoder structure to encode 2D images and decode to 3D voxel volumes. Following works uses octrees to allow hierarchical approach to save computations. </p>"},{"location":"cs294173/supervised3d.html#implicit-volumetric-representations","title":"Implicit Volumetric Representations","text":"<p>Accounting for the resolution bottleneck on voxel grid, another set of works represents the 3D space as a implicit filed that maps 3D location \\(\\mathbf x\\) to a scalar output. The output can be </p> <ul> <li>Signed distance (DeepSDF), the closest distance to the surface. positive distance means outside of the surface and negative means inside. Surface at 0-level..</li> <li>Occupancy function or Inside/outside score (Occupancy Networks, Learning Implicit Fields for Generative Shape Modeling), \\(0\\) if outside the shape, \\(1\\) is inside. Surface at \\(\\frac{1}{2}\\)-level. </li> </ul> <p>The core idea is similar, encode 2D images to a latent code, and then add position \\((x,y,z)\\) (possibly with a positional encoding) to the latent code, and final decode them into the scalar value. Given a 3D scalar field, the surface is represented by the level-set. Marching-cubes algorithm is an effective way to extract meshes from 3D scalar field, given the actual level. </p>"},{"location":"cs294173/supervised3d.html#parametric-surface","title":"Parametric surface","text":"<p>AtlasNet: A Papier-Mache Approach to Learning 3D Surface Generation</p> <p>Suppose that a 3D surface can be represented by multiple pieces of manifold surfaces. Each manifold surface is a manifold mapping from \\((u,v)\\) to \\((x,y,z)\\) and the 3D surface can be represented by \\(n\\) pieces of manifold with overlapping. </p> <p>To allow learning of such manifold, the latent code is padded with randomly sampled point cloud from \\([0,1]\\times [0, 1]\\) sheet and decoded into corresponding point cloud location. Since the point cloud is uniformly sampled from a 2D square, we can increase the resolution by increase the number of samples. If the whole object can be represented by deforming a sphere, then we can also sample from a unit sphere. Note that in this case, the supervision is point cloud sampled from 3D mesh, and the loss is3 the Chamfer distance loss. </p> Chamfer distance <p>given two point cloud \\(\\mathcal P\\) and \\(\\mathcal Q\\), the Chamfer distance loss is</p> \\[L_C(\\mathcal P,\\mathcal Q) = \\sum_{p\\in \\mathcal P} \\min\\{d(p, q) | q\\in\\mathcal Q\\} + \\sum_{p\\in \\mathcal P} \\min\\{d(p, q) | p\\in\\mathcal P\\}\\] <p>Note that there's no correspondence between point cloud and the points are orderless. In addition, the number of points in two point cloud can be different. </p>"},{"location":"csc265/amortized.html","title":"Amortized Analysis","text":""},{"location":"csc265/amortized.html#amortized-complexity","title":"Amortized Complexity","text":"<p>Let \\(\\sigma = (\\sigma_i)_1^m\\) be a sequence of operations on data structure \\(D\\), and \\(t_i\\) be the time to execute \\(\\sigma_i\\). Let \\(S\\) be the set of all valid sequence of operations on \\(D\\) so that \\(\\sigma\\in S\\). </p> <p>Sequence complexity \\(C(m) = \\max\\{\\sum_1^m t_i: \\sigma \\in S\\}\\).  Amortized complexity \\(A(m) = \\frac{C(m)}{m}\\) time each operation takes in average. </p>"},{"location":"csc265/amortized.html#aggregate-analysis","title":"Aggregate Analysis","text":"<p>In aggregate analysis, we show that for all \\(n\\), a sequence of \\(n\\) operations takes worst-case time \\(T(n)\\) in total. In the worst case, the average cost, or amortized cost, per operation is therefore \\(T(n)/n\\).</p>"},{"location":"csc265/amortized.html#accounting-method","title":"Accounting Method","text":"<p>We assign differing charges to different operations, with some operations charged more or less than they actually cost. The amount charged on each operation is called amortized cost. When an operation's amortized cost exceeds its actual cost, the differnt is assigned to specific objects in the data structure as credit. </p> <p>In the worst case analysis, we have to make such that the the amortized cost provides an upper bound on the actual cost. Which mean that for all sequences of \\(n\\) operations. \\(\\sum^n \\hat c_i \\geq \\sum^n c_i\\) where \\(\\hat c_i\\) is the amortized cost for ith operation, \\(c_i\\) is the actual cost. Then, the total credit stored is \\(\\sum^n \\hat c_i - \\sum^n c_i \\geq 0\\) all the time. </p>"},{"location":"csc265/amortized.html#potential-method","title":"[+] Potential Method","text":"<p>As an extension to accounting method, we represents the prepaid work (credit in accounting method) as \"potential\". We associate the potential with the data structure instead of specific objects within the data structure. </p> <p>Let the state of an initial data structure as \\(D_0\\), and \\(D_i\\) be the state of the data structre after applying the ith operation to \\(D_{i-1}\\). Define a potential function</p> \\[\\Phi: \\{D_0, ..., D_{n-1}\\}\\rightarrow \\mathbb R\\] <p>to be the potential associated with data structure \\(D_i\\). Then, the amortized cost \\(\\hat c_i\\) of the ith operation w.r.t. potential function \\(\\Phi\\) is defined by </p> \\[\\hat c_i = c_i + \\Phi(D_i) - \\Phi(D_{i-1})\\] <p>and the total amortized cost of the n operations is </p> \\[\\sum_{i=1}^n \\hat c_i = \\sum_{i=1}^n (c_i + \\Phi(D_i) - \\Phi(D_{i-1})) = \\sum_{i=1}^n c_i + \\Phi(D_n) - \\Phi(D_0)\\]"},{"location":"csc265/amortized.html#example-binary-counter","title":"Example: Binary Counter","text":"<p>Consider a binary counter increment, which keeps a number \\(x\\mod 2^k\\) be increament. The data structure used is a sequence \\(A:=(b_0, ..., b_{k-1}), b_i \\in \\{0, 1\\}. x = \\sum_{i=0}^{k-1} a_i 2^i\\). </p> increment(A)<pre><code>j = 0\nwhile j &lt; A.k and A[j] == 1:\n    A[j] = 0\n    j += 1\nif j == k:\n    A[j] = 0\nA[j] = 1\n</code></pre>"},{"location":"csc265/amortized.html#aggregate-analysis_1","title":"Aggregate Analysis","text":"<p>Consider the number of bits flipped in each increment be the actual cost, we know that \\(A[i]\\) flipped every \\(2^i\\) time.  Thus, let \\(N_j\\) be the number of jth bit flips. we have the total cost</p> \\[C(m) = \\sum_{j=0}^{k-1} N_j = \\sum_{j=0}^{k-1} \\lfloor \\frac{m}{2_i}\\rfloor \\leq m\\sum_{j=0}^{k-1} 2^{-i} = 2m\\] \\[A(m) = C(m)/m = 2\\in O(1)\\]"},{"location":"csc265/amortized.html#accounting-method_1","title":"Accounting Method","text":"<p>charge an amortized cost of 2 dollars to set a bit to \\(1\\). When a bit is set we use 1 dollar, and the other 1 dollar as credit to be used later when we flip the bit back to 0. Therefore, every 1 in the counter has a dollar of credit, so that we charge nothing to reset a bit to \\(0\\). Since there will always be 1 in \\(A\\), the amount of credits is always positive. Thus, the total amortized cost is \\(C(n)\\in O(n)\\) and  \\(A(n)\\in O(1)\\)</p>"},{"location":"csc265/amortized.html#potential-method_1","title":"[+] Potential Method","text":"<p>Suppose that ith operation set \\(t_i\\) bits from 1 to 0. Then \\(c_i \\leq t_i + 1\\). Define \\(\\Phi(A_i) := \\sum_{j=0}^{k-1}\\mathbb I(b_j = 1)\\), i.e. the number of 1's in \\(A_i\\).  Then, \\(\\Phi(A_i) - \\Phi(A_{i-1}) \\leq -t_i + 1\\) since at least \\(t_i\\) number of 1's in \\(A_{i-1}\\) so that they can be reset by ith operation, and at most one 0-bit can be flipped to \\(1\\) in each operation.  Therefore</p> \\[\\hat c_i = c_i +\\Phi(A_i) - \\Phi(A_{i-1}) \\leq t_i + 1 -t_i + 1  = 2\\] <p>We know that \\(\\Phi(D_0) = 0, \\Phi(D_n) \\leq k\\) so that </p> \\[C(n) = 2n -k \\in O(n), A(n) \\leq 2 \\in O(1)\\]"},{"location":"csc265/amortized.html#example-dynamic-table","title":"Example: Dynamic Table","text":"<p>Instead of using linked list in chaining implementation, we instead us a dynamic sized table. For each bin, we store a pointer to an array <code>arr</code>, a number <code>size</code> to be the number of elements stored, and <code>maxsize</code> to be the array length. Then, insert is implemented as </p> insert(T, x)<pre><code>if T.size == T.maxsize:\n    allocate new_arr of size 2 * T.maxsize\n    T.maxsize *= 2\n    copy T.arr into new_arr # T.size ops\n    T.arr = new_arr\nT.arr[T.size] = x # 1 op\nT.size += 1\n</code></pre> <p>Let \\(d(T) := \\text{size}/\\text{maxsize}\\) be the load factor, assuming that we start with \\(\\text{maxsize} = 1\\). After each <code>insert</code>, we have that \\(0.5 &lt; d(T) \\leq 1\\) since we only double <code>maxsize</code> when <code>size == maxsize</code>. </p>"},{"location":"csc265/amortized.html#potential-method_2","title":"[+] Potential Method","text":"<p>Define \\(\\Phi(T_i) = 2 \\times\\) number of occupied slot in the second half of <code>arr</code>. </p> \\[\\Phi(T_i) = 2 (\\text{size} - \\text{maxsize}/2) = 2\\text{size} - \\text{maxsize}\\] <p>Then, we have two cases to consider </p> <p>If \\(T_i.\\text{size} = T_i.\\text{maxsize}\\), then we have the cost </p> \\[c_i = T_i.\\text{size} + 1\\] <p>Also we know that \\(T_{i-1}.\\text{size} = T_i.\\text{size} - 1, T_{i-1}.\\text{maxsize} = T_i.\\text{maxsize} / 2\\). Thus</p> \\[\\begin{align*} \\phi_i - \\phi_{i-1} &amp;= 2T_i.\\text{size} -  T_i.\\text{maxsize} - 2(T_i.\\text{size} - 1) + T_i.\\text{maxsize} / 2\\\\ &amp;= 2 - T_i.\\text{maxsize}/2 \\end{align*}\\] <p>From the <code>if</code> condition, we also have that $ T_i.\\text{maxsize}/2 =  T_i.\\text{size} - 1$</p> \\[c_i + \\phi_i - \\phi_{i-1} = T_i.\\text{size} + 1 + 2 - T_i.\\text{size} + 1 = 4\\in O(1)\\] <p>If \\(T_i.\\text{size} &lt; T_i.\\text{maxsize}\\), then we have the cost </p> \\[c_i + \\phi_i - \\phi_{i-1} = 1 + 2 = 3 \\in O(1)\\]"},{"location":"csc265/amortized.html#accounting-method_2","title":"Accounting Method","text":"<p>For each <code>insert</code>, we charge \\(3\\) dollars. 1 dollar for setting \\(x\\), and 2 dollar as credit. When the if branch runs, for each index \\(i\\), it pays for the cost of copying itself and the slot \\(i - T.\\text{maxsize} / 4\\). Consider one iteration before the <code>if</code> branch runs. Known that <code>T.size == T.maxsize</code> then the second half have 2 credits since they have never been copied. Thus, they have enough credit to pay for the cost of the first half. </p>"},{"location":"csc265/amortized.html#aggregate-analysis_2","title":"Aggregate Analysis","text":"<p>After \\(n\\) insert, the <code>maxsize</code> of the table will be \\(2^{\\lceil \\lg n\\rceil}\\), thus we have doubled the table size \\(\\lceil \\lg n\\rceil - 1\\) times. Each time copies \\(2^0, 2^1, ..., 2^{\\lceil \\lg n\\rceil - 1}\\) elements. Thus, the total costs will be </p> \\[C(n) = n + \\sum_{i=0}^{\\lceil \\lg n\\rceil - 1} 2^i = n + 2^{\\lceil \\lg n\\rceil} - 1 \\leq n + 2^{\\lg n + 1} = 3n\\]"},{"location":"csc265/avl.html","title":"Set: AVL Tree","text":""},{"location":"csc265/avl.html#abstract-data-type-set","title":"Abstract Data Type: Set","text":"<ul> <li>Object: <ul> <li>Set \\(S\\) of keys (distinct integers)</li> </ul> </li> <li>Operations:<ul> <li><code>delete(x)</code> removes \\(x\\) from the set.</li> <li><code>insert(x)</code> add <code>x</code> to \\(S\\).</li> <li><code>search(x)</code> return if \\(x\\) is in the set.</li> </ul> </li> </ul> <p>Note that a dictionary is just a set, instead of storing keys, dictionary stores key value pairs. </p>"},{"location":"csc265/avl.html#binary-search-tree","title":"Binary Search Tree","text":"<p>Binary search tree is the most basic way to implement a set. However, in worst case, <code>delete, insert, search</code> all take \\(O(n)\\) time. For example, inserting a sequence of increase numbers, the BST will become a chain. </p> BST basic implementation BST.py<pre><code>class BSTNode:\n\n    # Constructor to create a new node\n    def __init__(self, key):\n        self.k = key\n        self.l = None\n        self.r = None\n\ndef insert(node, k):\n\n    # If the tree is empty, return a new node\n    if node is None:\n        return BSTNode(k)\n\n    # Otherwise recur down the tree\n    if k &lt; node.k:\n        node.l = insert(node.l, k)\n    else:\n        node.r = insert(node.r, k)\n\n    # return the (unchanged) node pointer\n    return node\n\n\ndef search(node, k):\n    if node is None:\n        return False\n    if node.k == k:\n        return True\n    if k &lt; node.k:\n        return search(node.l, k)\n    else:\n        return search(node.r, k)\n\ndef delete(node, k):\n    if node is None:\n        return node\n    if k &lt; node.k:\n        node.l = delete(node.l, k)\n    elif k &gt; node.k:\n        node.r = delete(node.r, k)\n    elif node.l is None:\n        temp = node.r\n        node = None\n        return temp\n    elif node.r is None:\n        temp = node.l\n        node = None\n        return temp\n    else:\n        temp = node.r\n        while temp.l is not None:\n            temp = temp.l\n        node.k = temp.k\n        node.r = delete(node.r, temp.k)\n    return node\n</code></pre> <pre><code>from assets.AVL import *\nfrom assets.plot_trees import plot_tree, construct_tree_nodes\nfrom IPython.display import Image\nlabel_fn = lambda x: x.k\n</code></pre> <pre><code># in worst case, it becomes a chain\narr = [1, 2, 3, 4, 5, 6]\nroot = BST_insert(None, arr[0])\nfor x in arr[1:]:\n    BST_insert(root, x)\nplot_tree(\n    construct_tree_nodes(root, label_fn, ['l', 'r']),\n    \"./assets/avl_1.jpg\"\n)\n</code></pre> <p></p>"},{"location":"csc265/avl.html#avl-tree","title":"AVL Tree","text":"<p>For AVL tree \\(T\\), for node \\(u\\in T\\). Define \\(h(u)\\) be the height of \\(u\\), a.k.a. the shortest path from \\(u\\) to a leaf.  Define tree height \\(H(T) = h(\\text{root}), H(\\emptyset) = -1\\) </p> <p>Define balance factor \\(BF(u) = H(T_{u,l}) - H(T_{u,r})\\) where \\(T_{u,l}, T_{u,r}\\) are the left subtree and right subtree of \\(u\\). </p> <p>Finally, a AVL Tree \\(T\\) is a BST s.t. \\(\\forall u\\in T. BF(u) \\in \\{-1, 0, 1\\}\\).</p>"},{"location":"csc265/avl.html#properties","title":"Properties","text":"<p>Claim 1 Let \\(f(h)\\) be the smallest number of nodes in a AVL tree of height \\(h\\). Then, \\(f(h) = f(h-1) + f(h-2) + 1\\). proof. Left subtree + Right subtree + root, by balance factor constraint, the subtree height difference can be at most 1. </p> <p>Claim 2 Let \\(F\\) be Fibonacci series function, \\(F_0 = 0, F_1 = 1, F_n = F_{n-1} + F_{n-2}\\). Then \\(f(h) = F(h+3) - 1\\). </p> <p>proof. </p> \\[\\begin{align*} f(0) &amp;= F_3 - 1 = F_2 + F_1 - 1 = 1\\\\ f(1) &amp;= F_4 - 1 = 3 - 1 = 2\\\\ f(h) &amp;= f(h-1) + f(h-2) + 1 \\\\ &amp;= F_{h+2} -1 + F_{h+1} -1 + 1 = F_{h+3} - 1 \\end{align*}\\] <p>Claim 3 \\(h \\in O(\\log n)\\). proof. Let \\(n\\) be the number of nodes, \\(h\\) be the height</p> \\[\\begin{align*} n &amp;\\geq f(h) \\\\ &amp;= F_{h+3} - 1 \\\\ &amp;= \\frac{(1+\\sqrt{5})^{h+3} - (1-\\sqrt{5})^{h+3}}{2^{h+3}\\sqrt 5 } - 1\\\\ &amp;\\geq \\frac{1}{\\sqrt 5}(\\frac{1+\\sqrt{5}}{2})^{h+3} - 2\\\\ \\sqrt{5}(n+2)&amp;\\geq (\\frac{1+\\sqrt{5}}{2})^{h+3}\\\\ \\log_{\\frac{1+\\sqrt{5}}{2}}\\sqrt{5}(n+2) &amp;\\geq h+3\\\\ h &amp;\\leq \\log_{\\frac{1+\\sqrt{5}}{2}}\\sqrt{5}(n+2) -3\\\\ h&amp;\\in O(\\log n) \\end{align*}\\]"},{"location":"csc265/avl.html#insert","title":"<code>insert</code>","text":"<p>Consider the major properties of AVL tree. We want it to be a self-balanced BST. Therefore, after inserting an element as of regular BST, we want to make the tree balance.  </p> AVL_insert(root, x)<pre><code>    #precondition: root is the root of a AVL tree\n    #postcondition: root is the root of a AVL tree containing x\n 1  new_leaf = BST_insert(root, x)\n 2  curr = new_leaf\n 3  while curr is not root:\n 4      BF = height(curr.right) - height(curr.left)\n 5      if BF == 0:\n 6          return\n 7      elif BF == 2 or BF == -2:\n 8          fix_imbalance(curr)\n 9          return\n10      update curr height\n11      curr = curr.parent \n</code></pre>"},{"location":"csc265/avl.html#correctness","title":"Correctness","text":"<p>First note that by our definition of <code>height</code>, only \\(x\\)'s ancestors will need to update their height, and hence balance factor. </p> <p>Intuitively, there are 3 cases in the <code>while</code> loop</p> <p><code>BF = 0</code>: the newly added node makes the tree balanced. Thus, the lighter of the two subtrees add one node \\(x\\). This means that the current node's height won't change, since its height is <code>max(height(curr.left), height(curr.right)) + 1</code>. Thus, current node's ancestors won't need to update their height. </p> <p><code>BF = 1 or BF = -1</code>: AVL tree property is not violated, but the height need to be updated, hence we have to continue on looking up. </p> <p><code>BF = 2 or BF = -2</code>: AVL tree property got violated, we need to fix the imbalance on the branch, while maintaining BST properties. Moreover, we can make the current node's balance factor 0 after <code>fix_imbalance</code>, thus we can stop the while loop. </p>"},{"location":"csc265/avl.html#runtime","title":"Runtime","text":"<p><code>BST_insert</code> takes \\(O(h)\\) time. Each iteration of the <code>while</code> loop, the depth of the <code>curr</code> reduces by 1. Thusm there are at most \\(h\\) iterations. In each iteration, all branches take constant time. Thus, each <code>insert</code> is \\(\\in O(h)\\). Then, by AVL tree's self-balancing property claim 3, we have \\(\\in O(\\log n)\\)</p>"},{"location":"csc265/avl.html#rotation","title":"Rotation","text":"<p><code>left_rotate</code> and <code>right_rotate</code> can modify the BST in \\(O(1)\\) time, maintaining the BST property.</p> right_rotate(root)<pre><code> 1  r = root.right\n 2  rl = r.left\n 3  root.right = rl\n 4  r.left = root\n 5  root = r\n</code></pre> left_rotate(root)<pre><code> 1  l = root.left\n 2  lr = l.right\n 3  root.left = lr\n 4  l.right = root\n 5  root = l\n</code></pre> <p>Given an imbalanced BST with \\(\\pm 2\\) BF on root, and \\(\\pm 1, 0\\) BF on other nodes. We want a new BST that is balanced. WLOG assume the right subtree is heavier. There are two cases</p> <p><code>right, right</code> case: right subtree of right subtree is heavier. This is easy since after one <code>right_rotate</code>, it is balanced. </p> <p><code>right, left</code> case: if we do a right rotation, then it is still imbalanced. Thus, the idea is first left rotate the right subtree, so that the right subtree's right is heavier. Then, we go back to <code>right, right</code> case. </p> <p></p> <pre><code># a demonstration of BST\narr = [12, 3, 17, 1, 7, 14, 19, 5, 8, 10]\nroot = BST_insert(None, arr[0])\nfor x in arr[1:]:\n    new_node = BST_insert(root, x)\n    BST_update_height(new_node)\nplot_tree(\n    construct_tree_nodes(root, label_fn, ['l', 'r']),\n    \"assets/avl_2.jpg\"\n)\n</code></pre> <p></p> <pre><code>right_rotate(root.l)\nplot_tree(\n    construct_tree_nodes(root, label_fn, ['l', 'r']),\n    \"assets/avl_3.jpg\"\n)\n</code></pre> <p></p> <pre><code># a demonstration of BST\narr = [12, 3, 17, 1, 7, 14, 19, 5, 8, 10, 21]\nroot = AVL_insert(None, arr[0])\nfor x in arr[1:]:\n    new_node = AVL_insert(root, x)\nlabel_fn = lambda node: f\"{node.k}, {get_height(node)}, {get_BF(node)}\"\nplot_tree(\n    construct_tree_nodes(root, label_fn, ['l', 'r']),\n    \"assets/avl_4.jpg\"\n)\n# key, height, BF\n</code></pre> <p></p>"},{"location":"csc265/avl.html#delete","title":"[+] <code>delete</code>","text":"<p>The idea is similar to <code>insert</code>, but need more considerations on the details. </p> <p>First, <code>BST_insert</code> will only delete leaf node (otherwise swap child node until the to be deleted node is a leaf). Thus, call the to be deleted lead node <code>x</code>, then we trace up from <code>x.parent</code> to <code>root</code> as <code>curr</code> and consider the cases</p> <ol> <li>if <code>curr</code>'s old BF is \\(0\\), then removal <code>x</code> will shorten one of is subtree, but <code>curr.height</code> won't change. Thus, if BF changes from \\(0\\rightarrow \\pm 1\\), we are safe to stop. </li> <li>if <code>curr</code>'s old BF is \\(\\pm 1\\), then the new BF will either be \\(0\\) or \\(\\pm 2\\). </li> <li>In the case of \\(0\\), <code>curr.height</code> will update and we still need to go up.</li> <li>In the case of \\(\\pm 2\\), we will rotate to fix imbalance. <code>fix_imbalance</code> will update height thus we need to go up.</li> </ol> <p>The correctness and runtime justification is very similar to <code>insert</code>. </p>"},{"location":"csc265/avl.html#fix-imbalance","title":"Fix Imbalance","text":"<p>Imbalance happens when we delete a node from the lighter subtree, and there are 3 cases to consider. As shown below</p> <p></p>"},{"location":"csc265/avl_app.html","title":"[+] Augmented BST (AVL Tree)","text":"<p>Consider  our implementation of AVL tree, the height of a node is </p> \\[x.\\text{height} = \\max(x\\text{.left.height}, x\\text{.right.height}) + 1\\] <p>Such property depends only on the property of the child nodes and takes constant time to accumulate. Thus, when insert or delete one node, we only need to make a bottom-up updates from the leaf to root. Taking at most \\(O(h)\\in O(\\log n)\\) time. </p>"},{"location":"csc265/avl_app.html#ordered-set-with-rank","title":"Ordered Set with <code>rank</code>","text":""},{"location":"csc265/avl_app.html#adt-ordered-set","title":"ADT: Ordered Set","text":"<p>Suppose that we want our set to supports finding the \\(k\\)th largest (or smallest) element, a.k.a. a ordered set. Thus, we need to support the additional operations.</p> <ul> <li><code>rank(A, x)</code>: return the rank of \\(x\\in A\\) where \\(rank = r\\) IFF there are exactly \\(r-1\\) elements of \\(A\\) that is smaller than \\(x\\). </li> <li><code>select(k)</code>: return the key of node \\(x\\) where \\(rank(A, x) = k\\). </li> </ul>"},{"location":"csc265/avl_app.html#rank-and-size","title":"<code>rank</code> and <code>size</code>","text":"<p>Think about BST's property, for each node <code>x</code>, we have that <code>x</code> is larger than all of its left subtree and its right subtree. Then, </p> rank(x)<pre><code>if x is None:\n     return 0\nif x.parent is None:\n    return size(x.left) + 1\nif x is left child of x.parent:\n    return rank(x.parent) - size(x.right) - 1\nelse:\n    return rank(x.parent) + size(x.left) + 1\n</code></pre> <p>If we know the size of each node, we can retrieve <code>rank</code> is \\(O(h)\\) time. </p> <p>Thus we can augment the AVL tree with field <code>size</code>, where </p> \\[size(\\emptyset) = 0, size(x) = size(x\\text{.left}) + size(\\text{x.right}) + 1\\] <p>Similar to height, it takes constant time and only depends on its child nodes. Thus, when we modify the data structure, we can upadte <code>size</code> in \\(O(h)\\) time. </p>"},{"location":"csc265/avl_app.html#select","title":"<code>select</code>","text":"<p>Then, we apply the same idea for select, </p> select(node, k)<pre><code>left_size = node.left.size\nif k == left_size + 1:\n    return node\nelif k &lt; left_size + 1:\n    return select(node.left, k)\nelse:\n    return select(node.right, k - left_size)\n</code></pre> <p>for some node \\(x\\), it is larger than its left subtree, thus at least larger than \\(size(x.\\text{left})\\). Then, if the wanted node is in the right subtree, we know it is at least the size of the left subtree plus the node. </p> <pre><code>from assets.AVL import *\nfrom assets.plot_trees import plot_tree, construct_tree_nodes\n\ndef compute_size(node):\n    if node is None:\n        return 0\n    node.size = compute_size(node.l) + compute_size(node.r) + 1\n    return node.size\n\ndef rank(node):\n    if node is None:\n        return 0\n    get_size = lambda node: 0 if node is None else node.size\n    if node.parent is None:\n        return get_size(node.l)\n    if node.parent.l is node:\n        return rank(node.parent) - get_size(node.r) - 1\n    else:\n        return rank(node.parent) + get_size(node.l) + 1\n\ndef select(node, k):\n    left_size = node.l.size + 1 if node.l is not None else 1\n    if left_size == k:\n        return node.k\n    elif left_size &gt; k:\n        return select(node.l, k)\n    else:\n        return select(node.r, k - left_size) \n</code></pre> <pre><code># a demonstration of BST\narr = [12, 3, 17, 1, 7, 14, 19, 5, 8, 10, 21]\nroot = AVL_insert(None, arr[0])\nfor x in arr[1:]:\n    new_node = AVL_insert(root, x)\n\ncompute_size(root)\n\nlabel_fn = lambda node: f\"{node.k}, {node.size}, {rank(node)}\"\n\nplot_tree(\n    construct_tree_nodes(root, label_fn, ['l', 'r']),\n    \"assets/avl_app_1.jpg\"\n)\n# key, size, rank\n</code></pre> <p></p> <pre><code>print([select(root, i) for i in range(1, len(arr) + 1)])\n#&gt;&gt; [1, 3, 5, 7, 8, 10, 12, 14, 17, 19, 21]\n</code></pre>"},{"location":"csc265/bfs.html","title":"Graphs: Breadth-first Search","text":""},{"location":"csc265/bfs.html#graphs-and-abstract-data-type","title":"Graphs and Abstract Data Type","text":"<p>Mathmatically, a graph \\(G = (V, E)\\) is a collection of vertices \\(v\\in V\\) and edges \\((v_i, v_j) \\in E\\). If a graph is undirected, then \\((v_i, v_j) = (v_j, v_i)\\), if directed, then \\((v_i, v_j) \\neq (v_j, v_i)\\). </p> <p>Two vertices \\((v_i, v_j)\\) are neighbors, or adjacent if exists \\((v_i, v_j) \\in E\\). </p> <p>The abstract data type for (undirected) graph is defined as  - Object: A list of vertices \\(V\\) and their associated values. - Operations:   - <code>vertices(G)</code> return a list of all vertices   - <code>isadjacent(G, u, v)</code> given two vertices \\(u,v\\), return whether they are adjacent.</p>"},{"location":"csc265/bfs.html#implementation","title":"Implementation","text":"<p>Consider a graph of \\(G\\) with \\(n = |V|, m = |E|\\). </p> <p>Although we can store \\(V\\) and \\(E\\) as two lists, but this is obviously not efficient for <code>isadjacent</code>, which will take \\(O(m)\\) time. In the case of an almost fully connected graph of \\(n\\) vertices, \\(T(n, m) \\in O(n^2)\\).</p> <p>Also, we have to store all vertices, thus <code>vertices</code>, regardless of implementation, is at least \\(\\Omega(n)\\)</p>"},{"location":"csc265/bfs.html#adjacency-lists","title":"Adjacency Lists","text":"<ul> <li>Object: for each vertice \\(u\\), we store all vertices that is \\(u\\)'s neighbor as a list. </li> <li><code>isadjacent(G, u, v)</code> search for <code>v</code> in <code>u.neighbor</code></li> </ul> <p>The space complexity is \\(n + 2m\\) since for each edge we store it twice in both vertices' neighbors. </p> <p>The time complexity for <code>isadjacent</code> is \\(\\Omega(\\min(m,n))\\) since a vertex can have at most \\(\\min(m,n-1)\\) neighbors. However, if the vertices have some orderness, we can reduce the time complexity by sorting. (But takes longer to construct the adjacency lists). </p>"},{"location":"csc265/bfs.html#adjacency-matrix","title":"Adjacency Matrix","text":"<ul> <li>Object: A \\(n\\times n\\) matrix of booleans, <code>E[i][j] = 1</code> if there's an edge between them, otherwise \\(0\\). </li> <li><code>isadjacent(G, i, j)</code> return <code>G.E[i][j]</code></li> </ul> <p>The space complexity is \\(n^2\\). The time complexity is \\(O(1)\\). </p>"},{"location":"csc265/bfs.html#path-and-distance","title":"Path and Distance","text":"<p>A path between \\(u, w\\) is a sequence of edges \\(p = ((u = v_0, v_1), (v_1, v_2), ..., (v_{k-1}, v_k = w))\\) where \\(v_i\\)'s are distinct. Let the collection of all paths between \\(u, v\\) be \\(\\mathcal P(u,v)\\).  The length is the number of edges \\(|P|\\), which in this case is \\(k\\). The distance is defined as minimum number of edges among all paths between \\(u,v\\) if exists such path, otherwise \\(\\infty\\). </p> \\[\\delta(u, v) =\\begin{cases}\\min\\{|P|: p\\in \\mathcal P(u,v)\\}&amp;\\mathcal P(u,v)\\neq \\emptyset\\\\  0&amp;u=v\\\\\\infty&amp;\\mathcal P(u,v)= \\emptyset\\end{cases}\\] <p>A graph is connected if \\(\\forall u, v\\in V\\), \\(\\exists P(u,v)\\). Otherwise unconnected. </p>"},{"location":"csc265/bfs.html#degree","title":"Degree","text":"<p>Define degree to be the number of neighbours for some \\(v\\), denote it as \\(d_v\\). </p> <p>In adjacency list, <code>len(v.neighbors)</code>; in adjacency matrix, <code>sum(E[v])</code>. </p> <p>Lemma 1 (handshake lemma) \\(\\sum_v d_v = 2|E|\\) proof. each edge \\((u,v)\\) is counted twice on \\(u\\) and \\(v\\). </p> <pre><code>import igraph as ig\n# create a example graph, and add one unreachable point\ng = ig.Graph.Famous('Walther'); g.add_vertex()\nlayout = g.layout();\n</code></pre>"},{"location":"csc265/bfs.html#breadth-first-search","title":"Breadth-first Search","text":"<p>In many applications, we need to traverse a graph (of states, relationships, etc.) from a starting vertex to search for some targeted vertices, or obtain some graph properties (for example, search for AI states). </p> <p>BFS searches, from a starting vertex \\(s\\), by visiting all of its neighbors, and then neighbors of neighbors. More formally, it searches all vertices with distance \\(1\\) to \\(s\\) first, and then \\(2, 3, 4,...\\).</p> BFS(G, s)<pre><code>q = new Queue()\nv.queued = False for v in G.V\nq.enqueue(s)\ns.queued = True\ns.d = 0\nwhile queue is not empty:\n    v = q.dequeue()\n    examine v\n    for u in v.neighbors:\n        if not u.queued:\n            q.enqueue(u)\n            u.queued = True\n            u.d = v.d + 1\n</code></pre>"},{"location":"csc265/bfs.html#time-complexity","title":"Time Complexity","text":"<p>Note that from <code>queued</code> proerpty, we know that each \\(v\\) will be enqueued at most once, and each edge from the enqueued is examined once. Thus, the total time complexity is in \\(O(n+m)\\).</p>"},{"location":"csc265/bfs.html#shortest-path","title":"Shortest Path","text":"<p>Note that we have defined \\(\\delta(u,v)\\) be the minimum number of edges among all possible paths between \\(u,v\\). Thus, one of the functionality of <code>BFS</code> (and moreover Dijkstra's for shortest path of all vertices pairs) is to find the shortest path between starting vertex \\(s\\) to any vertex. In order to do so, we only need to add a new field <code>parent</code>, which initially set as <code>None</code>, and update it when we enqueue the vertex (similar to how we update distance). </p> <p>Then, we have to show some properties with the shorted path, and then we can show that the found path from BFS is actually the shortest path. </p> <p>Lemma 2 For \\(G=(V,E)\\), let \\(s\\in V\\) be some arbitrary vertex. Then, for any edge \\((u,v)\\in E\\), \\(\\delta(s,v) \\leq \\delta(s,u)+1\\) proof. Denote the path between \\(s, u\\) as \\(P(s,u)\\) and assume that it is a shortest path. If \\((u,v) \\in P(s,u)\\), then since path does not have cycle, then \\(P(s,u) = P(s, v) , (v, u)\\), thus \\(\\delta(s, v) \\leq \\delta(s, u) - 1\\). If \\((u,v) \\not\\in P(s,u)\\), then there at least exists \\(P(s, v) = P(s, u), (u, v)\\), thus \\(\\delta(s, v) \\leq \\delta(s, u) + 1\\).</p> <p>Lemma 3 Upon the termination of <code>BFS(G, s)</code> given the source vertex \\(s\\in V\\), we have that \\(\\forall v\\in V. v.d \\geq \\delta(s, v)\\).  proof. We will prove the loop invariant on each <code>enqueue</code> operation, since each <code>enqueue</code> follows a distance update.  First note that we start with \\(s.d = 0 = \\delta(s, s)\\). Then, consider the distance updates, \\(v.d = u.d + 1\\), by induction hypothesis we know that  \\(u.d \\geq \\delta(s, u)\\). Also, since \\(v\\) is a neighbor of \\(u\\), from lemma 2, \\(\\delta(s, v) \\leq \\delta(s, u) + 1\\). Thus</p> \\[v.d = u.d + 1 \\geq \\delta(s,u)+ 1\\geq \\delta(s,v)\\] <p>For the unvisited vertices, we have that \\(v.d = \\infty \\geq \\delta(s,v)\\)</p>"},{"location":"csc265/bfs.html#correctedness","title":"Correctedness","text":"<p>Lemma 4 For any time in the execution of <code>BFS(G, s)</code>, the queue \\(q = (v_0, v_1, ..., v_r)\\) always holds for that </p> \\[v_r.d \\leq v_1.d+1, v_i.d \\leq v_{i+1}.d\\] <p>proof. We will prove the loop invariance on each <code>enqueue</code> operation. At first, \\(q = (s)\\) obviously holds for the claim. Then, consider each <code>enqueue</code> in the <code>while</code> loop in <code>line 11</code>, consider the state of queue before the <code>dequeue</code> in <code>line 7</code> to be \\((v_0, v_1, ..., v_r)\\) and the claimed property holds. After the <code>dequeue</code>, we have the queue become \\((v_1, ..., v_r)\\), then consider each <code>enqueue</code> on <code>line 11</code>, for each enqueued vertex \\(u\\), from induction hypothesis, we have that that \\(u.d = v_0.d + 1 \\geq v_r.d\\) and \\(u.d = v_0.d + 1 \\leq v_1.d + 1\\). </p> <p>Corollary 5 For vertices \\(v_i, v_j \\in V\\), if \\(v_i\\) is enqueued before \\(v_j\\), then \\(v_i.d \\leq v_j.d\\). proof. Trivial from lemma 4. </p> <p>Claim 6 (BFS Correctedness) Let \\(G=(V,E)\\) be a undirected, connected graph, and <code>BFS(G, s)</code> runs from \\(s\\in V\\). Then, during the execution,  1. Upon termination \\(v.d = \\delta(s, v)\\).  2. BFS discovers all vertex \\(v\\in V\\) reachable from \\(s\\)</p> <p>proof. We will prove 1. using contradiction on minimum. First assume that some vertex \\(v\\) be the vertex with minimum \\(\\delta(s,v)\\) s.t. \\(v.d \\neq \\delta(s, v)\\). By lemma 3 and this assumption we have that \\(v.d &gt; \\delta (s,v)\\) and \\(v\\) is reachable from \\(s\\). Let \\(u\\) be the vertex immediately preceeding \\(v\\) on a shortest path from \\(s\\) to \\(v\\) so that \\(\\delta(s,v) = \\delta(s,u)+1\\). Because \\(\\delta(s,u) &lt; \\delta(s,v)\\), we must have that \\(u.d = \\delta(s,u)\\). Now, </p> \\[v.d &gt; \\delta(s,v) = \\delta(s.u) + 1 = u.d + 1\\] <p>However, consider BFS algorithm. At the time we <code>dequeue</code> \\(u\\), there are 3 possible states for \\(v\\).  - <code>v.queued = FALSE</code> and is not in queue yet, then we <code>enqueue(v)</code>, resulting \\(v.d = u.d + 1\\). - <code>v.queued = FALSE</code> and is in queue, then by lemma 4 we have that \\(v.d \\leq u.d + 1\\). - <code>v.queued = TRUE</code>, then by corollary 5, we have that \\(v.d \\leq u.d\\). In all cases, we have the contradiction. </p> <p>By 1. We have 2. Otherwise \\(v.d = \\infty &gt; \\delta(s, v)\\)</p> <pre><code>from assets.graph_search import traversal_order, BFS, plot_graph\ntraversal_order(BFS, g, 0)\nplot_graph(g, ['order', 'distance'], layout, \"assets/bfs_1.jpg\")\n</code></pre> <p></p>"},{"location":"csc265/dfs.html","title":"Graphs: Depth-first Search","text":""},{"location":"csc265/dfs.html#depth-first-search","title":"Depth-First Search","text":"<p>Consider a undirected graph \\(G=(V,E)\\) with \\(|V|=n, |E|=m\\)</p> <p>The strategy for DFS is to go as \"deeper\" as possible. Similar to BFS, we start with all vertices being undiscovered; then we \"discover\" it when we first encounter it from some vertex's neighborhood; finally, we finish processing a vertex when we finish processing all of its neighbors. Thus, this suggested a recursive algorithm. </p> <p>We also use a timestamp to record the time when a vertex is discovered as <code>v.d</code>, and when it is finished as <code>v.f</code>. We will explore its uses later in DFS properties. </p> DFS(G)<pre><code>for u in G.V:\n    u.state = undiscovered\ntime = 0\nfor u in G.V:\n    if u.state == undiscovered:\n        DFS_visit(G, u)\n</code></pre> DFS_visit(G, u)<pre><code>time += 1\nu.d = time\nu.state = discovered\nfor v in u.neighbors:\n    if v.state == undiscovered:\n        DFS_visit(G, v)\nu.state = FINISHED\ntime += 1\nu.f = time\n</code></pre> <p>Lemma 1 After the execution of <code>DFS</code>, for \\(v\\in V\\), \\(1 \\leq v.d &lt; v.f \\leq 2n\\). proof. The bound \\([1, 2n]\\) is trivial. Since timestamp only increment when we discover and finish one vertex. Thus, there are at most \\(n\\) vertices to visit, and we visit each vertex exactly once. Also, note that time cannot decrease, and by <code>DFS_visit line 8</code>, we must have \\(v.d &lt; v.f\\). </p>"},{"location":"csc265/dfs.html#time-complexity","title":"Time Complexity","text":"<p>Note that <code>DFS_visit</code> is called on each <code>undiscovered</code> vertex exactly once, and in each <code>DFS_visit</code>, we loop through all of its neighbors once. Thus, the time complexity is \\(O(n+m)\\)</p>"},{"location":"csc265/dfs.html#depth-first-trees-and-depth-first-forests","title":"Depth First Trees and Depth First Forests","text":"<p>Consider the DFS algorithm, for each recursive call on <code>DFS_visit</code> on <code>line 4-6</code>. we add an edge \\((u,v)\\). Thus, we have a new graph \\(G'=(V,E')\\). Note that on each search path, we won't go further if the point is already discovered. Therefore, the graph is acyclic, thus it is a forest consisting one or more trees. Calling \\(G'\\) depth first forest_; and the trees within \\(G' = \\{T_1,..., T_k\\}\\) be the __depth first trees. </p>"},{"location":"csc265/dfs.html#parenthsis-theorem","title":"Parenthsis Theorem","text":"<p>Theorem 1 For any two vertices \\(u,v\\) we have one of the following conditions - \\([u.d, u.f] \\cap [v.d, v.f] = \\emptyset\\), and they are not a descendant of each other in \\(G'\\).  - \\([u.d, u.f] \\subset [v.d, v.f]\\), and \\(u\\) is a descendant of \\(v\\) in some depth fisrt tree \\(T_i\\).</p> <p>proof.  WLOG we assume that \\(u.d &lt; v.d\\), then by lemma 1, we have two cases.   \\(v.d &lt; u.f\\), so \\(v\\) was discovered while \\(u\\) haven't finished, which implies that \\(v\\) is a descendant of \\(u\\). Moreover, \\(v\\) must finishes earlier than \\(u\\), otherwise the call stacks won't return. \\(u.f &lt; v.d\\), then the intervals are disjoint, and \\(u\\) must finish exploration before discovering \\(v\\), thus they are not a descendant of each other. </p> <p>Corollary 2 If \\(v\\) is \\(u\\)'s proper descendant, then \\(u.d &lt; v.d &lt; v.f &lt;u.f\\)</p>"},{"location":"csc265/dfs.html#white-path-theorem","title":"White Path Theorem","text":"<p>Theorem 3 In a depth first forest \\(G'\\), \\(v\\) is a descendant of \\(u\\) IFF at the time \\(u.d\\) that the search discovers \\(u\\), there is a path from \\(u\\) to \\(v\\) consisting entirely of undiscoverted vertices.</p> <p>proof. \\(\\Rightarrow\\). If \\(v=u\\), then the path contains only \\(u\\), and it's undiscoverted. Overwise, by corollary 2, \\(u.d &lt; v.d\\) so that \\(v\\) is undiscovered yet. </p> <p>\\(\\Leftarrow\\), consider any path between \\(u, v\\) formed by undiscovered vertices, and \\(v\\) does not become a descendant in \\(G'\\) yet. WLOG assume that every vertex other than \\(v\\) along the path becomes a descendant of \\(u\\). Let \\(w\\) be the predecessor of \\(v\\) in the path so that #w$ is a descendant of \\(u\\). By corollary 2m \\(w.f \\leq u.f\\). Because \\(v\\) must be discovered after \\(u\\) is discoverted but before \\(w\\) finishes, we have that \\(u.d &lt; v.d &lt; w.f \\leq u.f\\), while \\(v.f \\leq w.f\\). Thus, \\([v.d, v.f] \\subset [u.d, u.f]\\), it is a desendant of \\(u\\). </p> <pre><code>import igraph as ig\n# create a example graph, and add one unreachable point\ng = ig.Graph.Famous('Walther'); g.add_vertex()\nlayout = g.layout();\n\nfrom assets.graph_search import traversal_order, DFS, plot_graph, topological_sort\ntraversal_order(DFS, g, 0)\nplot_graph(g, ['d', 'f'], layout, \"assets/dfs_1.jpg\")\n</code></pre> <p> </p>"},{"location":"csc265/dfs.html#topological-sort","title":"[+] Topological Sort","text":"<p>Note that both BFS and DFS works the same on directed graph (only need to modify <code>neighbors</code> to all outwards edges). </p> <p>Now, consider a directed acyclic graph (dag), a topological sort of a dag is a linear ordering of all vertices s.t. if \\(G\\) contains an edge \\((u,v)\\), then \\(u\\) appears before \\(v\\) in the ordering. </p> <p>Then, note that DFS iterates through all vertices exactly once, and we have the algorithm for togological sort</p> <p>topological_sort(G)<pre><code># precondition: G is a dag\nL = linked_list()\ncall DFS(G)\nfor each vertex is finishes, prepend it into L\nreturn L\n</code></pre> Obviously, <code>topological_sort</code> runs in \\(\\Theta(V+E)\\) as <code>DFS</code> does. </p>"},{"location":"csc265/dfs.html#correctedness","title":"Correctedness","text":"<p>Lemma 4 A directed graph \\(G\\) is acyclic IFF <code>DFS(G)</code> yields no back edges.  proof. \\(\\Leftarrow\\) Suppose DFS has a back edge. Then \\(v\\) is an ancestor of \\(u\\) in \\(G'\\). Thus, \\(G\\) contains a path from \\(v\\) to \\(u\\), and we have a cycle \\(u\\rightsquigarrow v \\rightarrow u\\).  \\(\\Rightarrow\\) suppose \\(G\\) contians a cycle \\(C\\). Let \\(v\\) be the first vertex to be discovered in \\(c\\), and \\((u,v)\\) be the preceding edge in \\(c\\). At time \\(v.d\\), the vertices of \\(c\\) from a path of undiscovered vertices from \\(v\\) to \\(u\\). By claim 3, \\(u\\) is a descendant of \\(v\\) in \\(G'\\). Thus, \\((u,v)\\) is a back edge.</p> <p>Theorem 5 <code>topological_sort(G)</code> produces a topological sort given a dag \\(G\\).  </p> <p>proof. Note that the order of vertices in \\(L\\) is determined by \\(v.f\\). Thus, we only need to show that for any pair of \\(u,v\\in V, u\\neq v\\), if \\((u,v)\\in E\\), then \\(v.f &lt; u.f\\). </p> <p>Consider any edge \\((u,v)\\) explored by DFS. When this edge is explored, \\(v\\) cannot be discovered but not finished, since then it will produce a back edge, contradicting lemma 4. Therefore, \\(v\\) is either undiscovered or already finished. If \\(v\\) is white, then it becomes a descendant of \\(u\\) and \\(v.f &lt; u.f\\). If \\(v\\) is finished, so that \\(v.f\\) has already been set. But we are still iterating through \\(u\\)'s neighbor. Thus, \\(v.f &lt; u.f\\). </p> <pre><code>import igraph as ig\n# create a example graph, and add one unreachable point\ng = ig.Graph.Tree_Game(30, directed=True); \nlayout = g.layout(\"tree\");\n\nfrom assets.graph_search import traversal_order, DFS, plot_graph, topological_sort\ntopological_sort(g)\nplot_graph(g, ['order', 'f'], layout, \"assets/dfs_2.jpg\")\n</code></pre> <p></p>"},{"location":"csc265/dfs.html#circle-detection","title":"Circle Detection","text":"<p>A corollary of lemma 4 is that during DFS, if we find a back edge, then it contains a cycle. Note that this proof also applies to undirected graph, if we think of each undirected edge as two directed edge. </p> <p>Thus, we have the modified <code>DFS_visit</code> for detecting cycle that contains \\(u\\).</p> DFS_visit_undirected(G, u)<pre><code>    # ...\n    for v in u.neighbors:\n        if v.state == undiscovered:\n            u.parent = v\n            DFS_visit(G, v)\n        # u is discovered before so there is a path \n        # between u, v, now this if check gives \n        # that the edge (u, v) is not discovered\n        # thus closes the cycle\n        elif u != v.parent: \n            raise CYCLE\n    # ...\n</code></pre> DFS_visit_directed(G, u)<pre><code>    # ...\n    for v in u.neighbors:\n        if v.state == undiscovered:\n            u.parent = v\n            DFS_visit(G, v)\n        # if u is discovered but not finished\n        # meaning that there is a path from v \n        # to u, now the path is closed by a new\n        # (v, u) edge\n        elif u.state == discovered: \n            raise CYCLE\n    # ...\n</code></pre>"},{"location":"csc265/dfs.html#stongly-connected-components","title":"[+] Stongly Connected Components","text":"<p>Given a graph \\(G = (V,E)\\), the strongly connected component (SCC) of \\(G\\) is a maximal set of vertices \\(C\\subseteq V\\) s.t. \\(\\forall u,v \\in C. \\exists u\\rightsquigarrow v, v\\rightsquigarrow u\\). Note that each vertex itself is a connected component, as it is reachable to itself. </p> <p>Note that if we have two SCCs \\(C_1, C_2\\), and there exists edges from some vertices in \\(C_1\\) to \\(C_2\\), and \\(C_2\\) to \\(C_1\\), then the SCCs merged into a larger one. Thus, each graph has a unique SCC decomposition. </p> <p>Define the transpose of some graph \\(G=(V, E)\\) as \\(G^T = (V, E^T)\\) where \\(E^T = \\{(v,u): (u,v) \\in E\\}\\), i.e. reverse all edge directions. Note that this is a transpose because if we represent \\(G\\) using a adjacency matrix, then \\(G^T\\) is the transpose of the adjacency matrix. Note that \\(G\\) and \\(G^T\\) have exactly the same SCC.</p> <p>Then, the algorithm is as following</p> DFS(G)<pre><code>for u in G.V:\n    u.state = undiscovered\n    time = 0\n    for u in G.V:\n        if u.state == undiscovered:\n            DFS_visit(G, u)\n</code></pre> <p><code>c title=\"SCC(G)\" linenumes=\"1\" DFS(G) to compute u.f for each vertex compute G.T be the transpose DFS(G.T) but line 4 uses decreasing u.f order output vertices of each tree formed by DFS(G.T)</code></p>"},{"location":"csc265/dfs.html#time-complexity_1","title":"Time Complexity","text":"<p>DFS runs in \\(O(n+m)\\), transpose a graph takes \\(O(n+m)\\) time (reverse all edges in adjacency list). The final output of vertices takes at most \\(O(n)\\) time (When we return from all <code>DFS_visit</code> stacks in <code>DFS</code>, we have already found one completed tree). Thus the total time is still in \\(O(n+m)\\).</p>"},{"location":"csc265/dfs.html#correctedness_1","title":"Correctedness","text":"<p>Some extra notes: Consider <code>SCC line 3</code>, the decreasing \\(u.f\\) order is actually the topological sorted order.</p> <p>All the \\(u.d\\) and \\(u.f\\) mentioned here will be referring to <code>SCC line 1</code>. Actually, for <code>SCC line 3</code> we can omit the timestamps since it is not necessary. </p> <p>Lemma 6 If \\(C,C'\\) are two distinct SCC in directed graph \\(G\\). Then, if exists some path from \\(x\\in C\\) to \\(x'\\in C'\\), then we cannot have path from \\(y\\in C\\) to \\(y'\\in C'\\).  proof. As what our observation. </p> <p>Then, for \\(U\\subseteq V\\), define \\(d(U) = \\min\\{u.d: u\\in U\\}, f(U) = \\min\\{u.f: u\\in U\\}\\). </p> <p>Lemma 7 Let \\(C,C'\\) be distinct SCC in \\(G\\). If \\(\\exists (u,v)\\in E, u\\in C, v\\in C'\\), then \\(f(C) &gt; f(C')\\). proof. Consider two cases:  If vertices in \\(C\\) is discovered earlier than \\(C'\\), then \\(d(C) &lt; d(C')\\). Take \\(x\\in C. x.d = d(C)\\) be the first dicovered vertex. Then at time \\(x.d\\), all vertices in \\(C, C'\\) are undiscovered. By SCC definition, for any vertex \\(w\\in C'\\), exists a white path \\(x\\rightsquigarrow u\\rightarrow v\\rightsquigarrow w\\). By white-path theorem, all vertices in \\(C\\) and \\(C'\\) become descendant of \\(x\\) in \\(G'\\). Thus \\(x.f = f(C) &gt; w.f \\geq f(C')\\).</p> <p>If \\(d(C') &lt; d(C)\\), then by lemma 6 we cannot have a path from vertices \\(C'\\) to vertices in \\(C\\). Thus, all vertices in \\(C\\) is undiscovered until all vertices in \\(C'\\) finishes. Resulting \\(f(C) &gt; f(C')\\). </p> <p>Corollary 8 Let \\(C,C'\\) be distinct SCC in \\(G\\). If \\(\\exists (u,v)\\in E^T, u\\in C, v\\in C'\\), then \\(f(C) &lt; f(C')\\). proof. exactly the opposite of lemma 7.</p> <p>Theorem 9 (Correctedness of SCC) SCC correctly produce the SCC of \\(G\\).  </p> <p>proof. We will argue by induction on the number of depth-frst trees found in the DFS of \\(G^T\\), that the first \\(k\\) trees produced by <code>DFS(G.T)</code> is a SCC. </p> <p>For \\(k = 0\\), then exists no vertices, vacuously true. </p> <p>For \\(k &gt; 0\\), assume the first \\(k\\) DFT is SCC. Let the root of the \\((k+1)\\)'th tree be \\(u\\), and let \\(C\\) be the SCC that \\(u\\) belongs. By the topological order we have that \\(u.f = f(C) &gt; f(C')\\) for any SCC \\(C'\\neq C\\) and has yet to be visited. At time \\(u.d\\), all vertices in \\(C\\) is not discovered, thus by white-path theorem, \\(\\forall v\\in C\\) are in the depth-first tree rooted at \\(u\\). Moreover, the vertices in first \\(k\\) SCC must been finished. Thus, no vertex in any SCC other than \\(C\\) will be descendant of \\(u\\) during <code>DFS(G.T)</code>. </p>"},{"location":"csc265/dfs.html#component-graph","title":"Component Graph","text":"<p>From the observation above, we know that if there are edges incoming and outcoming between two SCCs, then they can be merged into one SCC. Thus, we define a component graph \\(G^{C} = (V^C, E^C)\\) for graph \\(G\\) with SCCs \\(C_1,...,C_k\\). We define \\(V^C = \\{v_1,...,v_k\\}\\) w.r.t. to each SCC, and exists \\((v_i, v_j) \\in E^C\\) if \\((x\\in C_i, y\\in C_j) \\in E\\), a.k.a. exists an edge from \\(C_i\\) to \\(C_j\\). </p> <p>Claim 10 \\(((G^T)^C)^T = G^C\\). proof. Note that \\(((G^T)^C)^T = (V^C, ((E^T)^C)^T)\\). Consider \\((E^T)^C\\), \\(\\forall (v_i, v_j) \\in E^C. (x\\in C_i, y\\in C_j) \\in E\\), thus \\((y\\in C_j, x\\in C_i) \\in E^T, (v_j, v_i) \\in (E^T)^C, (v_i, v_j) \\in ((E^T)^C)^T\\)</p> <p>Corollay 11 <code>DFS(G.T)</code> visits \\(G^C\\) in topologically sorted order. proof. Consider the topological orderness of vertices of \\(G\\), and since \\(((G^T)^C)^T = G^C\\).</p>"},{"location":"csc265/disjoint_set.html","title":"Disjoint Set","text":""},{"location":"csc265/disjoint_set.html#adt-disjoint-set","title":"ADT: Disjoint Set","text":"<ul> <li>Objects: a collection of disjoint sets \\(\\mathcal S = \\{S_1,..., S_k\\}\\) where each set has a representative \\(x_i\\in S_i\\). </li> <li>Operations:<ul> <li><code>makeset(x)</code> If \\(x\\) is not already in any sets of \\(\\mathcal S\\) (since disjoint), then create a new set of only \\(\\{x\\}\\) with representative \\(x\\). Otherwise return error. </li> <li><code>findset(x)</code> given a pointer to the element, return the representative of the set containing \\(x\\). </li> <li><code>union(x, y)</code> union the sets that contain \\(x\\in S_i, y\\in S_j\\), and choose any element within \\(S_i\\cup S_j\\) to be the new representative. Note that <code>x, y</code> can be in the same set, in which <code>union(x, y)</code> should do nothing. </li> <li><code>link(x, y)</code> save as union, but with the precondition that <code>x, y</code> must be representatives of different sets. </li> </ul> </li> </ul>"},{"location":"csc265/disjoint_set.html#connected-component","title":"Connected component","text":"<p>Connected components is a common problem in geometry processing. For graph \\(G = (V, E)\\). A connected component is a subset of vertices \\(V' \\subseteq V\\) s.t. for any pair of vertices \\(v_1, v_2 \\in V'\\), there exists a path between them. Thus, a graph can have several connected components. </p> <p>With a disjoint set, we can let the vertices be the elements, then <code>union</code> two sets together if there is an edge. </p> connected_components(G)<pre><code>for v in G.v:\n    makeset(v)\nfor (u, v) in G.E:\n    if findset(u) != findset(v):\n    union(u, v)\n</code></pre> <pre><code>from assets.disjointset import DisjointSet\nfrom random import randint, seed\n\n\nseed(21)\nNV = 25\nE = set()\nwhile len(E) &lt; NV * 2 // 3:\n    x, y = randint(0, NV - 1), randint(0, NV - 1)\n    if x != y and (y, x) not in E :\n        E.add((x, y))\nE = list(E)\nlabel_fn = lambda x: f\"{x.value}&lt;br&gt;{x.rank}\"\n\nimport igraph as ig\nimport matplotlib.pyplot as plt\n\nfig, ax = plt.subplots()\ng = ig.Graph()\ng.add_vertices(NV)\ng.add_edges(E)\nig.plot(g, target=ax)\nax.set_axis_off()\nfig.savefig(\"assets/disjoint_set_1.jpg\")\n</code></pre> <p></p>"},{"location":"csc265/disjoint_set.html#link","title":"Link","text":"<p>Note that <code>union(x, y) = if findset(x) != findset(y): link(findset(x), findset(y))</code>. We use <code>link</code> to decompose the <code>union</code> operation so that the time analysis is directly related to the number of <code>link</code> and <code>findset</code>. </p> <p>Claim 1 Define \\(T(m, n), m \\geq n\\) be the worst case time executing any set of \\(m\\) operations from <code>{makeset, findset, link}</code>, which \\(n\\) of the operations are <code>makeset</code>, starting from \\(\\mathcal S = \\emptyset\\).  Then, there are at most \\((n-1)\\) <code>link</code>'s.  proof. There are at most \\(n\\) sets, and each link merges two sets into one, thus reduces the number of sets by \\(1\\). </p>"},{"location":"csc265/disjoint_set.html#naive-implementation-set-of-trees-linked-lists","title":"Naive Implementation: Set of Trees / Linked lists","text":"<ul> <li>Objects: each set \\(S_i\\) is stored as a trees, where the representative is the root of the tree. Then, we store all roots in a list. </li> <li>Operation:<ul> <li><code>makeset(x)</code> make a new tree containing \\(x\\), and put into \\(\\mathcal S\\). Taking \\(O(1)\\) time. </li> <li><code>findset(x)</code> recursively traverse the parent until the root. Takeing \\(O(h)\\) time where \\(h\\) is the height of the tree. </li> <li><code>link(x, y)</code> take one root and connected to another</li> </ul> </li> </ul> <p>Note that linked list is also a tree, and it's the simplest implementation of the disjoint set. However, the runtime for <code>findset</code> with linked list implementation will be \\(O(n)\\). In worst case, \\(T(m,n) \\in O(mn)\\) which is far too large. </p> <p>Another implementation is to have 2 level trees, where all nodes except the root are children of the root. In this case, <code>findset(x)</code> takes \\(O(1)\\) time, while <code>link(x, y)</code> need to reconnected all nodes in one set to another. Resulting \\(T(m,n) \\in O(mn)\\) time again. </p> <p>Therefore, we need to keep a good balance in between <code>findset</code> and <code>link</code>.</p> <pre><code>ds = DisjointSet()\nfor v in range(NV):\n    ds.makeset(v)\nnodes = ds.nodes\nfor e in E:\n    ds.union_naive(nodes[e[0]], nodes[e[1]])\nds.plot(label_fn, \"assets/disjoint_set_2.jpg\")\n</code></pre> <p></p>"},{"location":"csc265/disjoint_set.html#heuristic-1-weighted-union","title":"Heuristic 1: Weighted Union","text":"<p>We want to make sure the tree height is small, while keeping <code>link</code> in \\(O(1)\\) time. Thus, the simplest idea is to attach the shorter tree to the root of the longer tree, thus the total height won't change. Thus, for each tree, we can store its height. </p> makeset(x)<pre><code>x.height = 0\nS.append(x)\n</code></pre> link(x, y)<pre><code>if x.height &gt; y.height:\n    y.parent = x\nelif x.height &lt; y.height:\n    x.parent = y\nelse:\n    y.parent = x\n    x.height += 1\n</code></pre>"},{"location":"csc265/disjoint_set.html#time-analysis","title":"Time Analysis","text":"<p>Claim 2 After any operation on the disjoint set, for any element \\(x \\in S_i \\in \\mathcal S\\), \\(|S_i| \\geq 2^{x.\\text{height}}\\). </p> <p>proof. Note that only <code>makeset</code> and <code>link</code> will modify \\(\\mathcal S\\). So that we will use induction on the operations. </p> <p>For <code>makeset</code>, the other sets won't change, and the newly added set is \\(\\{x\\}\\) with height 0, and we have that \\(2 \\geq 2^0\\).</p> <p>For <code>link(x, y)</code>, if their height is different, WLOG assume <code>x.height &gt; y.height</code> so that \\(S_x\\) is merged into \\(S_y\\), and the size of the new set is \\(|S_x| + |S_y| \\geq 2^{x.\\text{height}} + 2^{y.\\text{height}} \\geq  2^{y.\\text{height}}\\)</p> <p>For <code>link(x, y)</code> with the same height, then the size of the new set is </p> \\[|S_x| + |S_y| \\geq 2^{x.\\text{height}} + 2^{y.\\text{height}} = 2^{x.\\text{height} + 1}\\] <p>Corollary 3 For any \\(x\\in S_x\\in \\mathcal S, x.\\text{height} \\leq \\lfloor \\lg n\\rfloor\\) </p> <p>proof. from Claim 2 we have</p> \\[\\begin{align*} |S_x| &amp;\\geq  2^{x.\\text{height} + 1}\\\\ n &amp;\\geq 2^{x.\\text{height} + 1}\\\\ \\lg n &amp;\\geq x.\\text{height} + 1\\\\ \\lfloor \\lg n\\rfloor &amp;\\geq x.\\text{height}\\\\ \\end{align*}\\] <p>Corollary 4 \\(T(m,n) \\in O(m\\log n)\\)</p> <pre><code>ds = DisjointSet()\nfor v in range(NV):\n    ds.makeset(v)\nnodes = ds.nodes\nfor e in E:\n    ds.union_weighted(nodes[e[0]], nodes[e[1]])\nds.plot(label_fn, \"assets/disjoint_set_4.jpg\")\n</code></pre> <p></p>"},{"location":"csc265/disjoint_set.html#heuristic-2-path-compression","title":"Heuristic 2: Path Compression","text":"<p>When we do <code>findset</code>, we have already put the effort in tracing the parent, we can, at the same time, update all nodes on the traced path directly point to the root. </p> findset(x)<pre><code>if x.parent is None:\n    return x\nroot = findset(x.parent)\nx.parent = root\nreturn root\n</code></pre> <pre><code>ds = DisjointSet()\nfor v in range(NV):\n    ds.makeset(v)\nnodes = ds.nodes\nfor e in E:\n    ds.union_compression(nodes[e[0]], nodes[e[1]])\nds.plot(label_fn, \"assets/disjoint_set_5.jpg\")\n</code></pre> <p></p>"},{"location":"csc265/disjoint_set.html#time-analysis-log-star","title":"[+] Time Analysis (log-star)","text":"<p>Define the iterated logrithm \\(\\log^*\\) as </p> \\[\\log_{b}^* n = \\begin{cases}0 &amp;n \\in \\{0, 1\\}\\\\1+\\log^*(\\log n) &amp;n&gt;1\\end{cases}\\] <p>and its inverse function tetration</p> \\[b\\upuparrows n = \\begin{cases}0 &amp; n = 0\\\\b^{b\\upuparrows n-1} &amp;n&gt;0\\end{cases}\\] <p>For convinence, assume that \\(n\\) is a power of \\(2\\), so that we don't have to deal with floors or ceilings. </p> <p>Define find path be the sequence of nodes (and the edges connecting node and its parent) from a node \\(u\\), by recursively traversing its parent, to the root. </p> <p>Lemma 1 The rank on a <code>findset</code> path is increasing. proof. Since <code>link</code>, with weighted union, will always have higher rank nodes be the root. Then, <code>findset</code> will only attach to the root, while we have the assumption that all nodes has smaller rank than the root. </p> <p>Lemma 2 The max number of nodes of rank \\(r\\) is at most \\(\\frac{n}{2^r}\\). proof. First note that in our algorithm, rank cannot decrease and rank only changes in <code>link</code>. Then, by weighted union, to make a node of rank \\(r &gt; 0\\), you need at least two nodes of rank \\(r-1\\). Thus, recursively, you need at least \\(2^r\\) nodes to create a node of rank \\(r\\). And we have a total of \\(n\\) nodes. </p> <p>Claim 3 (log-star runtime) \\(T(m,n) \\in O(m\\log^* n)\\) </p> <p>proof. First, define the intervals \\(b_0 = [0, 0], b_i = [(2\\upuparrows {i-1}), (2\\upuparrows {i}) - 1]\\) for \\(i = 1, 2, ...\\) As some references, \\(b_0 = [0, 0], b_1 = [1, 1], b_2 = [2, 3], b_3 = [4, 15], b_4 = [16, 65535]\\).  Then we define the partition over \\(\\mathcal S\\) as some \"buckets\"</p> \\[B_i = \\{x\\in\\mathcal S: x.\\text{rank} \\in b_i\\}\\] <p>We have the following claims:</p> <ol> <li>the total number of buckets is at most \\(\\log_2^* n\\), as the inverse of tetration. </li> <li>the number of elements in bucket \\([B, 2^B - 1]\\) is at most \\(\\frac{2n}{2^B}\\). Following lemma 2 that \\(\\sum_{r=B}^{2^B-1}\\frac{n}{2^r} \\leq \\frac{2n}{2^B}\\)</li> </ol> <p>Let \\(F\\) be the sequence of all the edges traversed in \\(m\\) <code>findset</code> operations, we need to count the number of edges, which is the total number of cost.</p> <p>Let \\(T_1\\) be the set of the edges connected to a root, since each <code>findset</code> will eventually traversed to the root, we have that \\(|T_1| = m\\). </p> <p>Let \\(T_2\\) be the set of the edges that traversed between nodes from different bins. Since the rank on the find path has increasing ranks and we have at most \\(\\log^*n\\) bins, we have \\(|T_2| \\in O(m\\log^*n)\\)</p> <p>Let \\(T_3\\) be the set of the edges that traversed in the same bin. Consider any edge \\((u, v)\\), and the nodes \\(u, v, v_1, ..., v_k\\) on the find path to the root. Then none of the nodes are root, and they will never appear in \\(T_3\\) since they are directly attached to a root. Also, known that rank is strictly increasing, so that the length of each find path is at most \\(2^B-1-B &lt; 2^B\\). Therefore, for a bucket \\([B, 2^{B-1}]\\). The total number of edges is at most \\(\\frac{2n}{2^B}2^B = 2n\\). With at most \\(\\log^*n\\) bins, \\(|T_3| = 2n \\log^*n\\).</p> <p>Therefore, we can conclude that the worst case amortized run time is \\(T(m, n) = |T_1| + |T_2| + |T_3| \\in O(m\\log^* n)\\)</p>"},{"location":"csc265/disjoint_set.html#implementation","title":"Implementation","text":"DisjointSet implementation <pre><code># Note that this implementation add unnecessary parent for ploting\nfrom .plot_trees import plot_tree, construct_tree_nodes\n\nclass DNode:\n    def __init__(self, x) -&gt; None:\n        self.value = x\n        self.rank = 0\n        self.parent = self\n        # this is only for easy printing\n        self.children = []\n\nclass DisjointSet:\n\n    def __init__(self) -&gt; None:\n        self.nodes = []\n        self.n = 0\n\n\n    def makeset(self, x):\n        self.nodes.append(DNode(x))\n        self.n += 1\n\n    # path compression\n    def findset(self, node):\n        if node.parent is node:\n            return node\n        root = self.findset(node.parent)\n        if node.parent is not root:\n            root.children.append(node)\n            node.parent.children.remove(node)\n            node.parent = root\n        return root\n\n    def findset_naive(self, node):\n        if node.parent is node:\n            return node\n        return self.findset_naive(node.parent)\n\n    def union(self, x, y):\n        sx = self.findset(x)\n        sy = self.findset(y)\n        if sx is not sy:\n            self.__link(sx, sy)\n            self.n -= 1\n\n    def union_naive(self, x, y):\n        sx = self.findset_naive(x)\n        sy = self.findset_naive(y)\n        if sx is not sy:\n            self.__link_naive(sx, sy)\n            self.n -= 1\n\n    def union_weighted(self, x, y):\n        sx = self.findset_naive(x)\n        sy = self.findset_naive(y)\n        if sx is not sy:\n            self.__link(sx, sy)\n            self.n -= 1\n\n    def union_compression(self, x, y):\n        sx = self.findset(x)\n        sy = self.findset(y)\n        if sx is not sy:\n            self.__link_naive(sx, sy)\n            self.n -= 1\n\n    # weighted union\n    def __link(self, x, y):\n        if x.rank &gt; y.rank:\n            y.parent = x\n            x.children.append(y)\n        elif x.rank &lt; y.rank:\n            x.parent = y\n            y.children.append(x)\n        else:\n            y.parent = x\n            x.children.append(y)\n            x.rank += 1\n\n    def __link_naive(self, sx, sy):\n        sx.parent = sy\n        sy.children.append(sx)\n        sy.rank = max(sy.rank, sx.rank + 1)\n\n\n    def plot(self, label_fn, path):\n        counter, i = 0, 0\n        root = DNode(r\"$\\mathcal S$\")\n        while counter &lt; self.n:\n            node = self.nodes[i]\n            if node.parent is node:\n                root.children.append(node)\n                counter += 1\n            i += 1\n        return plot_tree(construct_tree_nodes(root, label_fn, children_attr=\"children\"), path)\n</code></pre> <pre><code>ds = DisjointSet()\nfor v in range(NV):\n    ds.makeset(v)\nnodes = ds.nodes\nfor e in E:\n    ds.union(nodes[e[0]], nodes[e[1]])\nds.plot(label_fn, \"assets/disjoint_set_6.jpg\")\n\nprint(\"Number of connected components\", ds.n)\n#&gt;&gt; Number of connected components 9\n</code></pre>"},{"location":"csc265/hashset.html","title":"Set: Hash Tables","text":""},{"location":"csc265/hashset.html#hash-table","title":"Hash Table","text":"<p>For hash table, we take some hash function \\(hash: \\mathcal U \\rightarrow\\{0, 1, ..., m - 1\\}\\) where \\(\\mathcal U\\) is the universe that keys are chosen from, and \\(m\\) is the number of bins pre-defined for hash table. </p> <p>Then, we use the hash function to determine which spot should we insert the (key, value) into. Thus, when we want to, we can directly find the spot by running the hash function again. </p> <p>However, in most cases \\(|\\mathcal U| &gt;&gt; m\\) so that there must exist \\(x_1 \\neq x_2\\neq \\cdots x_k. hash(x_1) = \\cdots hash(x_k)\\). In this case, we call it a collision. The main goal for hash table is to resolve collision problem, and to choose \"good\" hash function. </p>"},{"location":"csc265/hashset.html#hash-functions","title":"Hash Functions","text":"<p>Some good hash function examples for natural number keys \\(k\\) (note that any key is essentially bits, which can be seen as unsigned intergers) are </p> <ul> <li> <p>division method: \\(hash(k) = k\\mod p\\) where \\(p \\geq m\\) is a prime number.  </p> <ul> <li>We'd like to make such the remainder by a prime number, so that it's more likely to depent on all bits of the input key instead of a fixed range of bits. For example, if we use \\(k\\mod 2^i\\), then it will only depend on the last \\(i\\) bits of \\(k\\). </li> <li>Since it only takes one division operation, it is quite fast. </li> </ul> </li> <li> <p>multiplication method: \\(hash(k) = \\lfloor m (k A - \\lfloor kA\\rfloor)\\rfloor\\) where \\(0 &lt; A &lt; 1\\). </p> <ul> <li>This method fits well with binary based operations and when \\(m\\) is set to be a power of 2. </li> <li>Often \\(A\\approx (\\sqrt 5 - 1)/2 = 0.6180339887\\) is likely to work reasonably well. </li> </ul> </li> </ul>"},{"location":"csc265/hashset.html#closed-addressing-chaining","title":"Closed Addressing (Chaining)","text":"<p>To resolve a collision, the simplest way is to implement each bin as a linked list (or other list implementations, but runtime analysis will be harder), which is called chaining, or closed addressing.</p> <p>Specifically,</p> <ul> <li>Object: fix-sized array of bins</li> <li>Operation:   <ul> <li><code>search(H, x)</code>: return whether \\(x\\) it's in <code>H[hash(x)]</code></li> <li><code>insert(H, x)</code>: insert \\(x\\) into <code>H[hash(x)]</code></li> <li><code>delete(H, x)</code>: delete \\(x\\) from <code>H[hash(x)]</code></li> </ul> </li> </ul> <p>Assuming simple uniform hashing, i.e. the expected value of each bin size \\(a := E(b_i) = n/m\\), where \\(n\\) is the size of elements. Further assume that the hash function takes \\(O(1)\\) time. </p>"},{"location":"csc265/hashset.html#runtime-analysis","title":"Runtime analysis","text":"<p>Claim 1.1 <code>insert</code> takes worst case \\(O(1)\\) time. proof. insert the new node to the head (only update 2 pointers). </p> <p>Claim 1.2 Under simple uniform hashing assumption, <code>search</code> takes average-case \\(\\Theta(1+a)\\) time.  proof. For unsuccessful search, the time needed is 1 hash and traverse the linked list in the bin, which takes \\(a\\) time. </p> <p>For successful search, there are \\(n\\) cases for input \\(x\\), i.e. \\(n\\) keys in the table, assuming that each element in the table has \\(1/n\\) probability being searched. Define \\(I_{ij}\\) be the indicator random variable that the ith key and jth key being inserted to the same bin. By simple uniform hashing, the probability that \\(P(I_{ij} = 1) = 1/m\\), thus by Bernoulli distribution, \\(E(I_{ij}) = 1/m\\). Then, to find some node \\(i\\), we need to compare the searched key with all nodes before \\(i\\), and by our <code>insert</code>, those are nodes inserted after \\(i\\); and then \\(i\\). Therefore, for some node \\(i\\), the number of elements compared is  \\(1 + \\sum_{j=i+1}^n I_{ij}\\). We can write the average case time as </p> \\[\\begin{align*} E(T(n)) &amp;= E(\\sum_{i=1}^n \\frac{1}{n} (1 + \\sum_{j=i+1}^n I_{ij}))\\\\ &amp;= \\frac{1}{n} \\sum_{i=1}^n (1 + \\sum_{j=i+1}^n E(I_{ij}))\\\\ &amp;= \\frac{1}{n} \\sum_{i=1}^n (1 + \\sum_{j=i+1}^n \\frac{1}{m})\\\\ &amp;= 1 + \\frac{1}{nm}(n^2 - \\frac{n(n+1)}{2})\\\\ &amp;= 1 + \\frac{a}{2} - \\frac{a}{2n}\\\\ &amp;\\in \\Theta(1+a) \\end{align*}\\] <p>Claim 1.3 Under simple uniform hashing assumption, <code>delete</code> takes average-case \\(\\Theta(1+a)\\) time.  proof. search, and then constant time to update pointers. </p> <p>Note that, if \\(n/m\\) is proportional, then \\(a\\in O(1)\\). Thus, all operations take constant worst case expected time. </p>"},{"location":"csc265/hashset.html#open-addressing","title":"Open Addressing","text":"<p>For open addressing, we have </p> <ul> <li>Object: fix-sized array (key, value) of size \\(M\\)</li> <li>Operation:</li> <li><code>search(H, x)</code>: seach for each of <code>H[h(x, 0)], H[h(x, 1)], ...</code> until find or encountered <code>None</code></li> <li><code>insert(H, x)</code>: try <code>H[h(x, 0)], H[h(x, 1)], ...</code> until we find <code>None</code> or <code>DEL</code> at <code>i</code>, then insert into <code>H[h(x, 1)]</code>. </li> <li><code>delete(H, x)</code>: search for \\(x\\), say at <code>H[h(x, 0)]</code>, then make <code>H[h(x, 0)] = DEL</code></li> </ul> <p>The successively examining of <code>H[h(x, 0)], H[h(x, 1)], ...</code> is called probing. </p> <p>First, note that our array eventually have size \\(M \\geq n\\), otherwise we are unable to store all wanted key, value pairs, since the table can be filled up. </p> <p>Then, Note that in this case, \\(h(x, i)\\) take a key \\(x\\) and a counter \\(i\\). We define this as a parameterized hash function</p> \\[h: \\mathcal U \\times \\{0, ..., M - 1\\} \\rightarrow \\{0, ..., M - 1\\}\\] <p>Consider the sequence \\((h(x, i))_{i=0}^{M-1}\\), calling it the probe sequence of key \\(x\\). It must be a permutation of \\(\\{0, 1, \\cdots, M-1\\}\\) so that the sequence performs like the insertion order for \\(x\\).  We want to make the uniform hashing assumption, i.e. \\(h\\) generates probe sequence for each key being equally likely to be any of \\(M!\\) permutaions of \\(\\{0, ..., M-1\\}\\). Unfortunately, such assumption is hard to implement. </p> <p>The following techniques guarentee that the probe sequence for each key \\(x\\in\\mathcal U\\) is valid. However, the mast number of probe sequences that they can generate is \\(m^2\\). Thus none of them satisfy the uniform hashing assumption. </p> <p>All methods uses the ordinary hash function \\(hash\\) we defined above, refers to as auxiliary hash function. We know that \\(hash\\) will collide, so that we want to avoid collision by the additional \\(i\\). </p>"},{"location":"csc265/hashset.html#linear-probing","title":"Linear Probing","text":"\\[h(x, i) = (hash(x) + c_0i) \\mod M. \\quad c_0\\in\\mathbb N^+\\] <p>Consider the example of \\(c_0=1\\), then the probe sequence is simply \\((hash(k), hash(k) + 1, ...,M-1, 0, 1, ...)\\). However, linear probing suffers from primary clustering. Clusters arise because an empty slot preceded by i full slots gets filled next with probability \\((i+1)/M\\). Thus, the longer it runs, the occupied slots tend to get longer. Also, if \\(hash(x) = hash(y)\\), then their probe sequences are the same. </p>"},{"location":"csc265/hashset.html#quadratic-probling","title":"Quadratic Probling","text":"\\[h(x, i) = (hash(x) + c_0i + c_1i^2) \\mod M.\\quad c_0, c_1\\in\\mathbb N^+\\] <p>The same issue that if if \\(hash(x) = hash(y)\\), then their probe sequences are the same. Also, it still suffers clustering problem, but milder than linear probing.</p>"},{"location":"csc265/hashset.html#double-hashing","title":"Double Hashing","text":"\\[h(x, i) = (hash_1(x) + i \\cdot hash_2(x)) \\mod M\\] <p>It is \"double\" as we use two different hash functions. Thus, the chance that \\(hash_1, hash_2\\) both collide is much reduced. In practice, this tends to give the best results. </p>"},{"location":"csc265/hashset.html#universal-hashing","title":"Universal Hashing","text":"<p>With a fixed hash function, then a malicious adversary can choose \\(n\\) keys that all collide with each other. The only efficient way to improve the situation is to randomly sample hash functions in a way that is independent of the keys that are actually going to be stored. </p> <p>The approach is called universal hashing, at the beginning of hashtable construction, we randomly pick a hash function from a class of functions. Therefore, given a fixed sequence of input and opeartions, each execution will have different results, and guarentees a good average case time. </p> <p>Let \\(\\mathcal H\\) be a finite collection of hash functions \\(h: \\mathcal U \\rightarrow \\{0, 1,..., m-1\\}, \\mathcal H = \\{h_1,...,h_n\\}\\). \\(\\mathcal H\\) is universal if for each pair of distinct keys \\(k_1, k_2\\in\\mathcal U\\). There are at most \\(|\\mathcal H|/m\\) hash functions \\(h\\in\\mathcal H\\) s.t. \\(h(k_1) = h(k_2)\\). </p> <p>In other words, with a hash function randomly chosen from \\(\\mathcal H\\), for any pair of distinct keys \\(k_1, k_2\\),</p> \\[P(h(k_1) = h(k_2))\\leq 1/m\\]"},{"location":"csc265/hashset.html#universal-hashing-theorem","title":"Universal Hashing Theorem","text":"<p>Claim Using universal hasing and chaining. The worst case expected running time for <code>search</code> is \\(1 + \\frac{n}{m}\\). </p> <p>proof. The settings are very much similar to the average case running time for chaining, but note that we are now considering expected running time instead of average running time.  </p> <p>Let \\(h\\in\\mathcal H\\) be chosen randomly, let \\(n\\) be the size of data in the table \\(T\\), define \\(a = n/m\\) be the load factor. \\(I_{ij} =\\mathbb I(h(k_i) = h(k_j))\\). Since \\(h\\in\\mathcal H\\), \\(I_{ij} = P(h(k_i) = h(k_j))\\leq 1/m\\). </p> <p>Define the random variable \\(Y_i\\) be the number of keys other than \\(k_i\\) that is in the same bin as \\(k_i\\). Then, </p> \\[E(Y_i) = E[\\sum_{k_j\\in T, i\\neq j} I_{ij}] \\leq \\sum_{k_j\\in T, i\\neq j} \\frac{1}{m}\\] <p>Thus, for unsuccessful search, \\(k_i\\not\\in T, E(Y_i) \\leq  \\sum_{k_j\\in T} \\frac{1}{m} = \\frac{n}{m}\\).   for successul search, \\(E(Y_i) \\sum_{k_j\\in T, i\\neq j} \\frac{1}{m} = \\frac{n-1}{m}\\).</p> <p>Let the random variable \\(n_{h(k)}\\) be the number of linked list nodes traversed in bin \\(h(k)\\),  for unsuccessful search, it traverses all nodes, \\(E(n_{h(k_i)}) = E(Y_i) \\leq n/m\\); for successful search, it traverses all nodes plus \\(k_i\\) itself, \\(E(n_{h(k_i)}) = E(Y_i)  + 1\\leq \\frac{n-1}{m} +1 &lt; \\frac{n}{m}+1\\);  </p>"},{"location":"csc265/hashset.html#universal-class-of-hash-functions","title":"Universal Class of Hash Functions","text":"<p>For \\(\\mathcal U = \\{0, 1, ..., 2^{w} - 1\\}\\), for a hash table with \\(m=2^M\\) bins, the following family of hash functions is universal</p> \\[\\mathcal H_{M, w} = \\{h_{ab} = ((ak + b)\\mod 2^w) // 2^{w-M} : 0 &lt; a &lt; 2^w, a\\text{ is odd}, 0\\leq b &lt; 2^{w-M}\\}\\] <p>This family is very natural for binary based computers. Note that \\(\\mathcal U\\) represent is the set of binary representations of unsigned intergers with \\(w\\) bits, typically we have \\(w = 16, 32, 64\\). Then, \\(ak+b\\) is unsigned integer arithmetic, \\(\\mod 2^w\\) is simply taking the last \\(w\\) bits, and \\(// 2^{w-M}\\) is a bit shift, taking the \\(M\\) left most bits. </p> <p>For a hash table with \\(m\\) bins, let \\(p &gt; m\\) be prime, define \\(\\mathbb Z_{p} = \\{0, 1, ..., p-1\\}, \\mathbb Z_p^* =  \\{1, ..., p-1\\}\\). The following family of hash functions is universal</p> \\[\\mathcal H_{p, m} = \\{h_{ab} = ((ak + b)\\mod p)\\mod m : a\\in Z_p^*, b\\in Z_p\\}\\]"},{"location":"csc265/hashset.html#implementation","title":"Implementation","text":"Header for hashset hash.h<pre><code>#include &lt;assert.h&gt;\n#include &lt;math.h&gt;\n#include &lt;stdbool.h&gt;\n#include &lt;stddef.h&gt;\n\n#include &lt;string.h&gt;\n#include &lt;stdlib.h&gt;\n#include &lt;stdio.h&gt;\n\n#define BUCKET_SIZE 10\n\ntypedef struct bucket_t {\n  unsigned int size;\n  unsigned int max_size;\n  int *arr;\n};\n\ntypedef struct hash_table_t {\n  int size; // should be a prime number\n  bucket_t* buckets;\n};\n\nstatic bool is_prime(int n) {\n  assert(n &gt; 0);\n  for (int i = 2; i &lt;= sqrt(n); i++) {\n    if (n % i == 0)\n      return false;\n  }\n  return true;\n}\n\n// Get the smallest prime number that is not less than n (for hash table size\n// computation)\nint next_prime(int n) {\n  for (int i = n;; i++) {\n    if (is_prime(i))\n      return i;\n  }\n  assert(false);\n  return 0;\n}\n\n// Create a hash table with 'size' buckets; the storage is allocated dynamically\n// using malloc(); returns NULL on error\nhash_table_t *hash_create(int size) {\n  assert(size &gt; 0);\n\n  int tsize = next_prime(size);\n  hash_table_t *table = (hash_table_t *) malloc(sizeof(hash_table_t));\n  if (table == NULL) {\n    return NULL;\n  }\n  table-&gt;size = tsize;\n    table-&gt;buckets = (bucket_t *) malloc(tsize * sizeof(bucket_t));\n  if (table-&gt;buckets == NULL) {\n    free(table);\n    return NULL;\n  }\n  for (int i = 0; i &lt; tsize; i++) {\n    int *arr = (int *) malloc(BUCKET_SIZE * sizeof(int));\n    (table-&gt;buckets[i]).max_size = BUCKET_SIZE;\n    (table-&gt;buckets[i]).size = 0;\n    (table-&gt;buckets[i]).arr = arr;\n  }\n  return table;\n}\n\n// Release all memory used by the hash table, its buckets and entries\nvoid hash_destroy(hash_table_t *table) {\n  assert(table != NULL);\n\n  for (int i = 0; i &lt; table-&gt;size; i++) {\n    free((table-&gt;buckets[i]).arr);\n  }\n    free(table-&gt;buckets);\n    free(table);\n}\n\n// Returns 0 if key is not found, 1 if exist\nint hash_get(hash_table_t *table, int key) {\n  assert(table != NULL);\n\n    int hash_value = key % table-&gt;size;\n    bucket_t *bucket = &amp;(table-&gt;buckets[hash_value]);\n  for (int i = 0; i &lt; bucket-&gt;size; i++) {\n    if (bucket-&gt;arr[i] == key) return 1;\n  }\n\n  return 0;\n}\n\n// Returns 0 on success, -1 on failure\nint hash_put(hash_table_t *table, int key) {\n  assert(table != NULL);\n\n  int hash_value = key % table-&gt;size;\n    bucket_t *bucket = &amp;(table-&gt;buckets[hash_value]);\n\n  // if already exist, dont need to put\n  for (int i = 0; i &lt; bucket-&gt;size; i++) {\n    if ((bucket-&gt;arr)[i] == key) return 0;\n  }\n\n  if (bucket-&gt;size == bucket-&gt;max_size) {\n    bucket-&gt;max_size *= 2;\n    bucket-&gt;arr = (int *) realloc(bucket-&gt;arr, (bucket-&gt;max_size) * sizeof(int));\n  }\n  bucket-&gt;arr[bucket-&gt;size++] = key;\n  return 0;\n}\n</code></pre> <p>More about the dynamic list</p>"},{"location":"csc265/mq.html","title":"[+] Priority Queue: Mergeable Heap","text":"<p>A mergeable heap is a heap with the support of operations <code>union</code>. </p> <p>Consider the max/min heap, if we want to merge two heaps of size \\(n, m\\), it will take \\(O(n+m)\\) using <code>build_heap</code>. However, there is another implementation which <code>max</code> takes \\(O(\\lg n)\\) while <code>union</code> takes \\(O(\\lg(n+m))\\). </p>"},{"location":"csc265/mq.html#binomial-tree","title":"Binomial Tree","text":"<p>A binomial tree is constructed from structural induction.  - \\(B_0\\): the base case with one node - \\(B_k\\): two copies of \\(B_{k-1}\\), connecting the roots with an edge, and let the root of the one of the \\(B_{k-1}\\) be the root of \\(B_k\\). </p> <p>Claim 1 The number of \\(B_k\\) is \\(n = 2^k\\) </p> <p>Claim 2 The height of \\(B_k\\) is \\(h = k\\) proof. By induction, \\(h_0 = 0\\), assume \\(h_{k-1} = k-1\\), then note that \\(B_k\\) add one more edge from root to the root of \\(B_{k-1}\\) so that \\(h_k = k\\).  </p> <p>Claim 3 The root of \\(B_k\\) has subtrees \\(B_0,..., B_{k-1}\\). proof. By construction, the left most subtree of the root is \\(B_{k-1}\\). Then, the rest is a \\(B_{k-1}\\), which by induction hypothesis, has subtrees \\(B_0, ..., B_{k-2}\\).</p> <p>Claim 4 For level \\(k\\) of a \\(h\\) height binomial tree, the number of nodes at level \\(k\\) is \\(h\\choose k\\). proof. Since a tree of height \\(h\\) has subtrees \\(B_0, ..., B_{h-1}\\). For each of the tree, the number of nodes at level \\(k-1\\) is \\({m-1\\choose k-1}, m = k-1,...,h-1\\). Summing them together we have that </p> \\[\\sum_{m=k-1}^{h-1}\\frac{(m-1)!}{(k-1)!(m-k)!} =\\frac{h!}{(h-k)!k!}\\]"},{"location":"csc265/mq.html#binomial-forest","title":"Binomial Forest","text":"<p>A binomial forest is a set of distinct size binomial trees. </p> <p>Since each decimal number has a unique binary representation, for \\(n\\) nodes, we have a unique binomial forest \\(F_{n}\\). </p> <p>For example, \\(13 = b'1101\\), so that the binomial forest is \\(F_{13} =\\{B_0, B_2, B_3\\}\\)</p> <p>Claim 5 Let \\(k\\) be the number of binomial trees in the forest, \\(k = |F_n| \\leq \\lceil \\lg (n + 1)\\rceil\\).  proof. \\(k\\) is smaller than the number of digits in binary representation.</p>"},{"location":"csc265/mq.html#mergeable-min-heap","title":"Mergeable Min Heap","text":""},{"location":"csc265/mq.html#object","title":"Object","text":"<p>A binomial forest \\(F_n\\), for each tree in the forest, for each node in the tree, the key is greater than its children. </p>"},{"location":"csc265/mq.html#union","title":"<code>union</code>","text":"<p>Let \\(H_1, H_2\\) be two mergable heap of size \\(n_1, n_2\\). <code>union</code> will merge trees with the same size, by simplying compare the root, where the smaller one becomes the new root.</p>"},{"location":"csc265/mq.html#correctness","title":"Correctness","text":"<p>Merging trees is equivalent to bitwise add \\(n_1 + n_2\\), thus the resulting forest is still a binomial forest. For each tree in the new forest. If it merges with some other trees, then the new root is smaller of equal to its original children, and by the comparison, is also smaller than or equal to its new child from the other tree. </p>"},{"location":"csc265/mq.html#runtime","title":"Runtime","text":"<p>Each tree-wise merge takes constant time, the number of tree merge equals the max bits in bit-wise add. Thus the time is \\(O(\\lg(\\max(n_1, n_2))) \\in O(\\lg(n_1 + n_2))\\)</p>"},{"location":"csc265/mq.html#min-extract_min-insert","title":"<code>min</code>, <code>extract_min</code>, <code>insert</code>","text":"<p><code>min</code> will simply return the minimum of roots of all trees in the forest. Taking \\(O(\\lg n)\\) time since there are at most \\(\\lg (n+1)\\) trees. </p> <p><code>extract_min</code> will remove the minimum root, by claim 3, that tree will be broken into a forest. Thus, merge the two new forest. Taking \\(O(\\lg n)\\) time since the two forests two merge is at most size \\(\\lg(n)-1, \\lg(n)\\).</p> <p><code>insert</code> merge the forest with a new forst of \\(F_1 = \\{B_1\\}\\). Taking \\(O(\\lg n)\\) time. </p>"},{"location":"csc265/mq.html#implementation","title":"Implementation","text":"Implementation code mergeable_heap.py<pre><code>from math import inf\n\nfrom matplotlib.pyplot import plot\nfrom .priority_queue import PriorityQueue\nfrom .plot_trees import TreeNode, construct_tree_nodes, plot_tree\n\nclass Node:\n    def __init__(self, k):\n        self.k = k\n        self.children = []\n\n    @property\n    def degree(self):\n        return len(self.children)\n\n    def add(self, other):\n        if self.degree != other.degree:\n            raise ValueError(\"Cannot merge binomial trees of different size\")\n        self.children.insert(0, other)\n\n\nclass MergableHeap(PriorityQueue):\n    def __init__(self, arr=[], ascending=False):\n        self.trees = dict()\n        self.bit = -1\n        self.ascending = ascending\n        if ascending:\n            self.compare = lambda x, y: x &lt; y\n        else:\n            self.compare = lambda x, y: x &gt; y\n        for x in arr:\n            self.insert(x)\n\n    def from_trees(self, trees):\n        for tree in trees:\n            self.trees[tree.degree] = tree\n            if tree.degree &gt; self.bit:\n                self.bit = tree.degree\n\n    def __peek_node(self):\n        m, trees_k = inf, -1\n        for i, node in self.trees.items():\n            if self.compare(node.k, m):\n                m = node.k\n                trees_k = i\n        return self.trees[trees_k]\n\n    def peek(self):\n        return self.__peek_node().k\n\n    def union(self, other):\n        if self.ascending != other.ascending:\n            raise ValueError(\"Two Mergable Heaps have different order\")\n        if other.bit == -1:\n            return\n        if self.bit == -1:\n            self.trees = other.trees\n            self.bit = other.bit\n            return\n        bit, max_bit = 0, max(self.bit, other.bit)\n        new_trees = dict()\n        reg = []\n        while bit &lt;= max_bit:\n            if self.trees.get(bit):\n                reg.append(self.trees[bit])\n            if other.trees.get(bit):\n                reg.append(other.trees[bit])\n            if len(reg) == 1:\n                new_trees[bit] = reg.pop()\n            elif len(reg) == 2:\n                t1 = reg.pop()\n                t2 = reg.pop()\n                if self.compare(t1.k, t2.k):\n                    t1.add(t2)\n                    reg.append(t1)\n                else:\n                    t2.add(t1)\n                    reg.append(t2)\n            elif len(reg) == 3:\n                new_trees[bit] =  reg.pop()\n                t1 = reg.pop()\n                t2 = reg.pop()\n                if self.compare(t1.k, t2.k):\n                    t1.add(t2)\n                    reg.append(t1)\n                else:\n                    t2.add(t1)\n                    reg.append(t2)\n            bit += 1\n        if len(reg) == 1:\n            new_trees[bit] = reg.pop()\n        else:\n            bit -= 1\n        self.trees = new_trees\n        self.bit = bit\n\n    def insert(self, k):\n        if len(self.trees) == 0:\n            self.trees[0] = Node(k)\n            self.bit = 0\n        else:\n            new_mh = MergableHeap([k], ascending=self.ascending)\n            self.union(new_mh)\n\n\n    def pull(self):\n        m = self.__peek_node()\n        del self.trees[m.degree]\n        other = MergableHeap(ascending=self.ascending)\n        other.from_trees(m.children)\n        self.union(other)\n        return m.k\n\n    def plot(self, path):\n        forest_root = TreeNode(\"\")\n        for tree in self.trees.values():\n            forest_root.children.append(construct_tree_nodes(tree, label_fn=lambda x: x.k, children_attr =\"children\"))\n        return plot_tree(forest_root, path)\n</code></pre> <pre><code>from assets.mergeable_heap import MergableHeap\narr1 = [3, 2, 4]\nmq1 =  MergableHeap(arr1, ascending=True)\nmq1.plot(\"./assets/mq_1.jpg\")\n</code></pre> <pre><code>arr2 = [6, 1, 8, 5, 9, 7, 10]\nmq2 = MergableHeap(arr2, ascending=True)\nmq2.plot(\"./assets/mq_2.jpg\")\n</code></pre> <pre><code>mq1.union(mq2)\nmq1.plot(\"./assets/mq_3.jpg\")\n</code></pre> <pre><code>print(mq1.pull())\n#&gt;&gt; 1\nmq1.plot(\"./assets/mq_4.jpg\")\n</code></pre>"},{"location":"csc265/pq.html","title":"Priority Queue: Max Heap","text":""},{"location":"csc265/pq.html#abstract-data-type","title":"Abstract Data Type","text":"<ul> <li>Object: <ul> <li>Set \\(S\\) of keys (integers) and values</li> </ul> </li> <li>Operations:<ul> <li><code>peek()</code> return the element of \\(S\\) with the highest priority</li> <li><code>pull()</code> return highest priority and removes it from \\(S\\)</li> <li><code>insert(key, value)</code> add <code>key, value</code> to \\(S\\)</li> </ul> </li> </ul>"},{"location":"csc265/pq.html#data-structure-maxmin-heap","title":"Data Structure: Max(Min) Heap","text":"<p>Max heap, as the data structure, is a implementament of priority queue. </p>"},{"location":"csc265/pq.html#object","title":"Object:","text":"<ul> <li>A complete binary tree \\(T\\) of size \\(n\\) where the key of every node is at least the keys of its children.</li> <li>By \"complete\",  every level is full except possibly the last one where all nodes are fully to the left.</li> <li>Note that, since \\(T\\) is complete, we can store it as an array instead of a tree object. </li> </ul> <p>Claim 1 The max level of a complete binary tree with \\(n\\) nodes is \\(\\lfloor\\lg n\\rfloor\\)</p> <p>proof. Consider the number of nodes for a complete binary tree with height \\(h\\)</p> \\[\\sum_{i=0}^{h-1} 2^i + 1 \\leq n \\leq \\sum_{i=0}^{h} 2^i\\] <p>where the lower bound is the tree with 1 leaf at \\(h\\) level, and upper bound will have \\(2^h\\) leaves at \\(h\\) level. </p> <p>Thus, \\(h = \\lfloor\\lg n\\rfloor\\)</p> <p>Observation We can map a complete binary tree to an array via - \\(\\text{root} = 0\\) - \\(\\text{parent}(i) = \\lfloor i / 2\\rfloor\\) - \\(\\text{left}(i) = 2i + 1\\) - \\(\\text{right}(i) = 2i + 2\\)</p>"},{"location":"csc265/pq.html#operations","title":"Operations","text":"<ul> <li><code>max()</code> return the root of the tree.</li> <li><code>insert(x)</code> put \\(x\\) in the leftmost position of the leaf level, then swap with its parent if it is smaller than its parent. </li> <li><code>extract_max()</code> remove the root, move the leftmost to the root. Swap with its larger child till it's large than its children. </li> </ul>"},{"location":"csc265/pq.html#heapify","title":"Heapify","text":"<p>The <code>swap</code> operation can be considered as a structural induction <code>heapify(A, i)</code></p> <ul> <li>precondition: left and right subtree of \\(A[i]\\) are both heap. </li> <li>postcondition: The tree rooted at \\(A[i]\\) is a heap. </li> </ul> max_heapify(A: heap, i: index)<pre><code>l = left(i)\nr = right(i)\nlargest = i\nif i has left child and A[l] &gt; A[largest]:\n    largest = l\nif i has right child and A[r] &gt; A[largest]:\n    largest = r\nif largest != i:\n    swap(A[i], A[largest])\n    max_heapify(A, largest)\n</code></pre>"},{"location":"csc265/pq.html#correctness","title":"Correctness","text":"<p>Base case:  - \\(H[i]\\) is a leaf, then it is a heap. </p> <p>Induction Step:  </p> <ul> <li>If \\(A[i]\\) does not violate max heap property, meaning it is larger than or equal to both children. Then none of the if conditions are satisfied and we are done. </li> <li>If \\(A[i]\\) is smaller than any of its children, than by <code>line 4-7</code>, <code>largest</code> will be set to the index of the larger child. By <code>line 8-10</code>, WLOG assume that <code>largest = r</code>. Then, after the swap<ul> <li>Left subtree is untouched, by precondition, it is a max heap. </li> <li>Right subtree is a max heap by <code>line 10</code> and induction hypothesis.</li> <li>The new root, by <code>line 4-9</code>, is greater than or equal to its children.</li> </ul> </li> </ul>"},{"location":"csc265/pq.html#runtime","title":"Runtime","text":"<p>Each time, a swap happens between a parent and child, in the worst case, we swap from root to a leaf, by Claim 1, resulting \\(T(n) \\in O(h)\\).</p>"},{"location":"csc265/pq.html#build-heap-from-array","title":"Build Heap from Array","text":"build_heap(A)<pre><code>n = len(A)\nfor i in [floor(n / 2): 0: -1]:\n    max_heapify(A, i)\n</code></pre>"},{"location":"csc265/pq.html#correctness_1","title":"Correctness","text":"<p>Consider the for loop, we want to prove the loop invariance that the tree rooted at \\(A[i]\\) is a max heap. </p> <p>For \\(i = \\lfloor n / 2\\rfloor\\). Note that \\(h = \\lfloor \\lg n\\rfloor\\), thus the node is in the second last level. It subtrees are either leaf or empty, hence by correctness of <code>max_heapify</code>, the loop invariance is met. </p> <p>For \\(i &lt; \\lfloor n / 2\\rfloor\\), by loop counter, since \\(\\text{left}(i) = 2i + 1 &gt; i, \\text{right}(i) = 2i+2 &gt; i\\), both subtrees either empty or have already run <code>max_heapify</code>, thus, the loop invariance is met. </p>"},{"location":"csc265/pq.html#runtime_1","title":"Runtime","text":"<p>Claim \\(T(n)\\in O(n)\\) Consider the for loop, at each height \\(h\\), there are at most \\(n/2^h\\) nodes, and each <code>max_heapify</code> takes \\(O(h) = ch\\) time. Therefore,</p> \\[T(n) \\leq \\sum_{h=0}^{\\lfloor \\lg n\\rfloor} \\frac{n}{2^h} c h= cn \\sum_{h=0}^{\\lfloor \\lg n\\rfloor}\\frac{h}{2^h}\\leq \\sum_{h=0}^{\\infty}\\frac{h}{2^h}\\] <p>Note that by Maclaurin series</p> \\[\\sum_{h=0}^{\\infty}\\frac{h}{2^h} =  \\sum_{h=1}^{\\infty}[\\left.\\frac{d}{dx}(\\frac{1}{x})^h\\right\\vert_{x=1/2}] = \\left.\\frac{d}{dx}(\\frac{1}{1-x})\\right\\vert_{x=1/2} = \\frac{1/2}{(1-1/2)^2} = 2\\] <p>Therefore, we have that </p> \\[T(n) \\leq 2cn \\in O(n)\\]"},{"location":"csc265/pq.html#heap-sort","title":"Heap Sort","text":"<p>Given an array, return a sorted array using the heap</p> heap_sort(A)<pre><code>H = build_heap(A)\nsorted = []\nfor i in [0:n:1]:\n    sorted.append(extract_max(H))\nreturn sorted\n</code></pre> <p>Correctness is trivial, since \\(H\\) is a heap and we always extract the largest element. </p> <p>Runtime: <code>line 1</code> takes \\(O(n)\\), the for loop in <code>line 3-4</code> takes \\(n \\times O(h) \\in O(n \\lg n)\\). Thus, \\(T(n) \\in O(n\\lg n)\\). </p>"},{"location":"csc265/pq.html#implementation","title":"Implementation","text":"Implementation code priority_queue.py<pre><code>class PriorityQueue:\n    def __init__(self, arr):\n        \"\"\" Create a priority queue from a given \n            list of priorities\n        \"\"\"\n        raise NotImplementedError\n\n    def peek(self):\n        \"\"\" return the highest priority\n        \"\"\"\n        raise NotImplementedError\n\n    def pull(self):\n        \"\"\" return the highest priority and extract it\n        \"\"\"\n        raise NotImplementedError\n\n    def insert(self, priority):\n        \"\"\" insert a priority \n        \"\"\"\n        raise NotImplementedError\n</code></pre> heap.py<pre><code>from .priority_queue import PriorityQueue\n\nclass Heap(PriorityQueue):\n    def __init__(self, arr, ascending=False):\n        self.H = arr\n        if ascending:\n            self.compare = lambda x, y: x &lt; y\n        else:\n            self.compare = lambda x, y: x &gt; y\n        for i in range(len(arr) // 2, -1, -1):\n            self.__heapify(i)\n\n\n    @staticmethod\n    def left(i):\n        \"given the index, return the index of left child\"\n        return 2 * i + 1\n\n    @staticmethod\n    def right(i):\n        \"given the index, return the index of right child\"\n        return 2 * i + 2\n\n    @staticmethod\n    def parent(i):\n        \"given the index, return the index of parent\"\n        return (i - 1)// 2\n\n    def __heapify(self, i):\n        \"\"\" Given the complete binary tree H and index i, \n            assume that the left subtree and right subtree of \n            of H[i] are max-heap.\n            make H a max heap\n        \"\"\"\n        largest = i\n        l, r = Heap.left(i), Heap.right(i)\n        if l &lt; len(self.H) and self.compare(self.H[l], self.H[largest]):\n            largest = l\n        if r &lt; len(self.H) and self.compare(self.H[r], self.H[largest]):\n            largest = r\n        if largest != i:\n            self.H[i], self.H[largest] = self.H[largest], self.H[i]\n            self.__heapify(largest)\n\n    def peek(self):\n        return self.H[0]\n\n    def pull(self):\n        m = self.H[0]\n        self.H[0] = self.H[-1]\n        self.H.pop()\n        self.__heapify(0)\n        return m\n\n    def insert(self, x):\n        self.H.append(x)\n        i = len(self.H) - 1\n        while self.compare(self.H[i], self.H[Heap.parent(i)]) and i != 0:\n            self.H[i], self.H[Heap.parent(i)] = self.H[Heap.parent(i)], self.H[i]\n            i = Heap.parent(i)\n\n    def plot(self, path):\n        from .plot_trees import TreeNode, plot_tree\n        if len(self.H) == 0:\n            return\n        nodes = [TreeNode(str(self.H[0]))]\n        for i in range(1, len(self.H)):\n            new_node = TreeNode(str(self.H[i]), nodes[Heap.parent(i)])\n            nodes.append(new_node)\n            nodes[Heap.parent(i)].children.append(new_node)\n\n        return plot_tree(nodes[0], path)\n\ndef heap_sort(arr, ascending=False):\n    H = Heap(arr, ascending=ascending)\n    sorted = []\n    for _ in range(len(arr)):\n        sorted.append(H.pull())\n    return sorted\n</code></pre> <pre><code>from assets.heap import Heap, heap_sort\n</code></pre> <pre><code>arr = [2, 11, 10, 8, 3, 3, 7]\n# ascending = True =&gt; min heap\n# ascending = False =&gt; max heap\n# default max heap\nh = Heap(arr, ascending=True)\nh.plot(\"./assets/pq_0.jpg\")\n</code></pre> <pre><code>h.insert(17)\nh.plot(\"./assets/pq_1.jpg\")\n</code></pre> <pre><code>print(f\"{h.pull()}\")\n#&gt;&gt; 2\n\nh.plot(\"./assets/pq_2.jpg\")\n</code></pre> <pre><code>print(f\"&gt; {h.pull()}\")\n#&gt;&gt; 3\n\nh.plot(\"./assets/pq_3.jpg\")\n</code></pre> <pre><code>print(f\"&gt; {heap_sort(arr)}\")\n#&gt;&gt;  [17, 11, 10, 8, 7, 3]\n</code></pre>"},{"location":"csc265/pq.html#usage-example","title":"[+] Usage Example","text":"<p>Given a sorted array \\(A\\) of distinct integers of length \\(n &gt; 1\\). Design a data structure that supports - <code>perprocess(A)</code> initialize the data structure. This operation should run in \\(O(n)\\) - <code>next(A)</code> return the next pair \\((i,j), 1\\leq i\\leq j\\leq n\\) in non-decreasing order \\(A[j] - A[i]\\). Each call should run in \\(O(\\log n)\\)</p> <p>We can assume that <code>preprocess</code> is called once, following at most \\(n\\choose 2\\) calls to <code>next</code>.</p>"},{"location":"csc265/pq.html#observations","title":"Observations","text":"<p>Lemma 1 Let \\((i_i, j_i)\\) be the return value of the ith call to <code>next</code>. Then, \\(j_1 - i_1 = 1, j_2 - i_2 = 1\\). proof. Since \\(A\\) is sorted. If \\(j_1 - i-1 &gt; 1\\), then there exists \\(A[j_1 - 1] - A[i_1] &lt; A[j_1] - A[i_1]\\). Moreover, consider \\((i_2, j_2)\\), \\(A[j_2] - A[i_2 + 1] &lt; A[j_2] - A[i_2]\\).</p> <p>Lemma 2 Let \\(\\sigma = [(i_1, j_1),...,(i_N, j_N)]\\) be the sequence of return values from \\(N = {n\\choose 2}\\) calls of <code>next</code>. Then for any \\(k\\), either \\(j_k - i_k = 1\\) or \\((i_k, i_{k+1})\\) and \\((i_{k+1},j_k)\\) appear before \\((i_k,j_k)\\) in \\(\\sigma\\). </p>"},{"location":"csc265/pq.html#implementataion","title":"Implementataion","text":"preprocess(A)<pre><code> 1  L = [ ( A[i+1] - A[i], (i, i+1) ) for i in [0: n-1: 1] ]\n 2  return build_min_heap(L)\n</code></pre> next(A)<pre><code> 1  (i, j) = extract_min(A)\n 2  if i &gt; 0:\n 3      insert(A[j] - A[i-1], (i-1, j))\n 4  return (i, j)\n</code></pre>"},{"location":"csc265/pq.html#analysis","title":"Analysis","text":"<p>Runtime analysis is trivial. </p> <p>For correctness, </p> <p>Claim 1 each pair \\((i,j)\\) inserted is distinct.  proof. For pairs inserted in <code>preprocess</code> this is obvious. For any pair inserted in <code>next</code>, we must have that \\(j-i &gt; 1\\), considering <code>line 3</code>. Suppose that \\((i,j)\\) is the first pair that got inserted twice, then it must exists \\((i, j-1)\\) be extracted twice, meaning it is inserted before, contradiction. </p> <p>Claim 2 For each \\((i_k, j_k)\\), \\(A[j_k] - A[i_k]\\) is the \\(k\\)th smallest.  proof. For \\(k = 1\\), by lemma 1 this is proven. Then, assume \\(k&gt;1\\) is the first step where \\(A[j_k] - A[i_k]\\) is not minimal. Let \\((i,j)\\) be the pair that should be extracted at \\(k\\)th step, \\((i,j)\\neq (i_k, j_k), A[j]-A[i] &lt; A[j_k] - A[i_k]\\). Note that \\(j - i &gt; 1\\), otherwise it is inserted through <code>preprocess</code> and must be extracted. Then, for \\(j - i &gt; 1\\), then by lemma 2, \\((i+1, j)\\) must have been extracted. Therefore \\((i,j)\\) must have been inserted. By the property of heap, \\((i,j)\\) must be extracted at \\(k\\)th step. </p>"},{"location":"csc265/random.html","title":"Randomized QuickSort","text":""},{"location":"csc265/random.html#quicksort","title":"QuickSort","text":"<p>Given a unsorted array \\(A\\) <code>quicksort</code> sort the array by first choose a pivot \\(x\\) from \\(A\\), then partition the array into \\(A[0:q-1] \\leq A[q] =  x \\leq A[q+1:n]\\) and then recursively sort the left partition and right partition. </p> <p>As a reference, the algorithm is provided below</p> <pre><code>from random import shuffle, randint\n\ndef partition(A, l, r):\n    global NUM_COMPS # for runtime demo\n    x = A[r]\n    i = l - 1\n    for j in range(l, r):\n        NUM_COMPS += 1 # for runtime demo\n        if A[j] &lt;= x:\n            i += 1\n            A[i], A[j] = A[j], A[i]\n    A[i+1], A[r] = A[r], A[i+1]\n    return i + 1\n\ndef quicksort(A, l, r):\n    if l &lt; r:\n        q = partition(A, l, r)\n        quicksort(A, l, q-1)\n        quicksort(A, q + 1, r)\n</code></pre>"},{"location":"csc265/random.html#runtime-analysis","title":"Runtime Analysis","text":"<p>Assuming that we don't do early returning (otherwise we can check whether \\(A[l:r]\\) is already sorted). As of a best case, the chosen pivot is always in the middle, then it takes \\(O(n\\lg n)\\) time. Also, in average, <code>quicksort</code> takes \\(\\Theta(n\\log n)\\) time (proof omitted). </p> <p>However, if the pivot is chosen unwisely, the running time is quite bad. As an example, we choose the rightmost element of \\(A[l:r]\\) as our pivot, if \\(A\\) is already sorted (either ascending or descending), then all elements go to either \\(A[l:r-1]\\) or \\(A[l+1:r]\\), thus <code>quicksort</code> takes \\(\\frac{n(n-1)}{2}\\in O(n^2)\\) comparisons. </p> <pre><code>N = 25\nA = list(range(N))\nNUM_COMPS = 0\nquicksort(A, 0, N-1)\nprint(\"\\nNumber of comparisons: \", NUM_COMPS)\n#&gt;&gt; Number of comparisons:  300\n</code></pre> <pre><code>ncomps_sum = 0\nfor _ in range(100):\n    NUM_COMPS = 0\n    shuffle(A)\n    quicksort(A, 0, N-1)\n    ncomps_sum += NUM_COMPS\nprint(\"Average number of comparisons: \", ncomps_sum / 100)\n#&gt;&gt; Average number of comparisons:  97.4\n</code></pre> <p>Note that whichever pivot strategy we choose, there is always a way to a construct worst case array. </p>"},{"location":"csc265/random.html#randomized-quicksort_1","title":"Randomized Quicksort","text":"<p>Intuitively, to avoid running into the worst case scenario, we can randomly permute \\(A\\) before entering <code>quicksort</code>, so that we are expecting an average case running time on the permuted array \\(A\\). </p> <p>Alternatively, we know that the fixed choice of pivot causes the problem, so that we randomly pick pivot from \\(A[l:r]\\). </p> <p>The two thought led to the two different implementations</p> <pre><code># implementation 1: permute A before quick sort\ndef rand_quicksort_permute(A, l, r):\n    shuffle(A)\n    quicksort(A, l, r)\n\n# implementation 2: randomly pick pivot for each partition\ndef rand_partition(A, l, r):\n    global NUM_COMPS # for runtime demo\n    x = A[randint(l, r)]\n    i = l - 1\n    for j in range(l, r):\n        NUM_COMPS += 1 # for runtime demo\n        if A[j] &lt;= x:\n            i += 1\n            A[i], A[j] = A[j], A[i]\n    A[i+1], A[r] = A[r], A[i+1]\n    return i + 1\n\n\ndef rand_quicksort_partition(A, l, r):\n    if l &lt; r:\n        q = rand_partition(A, l, r)\n        quicksort(A, l, q-1)\n        quicksort(A, q + 1, r)\n</code></pre>"},{"location":"csc265/random.html#worst-case-expected-runtime","title":"Worst Case Expected Runtime","text":"<p>Since we introduced randomness into our algorithm, the running time for the same input in different runs will be different. Thus, let \\(T_x\\) be the running time of the algorithm given a fixed input \\(x\\). Thus, thee expected running time is very intuitively defined as \\(E(T_x)\\). </p> <p>As a refresher, worst case running time for an algorithm is \\(T(n):=\\max\\{T_x: |x| = n\\}\\), and the average running time for a algorithm with finite input space is \\(\\bar T(n):=\\frac{1}{|\\mathcal X(n)|}\\sum_{x\\in \\mathcal X(n)} T_x\\) where \\(\\mathcal X(n)\\) is the set of all input of size \\(n\\). For infinite sied input space, then it is defined as a weighted average \\(\\bar T(n) = \\sum_{x\\in\\mathcal X(n)}p_xT_{x}\\) where \\(p_x\\) is the probability of input \\(x\\) be chosen, \\(\\sum_{x\\in\\mathcal X(n)} p_x = 1\\).</p> <p>Thus, the worst case expected running time is defined as </p> \\[E(T(n)) := \\max\\{E(T_x): |x| = n\\}\\] <p>For permutation based randomized quicksort, since we randomly permute the input, \\(E(T_x) = \\bar T(n)\\) for any input \\(x\\) of size \\(n\\). Thus, \\(E(T(n)) = \\bar T(n) \\in \\Theta(n\\log n)\\). </p> <p>For pivot based randomized quicksort, the proof is similar to average case proof. The key idea is that after each <code>rand_partition(A, l, r)</code>, the expected number of elements in \\(A[l: q-1], A[q+1:r]\\) is the same as the average number of elements after <code>partition(A, l, r)</code> over all \\(A\\) of size \\(n\\).</p> <pre><code>shuffle(A)\n\nncomps_sum = 0\nfor _ in range(100):\n    A_fixed = A.copy()\n    NUM_COMPS = 0\n    rand_quicksort_permute(A_fixed, 0, N-1)\n    ncomps_sum += NUM_COMPS\nprint(\"Average number of comparisons for partition based: \", ncomps_sum / 100)\n#&gt;&gt; Average number of comparisons for partition based:  96.51\n\nncomps_sum = 0\nfor _ in range(100):\n    A_fixed = A.copy()\n    NUM_COMPS = 0\n    rand_quicksort_partition(A_fixed, 0, N-1)\n    ncomps_sum += NUM_COMPS\nprint(\"Average number of comparisons for partition based: \", ncomps_sum / 100)\n\n#&gt;&gt; Average number of comparisons for partition based:  96.75\n</code></pre>"},{"location":"csc311/bvd.html","title":"Bias-Variane Decomposition","text":""},{"location":"csc311/bvd.html#loss-function","title":"Loss function","text":"<p>A loss function \\(L(y,t)\\) defines how bad it is if, for some example \\(x\\), the algorithm predicts \\(y\\), but the target is actually \\(t\\)</p>"},{"location":"csc311/bvd.html#bias-variance-decomposition","title":"Bias-Variance Decomposition","text":"<p>Suppose the training set \\(\\mathcal D\\) consists of \\(N\\) pairs sampled IID from a sample generating distribution, i.e. \\((x^{(i)}, t^{(i)})\\sim p_{sample}\\) Let \\(p_{\\mathcal D}\\) denote the induced distribution over training set \\(i.e. \\mathcal D\\sim p_{\\mathcal D}\\) </p> <p>Pick a fixed query point \\(\\vec x\\), then consider an experiment where we sample lots of (say \\(m\\) times) training datasets IID from \\(p_{\\mathcal D}\\)</p> <p>Then, we can produce \\(h_{k,\\mathcal D}, k\\in\\{1,2,...,m\\}\\) and we compute each classifier's prediction \\(h_{k,\\mathcal D}(\\vec x) = y\\) at the chosen query point \\(\\vec x\\)</p> <p>Therefore, \\(y\\) is a random variable, i.e. \\(D\\Rightarrow h_{\\mathcal D}\\Rightarrow h_{\\mathcal D}(\\vec x)=y, \\mathcal D\\) is randomly chosen.</p>"},{"location":"csc311/bvd.html#basic-setup","title":"Basic setup","text":"<p>Assume \\(t\\) is deterministic given \\(x\\) There is a distribution over the loss at \\(\\vec x\\), with \\(E_{\\mathcal D\\sim p_{\\mathcal D}}(L(h_{\\mathcal D}(x), t))\\) For each query point, the expected loss is different. We are interested in quantifying how well our classifier does over \\(p_{sample}\\) average over training set \\(E_{\\vec x\\sim p_{sample}, \\mathcal D\\sim p_{\\mathcal D}}(L(h_{\\mathcal D}(\\vec x), t))\\) Then, we can decompose the expected loss </p> \\[\\begin{align*} E_{x,D}[(h_D(x) - t)^2] &amp;= E_{x,D}\\bigg[\\big(h_D(x) - E_D(h_D(x)\\mid x) +E_D(h_D(x)\\mid x)-t\\big)^2\\bigg]\\\\ &amp;= \\quad E_{x,D}\\bigg[\\big(h_D(x) - E_D(h_D(x)\\mid x)\\big)^2\\bigg] \\\\ &amp;\\quad+2E_{x,D}\\bigg[\\big(h_D(x) - E_D(h_D(x)\\mid x)\\big)\\big(E_D(h_D(x)\\mid x)-t\\big)\\bigg]\\\\ &amp;\\quad + E_{x,D}\\bigg[\\big(E_D(h_D(x)\\mid x)-t\\big)^2\\bigg]\\\\ &amp;= E_x\\bigg[E_D\\big[\\big(h_D(x) - E_D(h_D(x)\\mid x)\\big)^2 \\\\ &amp;\\qquad\\quad +2E_D\\bigg[\\big(h_D(x) - E_D(h_D(x)\\mid x)\\big)\\big(E_D(h_D(x)\\mid x)-t\\big)\\mid x \\bigg] &amp;(*)\\\\ &amp;\\qquad\\quad + \\big(E_D(h_D(x)\\mid x)-t\\big)^2\\mid x\\big]\\bigg]\\\\ &amp;= E_{x,D}\\bigg[\\big(h_D(x)-E_D[h_D(x)\\mid x]\\big)^2\\bigg] + E_{x}\\bigg[\\big(E_D[h_D(x)\\mid x] -t\\big)^2\\bigg]\\\\ &amp;= variance + bias \\end{align*}\\] \\[\\begin{align*} (*) \\quad &amp;= (E_D(h_D(x)\\mid x)-t\\big)&amp;\\text{indep. of }D\\\\ &amp;\\qquad \\times 2E_D\\bigg[\\mid x \\bigg] &amp; (**)\\\\ (**) &amp;= E_{D,x}\\big(h_D(x) - E_{D_x}[h_D(x)]\\big) = 0\\\\ (*) &amp;= 0 \\end{align*}\\] <p>Bias defines on average, how close is the classifier to true target</p> <p>Variance defines how widely dispersed are the prediction as we generate new datasets</p>"},{"location":"csc311/bvd.html#bayes-optimality","title":"Bayes Optimality","text":"<p>What if \\(t\\) is not deterministic given \\(\\vec x\\), i.e. \\(p(t\\mid \\vec x)\\). </p> <p>Since there is a distribution over targets, we measure distance from \\(y_*(x) = E(t\\mid \\vec x)\\) </p> \\[\\begin{align*} E[(y-t)^2 \\mid \\vec x] &amp;= E(y^2\\mid x) - 2E(yt\\mid x) + E(t^2\\mid x)\\\\ &amp;= y^2 - 2yE(t\\mid x) + E(t\\mid x)^2 + var(t\\mid x)\\\\ &amp;= y^2 - 2yy_*(x) + y_*(x)^2 + var(t\\mid x)\\\\ &amp;=(y - y_*(\\vec x))^2 + var[t\\mid \\vec x] \\end{align*}\\] <p>The first term show that s \\(y=y_*(\\vec x)\\) is the minimized value  </p> <p>Bayes error / irreducible error The second term is the inherent unpredicatability, or noise, of the target. </p> <p>If \\(Var[t|x] = 0\\), the algorithm is Bayes optimal. </p> <p>We can then decompose the non-deterministic form</p> \\[\\begin{align*} E_{x,D,t|x}\\bigg[(h_D(x) - t)^2\\bigg] &amp;= E_D\\bigg[E_{x,t|x}\\big[(h_D(x)-y)^2 \\mid D\\big]\\bigg]\\\\ &amp;= \\quad E_{x,D}\\big[(h_D(x) - E_t[t|x])^2\\big] \\\\ &amp;\\quad\\quad + E_{x,t|x}\\big[(E_{t|x}[t|x] - t)^2\\big]\\\\ &amp;= \\quad E_{x,D}\\big[(h_D(x) - E_t[t|x])^2\\big]  + E_x\\big[var(t\\mid x)\\big] \\end{align*}\\] <p>Hence we decompose out the Bayes error, where the first two terms are identical to the deterministic case, and will be decomposed into bias and variance</p>"},{"location":"csc311/bvd.html#bagging","title":"Bagging","text":""},{"location":"csc311/bvd.html#intuition","title":"Intuition","text":"<p>Suppose we sample \\(m\\) indep. training set \\(D_i\\) from \\(p_d\\), we could then learn a predictor \\(h_i := h_{D_i}\\) based on each one, then take the average \\(h = \\sum^m h_i /m\\) </p> <p>Bias unchanged</p> \\[E_{D_1,...,D_m \\sim p_d}h(x) = \\frac{1}{m} \\sum^m E_{D_i\\sim p_{d}}h_i(x) = E_{D\\sim p_d} h_D(x)\\] <p>Variance becomes \\(1/m\\) of the original </p> \\[var_{D_1,...,D_m}(h(x)) = \\frac{1}{m^2}var_{D_i}(h_i(x)) = \\frac{1}{m}var_D(h_D(x))\\] <p>However, we don't such iid datasets from \\(p_d\\)</p> <p>So we have to take a single dataset \\(D\\) with \\(n\\) examples Generate \\(m\\) new datasets by sampling \\(n\\) training examples from \\(D\\), with replacement Average the predictions of models trained on each of these</p>"},{"location":"csc311/bvd.html#problem-with-independence","title":"Problem with independence","text":"<p>Let correlation be \\(\\rho\\), the variance with correlated datasets is </p> \\[var(h(x)) = \\frac{1}{m}(1-\\rho)\\sigma^2 + \\rho\\sigma^2\\] <p>Ironically, introduce additional variability reduces correlation between samples  - invest a diversified portfolio, not just one stock  - help to use average over multiple algorithms, or multiple configurations</p>"},{"location":"csc311/bvd.html#example-random-forests","title":"Example: Random Forests","text":"<p>When choose each node of the decision tree, choose a random set of \\(d\\) features, and only consider splits on those features</p> <p>The main idea is to improve the variance reduction of bagging by reducing the correlation between the trees</p> <p>One of the example for black-box ML algorithm</p>"},{"location":"csc311/classification.html","title":"Classification Optimization","text":""},{"location":"csc311/classification.html#binary-linear-classification","title":"Binary Linear Classification","text":"<p>Let the target \\(t \\in \\{0,1\\}\\) be the binary classification, using a linear function model \\(z = \\vec w^T \\vec x\\) with threshold \\(I(z \\geq 0)\\) where \\(\\vec x\\) is the training data with one more dummy variable \\(1\\) so that the threshold is always \\(0\\)</p>"},{"location":"csc311/classification.html#geometric-picture","title":"Geometric Picture","text":"<p>Given input \\(t = \\text{NOT } x, x\\in\\{0,1\\}\\)</p>"},{"location":"csc311/classification.html#input-space","title":"Input space","text":"<p>the weights (hypothesis) can be represented by half-spaces </p> \\[H_+ = \\{\\vec x\\mid \\vec w^T \\vec x \\geq 0\\}, H_- = \\{\\vec x\\mid \\vec w^T \\vec x &lt; 0\\}\\] <p>The boundary is the decision boundary \\(\\{\\vec x\\mid \\vec w^T \\vec x = 0\\}\\)</p> <p>If the training example can be perfectly separated by a linear decision rule, we say the data is linearly separable</p>"},{"location":"csc311/classification.html#weight-space","title":"weight space","text":"<p>each training example \\(\\vec x\\) specifies a half space \\(\\vec w\\) must lie in to be correctly classified: \\(w^Tx &gt;0\\) if \\(t = 1\\) The region satisfying all the constraints is the feasible region. The problem is feasible is the region \\(\\neq \\emptyset\\), otw infeasible</p> <p>Note that if training set is separable, we can solve \\(\\vec w\\) using linear programming</p>"},{"location":"csc311/classification.html#loss-function","title":"Loss Function","text":""},{"location":"csc311/classification.html#0-1-loss","title":"0-1 Loss","text":"<p>Define the 0-1 Loss be </p> \\[\\mathcal L_{0-1}(y,t) = \\mathbb I(y\\neq t)\\] <p>Then, the cost is </p> \\[\\mathcal J = \\frac{1}{N}\\sum^N \\mathbb I(y^{(i)}\\neq t^{(i)})\\] <p>However, such loss is hard to optimize (NP-hard considering integer programming)</p> <p>Note that \\(\\partial_{w_j} \\mathcal L_{0-1} = 0\\) almost everywhere (since \\(\\mathcal L\\) is a step function w.r.t \\(z\\))</p>"},{"location":"csc311/classification.html#surrogate-loss-function","title":"Surrogate loss function","text":"<p>If we treat the model as a linear regression model, then </p> \\[\\mathcal L_{SE}(y,t) = \\frac{(y-t)^2}{2}\\] <p>However, the loss function will give large loss if the prediction is correct with high confidence. </p>"},{"location":"csc311/classification.html#logistic-activation-function","title":"Logistic Activation Function","text":"<p>Using logistic function \\(\\sigma(z) = (1+e^{-z})^{-1}\\) to transform \\(z = \\vec w^T\\vec x + b\\). </p> \\[\\mathcal L_{SE}(y,t) = \\frac{\\sigma(\\vec w^T \\vec x + b) - t}{2}\\] <p>A linear model with a logistic nonlinearity is known as log-linear In this way, \\(\\sigma\\) is called an activation function</p> <p>However, for \\(z\\rightarrow \\pm\\infty, \\sigma(z)\\approx 0\\) If the prediction is really wrong, you should be far from a critical point</p> <p></p>"},{"location":"csc311/classification.html#cross-entropy-loss-log-loss","title":"Cross-entropy loss (log loss)","text":"<p>More loss if the prediction is \"more\" confident about \"wrong\" answers and not punishing the correct one even not confident</p> \\[\\mathcal L_{CE}(y,t) = -t\\log(y) - (1-t)\\log(1-y)\\] <p>where \\(t\\in\\{0,1\\}\\)</p>"},{"location":"csc311/classification.html#logistic-regression","title":"Logistic Regression","text":"<p>\\(z = \\vec w^T \\vec x+b\\) \\(y = \\sigma(z) = (1+\\exp(-z))^{-1}\\) \\(\\mathcal L_{CE}(y,t) = -t\\log(y) - (1-t)\\log(1-y)\\)</p> <p></p>"},{"location":"csc311/classification.html#gradient-descent","title":"Gradient Descent","text":"<p>Initialize the weights to something reasonable and repeated adjust them in the direction of steepest descent</p> \\[\\vec w \\leftarrow \\vec w - \\alpha \\partial_{\\vec w} J\\] <p>\\(\\alpha \\in (0, 1]\\) is the learning rate (step size) When \\(J\\) converges, \\(\\partial_{\\vec w} J = 0\\) at the critical point</p>"},{"location":"csc311/classification.html#under-l2-regularization","title":"Under L<sup>2</sup> Regularization","text":"<p>The gradient descent update to minimize the regularized cost \\(\\mathcal J + \\lambda \\mathcal R\\) results in weight decay</p> \\[\\vec w\\leftarrow \\vec w - \\alpha\\partial_{\\vec w}(\\mathcal J+\\lambda \\mathcal R) = (1-\\alpha\\lambda)\\vec w - \\alpha \\partial_{\\vec w} \\mathcal J\\]"},{"location":"csc311/classification.html#learning-rate","title":"Learning rate","text":"<p>In gradient descent, the learning rate \\(\\alpha\\) is a hyper parameter. (TUT3)</p>"},{"location":"csc311/classification.html#training-curves","title":"Training Curves","text":"<p>To diagnose optimization problems, plot the training cost as a function of iteration.</p> <p>However, it's very hard to tell whether an optimizer has converged. </p>"},{"location":"csc311/classification.html#example-gradient-of-logistic-loss","title":"Example: Gradient of logistic loss","text":"\\[\\begin{align*} \\frac{\\partial\\mathcal L_{CE}}{\\partial w_j} &amp;= \\partial_y \\mathcal L_{CE}\\cdot \\partial_zy \\cdot \\partial_{wj}z \\\\ &amp;= (\\frac{-t}{y} + \\frac{1-t}{1-y})\\cdot y(1-y)\\cdot x_j\\\\ &amp;= (y-t)x_j\\\\ w_j &amp;\\rightarrow w_j - \\frac{\\alpha}{N} \\sum^N(y^{(i)} - t^{(i)})x_j^{(i)} \\end{align*}\\]"},{"location":"csc311/classification.html#multiclass-classification-softmax-regression","title":"Multiclass Classification (Softmax Regression)","text":""},{"location":"csc311/classification.html#one-hot-vector-one-of-k-encoding-target","title":"One-hot vector/ one-of-K encoding Target","text":"<p>Targets from a discrete set \\(\\{1,...,K\\}\\) For convenience, let \\(t\\in\\mathbb R^K, t_i= \\mathbb I(i=k)\\) where \\(k\\) is the classification. </p>"},{"location":"csc311/classification.html#linear-predictions","title":"Linear predictions","text":"<p>\\(D\\) input, \\(K\\) output, hence we need a weight matrix \\(W\\) </p> \\[\\vec{z}_{K\\times 1} = W_{K\\times D}\\vec{x}_{D\\times 1} + \\vec{b}_{K\\times 1}\\] <p>Otherwise \\(Z= Wx^*\\) where \\(x^*\\) is \\(x\\) padded a column of \\(1\\)'s.</p>"},{"location":"csc311/classification.html#softmax-function-activation","title":"Softmax Function Activation","text":"<p>A generalization of the logistic function</p> \\[y_k = softmax(z_1,...,z_K)_k = \\frac{e^{z_k}}{\\sum_{k'} e^zk'}\\] <p>The input \\(z_k\\) are the logits</p>"},{"location":"csc311/classification.html#properties","title":"Properties","text":"<ul> <li>Outputs are positive and sum to \\(1, (\\sum_k y_k = 1)\\) so that can be interpreted as probabilities</li> <li>If one of \\(z_k\\) is much larger, than \\(softmax(z)_k \\approx 1\\)</li> </ul>"},{"location":"csc311/classification.html#cross-entropy-loss","title":"Cross Entropy Loss","text":"<p>Use cross-entropy as the loss function, as from logistic regression </p> \\[\\mathcal L_{CE}(\\vec y, \\vec t) = -\\sum_{k=1}^K t_k \\log y_k = -\\vec t^T \\log(\\vec y)\\] <p>Log is applied element-wise</p>"},{"location":"csc311/classification.html#gradient-descent_1","title":"Gradient descent","text":"<p>Updates can be derived for each row of \\(W\\)</p> \\[\\frac{\\partial L}{\\partial w_k} = \\frac{\\partial L}{\\partial z_k}\\frac{\\partial z_k}{\\partial w_k} = (y_k - t_k)\\cdot \\vec x\\] \\[w_k \\leftarrow w_k - \\alpha N^{-1} \\sum^N (y^{(i)}_k - t^{(i)}_k)\\vec x^{(i)}\\] Source code <pre><code>import matplotlib.pyplot as plt\nimport numpy as np\n\nz = np.arange(-10, 10, 0.01)\nloss = (np.exp(-z) + 1) ** -1\nplt.figure(figsize=(4, 3))\nplt.plot(z, loss)\nplt.title(\"logistic function\")\nplt.tight_layout()\nplt.savefig(\"../assets/classification_logistic.jpg\")\n\n\nplt.figure(figsize=(4, 3))\nplt.plot(z, -1 * np.log(loss), label=\"t = 1\")\nplt.plot(z, -1 * np.log(1 - loss), label=\"t = 0\")\nplt.legend()\nplt.tight_layout()\nplt.savefig(\"../assets/classification_loss.jpg\")\n</code></pre>"},{"location":"csc311/decision_trees.html","title":"Decision Trees","text":"<p>make prediction by recursively splitting on different attributes according to a tree structure</p> <p>Internal nodes: test attributes Branching: attribute values Leaf: output (predictions)  </p> <p>NOTE   - can have repeated attributes, but not the same attribute values  - can be imaged as splitting the space into rectangular subspaces</p>"},{"location":"csc311/decision_trees.html#classification-and-regression","title":"Classification and Regression","text":"<p>Each path from root to a leaf defines a region \\(R_m\\) of input space, let \\(\\{(x^{(m_1)}, t^{(m_1)}), ..., (x^{(m_k)}, t^{(m_k)})\\}\\) be the training examples that fall into \\(R_m\\)</p> <p>Classification Tree set leaf value \\(y^m\\) be the most common value in \\(\\{t^{(m_1)}, ..., t^{(m_k)}\\}\\), hence discrete output</p> <p>Regression Tree set leaf value \\(y^m\\) be the mean value in \\(\\{t^{(m_1)}, ..., t^{(m_k)}\\}\\), hence continuous output</p>"},{"location":"csc311/decision_trees.html#learning-constructing-decision-trees","title":"Learning (Constructing) Decision Trees","text":"<p>Note that learning the simplest decision tree which correctly classifies training set is NPC</p>"},{"location":"csc311/decision_trees.html#general-idea","title":"General Idea","text":"<p>Greedy heuristic start with empty and complete training set by split on the \"best\" attribute and recurse on subpartitions</p>"},{"location":"csc311/decision_trees.html#accuracy-loss-based","title":"Accuracy (Loss) based","text":"<p>Let loss \\(L:=\\) misclassification rate, say region \\(R\\rightarrow R_1, R_2\\) based on loss \\(L(R)\\) and the accuracy gain is \\(L(R) - \\frac{\\|R_1\\|L(R_1) + \\|R_2\\|L(R_2)}{\\|R_1\\| + \\|R_2\\|}\\)</p> <p>Problem sometimes loss in misclassfication rate will have reduced uncertainty significantly. </p>"},{"location":"csc311/decision_trees.html#uncertainty-based","title":"Uncertainty based","text":"<p>Low uncertainty: all examples in leaf have same class High uncertainty: each class has same amount of examples in leaf</p> <p>Idea use counts at leaves to define probability distributions, and use information theory to measure uncertainty</p>"},{"location":"csc311/decision_trees.html#entropy","title":"Entropy","text":"<p>measure of expected \"surprise\", a.k.a. how uncertain are we of the value of a draw from this distribution  </p> \\[H(\\vec X) = -E_{X \\sim p}[\\log_2 p(X)] = -\\sum_{\\vec x\\in X} p(\\vec x)\\log_2 p(\\vec x)\\] <p>Average over information content of each observation Unit = bits (based on the base of log) A fair coin flip has 1 bit of entropy, i.e.  </p> \\[H(X) = -\\frac{1}{2}\\log_2 \\frac{1}{2} - -\\frac{1}{2}\\log_2 \\frac{1}{2} = -\\log_2 \\frac{1}{2} = 1\\] Source code <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nimport scipy.stats as sps\n\nplt.figure(figsize=(4, 3))\nx = np.arange(0.01, 1, 0.01)\ny = - 1 * x * np.log(x) - (1- x) * np.log(1- x)\nplt.plot(x, y)\nplt.title(\"Entropy of Bernoulli(0, 1)\")\nplt.tight_layout()\nplt.savefig(\"../assets/decision_trees.jpg\")\n</code></pre> <p></p>"},{"location":"csc311/decision_trees.html#high-entropy","title":"High Entropy","text":"<p>Variable has uniform-ish distribution Flat histogram Values sampled from it are less predicatable</p>"},{"location":"csc311/decision_trees.html#low-entropy","title":"Low Entropy","text":"<p>Variable has peaks and valleys Histogram with low and highs Values sampled from it are more predicatable</p>"},{"location":"csc311/decision_trees.html#example","title":"Example","text":"<p>Let X = {Raining, Not raining}, Y = {Cloudy, Not cloudy}  </p> C NC R 24 1 NR 25 50"},{"location":"csc311/decision_trees.html#entropy-of-a-joint-distribution","title":"entropy of a joint distribution","text":"\\[\\begin{align*} H(X,Y) &amp;= - \\sum_{\\vec x\\in X}\\sum_{y\\in Y}p(x,y)\\log_2 p(x,y)\\\\ &amp;= -\\frac{24}{100}\\log_2\\frac{24}{100} -\\frac{1}{100}\\log_2\\frac{1}{100}-\\frac{25}{100}\\log_2\\frac{25}{100} -\\frac{50}{100}\\log_2\\frac{50}{100}\\\\ &amp;\\approx 1.56 bits \\end{align*}\\]"},{"location":"csc311/decision_trees.html#conditional-entropy","title":"conditional entropy","text":"<p>Given it is raining, what is the entropy of cloudiness</p> \\[\\begin{align*} H(Y\\mid X = R) &amp;= -\\sum_{y\\in Y} p(y\\mid R)\\log_2 p(y\\mid R) \\\\ &amp;= -\\frac{24}{25}\\log_2\\frac{24}{25} -\\frac{1}{25}\\log_2\\frac{1}{25} \\\\ &amp;\\approx 0.24 bits \\end{align*}\\]"},{"location":"csc311/decision_trees.html#expected-conditional-entropy","title":"expected conditional entropy","text":"\\[\\begin{align} H(Y\\mid X) &amp;= E_{X\\sim p(x)[H(Y\\mid X)]} \\\\ &amp;= \\sum_{x\\in X}p(x)H(Y\\mid X= x) \\\\ &amp;= - \\sum_{x\\in X}\\sum_{y\\in Y}p(x,y)\\log_2 p(y\\mid x)\\\\ &amp;= -E_{(X,Y)\\sim p(x,y)}[\\log_2 p(Y\\mid X)] \\end{align}\\] <p>What is the entropy of cloudiness, given whether it is raining</p> \\[\\begin{align*} H(Y\\mid X) = \\sum_{x\\in X}p(x)H(Y\\mid X=x) \\\\ &amp;= \\frac{1}{4}H(Y\\mid R) + \\frac{3}{4} + H(Y\\mid NR)\\\\ &amp;\\approx 0.75 bits \\end{align*}\\]"},{"location":"csc311/decision_trees.html#properties","title":"Properties","text":"<p>for the discrete case</p> <ul> <li>\\(H\\geq 0\\)</li> <li>\\(H(X,Y)= H(X\\mid Y) + H(Y) = H(Y\\mid X) + H(X)\\) </li> <li>\\(X,Y\\) indep. \\(\\Rightarrow H(Y\\mid X) = H(Y)\\) </li> <li>\\(H(Y\\mid Y) = 0\\) </li> <li>\\(H(Y\\mid X)\\leq H(Y)\\)</li> </ul>"},{"location":"csc311/decision_trees.html#information-gain","title":"Information Gain","text":"<p>In \\(Y\\) due to \\(X\\) (or mutual information of \\(Y\\) and \\(X\\))is defined as </p> \\[IG(Y\\mid X) = H(Y) - H(Y\\mid X)\\] <p>Since \\(H(Y\\mid X )\\leq H(Y), IG\\geq 0\\) \\(X\\) is completely uninformative about \\(Y\\Rightarrow IG(Y\\mid X)= 0\\) \\(X\\) is completely informative about \\(Y\\Rightarrow IG(Y\\mid X) = H(Y)\\)</p> <p>Then, foe each decision, we gain some \\(X\\), so that we can calculate \\(IG\\)</p>"},{"location":"csc311/decision_trees.html#algorithm","title":"Algorithm","text":"<p>Start with empty decision tree and complete training set Split on the most informative attribute (most \\(IG\\)), partitioning dataset Recurse on subpartitions Possible termination condition: end if all examples in current subpartition share the same class</p>"},{"location":"csc311/decision_trees.html#what-makes-a-good-tree","title":"What makes a \"Good\" tree","text":"<p>Small Tree can't handle important but possibly subtle distinctions in data Big tree bad computational efficiency, over-fitting, human interpretability </p> <p>Occam's Razor find the simplest hypothesis that fits the observations</p>"},{"location":"csc311/decision_trees.html#expressiveness","title":"Expressiveness","text":"<ul> <li>Discrete input &amp; output case: can express any function of input attributes  </li> <li>Continuous input &amp; output: can approximate any function arbitrarily closely</li> </ul> <p>There's a consistent decision tree for any training set with one path to leaf for each example, while won't generalize to new examples</p>"},{"location":"csc311/decision_trees.html#miscellany","title":"Miscellany","text":""},{"location":"csc311/decision_trees.html#problems","title":"Problems","text":"<ul> <li>exponentially less data at lower levels  </li> <li>Too big tree =&gt; overfit</li> <li>Greedy don't necessarily yield the global optimum </li> <li>Mistakes at top-level propagate down tree</li> </ul> <p>For continuous attributes, must be split based on thresholds, which is more computational intensive in choosing more parameters</p> <p>With regression, use MSE as splits instead of IG</p>"},{"location":"csc311/decision_trees.html#decision-tree-vs-knn","title":"Decision Tree vs. kNN","text":"<p>Advantages of Decision Tree </p> <ul> <li>Good with discrete attributes</li> <li>Easily deals with missing values</li> <li>Robust to scale of inputs</li> <li>Test time is fast</li> <li>More interpretable</li> </ul> <p>Advantages of kNN </p> <ul> <li>Able to handle attributes/feature with interactions in complex ways</li> <li>Can incorporate interesting distance measures</li> </ul>"},{"location":"csc311/gradient_descent.html","title":"Gradient Descent","text":""},{"location":"csc311/gradient_descent.html#stochastic-gradient-descent","title":"Stochastic gradient descent","text":"<p>Update the parameters based on the gradient for a single training example</p> \\[1. \\text{Choose i uniformly}, 2. \\theta \\leftarrow \\theta - \\alpha \\frac{\\partial \\mathcal L^{(i)}}{\\partial \\theta}\\] <p>Therefore, for extremely large dataset, you can see some progress before seeing all the data. </p> <p>Such gradient is a biased estimate of the batch gradient</p> \\[E(\\partial_\\theta L^{(i)}) = N^{-1}\\sum_{i=1}^n \\partial_\\theta L^{(i)} = \\partial_\\theta J\\] <p>Note that by this expectation, we should do sampling with replacement</p>"},{"location":"csc311/gradient_descent.html#potential-issues","title":"Potential Issues","text":"<ul> <li>Dependent on the order </li> <li>Variance can be high, considering some points goes in one direction, while the others goes another</li> </ul>"},{"location":"csc311/gradient_descent.html#mini-batch","title":"mini-batch","text":"<p>Compute the gradients on a randomly chosen medium-sized set of training example.  </p> <p>Let \\(M\\) be the size of the mini batch,   - \\(M\\uparrow\\): computation  - \\(M\\downarrow\\): can't exploit vectorization, high variance</p>"},{"location":"csc311/gradient_descent.html#learning-rate","title":"Learning Rate","text":"<p>The learning rate also influences the fluctuations due to the stochasticity of the gradients.</p>"},{"location":"csc311/gradient_descent.html#strategy","title":"Strategy","text":"<p>Start large, gradually decay the learning rate to reduce the fluctuations</p> <p>By reducing the learning rate, reducing the fluctuations can appear to make the loss drop suddenly, but can come at the expense of long-run performance.</p>"},{"location":"csc311/gradient_descent.html#non-convex-optimization","title":"Non-convex optimization","text":"<p>Have a chance of escaping from local (but non global) minima. If the step-size is too small, it will likely to fall into the local minimum. </p>"},{"location":"csc311/gradient_descent.html#gd-with-momentum","title":"GD with Momentum","text":"<p>compute an exponentially weighted average of the gradient, and the use the gradient to update the weights</p>"},{"location":"csc311/gradient_descent.html#algorithm","title":"Algorithm","text":"<p>initialize \\(V= 0\\)</p> \\[V\\leftarrow \\beta V + (1-\\beta)\\frac{\\partial E}{\\partial w}\\] \\[w \\leftarrow w - \\alpha V\\] <p>where \\(\\alpha\\) is the learning rate, \\(\\beta\\) is the momentum. Commonly, \\(\\beta\\) is around 0.9</p>"},{"location":"csc311/gradient_descent.html#demo-gradient-based-optimization-for-machine-learning","title":"Demo: (Gradient Based) Optimization for Machine Learning","text":"<p>We will cover how to use the python package autograd to solve simple optimization problems. </p> <p>Autograd is a lightweight package that automates differentiating through numpy code. Understanding how to use it (and how it works) is not only a useful skill in itself, but also will it help you understand the inner workings of popular deep learning packages such as PyTorch. </p> <p>(optional) Check out the github page of autograd: https://github.com/HIPS/autograd. Specifically, check out the \"examples\" directory, where you can find transparent implementations of many interesting machine learning methods. </p> <p>This tutorial explores optimization for machine learning.</p> <p>We will implement a function and then use gradient-based optimization to find it's minimum value.  For ease of visualization we will consider scalar functions of 2-d vectors.  In other words \\(f: \\mathbb{R}^2 \\rightarrow \\mathbb{R}\\)</p> <p>One example of such a function is the simple quadratic </p> \\[f(x) = \\frac{1}{2} x^T x = \\frac{1}{2}(x_0^2 + x_1^2)\\] <p>where \\(x_i\\) represents the \\(i\\)-th entry of the vector \\(x\\).</p> <p>Question: is this function convex?</p> <p>Let's implement this in code and print \\(f(x)\\) for a few random \\(x\\)</p> <pre><code>import numpy as np\nimport autograd\n\ndef f(x):\n    return 0.5*np.dot(x.T, x)\n\nfor _ in range(5):  \n    x = np.random.randn(2)  # random 2-d vector\n    print('x={}, f(x)={:.3f}'.format(x, f(x)))\n\n#&gt;&gt; x=[ 1.624 -0.612], f(x)=1.506\n#&gt;&gt; x=[-0.528 -1.073], f(x)=0.715\n#&gt;&gt; x=[ 0.865 -2.302], f(x)=3.023\n#&gt;&gt; x=[ 1.745 -0.761], f(x)=1.812\n#&gt;&gt; x=[ 0.319 -0.249], f(x)=0.082\n</code></pre> <p>This simple function has minimum value \\(f(x^*)=0\\) given by \\(x^* = [0, 0]^T\\). </p> <p>Let's look at the more general quadratic </p> \\[f(x) = \\frac{1}{2}x^T A x\\] <p>where \\(A \\in \\mathbb{R}^{2 \\times 2}\\) is a positive semi-definite matrix. (Intuitively, these are matrices that \"stretch or shrink [eigenvectors] without rotating them. \")</p> <p>Notice that we get the previous function when we set \\(A\\) to the identity matrix \\(I\\).</p> <p>We can think of this function as a quadratic bowl whose curvature is specified by the value of \\(A\\). This is evident in the isocontour plots of \\(f(x)\\) for various \\(A\\). Let's take a look.</p> <p></p> <p>Now let's learn how to find the minimal value of \\(f(x)\\).  We need to solve the following optimization problem:</p> \\[x^* = \\underset{x}{\\arg \\min} \\quad \\frac{1}{2}x^T A x = \\underset{x}{\\arg \\min} \\quad f(x)\\] <p>Modern machine learning optimization tools rely on gradients.  Consider gradient descent, which initializes \\(x_0\\) at random then follows the update rule </p> \\[ x_{t+1} \\leftarrow x_t - \\eta \\nabla_x f(x_t)\\] <p>where \\(\\eta\\) represents the learning rate.</p> <p>So we need to compute \\(\\nabla_x f(x)\\). For simple functions like ours, we can compute this analytically:</p> \\[\\nabla_x f(x) = \\nabla_x \\left( \\frac{1}{2}x^T A x \\right) = A x\\] <p>In other words, to compute the gradient of \\(f(x)\\) at a particular \\(x=x'\\), we matrix multiply \\(A\\) with \\(x'\\)</p> <p>But deriving the analytic gradients by hand becomes painful as \\(f\\) gets more complicated. Instead we can use automatic differentation packages like <code>autograd</code> to do this hard work for us. All we need to do is specify the forward function.</p> <p>Let's take a look at the two approaches:</p> <pre><code># define df/dx via automatic differentiation\ndf_dx = autograd.grad(f, 0)\n# ^ the second argument of grad specifies which argument we're differentiating with respect to (i.e. x, \n# or A in this case). Note that f(x, A) has 2 arguments: x, A. So does grad{f(x,A)}, also x and A. \n\n# define df/dx analytically \ndef analytic_gradient(x, A):\n    return np.dot(A, x)\n\nfor A in [np.zeros((2, 2)), np.eye(2), random_psd_matrix()]:  # try a few values of A\n    x = np.random.randn(2)  # generate x randomly\n    print('')\n    print('x={}\\nA={}\\nf(x,A)={:.3f}\\ndf/dx={}'.format(x, A, f(x,A), df_dx(x,A)))\n    assert np.isclose(np.sum((df_dx(x, A) - analytic_gradient(x, A)))**2, 0.), 'bad maths'  # unit test\n</code></pre> <pre><code>x=[-1.35  -1.175]\nA=[[0. 0.]\n [0. 0.]]\nf(x,A)=0.000\ndf/dx=[0. 0.]\n\nx=[-0.176 -0.014]\nA=[[1. 0.]\n [0. 1.]]\nf(x,A)=0.016\ndf/dx=[-0.176 -0.014]\n\nx=[-0.577 -1.089]\nA=[[0.81  0.393]\n [0.393 0.191]]\nf(x,A)=0.494\ndf/dx=[-0.895 -0.434]\n</code></pre> <p>Now that we know how to compute \\(\\nabla_x f(x)\\) using <code>autograd</code>, let's implement gradient descent.</p> <p>To make the implementation of GD crystal clear, let's break this update expression from above into two lines:</p> \\[ \\delta_{t+1} \\leftarrow - \\eta \\nabla_x f(x_t)\\] \\[ x_{t+1} \\leftarrow x_t + \\delta_{t+1}\\] <p>This yields the following <code>autograd</code> implementation:</p> Gradient descent<pre><code>for i in range(32):\n    x_old = np.copy(x)\n    delta = -LEARNING_RATE*df_dx(x, A)  # compute gradient times learning rate\n    x += delta  # update params\n</code></pre> <p></p> <p>Cool! Now let's try gradient descent with momentum. The hyperparameters are the learning rate \\(\\eta\\) and momentum value \\(\\alpha \\in [0, 1)\\).</p> <p>We randomly initialize \\(x_0\\) like before. We initialize \\(\\delta_0\\) to the zero vector. Then proceed with updates as:</p> \\[\\delta_{t+1} \\leftarrow -\\eta \\nabla_x f(x) + \\alpha \\delta_t\\] \\[x_{t+1} \\leftarrow x_t + \\delta_{t+1}\\] <p>This yields the following <code>autograd</code> implementation:</p> GD with momentum<pre><code>for i in range(32):\n    x_old = np.copy(x)\n    delta_old = np.copy(delta)\n\n    g = df_dx(x, A)  # compute standard gradient\n    delta = -LEARNING_RATE*g + ALPHA*delta_old  # update momentum term\n    x += delta  # update params\n</code></pre> <p></p> <p>(optional) Distill is an amazing resource to both learn concepts about machine learning, but also a new format for serious scientific discourse. If you are interested in learning more about why momentum is very effective, check this out</p> Source code <pre><code>import autograd.numpy as np\nimport autograd\nimport matplotlib.pyplot as plt\nnp.set_printoptions(precision=3)\nnp.random.seed(1)\n\n# helper function yielding a random positive semi-definite matrix\ndef random_psd_matrix(seed=None):\n    \"\"\"return random positive semi-definite matrix with norm one\"\"\"\n    np.random.seed(seed)\n    A = np.random.randn(2,2)\n    A = np.dot(A.T,A)\n    A = np.dot(A.T,A)\n    A = A / np.linalg.norm(A, ord=2)\n    return A\n\n# define forward function\ndef f(x, a):\n    \"\"\"f(x) = x^T A x\"\"\"\n    y = 0.5*np.dot(x.T, np.dot(a, x))\n    return y\n\n# helper function for isocontour plotting\ndef plot_isocontours(one_d_grid, g, ax):\n    \"\"\"\n    first makes a 2d grid from the 1d grid\n    then plots isocontours using the function g\n    \"\"\"\n    X,Y = np.meshgrid(one_d_grid, one_d_grid)  # build 2d grid\n    Z = np.zeros_like(X)\n    # numpy bonus exercise: can you think of a way to vectorize the following for-loop?\n    for i in range(len(X)):\n        for j in range(len(X.T)):\n            Z[i, j] = g(np.array((X[i, j], Y[i, j])))  # compute function values\n    ax.set_aspect(\"equal\")\n    ax.contour(X, Y, Z, 100)\n\nmatrices = [np.ones((2, 2)), random_psd_matrix(0), random_psd_matrix(1), random_psd_matrix(11)]\nfig, axes = plt.subplots(2, 2, figsize=(8, 9))\nfig.tight_layout()\nfor i, A in enumerate(matrices):\n    ax = axes[i//2][i%2]\n    plot_isocontours(np.linspace(-0.5, 0.5, 200), lambda x: f(x, A), ax)\n    ax.set_title(rf\"{A}\")\nfig.savefig(f\"../assets/gradient_descent_1.jpg\")\n\n\ndf_dx = autograd.grad(f, 0)\n\nA = random_psd_matrix(0)  # argument specifies the random seed\nfig, ax = plt.subplots(figsize=(4, 4))\nplot_isocontours(np.linspace(-5, 5, 100), lambda x: f(x, A), ax)  # plot function isocontours\n\n# hyperparameters\nLEARNING_RATE = 2.0 \nINITIAL_VAL = np.array([4., -4.])  # initialize\n\nx = np.copy(INITIAL_VAL)\nax.plot(*x, marker='.', color='r', ms=25)  # plot initial values\n\n# --8&lt;-- [start:grad]\nfor i in range(32):\n    x_old = np.copy(x)\n    delta = -LEARNING_RATE*df_dx(x, A)  # compute gradient times learning rate\n    x += delta  # update params\n# --8&lt;-- [end:grad]\n    # plot\n    # plot a line connecting old and new param values\n    ax.plot([x_old[0], x[0]], [x_old[1], x[1]], linestyle='-', color='k',lw=2)  \n    fig.canvas.draw()\n    ax.set_title('i={}, x={} | f(x)={}'.format(i, x, f(x, A)))\nfig.tight_layout()\nfig.savefig(f\"../assets/gradient_descent_2.jpg\")\n\n\nfig, ax = plt.subplots(figsize=(4, 4))\nplot_isocontours(np.linspace(-5, 5, 100), lambda x: f(x, A), ax)  # plot function isocontours\n\n# initialize\nx = np.copy(INITIAL_VAL)\ndelta = np.zeros(2) \nax.plot(*x, marker='.', color='r', ms=25)  # plot initial values\n\n# hyperparameters\nLEARNING_RATE = 2.0\nALPHA = 0.5\n\n# --8&lt;-- [start:momentum]\nfor i in range(32):\n    x_old = np.copy(x)\n    delta_old = np.copy(delta)\n\n    g = df_dx(x, A)  # compute standard gradient\n    delta = -LEARNING_RATE*g + ALPHA*delta_old  # update momentum term\n    x += delta  # update params\n# --8&lt;-- [end:momentum]\n    # plot\n    ax.plot([x_old[0], x[0]], [x_old[1], x[1]],'-k',lw=2)  # plot a line connecting old and new param values\n    fig.canvas.draw()\n    ax.set_title('i={}, x={} | f(x)={}'.format(i, x, f(x, A)))\n\nfig.tight_layout()\nfig.savefig(f\"../assets/gradient_descent_3.jpg\")\n</code></pre>"},{"location":"csc311/kM_EM.html","title":"k-Means and EM Algorithm","text":""},{"location":"csc311/kM_EM.html#clustering-problem","title":"Clustering Problem","text":"<p>Grouping data points into clusters, without observed labels. </p> <p>Samples within a cluster are similar to each other, and samples in different clusters are dissimilar. Such data forms a multimodal distribution, as having multiple modes. </p>"},{"location":"csc311/kM_EM.html#k-means","title":"K-Means","text":"<p>Given data points \\(x^{(1)},...,x^{(N)}\\in \\mathbb R^D\\), want to find cluster centers \\(\\{m_k\\}^K\\in\\mathbb R^D\\) and assignments \\(\\{r^{(n)}\\}^N\\in \\mathbb R^K\\) being 1-K encoding.  </p> <p>The objective is to minimize the cost as the squared sum distance of data points to their assigned centers, i.e. </p> \\[\\min_{\\{m_k\\},\\{r^{(n)}\\}}J(\\{m_k\\},\\{r^{(n)}\\}) = \\min \\sum^N\\sum^K r^{(n)}\\|m_k - x^{(n)}\\|^2\\] <p>Because \\(r\\) works as an encoding, i.e. integer optimization problem, optimize both \\(r,m\\) will be NP-hard. </p> <p>However, if we fix \\(m\\), finding optimal assignments \\(r\\) is easy, i.e. </p> \\[\\min_{\\{r^{(n)}\\}}\\sum^K r_k^{(n)}\\|m_k-x^{(n)}\\|^2\\Rightarrow r_k = \\mathbb I(k=\\arg\\min_k\\|x^{(n)}-m_j\\|^2)\\] <p>If we fix \\(r\\), then we can minimize by taking gradient and set to 0 </p> \\[\\begin{align*} \\partial_{m_l}\\sum^N\\sum^K r_k^{(n)}\\|m_k - x^{(n)}\\|^2 &amp;= 0\\\\ 2\\sum^Nr_l^{(n)}(m_l - x^{(n)}) &amp;= 0\\\\ m_l &amp;= \\frac{\\sum^N r_l^{(n)}x^{(n)}}{\\sum^Nr_l^{(n)}} \\end{align*}\\] <p>Therefore, it is possible to do alternating minimization, i.e. minimize \\(J\\) w.r.t. \\(m, k\\) individually .</p>"},{"location":"csc311/kM_EM.html#alternating-minimization-algorithm","title":"Alternating Minimization Algorithm","text":"<ul> <li>Initialize with random cluster centers</li> <li>For each iterative step<ul> <li>Assignment step: assign data point to closest cluster by nearest neighbor</li> <li>Refitting step: move each cluster center to the mean of the data assigned to it.</li> </ul> </li> </ul> <pre><code>def kMeans(x, k, step, m=None):\n\n    # initialization\n    if m is None:\n        m = x[np.random.choice(x.shape[0], k)]\n\n    # iterative step\n    for i in range(step):\n        # assignment step\n        kd = sp.spatial.KDTree(m)\n        r = kd.query(x)[1]\n\n        # refitting step\n        for c in range(k):\n            m[c] = x[r == c].mean(axis=0)\n    return r, m\n</code></pre>"},{"location":"csc311/kM_EM.html#convergence","title":"Convergence","text":"<p>Note that \\(J\\) is lower bounded by 0 as a squared distance. Also, consider each assignment step, as \\(m\\) fixed, for each \\(x^{(n)}\\), \\(r_k^{(n)}\\) guarantees \\(x^{(n)}-m_k\\) is minimum. Consider each minimization step, as \\(r\\) fixed, for each \\(\\sum r_k( x^{(n)} - E(x^{(n)}))^2\\) is minimized. </p> <p>Therefore, by monotone convergence theorem, this will converge.</p> <p>However, this will not guarantee the global minimum. </p> <p></p>"},{"location":"csc311/kM_EM.html#soft-k-means","title":"Soft k-Means","text":"<p>Instead of making 0-1, we can make soft assignment, which we assign the clusters by responsibility (probability). </p>"},{"location":"csc311/kM_EM.html#algorithm","title":"Algorithm","text":"<ul> <li>Initialize \\(m\\) random</li> <li> <p>Iterative step</p> <ul> <li>Assignment</li> </ul> \\[r_k^{(n)} = \\frac{\\exp(-\\beta\\|m_k-x^{(n)}\\|^2)}{\\sum_j\\exp (-\\beta \\|m_j-x^{(n)}\\|^2)} = softmax(-\\beta\\{\\|m_k-x^{(n)}\\|^2\\}^K)\\] <ul> <li>Refitting</li> </ul> \\[m_k = \\frac{\\sum_n r_k^{(n)}x^{(n)}}{\\sum_n r_k^{(n)}}\\] </li> </ul> <p>Note that </p> \\[\\begin{align*} \\lim_{\\beta\\rightarrow \\infty}r_k^{(n)}&amp;= \\lim_{\\beta\\rightarrow\\infty}\\frac{\\exp(-\\beta\\|m_k-x^{(n)}\\|^2)}{\\sum_j\\exp (-\\beta \\|m_j-x^{(n)}\\|^2)}\\\\ &amp;= \\lim_{\\beta\\rightarrow\\infty}\\frac{\\exp(-\\beta\\|m_k-x^{(n)}\\|^2)}{\\exp(-\\beta\\|m_k-x^{(n)}\\|^2) + \\sum_{j\\neq k}\\exp (-\\beta \\|m_j-x^{(n)}\\|^2)}\\\\ &amp;= \\lim_{\\beta \\rightarrow \\infty}\\frac{1}{1+\\sum_{j\\neq k}\\exp \\bigg(-\\beta (\\|m_j-x^{(n)}\\|^2 - \\|m_k-x^{(n)}\\|^2)\\bigg)}&amp;(*)\\\\ &amp;= \\mathbb I(k=\\arg\\min_k\\|m_k-x^{(n)}\\|^2) \\end{align*}\\] <p>\\((*)\\) when \\(k\\) is the argmin, then \\(d:=\\|m_j-x^{(n)}\\|^2 - \\|m_k-x^{(n)}\\|^2 &gt; 0, -\\beta d\\rightarrow -\\infty, \\exp(-\\beta d)\\rightarrow 0\\). Otherwise, exists some \\(d&lt;0, \\exp(-\\beta d) \\rightarrow \\infty, \\lim r^{(n)}_k = 0\\)</p> <p>However, this raises questions about how to set \\(\\beta\\), and how to cluster unequal weight and width. </p>"},{"location":"csc311/kM_EM.html#termination-condition","title":"Termination condition","text":"<p>We can test whether we can terminate the k-Mean's iterative steps by checking whether the assignment change. </p>"},{"location":"csc311/kM_EM.html#generative-model","title":"Generative Model","text":"<p>Consider the generative model for \\(\\mathcal D\\): Assume data point \\(x\\) is generated as  - choose a cluster \\(z\\) from \\(\\pi = [\\pi_1,...,\\pi_K]^T, p(z=k)=\\pi_k\\)  - Given \\(z\\), sample \\(x\\) from \\(N(x|\\mu_z, I)\\)</p> \\[p(z=k)=\\pi_k, p(x|z=k)=N(x|\\mu_k, I)\\] <p>Then the joint distribution \\(p(z,x) = p(z)p(x|z)\\) with parameters \\(\\{\\pi_k, \\mu_k\\}^K\\),   marginal probability of \\(x, p(x)=\\sum_zp(z,x)\\) the probability \\(x\\) came from kth cluster is \\(p(z=k|x) = \\frac{p(x|z=k)p(z=l)}{p(x)}\\)</p>"},{"location":"csc311/kM_EM.html#gaussian-mixture-model-gmm","title":"Gaussian Mixture Model (GMM)","text":"<p>Using MLE approach, since we can't see the cluster assignments \\(z\\), we only see \\(x\\), i.e. the pbjective is </p> \\[\\begin{align*} p(\\mathcal D) \\Rightarrow \\log(p(\\mathcal D)) &amp;= \\sum^N \\log(p(x^{(n)})) \\\\ &amp;= \\sum^N\\sum^K p (z=k)p(x^{(n)}|z=k)\\\\ &amp;= \\sum^N\\sum^K \\pi_k N(x|\\mu_k, I) \\end{align*}\\] <p>Such distribution is an example of Gaussian Mixture Model (GMM) and \\(\\pi_k\\) are the mixing coefficients. </p> <p>In general, each Gaussian will have different covariance, i.e. \\(N(x|\\mu_k, \\sigma_k)\\). </p> <p>Note that \\(\\log(p(\\mathcal D)) = \\sum^N\\log \\bigg(\\sum^K \\pi_k N(x^{(n)}| \\mu_k, I)\\bigg)\\)</p> <p>Similar to k-Means, if we knew \\(z^{(n)}\\), i.e. assignment, then the log likelihood is similar to a classification question, i.e. </p> \\[\\log p(\\mathcal D_{complete}) = \\sum^N \\log p(z^{(n)}, x^{(n)}) = \\sum^N\\sum^K \\mathbb I(z^{(n)} = k)\\bigg[\\log(N(x^{(n)}|\\mu_k, I) + \\log\\pi_k)\\bigg]\\] <p>The maximization results in </p> \\[\\hat\\mu_k = \\frac{\\sum^N \\mathbb I(z^{(n)}=k)x^{(n)}}{\\sum^N \\mathbb I(z^{(n)}=k)} = \\text{class mean}\\] \\[\\hat\\pi_k = N^{-1}\\sum^N \\mathbb I(z^{(n)}=k) = \\text{class proportions}\\] <p>Then, we replace assignment since \\(E(z^{(n)}) = p(z=k|x)\\) where \\(p(z=k|x)\\) is computed from Bayes rule</p> \\[\\begin{align*} p(z=k|x) &amp;= \\frac{p(z=k)p(x|z=k)}{p(x)}\\\\ &amp;= \\frac{p(z=k)p(x|z=k)}{\\sum^K p(z=j)p(x|z=j)}\\\\ &amp;= \\frac{\\pi_k N(x|\\mu_k, I)}{\\sum^K_j \\pi_jN(x|\\mu_k, I)} \\end{align*}\\] <p>Therefore, let \\(r_k^{(n)} := p(z^{(n)}=k|x^{(n)})\\) replace \\(\\mathbb I(z^{(n)}=k)\\), and </p> \\[\\log p(\\mathcal D_{complete}) = \\sum^N \\log p(z^{(n)}, x^{(n)}) = \\sum^N\\sum^K  r_k^{(n)}\\bigg[\\log(N(x^{(n)}|\\mu_k, I) + \\log\\pi_k)\\bigg]\\] <p>The maximization results in </p> \\[\\hat\\mu_k = \\frac{\\sum^N r_k^{(n)} x^{(n)}}{\\sum^N r_k^{(n)}} = \\text{class mean}\\] \\[\\hat\\pi_k = N^{-1}\\sum^N r_k^{(n)} = \\text{class proportions}\\]"},{"location":"csc311/kM_EM.html#expectation-maximization-algorithm","title":"Expectation Maximization Algorithm","text":"<ul> <li>E-step compute the posterior probabilities \\(r_k^{(n)} = p(z^{(n)}=k|x^{(n)})\\) given our current model</li> <li>M-step update \\(\\mu, \\pi\\)</li> </ul>"},{"location":"csc311/kM_EM.html#possible-problems","title":"Possible Problems","text":"<ul> <li>Singularities: arbitrarily large likelihood when a Gaussian explains a single point with variance shrinking to zero</li> <li>Non-convex</li> </ul> Source code <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import make_blobs\nimport scipy as sp\n\n\n# --8&lt;-- [start:kmeans]\ndef kMeans(x, k, step, m=None):\n\n    # initialization\n    if m is None:\n        m = x[np.random.choice(x.shape[0], k)]\n\n    # iterative step\n    for i in range(step):\n        # assignment step\n        kd = sp.spatial.KDTree(m)\n        r = kd.query(x)[1]\n\n        # refitting step\n        for c in range(k):\n            m[c] = x[r == c].mean(axis=0)\n    return r, m\n# --8&lt;-- [end:kmeans]\n\n\n\nsize = 3\ncenters = np.random.uniform(-5, 5, (size, 2))\nsds = np.random.uniform(0.8, 1, size)\n\nx, t = make_blobs(n_samples=500, centers=centers, n_features=2, cluster_std = sds, random_state=0)\n\nplt.figure(figsize=(12, 18))\nplt.subplot(321)\nfor i in range(size):\n    plt.scatter(x[t==i, 0], x[t==i, 1])\nplt.title(\"group truth\")\nplt.axis(\"off\");\n\nr, m = kMeans(x, 3, 1)\nfor iteration in range(4):\n    plt.subplot(3, 2, 3 + iteration)\n    plt.title(\"iteration \" + str(iteration))\n    plt.axis(\"off\");\n    for i in range(r.max() + 1):\n        plt.scatter(x[r==i, 0], x[r==i, 1])\n        r, m = kMeans(x, 3, 1, m)\nplt.tight_layout()\nplt.savefig(\"../assets/kmeans_1.jpg\")\n\n\n\ncenters = np.array([[ 1.69653073,  4.85908807], [ 0.13447474,  3.7240316 ], [-2.02381919, -2.05536678]])\nsds = 0.8119142178109643\nm = np.array([[ 0.84192262,  4.26238333], [-1.67684624, -2.56679316], [-2.44040286, -1.48432092]])\n\nx, t = make_blobs(n_samples=500, centers=centers, n_features=2, cluster_std = sds, random_state=0)\n\nplt.figure(figsize=(12, 18))\nplt.subplot(321)\nfor i in range(size):\n    plt.scatter(x[t==i, 0], x[t==i, 1])\nplt.title(\"group truth\")\nplt.axis(\"off\");\n\nr, m = kMeans(x, 3, 1, m)\nfor iteration in range(4):\n    plt.subplot(3, 2, 3 + iteration)\n    plt.title(\"iteration \" + str(iteration))\n    plt.axis(\"off\");\n    for i in range(r.max() + 1):\n        plt.scatter(x[r==i, 0], x[r==i, 1])\n        r, m = kMeans(x, 3, 1, m)\nplt.tight_layout()\nplt.savefig(\"../assets/kmeans_2.jpg\")\n</code></pre>"},{"location":"csc311/knn.html","title":"k-Nearest Neighbors","text":""},{"location":"csc311/knn.html#supervised-learning","title":"Supervised learning","text":"<p>Given a training set consisting of inputs and corresponding labels. </p>"},{"location":"csc311/knn.html#input-vectors","title":"Input vectors","text":"<p>To handle different types of data, we need to represent the input as input vector in \\(\\mathbb R^d\\), i.e. Representation mapping to another space that's easy to manipulate</p> <p>The training set consists of a collection of pairs of an input vector \\(\\vec x \\in \\mathbb R^d\\) and its corresponding target (label) \\(t\\) where \\(t\\) can be. Regression \\(t\\in\\mathbb R\\) Classification \\(t\\in \\{1,2,...,C\\}\\) More structured object </p> <p>The training set is denoted as \\(\\{(\\vec x^{(i)}, t^{(i)})| i\\in\\ \\{1,2,...,N\\}\\}\\)</p>"},{"location":"csc311/knn.html#k-nearest-neighbors_1","title":"k-Nearest Neighbors","text":"<p>Given a novel input \\(\\vec x\\) to be classified, to find the nearest input vector to \\(\\vec x\\) in the training set and copy its label.  </p> <p>nearest for example, this can be formalized by the Euclidean distance \\(\\|\\vec x^{(a)} - \\vec x^{(b)}\\|_2\\)</p> <p>Decision boundary the boundary between regions of input space assigned to different categories</p>"},{"location":"csc311/knn.html#1nn","title":"1NN","text":"<p>Find example \\((\\vec x^*, t^*)\\) (from the stored training set) closest to \\(\\vec x\\). i.e. </p> \\[\\vec x^* = arg\\min_{\\vec x^{(i)}\\in T} d(\\vec x^{(i)}, \\vec x)\\] <p>Then output \\(t^*\\)</p> <p>Note that \\(d\\) can be directly computed from the square, since we only want the argmin</p>"},{"location":"csc311/knn.html#problem","title":"Problem","text":"<p>it is sensitive to noise of mis-labeled data</p>"},{"location":"csc311/knn.html#knn","title":"kNN","text":"<p>To smooth the noise effect, we can having \\(k\\) nearest neighbors to vote</p> <p>Find \\(k\\) example \\(\\{\\vec x^{(i)}, t^{(i)}\\}\\) closest to the test instance \\(\\vec x\\), then classification output is majority class </p> \\[y = arg\\max_{t^{(z)}}\\sum_{r=1}^k \\mathbb I(t^{(z)}, t^{(r)})\\] Source code <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import ListedColormap\nfrom sklearn import neighbors, datasets\n\n# import some data to play with\niris = datasets.load_iris()\n\n# we only take the first two features. We could avoid this ugly\n# slicing by using a two-dim dataset\nX = iris.data[:, :2]\ny = iris.target\n\nh = .02  # step size in the mesh\n\n# Create color maps\ncmap_light = ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAFF'])\ncmap_bold = ListedColormap(['#FF0000', '#00FF00', '#0000FF'])\n\n# we create an instance of Neighbours Classifier and fit the data.\nfig, axs = plt.subplots(2, 2, figsize=(8, 8))\nk_set = [1, 5, 15, 50]\nfor i in range(4):\n    clf = neighbors.KNeighborsClassifier(k_set[i], weights='distance')\n    clf.fit(X, y)\n\n    # Plot the decision boundary. For that, we will assign a color to each\n    # point in the mesh [x_min, x_max]x[y_min, y_max].\n    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                             np.arange(y_min, y_max, h))\n    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n\n    # Put the result into a color plot\n    Z = Z.reshape(xx.shape)\n    axs[i//2][i%2].pcolormesh(xx, yy, Z, cmap=cmap_light)\n\n    # Plot also the training points\n    axs[i//2][i%2].scatter(X[:, 0], X[:, 1], c=y, cmap=cmap_bold,\n                    edgecolor='k', s=20)\n    axs[i//2][i%2].set_xlim(xx.min(), xx.max())\n    axs[i//2][i%2].set_ylim(yy.min(), yy.max())\n    axs[i//2][i%2].set_title(\"(k = %i)\" % k_set[i])\nplt.tight_layout()\nfig.savefig(\"../assets/knn.jpg\")\n</code></pre> <p></p>"},{"location":"csc311/knn.html#pitfalls","title":"Pitfalls","text":""},{"location":"csc311/knn.html#tradeoffs-with-k","title":"Tradeoffs with k","text":"<p>Small k is good at capturing fine-grained patterns May overfit, i.e. be sensitive to random idiosyncrasies in the training data</p> <p>Large k makes stable predictions by averaging over lots of examples May underfit, i.e. fail to capture important regularities</p> <p>Rule of thumb \\(k &lt; \\sqrt n\\), where \\(n\\) is the number of training examples</p>"},{"location":"csc311/knn.html#training-set-test-set-and-validation-set","title":"Training set, test set, and validation set","text":"<p>To generalize the algorithm to data haven't been seen before, we can measure the generalization error using a test set. </p> <p>Note that one test set can only be used to \"test\" the final error rate, it cannot be used to improve the algorithm, otherwise it will be the same as a training set, hence meaninglessly cheating. </p> <p>\\(k\\) is an example of a hyperparameter, i.e. not fit as part of the learning algorithm itself. So to get the \"better\" \\(k\\), we can use a validation set.</p> training set validation set test set train with k1 err = 7.3 train with k2 err = 1.1 we can test on this train with k3 err = 10.5"},{"location":"csc311/knn.html#the-curse-of-dimensionality","title":"The Curse of Dimensionality","text":"<p>Low-D visualizations are misleading, in high dimensions, \"most\" points are far apart. If we want the nearest neighbor to be closer than \\(\\epsilon\\) (i.e. including the non-categorized option), then since each single ball centered at \\(\\vec x\\) have \\(V(B_\\epsilon(\\vec x)) = O(\\epsilon^d)\\) and the total volume of \\([0,1]^d\\) is 1. Therefore, \\(O(\\epsilon^{-d})\\) balls are needed to cover the volume. </p> <p>In high dimensions, \"most\" points are approximately the same distance. However, at many times, data points are not uniformly distributed in the space, we may have low intrinsic dimension i.e. lies on or near a low-dimensional manifold. So nearest neighbors sometimes still works in high dimensions. </p> <p>For example, considering the digit identification case, each image of number is given as \\(64\\times 64\\)px image, while the corners will never have ink, hence it is not uniformly distributed and we can reduce the dimensionality. </p>"},{"location":"csc311/knn.html#normalization","title":"Normalization","text":"<p>The units of each dimension are often arbitrary, i.e. kNN can be sensitive to the ranges of different features. </p> <p>Fix: normalize each dimension with \\(N(0, 1)\\) i.e. for each dimension, compute \\(\\mu_j, \\sigma_j\\), then \\(\\tilde x_j = \\frac{x_j - \\mu_j}{\\sigma_j}\\)</p>"},{"location":"csc311/knn.html#computation-cost","title":"Computation cost","text":"<p>training time: 0 test time per query (naive algorithm): Calculate D-dimensional Euclidean distance with \\(N\\) data points \\(\\in O(ND)\\)  Sort the distance \\(\\in O(N\\log N)\\)</p> <p>Also, have to store all the dataset in the memory, for high dimension datasets that will be even more huge. </p>"},{"location":"csc311/knn.html#conclusion","title":"Conclusion","text":"<p>Simple algorithm that does not have \"learning\" Can control the complexity by varying k Curse of dimensionality</p>"},{"location":"csc311/linear_regression.html","title":"Linear Regression","text":""},{"location":"csc311/linear_regression.html#modular-approach-of-a-question","title":"Modular approach of a question","text":"<ul> <li>Choose a model describing the relationships between variables of interest</li> <li>loss function quantifying how bad is the fit</li> <li>regularizer saying how much we prefer different candidate explanations</li> <li>fit the model using an optimization algorithm</li> </ul> <p>For supervised learning Given: target \\(t\\in\\mathcal T\\) (response, outcome, output, class) features \\(x\\in\\mathcal X\\) (inputs, covariates, design) Objective to learn a function \\(f:\\mathcal X \\rightarrow \\mathcal T\\) .s.t. \\(t\\approx y = f(x)\\)</p>"},{"location":"csc311/linear_regression.html#linear-regression_1","title":"Linear Regression","text":""},{"location":"csc311/linear_regression.html#model","title":"Model","text":"\\[y = f(\\vec x) = \\sum_{j} w_j x_j +b\\] <p>where \\(\\vec x\\) is the input, \\(y\\) is prediction, \\(\\vec w\\) is wights, \\(b\\) is the bias \\(\\vec w, b\\) are the parameters </p> <p>In matrix form \\(y = XW\\) where </p> \\[ X= \\begin{bmatrix}   1&amp;[x^{(1)}]^T\\\\ ...&amp;...\\\\ 1&amp;[x^{(D)}]^T \\end{bmatrix}, W = \\begin{bmatrix}b&amp;w_1&amp;w_2&amp;...&amp;w_D\\end{bmatrix}^T\\]"},{"location":"csc311/linear_regression.html#loss-function-squared-error","title":"Loss Function (Squared error)","text":"<p>\\(\\mathcal L(y, t) = \\frac{(y-t)^2}{2}\\) where \\(y-t\\) is the residual and \\(\\frac{1}{2}\\) is just to make the calculations convenient. </p> <p>Therefore, define cost function to be the average over all training examples  </p> \\[\\mathcal J(\\vec w, b) = \\frac{\\sum^N (y^{(i)}- t^{(i)})^2}{2} = \\frac{1}{2} \\sum^N (\\vec w^T \\vec x^{(i)} + b - t^{(i)})^2\\] <p>To minimize the loss/cost, calculate \\(\\partial_{w_j} \\mathcal J := 0, \\forall j \\in \\{0,1,2,.., N\\}, w_0 = b\\)  The resulted </p> \\[\\vec w^{L.S.} = (X^TX)^{-1}X^Tt\\]"},{"location":"csc311/linear_regression.html#improving-model-polynomial-curve-fitting","title":"Improving model: Polynomial curve fitting","text":"<p>Consider feature mapping \\(\\psi(x):\\mathbb R^D\\rightarrow \\mathbb R^M\\), for example \\(x\\in\\mathbb R, \\psi(x) = [1, x, x^2]^T\\), so that we get a new \\(\\vec x'\\) and can be used into fit</p>"},{"location":"csc311/linear_regression.html#underfit-and-overfit","title":"Underfit and Overfit","text":"<p>Underfit: model is too simple to fit the data Overfit: too complex so that fit the data perfectly</p>"},{"location":"csc311/linear_regression.html#improving-model-l2-regularization","title":"Improving model: L<sup>2</sup> Regularization","text":"<p>A function that quantifies how much we prefer one hypothesis vs. another</p> <p>We encourage the weights to be small by choosing as our regularizer the \\(L^2\\) penalty \\(\\mathcal R(\\vec w) := \\frac{\\|\\vec w\\|^2_2}{2}\\) The regularized cost function makes a trade-off between fit to the data and the norm of the weights  </p> \\[\\mathcal J_{reg}(\\vec w) = \\mathcal J(\\vec w) + \\lambda \\mathcal R(\\vec w)\\] <p>Hence \\(\\lambda\\) is a hyperparameter that we can tune with a validation set and allows to vary penalty on dimensionality. </p> <p>When measuring the validation rate, we still measure \\(\\mathcal J(\\vec w)\\), but for training we will use \\(\\mathcal J_{reg}(\\vec w)\\) for determining \\(M\\)</p> <p>Probelms need to make sure \\(x_i\\)'s have approximately the same unit so that \\(\\mathcal R(\\vec w)\\) is not dominated by some feature weights</p> <p>For LS, regularized cost gives </p> \\[\\vec w_\\lambda^{Ridge} = arg\\min_{\\vec w} \\mathcal J_{reg}(\\vec w)= (X^T X+\\lambda I)^{-1}X^T t\\]"},{"location":"csc311/linear_regression.html#l1-regularization","title":"L<sup>1</sup> Regularization","text":"<p>\\(\\mathcal R_{L^1} = \\sum |w_i|\\) encourages weights to be exactly zero, we can design regularizers based on whatever property we'd like to encourage. </p>"},{"location":"csc311/linear_regression.html#conclusion","title":"Conclusion","text":"<ul> <li>Choose a model and a loss function</li> <li>Formulate an optimization problem</li> <li>Solve the minimization problem using either direct solution (set derivative to zero) or gradient descent (move \\(\\vec w\\), start with a guess, slowly changes to minimize cost, when direct solution is unavailable)</li> <li>vectorize </li> <li>use features to get a more powerful linear model</li> <li>improve the generalization by adding a regularizer</li> </ul>"},{"location":"csc311/matrix_factorization.html","title":"Matrix Factorization","text":""},{"location":"csc311/matrix_factorization.html#pca-as-a-matrix-factorization","title":"PCA as a matrix factorization","text":"<p>Given PCA reconstruction </p> \\[x^{(i)}\\approx Uz^{(i)}\\] <p>Write all the observations into matrix form, </p> \\[\\underset{X\\in \\mathbb R^{N\\times D}}{\\begin{bmatrix}[x^{(1)}]^T \\\\ \\vdots \\\\ [x^{(N)}]^T \\end{bmatrix}}\\approx  \\underset{Z\\in\\mathbb R^{N\\times K}}{\\begin{bmatrix}[z^{(1)}]^T \\\\ \\vdots \\\\ [z^{(N)}]^T  \\end{bmatrix}}U^T\\] <p>Using Frobenius norm \\(\\|Y\\|_F^2 = \\|Y^T\\|^2_F = \\sum_{i,j}y_{ij}^2 = \\sum_i \\|y^{(i)}\\|^2\\), define the squared error being \\(\\|X-ZU^T\\|_F^2\\)</p>"},{"location":"csc311/matrix_factorization.html#singular-value-decomposition","title":"Singular Value Decomposition","text":"<p>Given \\(X\\), the SVD is \\(X = QSU^T\\) where  \\(Q\\in\\mathbb R^{N\\times D}\\) with orthonormal columns, i.e. \\(Q^TQ = I_D\\).  \\(S\\in\\mathbb R^{D\\times D}\\) is the diagonal matrix \\(U\\in\\mathbb R^{D\\times D}\\) is the orthonormal matrix, \\(U^T = U^{-1}\\)</p>"},{"location":"csc311/matrix_factorization.html#properties-of-covariance-matrices","title":"Properties of covariance matrices","text":"<p>Construct two positive semi-definite matrices \\(XX^T (N\\times N)\\) and \\(X^TX(D\\times D)\\) \\(XX^T = QSU^T(QSU^T)^T = QSU^TUSQ^T = QSISQ^T = QS^2Q^T\\) is an eigendecomposition of \\(XX^T\\), similarly \\(X^TX = US^2U^T\\) Assuming \\(N\\geq D\\), then \\(XX^T,X^TX\\) will share \\(D\\) eigenvalues and the remaining \\(N-D\\) eigenvalues will be \\(0\\) </p> <p>Therefore, consider \\(\\hat\\Sigma\\), </p> \\[\\begin{align*} \\hat\\Sigma &amp;= N^{-1}X^TX \\\\ &amp;= N^{-1}USQ^TQSU^T\\\\ &amp;= U(S^2/N)U^T \\end{align*}\\] <p>the eigenvalues \\(\\lambda_i\\) are related to the singular values \\(\\lambda_i = s_i^2 / N\\), the SVD gives \\(U\\) which is equivalent to the learned basis of PCA. </p>"},{"location":"csc311/matrix_factorization.html#recommender-systems-matrix-completion","title":"Recommender Systems (Matrix Completion)","text":"<p>Compare to a PCA problem, where all the observations are known. Recommender systems are often sparse matrices with many unknown, i.e. Matrix completion problem. </p> <p>Consider the movie rating problem, which given users, movies, and user rates on some pf the movies. Want to predict the ratings of the movies that the user haven't watched. </p>"},{"location":"csc311/matrix_factorization.html#alternating-least-squares","title":"Alternating Least Squares","text":"<p>Let the representation of user \\(n\\) in the \\(K-\\)dimensional space be \\(\\vec u_n\\) and the representation of movie \\(m\\) be \\(z_m\\). Assume the rating user \\(n\\) gives to movie \\(m\\) is given by \\(R_{nm}\\approx u_n^T z_m\\), then this gives </p> \\[R = \\underset{U}{\\begin{bmatrix}u_1^T\\\\\\vdots\\\\u_N^T\\end{bmatrix}}\\underset{Z^T}{\\begin{bmatrix}z_1&amp;...&amp;z_M\\end{bmatrix}}\\] Source code <pre><code>import matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sn\n\n\nN, M, K = 6, 8, 3\nU = np.random.normal(2, 1, (N, K))\nU[U &lt; 2.5] = 0\nZ_T = np.random.normal(2, 1, (K, M))\nZ_T[Z_T&lt;2.5] = 0\nplt.figure(figsize=(12, 4))\nplt.subplot(131)\nplt.title(r\"$R$\")\nplt.imshow(U@Z_T, cmap=\"Greens\")\nplt.subplot(132)\nplt.title(r\"$U$\")\nplt.imshow(U, cmap=\"Greens\")\nplt.subplot(133)\nplt.title(r\"$Z^T$\")\nplt.imshow(Z_T, cmap=\"Greens\")\n\nplt.tight_layout()\nplt.savefig(\"../assets/matrix_factorization_1.jpg\")\n</code></pre> <p></p> <p>To enforce \\(R\\approx UZ^T\\), i.e. \\(\\min_{U,Z} \\sum _{i,j}(R_{ij} - u_i^Tz_j)^2\\), while most \\(R_{ij}\\) are unknown, so we can only minimize the observed parts, i.e. </p> \\[\\min_{U,Z} \\sum_{(n,m)\\in \\mathcal O} (R_{nm}- u_n^Tz_m)^2\\|, \\mathcal O := \\{(n,m), R_{nm}\\text{ is observed}\\}\\] <p>However, such objective is non-convex and generally NP-hard to solve, so we have to use alternating approach, i.e. optimize \\(U,Z\\) individually and alternatively. </p> <p>The generally algorithm follows that </p> <ul> <li>initialize \\(U,Z\\) randomly</li> <li> <p>repeat until convergence </p> <ul> <li>for \\(n = 1..N\\)</li> </ul> \\[u_n = (\\sum_mz_mz_m^T)^{-1}\\sum_m R_{nm}z_m\\] <ul> <li>for \\(m = 1..M\\) </li> </ul> \\[z_m = (\\sum_n u_n u_n^T)^{-1}\\sum_n R_{nm}u_n\\] </li> </ul> <p>where \\((n,m)\\in \\mathcal O\\). </p> <p>With such algorithm, we can do gradient descent Full gradient descent is \\(\\begin{bmatrix}U\\\\Z\\end{bmatrix} \\leftarrow \\begin{bmatrix}U\\\\Z\\end{bmatrix} - \\alpha \\nabla f(U,Z)\\).  </p> <p>For large dataset, we can use stochastic gradient descent </p> \\[\\begin{bmatrix}u_n\\\\z_m\\end{bmatrix} \\leftarrow \\begin{bmatrix}u_n\\\\z_m\\end{bmatrix} - \\alpha \\begin{bmatrix}(R_{nm}-u_n^Tz_m)z_m\\\\(R_{nm}-u^T_nz_m)u_n\\end{bmatrix}\\] <p>So we can change the updating step to </p> <ul> <li>randomly select a pair \\((n,m)\\in\\mathcal O\\) among observed elements of \\(R\\). </li> <li>\\(u_n \\leftarrow u_n - \\alpha (R_{nm} - u_n^T z_m)z_m\\)</li> <li>\\(z_m \\leftarrow z_m - \\alpha (R_{nm} - u_n^Tz_m)u_n\\)</li> </ul>"},{"location":"csc311/matrix_factorization.html#k-means-as-a-matrix-factorization","title":"K-Means as a matrix factorization","text":"<p>Stack assignment vectors \\(r_i\\) into \\(N\\times K\\) matrix R, and the cluster means \\(m_k\\) into a matrix \\(K\\times D\\) matrix \\(M\\). Then, the reconstruction with its cluster is given by \\(RM\\). </p> <p>Taking one step further, feature dimensions can be redundant and some feature dimensions cluster together. </p> <p>Co-clustering clusters both the rows and columns of a data matrix, giving a block structure. We can represent this as the indicator matrix for rows, times the matrix of means for each block, times the indicator matrix for columns. </p>"},{"location":"csc311/matrix_factorization.html#sparse-coding","title":"Sparse Coding","text":"<p>This algorithm works on small image patches, which we reshape into vectors. Suppose we have a dictionary of basis functions \\(\\{a_k\\}_{k=1}^K\\) which can be combined to model each patch.  Each patch is approximated as a linear combination of a small number of basis functions </p> \\[x = \\sum^K s_k a_k = As\\] <p>This is an overcomplete representation, in that typically \\(K&gt;D\\) for sparse coding problems, the requirement that \\(s\\) is sparse makes things interesting.  </p> <p>We'd like choose \\(s\\) to accurately reconstruct the image \\(x\\approx As\\) but encourage sparsity in \\(s\\). What cost function should we use? Inference in the sparse coding model: </p> \\[\\min_s \\|x-As\\|^2 + \\beta \\|s\\|_1\\] <p>Here, \\(\\beta\\) is a hyper-parameter that trades off reconstruction error v. sparsity.  There are efficient algorithms for minimizing this cost function.</p>"},{"location":"csc311/matrix_factorization.html#learning-the-dictionary","title":"Learning the dictionary","text":"<p>We can learn a dictionary by optimizing both \\(A\\) and \\(\\{s_i\\}_{i=1}^N\\) to trade off reconstruction error and sparsity </p> \\[\\min_{\\{s_i\\}, A} \\sum_{i=1}^N \\|x^{(i)}- As_i\\|^2 + \\beta\\|s_i\\|_1\\] <p>subject to \\(\\|a_k\\|^2 \\leq 1, \\forall k\\). Reconstruction term can be written in matrix form as \\(\\|X-AS\\|_F^2\\) where \\(S\\) combines the \\(s_i\\) as columns.  Can fit using an alternating minimization scheme over \\(A\\) and \\(S\\) just like \\(K\\)-means, \\(EM\\), low-rank matrix completion, etc. </p> <p>The sparse components are oriented edges, similar to what a neural networks learn. But the learned dictionary is much more diverse than the first-layer neural net representations. Tiles the space of location, frequency, and orientation in an efficient way.  </p>"},{"location":"csc311/neural_nets.html","title":"Neural Networks","text":""},{"location":"csc311/neural_nets.html#inspiration-and-introduction","title":"Inspiration and Introduction","text":"<p>Unit simulates a model neuron by </p> \\[y = \\phi(\\vec w^T \\vec x + b)\\] \\[\\text{output} = \\text{Activation function}(\\text{weights}^T\\cdot \\text{inputs}) + \\text{bias}\\] <p>The, by throwing together lots of processing unit, we can form a neural net. </p>"},{"location":"csc311/neural_nets.html#structure","title":"Structure","text":"<p>A NN have 3 types of layers: input, hidden layer(s), output</p>"},{"location":"csc311/neural_nets.html#feed-forward-neural-network","title":"Feed-forward neural network","text":"<p>A directed acyclic graph of connected neurons, starts from input layer, and ends with output layer. Each layer is consisted of many units.  </p> <p>Recurrent neural networks allows cycles</p>"},{"location":"csc311/neural_nets.html#multilayer-perceptrons","title":"Multilayer Perceptrons","text":"<p>A multiplayer network consisting of fully connected layers. </p> <ul> <li>Each hideen layer \\(i\\) connects \\(N_{i-1}\\) input units to \\(N_i\\) output units. </li> <li>Fully connected layer in the simplest case, all input units are connected to all output units.</li> <li>Note that the inputs and outputs for a layer are distinct from the inputs and outputs to the network.</li> </ul>"},{"location":"csc311/neural_nets.html#activation-functions","title":"Activation Functions","text":"<ul> <li>identity \\(y =z\\)</li> <li>Rectified Linear Unit (ReLU) \\(y = \\max(0,z)\\)</li> <li>Soft ReLU \\(y = \\log(1 + e^z)\\)</li> <li>Hard Threshold \\(y = \\mathbb I(z &gt; 0)\\)</li> <li>Logistic \\(y = (1+e^{-z})^{-1}\\)</li> <li>Hyperbolic Tangent (tanh) \\(y = \\frac{e^z - e^{-z}}{e^z + e^{-z}}\\)</li> </ul> Source code <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\n\nx = np.arange(-5, 5, 0.01)\nsrelu = np.log(1+np.exp(x))\nlogit = (1 + np.exp(-x)) ** (-1)\na, b = np.exp(x), np.exp(-x)\ntanh = (a - b) / (a + b)\nfig, axs = plt.subplots(1, 3, figsize=(12, 4), sharey=True)\naxs[0].plot(x, srelu); axs[0].set_title(\"Soft ReLU\"); axs[0].set_ylim(-3, 3)\naxs[1].plot(x, logit); axs[1].set_title(\"Logistic\")\naxs[2].plot(x, tanh); axs[2].set_title(\"tanh\")\nfig.tight_layout()\nfig.savefig(\"../assets/neural_nets_activations.jpg\")\n</code></pre> <p> Examples of activation functions</p> <p>Consider the layers,   </p> \\[\\begin{align*} h^{(1)} &amp;= f^{(1)}(x) = \\phi(X W^{(1)} + b^{(1)}) &amp;\\text{base}\\\\ h^{(i)} &amp;= f^{(i)}(x) = \\phi(h^{(i-1)} W^{(i)} + b^{(i)}) &amp;\\text{induction}\\\\ y &amp;= f^{(L)}(h^{(L-1)}) &amp;\\text{ending} \\end{align*}\\] <p>Therefore, we can consider the network as a composition of functions </p> \\[y = f^{(L)}\\circ ...\\circ f^{(1)}(X)\\] <p>Therefore, neural nets provide modularity: we can implement each layer's computations as a black box. </p> <p>For the last layer, if the task is regression, then the last activation function is identity function; if the task is binary classification, then is sigmoid function</p>"},{"location":"csc311/neural_nets.html#neural-network-vs-linear-regression","title":"Neural Network vs. Linear regression","text":"<p>Suppose a layer's activation was the identity, so is equivalent to an affine transformation of the input, then we call it a linear layer</p> <p>Consider a sequence of linear layer, it is equivalent to a single linear layer, i.e. a linear regression</p> \\[y = XW^{(h)}\\circ...\\circ W^{(1)} \\equiv X\\tilde W, W^{(i)}\\in\\text{affine}\\]"},{"location":"csc311/neural_nets.html#universality","title":"Universality","text":"<p>However, multilayer feed-forward neural nets with nonlinear activation function are universal function approximators, they can approximate any function arbitrarily well. </p>"},{"location":"csc311/neural_nets.html#problems","title":"Problems","text":"<p>this theorem does not tell how large the network well be we need to find appropriate weights will eventually lead to overfit, since it will fit the training set perfectly </p>"},{"location":"csc311/neural_nets.html#back-propagation","title":"Back-propagation","text":"<p>Given a NN model (with number of layers, activation function for each layer). We then have the weight space being one coordinate for each weight/bias of the network, in all the layers. Then, we have to compute the gradient of the cost \\(\\partial \\mathcal J / \\partial \\vec w\\), a.k.a. \\(\\partial \\mathcal L / \\partial \\vec w\\)</p> <p>Consider the NN </p> \\[y = f^{(L)}...f^{(1)}(X), f^{(i)} = \\phi^{(i)}(h^{(i-1)}W^{(i)} + b)\\] \\[\\mathcal L(\\vec w,b)=\\frac{(y-t)^2}{2}\\] <p>By chain rule</p> \\[\\frac{\\partial \\mathcal L}{\\partial w^{(1)}} = \\frac{\\partial \\mathcal L}{\\partial y}\\frac{\\partial y}{\\partial f^{(i)}}\\frac{\\partial f^{(i)}}{\\partial f^{(i-1)}}...\\frac{\\partial f^{(1)}}{\\partial h^{(1)}}\\frac{\\partial h^{(1)}}{\\partial w^{(1)}}\\]"},{"location":"csc311/neural_nets.html#univariate-example","title":"Univariate Example","text":"\\[z=wx+b, y = \\sigma(z), \\mathcal L = \\frac{(y-t)^2}{2}\\] <pre><code>flowchart LR\n    w --&gt; z\n    x --&gt; z\n    b --&gt; z\n    z --&gt; y\n    t --&gt; L\n    y --&gt; L</code></pre> <p>Denote \\(\\bar y := \\frac{\\partial L}{\\partial y}\\), or the error signal. This emphasizes that the error signals are just values out program is computing, rather than a mathematical operation.  Then, </p> \\[\\begin{align*} \\bar y &amp;=\\partial_y\\mathcal L  = y - t\\\\ \\bar z &amp;= \\bar y \\partial_z\\sigma(z)\\\\ \\bar w &amp;= \\bar z x\\\\ \\bar b &amp;= \\bar z \\end{align*}\\]"},{"location":"csc311/neural_nets.html#multivariate-perceptron-example","title":"Multivariate Perceptron example","text":"<pre><code>flowchart LR\n    W1 --&gt; z\n    x --&gt; z\n    b1 --&gt; z\n    z --&gt; h\n    W2 --&gt; y\n    h --&gt; y\n    b2 --&gt; y\n    t --&gt; L\n    y --&gt; L</code></pre> <p>Forward pass</p> \\[\\begin{align*} \\vec z &amp;= XW^{(1)} + b^{(1)} &amp;z_i&amp;=\\sum_{j}w_{ij}^{(1)}x_j + b_i^{(1)}\\\\ \\vec h &amp;= \\sigma(z) &amp;h_i&amp;=\\sigma(z_i)\\\\ \\vec y &amp;= hW^{(2)} + b^{(2)} &amp;y_k &amp;= \\sum_{i}w_{ki}^{(2)}h_i + b_k^{(2)}\\\\ \\mathcal L &amp;= \\frac{\\|\\vec t - \\vec y\\|^2}{2} &amp;\\mathcal L &amp;= \\frac{1}{2}\\sum_k (y_k-t_k)^2 \\end{align*}\\] <p>Backward pass</p> \\[\\begin{align*} \\bar{\\mathcal L} &amp;= 1&amp;...\\\\ \\bar{y} &amp;= \\bar{\\mathcal L}(\\vec y -\\vec t) &amp;\\bar{y_k} &amp;= \\bar{\\mathcal L}(y_k - t_k)\\\\ \\bar{W^{(2)}} &amp;= h^T\\bar y &amp;\\bar{w_{ki}^{(2)}}&amp;=\\bar y_k h_i\\\\ \\bar{b^{(2)}} &amp;= \\bar y  &amp;\\bar{b_k^{(2)}}&amp;=\\bar y_k\\\\ \\bar h &amp;= \\bar y (W^{(2)})^T &amp;\\bar h_i&amp;= \\sum_k \\bar y_k w_{ki}^{(2)}\\\\ \\bar z &amp;= \\bar h \\cdot \\partial_z\\sigma(z) &amp;\\bar z_i&amp;=\\bar h_i d_{z_i}\\sigma(z_i)\\\\ \\bar{W^{(1)}} &amp;= x^T\\bar z&amp;...\\\\ \\bar{b^{(1)}} &amp;= \\bar z&amp;... \\end{align*}\\]"},{"location":"csc311/neural_nets.html#computational-cost","title":"Computational Cost","text":"<p>Forward: one add-multiply operation per weight Backward: two add-multiply operations per weight \\(\\bar w, \\bar h\\)</p> <p>Therefore, let the number of layers be \\(L\\), number of units for the \\(l\\)th layer be \\(m_l\\), then the computation time \\(\\in O(\\sum_{l=1}^{L-1} m_im_{l+1})\\), since each unit is fully connected with the next layer, and takes the weights as the number of units in its layer. </p>"},{"location":"csc311/neural_nets.html#overfitting-preventions","title":"Overfitting Preventions","text":""},{"location":"csc311/neural_nets.html#reduce-size-of-the-weights","title":"Reduce size of the weights","text":"<p>Adding regularizations onto each regression</p> <p>Prevents unnecessary weights Helps in improving generalization Makes a smoother model in which the conditioning is good </p>"},{"location":"csc311/neural_nets.html#early-stopping","title":"Early Stopping","text":"<p>Starts with small weights and let it grow until the performance on the validation set starts getting worse</p> <p>Because when the weights are very small, every hidden unit is in its linear range, so a net with a large layer of hidden units is linear, and it has no more capacity than a linear net in which the inputs are directly connected to the outputs. </p> <p>While when the weights grow, the hidden units starts to use their non-linear ranges so the capacity grows.</p>"},{"location":"csc311/pca.html","title":"Dimension Reduction - PCA and Auto Encoders","text":""},{"location":"csc311/pca.html#dimension-reduction","title":"Dimension Reduction","text":"<p>Loss some information (e.g. spread / \\(\\sigma\\)) by projecting higher dimensions onto lower ones. IN practice, the important features can be accurately captured in a low dimensional subspace.</p> <p>Let \\(\\mathcal D = \\{x^{(1)},...,x^{(N)}\\}\\subset \\mathbb R^D\\), so that \\(N\\) instances will form matrix </p> \\[X = \\begin{bmatrix}[\\vec x^{(1)}]^T\\\\...\\\\ [\\vec x^{(N)}]^T\\end{bmatrix}\\] <p>each row will be one observation of \\(D\\) faetures, Let \\(x^{(i)}\\sim N(\\mu, \\Sigma)\\)</p>"},{"location":"csc311/pca.html#projection-onto-a-subspace","title":"Projection onto a subspace","text":"<p>Given \\(\\mathcal D, \\hat\\mu=N^{-1}\\sum^N x^{(i)}\\) be the sample mean.  </p> <p>Want to find a $K &lt;D $ dimensional subspace \\(S\\subset \\mathcal R^D\\) s.t. \\(x^{(n)}-\\hat\\mu\\) is \"well represented\" by a projection onto \\(K\\)-dimensional \\(S\\). </p> <p>Where projection is to find the closest point \\(\\tilde x\\) on \\(S\\) s.t. \\(\\|x-\\tilde x\\|\\) is minimized. </p> <p>In a 2-dimensional problem, we are looking for direction \\(u_1\\) along with the data is well-represented, such as direction of higher variance or the direction of min difference after projection, which turns to be the same. </p>"},{"location":"csc311/pca.html#euclidean-projection","title":"Euclidean Projection","text":"<p>\\(\\text{Proj}_S(x):=\\) projection of \\(x\\) on \\(S\\). </p> <p>In 2D case, \\(S\\) is the line along the unit vector \\(u\\) (1-D subspace). \\(u\\) is a basis for \\(S\\). </p> <p>Since \\(x^Tu = \\|x\\|\\|u\\|\\cos\\theta =\\|x\\|\\cos\\theta\\)</p> \\[\\text{Proj}_S(x) = x^Tu\\cdot u = \\|\\tilde x\\|u\\] <p>In K-D case, we have \\(K\\) basis \\(u_1,...,u_K \\in \\mathcal R^D\\). and the projection will be </p> \\[\\text{Proj}_S(x) = \\sum^K (x^Tu_i) u_i = \\sum^K z_i u_i\\]"},{"location":"csc311/pca.html#center-data","title":"Center data","text":"<p>Centering by subtract the mean \\(\\hat\\mu\\). i.e. the mean (center) be the origin. We need to center the data since we don't want location of data to influence the calculations. </p>"},{"location":"csc311/pca.html#representationcode","title":"Representation/code","text":"<p>Combine the two above together, we have </p> \\[\\tilde x = \\hat\\mu + \\text{Proj}_S(x-\\hat\\mu) = \\hat\\mu + \\sum^K z_i \\vec u_i\\] <p>where \\(z_k = \\vec u_k^T(x-\\hat\\mu)\\) </p> <p>Define matrix \\(U_{D\\times K} = [\\vec u_1, ..., \\vec u_K]\\), then</p> \\[\\vec z = U^T(x-\\hat\\mu), \\tilde x = \\hat\\mu + U\\vec z = \\hat\\mu + UU^T(x-\\hat\\mu)\\] <p>We call \\(UU^T\\) the projector on \\(S\\), \\(U^TU = I\\)</p> <p>Both \\(x,\\tilde x\\in \\mathbb R^D\\) but \\(\\tilde x\\) is a linear combination of vectors in a lower dimensional subspace with representation \\(\\vec z \\in \\mathbb R^K\\). </p> <p>We call \\(\\tilde x\\) reconstruction of \\(x\\), \\(\\vec z\\) be its representation(code). </p>"},{"location":"csc311/pca.html#learning-a-subspace","title":"Learning a Subspace","text":"<p>Since we will definitely lose partial information by dimension reduction, we want a good \\(D\\times K\\) matrix \\(U\\) with orthonormal columns. </p> <p>To measure how \"good\" the subspace is, propose two criteria:</p> <p>Minimize reconstruction error: find vectors in a subspace that are closest to data points </p> \\[\\arg\\min_U \\frac 1 N \\sum^N \\|x^{(i)} - \\tilde x^{(i)}\\|^2\\] <p>Maximize variance of reconstructions find a subspace where data has the most variability</p> \\[\\arg\\max_U \\frac 1 N \\sum^N \\|\\tilde x^{(i)} - \\hat\\mu\\|^2\\] <p>Noting that </p> \\[\\begin{align*} E(\\tilde x) &amp;= E(\\hat\\mu + UU^T(x-\\hat\\mu))\\\\ &amp;= \\hat\\mu + UU^T(E(x)-\\hat\\mu)\\\\ &amp;= \\hat\\mu + UU^T0\\\\ &amp;= \\hat\\mu \\end{align*}\\] <p>So that we can still use mean of \\(x\\) to calculate variance of the reconstruciton</p>"},{"location":"csc311/pca.html#equivalence-of-the-criteria","title":"Equivalence of the criteria","text":"<p>lemma1 Norm of centered reconstruction is equal to norm of representation</p> \\[\\begin{align*} \\|\\tilde x^{(i)} - \\hat\\mu\\|^2 &amp;= (U\\vec z^{(i)})^T(U\\vec z^{(i)})\\\\ &amp;=  (\\vec z^{(i)})^T U^TU\\vec z^{(i)}\\\\ &amp;= (\\vec z^{(i)})^T\\vec z^{(i)}&amp;U^TU = I\\\\ &amp;= \\|z^{(i)}\\| \\end{align*}\\] <p>lemma2 \\(\\tilde x^{(i)}-\\hat\\mu\\) is orthogonal to \\(\\tilde x^{(i)} - x^{(i)}\\)</p> \\[\\begin{align*} (\\tilde x^{(i)}-\\hat\\mu)^T(\\tilde x^{(i)} - x^{(i)}) &amp;= (\\hat\\mu+UU^T(x^{(i)}-\\hat\\mu)-\\hat\\mu)^T(\\hat\\mu+UU^T(x^{(i)}-\\hat\\mu) - x^{(i)})\\\\ &amp;= (x^{(i)}-\\hat\\mu)^TUU^T(\\hat\\mu- x^{(i)}+UU^T(x^{(i)}-\\hat\\mu) )\\\\ &amp;= (x^{(i)}-\\hat\\mu)^TUU^T(\\hat\\mu - x^{(i)}) + (x^{(i)}-\\hat\\mu)^TUU^TUU^T(x^{(i)}-\\hat\\mu))\\\\ &amp;= (x^{(i)}-\\hat\\mu)^TUU^T(\\hat\\mu - x^{(i)}) + (x^{(i)}-\\hat\\mu)^TUU^T(x^{(i)}-\\hat\\mu))\\\\ &amp;= 0 \\end{align*}\\] <p>Proposition The two criteria is equivalent \\(\\frac 1 N \\sum^N\\|x^{(i)}- \\tilde x^{(i)}\\|^2 = C - \\frac1N\\sum^N\\|\\tilde x^{(i)}-\\hat\\mu\\|^2\\)</p> <p>By lemma2, since the two vectors are orthogonal, by Pythagorean Theorem</p> \\[\\begin{align*} \\|\\tilde x^{(i)} - \\hat\\mu\\|^2 + \\|x^{(i)} - \\tilde x^{(i)}\\|^2 &amp;= \\|x^{(i)}-\\hat\\mu\\|^2\\\\ \\frac1N\\sum^N \\|\\tilde x^{(i)} - \\hat\\mu\\|^2 + \\frac1N\\sum^N\\|x^{(i)} - \\tilde x^{(i)}\\|^2 &amp;= \\frac1N\\sum^N\\|x^{(i)}-\\hat\\mu\\|^2\\\\ \\text{projected variance} + \\text{reconstruction error} &amp;= C \\end{align*}\\]"},{"location":"csc311/pca.html#pca","title":"PCA","text":""},{"location":"csc311/pca.html#spectral-decomposition-eigendecomposition","title":"Spectral Decomposition (Eigendecomposition)","text":"<p>If \\(A_{n\\times n}\\) is a symmetric matrix (so that has a full set of eigenvectors). Then, \\(\\exists Q_{n\\times n}, \\Lambda_{n\\times n}\\) s.t. \\(A = Q\\Lambda Q^T\\) where \\(Q\\) is orthogonal matrix formed by \\(n\\) eigenvectors and \\(\\Lambda\\) is diagonal with \\(\\lambda_1,...,\\lambda_n\\). </p> <p>Using Eigendecomposition on the empirical convariance matrix \\(\\hat\\Sigma = \\frac1N \\sum^N (x^{(i)}-\\hat\\mu)(x^{(i)}-\\hat\\mu)^T\\), the optimal PCA subspace is then spanned by some \\(K\\) eigenvectors of \\(\\hat\\Sigma\\)</p> <p>These eigencectors are called principal components, analogous to the principal axes of an ellipse. </p>"},{"location":"csc311/pca.html#deriving-pca-for-k-1","title":"Deriving PCA for K = 1","text":"\\[\\begin{align*} \\frac 1N \\sum^N\\|\\tilde x^{(i)} - \\hat\\mu\\|^2 &amp;= \\frac1N\\sum^N [z^{(i)}]^2\\\\ &amp;= \\frac1N\\sum^N(u^T(x^{(i)}-\\hat\\mu))^2\\\\ &amp;=  \\frac1N\\sum^Nu^T(x^{(i)}-\\hat\\mu)(x^{(i)}-\\hat\\mu)^Tu\\\\ &amp;= u^T\\bigg[\\frac1N\\sum^N(x^{(i)}-\\hat\\mu)(x^{(i)}-\\hat\\mu)^T\\bigg]u\\\\ &amp;= u^T\\hat\\Sigma u= u^TQ\\Lambda Q^Tu=a^T\\Lambda a= \\sum^D\\lambda_j a_j^2 \\end{align*}\\] <p>For the goal of maximize \\(\\sum^D \\lambda_j a_j^2, \\vec a = Q^Tu\\), noting that \\(\\sum a_j = a^Ta = u^TQQ^Tu = u^Tu = 1\\). Therefore, choosing the largest \\(\\lambda_k\\), </p> \\[\\sum \\lambda_ja_j^2 \\leq \\sum \\lambda_k a_j^2 = \\lambda_d\\sum a_j^2 = \\lambda_k\\] <p>And such maximum can be obtained by setting \\(a_i = \\mathbb I(i = k)\\). Therefore, \\(\\vec u = Q\\vec a = q_k\\)</p>"},{"location":"csc311/pca.html#decorrelation","title":"Decorrelation","text":"\\[\\begin{align*} cov(\\vec z) &amp;= cov(U^T(x-\\mu))\\\\ &amp;= U^Tcov(x)U\\\\ &amp;= U^TQ\\Lambda Q^TU\\\\ &amp;= [I_k, 0_{n-k}]\\Lambda [I_k, 0_{n-k}]^T&amp;\\text{orthogonality}\\\\ &amp;= \\text{Top left } K\\times K\\text{ block of }\\Lambda  \\end{align*}\\]"},{"location":"csc311/pca.html#autoencoder","title":"Autoencoder","text":"<p>An autoencoder is a feed-forward neural net to take input/target pair \\((\\vec x, \\vec x)\\). In such NN, we add a bottleneck layer to reduce the dimensionality so that the weights on such layer will be our code vector. </p> <p>The whole process goes through </p> <pre><code>x =&gt; Encoder =&gt; Bottleneck(code_vector) =&gt; Decoder =&gt; x_estimated\n</code></pre> <p>By doing such, we learn abstract features in an unsupervised way, and can transfer to supervised tasks. </p>"},{"location":"csc311/pca.html#linear-autoencoders","title":"Linear autoencoders","text":"<p>If we use linear activations and squared error loss. Say we have 1 hidden layer of \\(k\\) weights, so that \\(\\tilde x = W_2W_1x, W_2:D\\times K, W_1: K\\times D\\), then \\(W_2\\) is the PCA subspace</p>"},{"location":"csc311/pca.html#nonlinear-autoencoder","title":"Nonlinear Autoencoder","text":"<p>If we use non-linear activations, then they can be more powerful for a given dimensionality, comparing to PCA (but also much more computational heavy in finding an optimal subspace).</p>"},{"location":"csc311/probabilistic_models.html","title":"Probabilistic Models","text":""},{"location":"csc311/probabilistic_models.html#likelihood-function","title":"Likelihood Function","text":"<p>The density of the observed data, as a function of parameters \\(\\theta\\). </p>"},{"location":"csc311/probabilistic_models.html#approaches-to-classification","title":"Approaches to classification","text":"<p>Discriminative approach estimate parameters of decision boundary / class separator directly from labeled examples</p> <ul> <li>How do I separate the classes</li> <li>learn \\(p(t|x)\\) directly (logistic regression models)</li> <li>learn mapping from inputs to classes</li> </ul> <p>Generative approach model the distribution of inputs characteristic of the class (Bayes classifier)</p> <ul> <li>What does each class \"look\" like?</li> <li>Build a model of \\(p(x|t)\\)</li> <li>Apply Bayes rule</li> </ul>"},{"location":"csc311/probabilistic_models.html#bayes-classifier","title":"Bayes Classifier","text":"<p>Given features \\(x = [x_1,...,x_D]^T\\), we want to compute class probabilities using Bayes Rule:</p> \\[p(c|x) = \\frac{p(x,c)}{p(x)} = \\frac{p(x|c)p(c)}{p(x)}\\] <p>or by text </p> \\[\\text{posterior} = \\frac{\\text{class likelihood} \\times {\\text{prior}}}{\\text{Evidence}}\\]"},{"location":"csc311/probabilistic_models.html#bayes-nets","title":"Bayes Nets","text":"<p>We can represent this model using an directed graphical model, or Bayesian network. </p> <p>This graph structure means the joint distribution factorizes as a product of conditional distribution for each variable given its parent(s). </p> <p>Intuitively, you can think of the edges as reflecting a causal structure. But mathematically, this doesn't hold without additional assumptions. </p> <p>The parameters can be learned efficiently because the log-likelihood decomposes into independent terms for each feature. </p> \\[\\begin{align*} \\mathcal l(\\theta) &amp;= \\sum_{i=1}^N \\log p(c^{(i)}, x^{(i)})\\\\ &amp;= \\sum_{i=1}^N \\log\\{p(x^{(i)}| c^{(i)})p(c^{(i)})\\}\\\\ &amp;= \\sum_{i=1}^N \\log\\{p(c^{(i)}) \\prod_{j=1}^D p(x_j^{(i)}| c^{(i)})\\}\\\\ &amp;= \\sum_{i=1}^N \\bigg[\\log p(c^{(i)}) + \\sum_{j=1}^D \\log p(x_j^{(i)}|c^{(i)})\\bigg]\\\\ &amp;= \\underset{\\text{Bernoulli log-likelihood of labels}}{\\sum_{i=1}^N \\log p(c^{(i)})} + \\underset{\\text{Bernoulli log-likelihood for feature }x_j}{\\sum_j^D\\sum_i^N \\log p(x_j^{(i)}|c^{(i)})} \\end{align*}\\] <p>Each of these log-likelihood terms depends on different set of parameters, so they can be optimized independently. </p>"},{"location":"csc311/probabilistic_models.html#bayes-inference","title":"Bayes Inference","text":"\\[p(c|x)\\propto p(c)\\prod_j^D p(x_j|c)\\] <p>For input \\(x\\), predict by comparing the values of \\(p(c)\\prod_j^D p(x_j|c)\\) for different \\(c\\). </p>"},{"location":"csc311/probabilistic_models.html#bayesian-parameter-estimation","title":"Bayesian Parameter Estimation","text":"<p>Bayesian approach treats the parameters as random variables. \\(\\beta\\) is the set of parameters in the prior distribution of \\(\\theta\\)</p> <p>To define a Bayesian model, we need to specify two distributions: prior distribution\\(p(\\theta)\\), which encodes our beliefs about the parameters before we observe the data. likelihood, same as in MLE</p> <p>When we update our beliefs based on the observations, we compute the posterior distribution using Bayes' rule. </p> \\[p(\\theta|\\mathcal D) = \\frac{p(\\theta)p(\\mathcal D|\\theta)}{\\int p(\\theta')p(\\mathcal D|\\theta')d\\theta'}\\]"},{"location":"csc311/probabilistic_models.html#maximum-a-posteriori-estimation","title":"Maximum A-Posteriori Estimation","text":"<p>Find the most likely parameter settings under the posterior</p> \\[\\begin{align*} \\hat{\\boldsymbol{\\theta}}_{\\mathrm{MAP}}&amp;=\\arg \\max _{\\boldsymbol{\\theta}} p(\\boldsymbol{\\theta} | \\mathcal{D})\\\\ &amp;=\\arg \\max _{\\boldsymbol{\\theta}} p(\\boldsymbol{\\theta}) p(\\mathcal{D} | \\boldsymbol{\\theta}) \\\\ &amp;=\\arg \\max _{\\boldsymbol{\\theta}} \\log p(\\boldsymbol{\\theta})+\\log p(\\mathcal{D} | \\boldsymbol{\\theta}) \\end{align*}\\]"},{"location":"csc311/probabilistic_models.html#gaussian-discriminant-analysis-gaussian-bayes-classifier","title":"Gaussian Discriminant Analysis (Gaussian Bayes Classifier)","text":"<p>Make decisions by comparing class posteriors. </p> \\[\\log p\\left(t_{k} | \\mathbf{x}\\right)=\\log p\\left(\\mathbf{x} | t_{k}\\right)+\\log p\\left(t_{k}\\right)-\\log p(\\mathbf{x})\\] <p>Expanded as </p> \\[\\begin{align*}\\log p\\left(t_{k} | \\mathbf{x}\\right) =  &amp;-\\frac{d}{2} \\log (2 \\pi) -\\frac{1}{2} \\log \\left|\\boldsymbol{\\Sigma}_{k}^{-1}\\right| \\\\ &amp;-\\frac{1}{2}\\left(\\mathbf{x}-\\boldsymbol{\\mu}_{k}\\right)^{T} \\boldsymbol{\\Sigma}_{k}^{-1}\\left(\\mathbf{x}-\\boldsymbol{\\mu}_{k}\\right) \\\\ &amp;+\\log p\\left(t_{k}\\right)-\\log p(\\mathbf{x}) \\end{align*}\\] <p>Decision Boundary </p> \\[\\begin{align*}&amp;\\log p\\left(t_{k} | \\mathbf{x}\\right)=\\log p\\left(t_{l} | \\mathbf{x}\\right) \\\\\\Rightarrow &amp;\\left(\\mathbf{x}-\\boldsymbol{\\mu}_{k}\\right)^{T} \\boldsymbol{\\Sigma}_{k}^{-1}\\left(\\mathbf{x}-\\boldsymbol{\\mu}_{k}\\right)=\\left(\\mathbf{x}-\\boldsymbol{\\mu}_{\\ell}\\right)^{T} \\boldsymbol{\\Sigma}_{\\ell}^{-1}\\left(\\mathbf{x}-\\boldsymbol{\\mu}_{\\ell}\\right)+C_{k, l} \\\\\\Rightarrow&amp;\\mathbf{x}^{T} \\boldsymbol{\\Sigma}_{k}^{-1} \\mathbf{x}-2 \\boldsymbol{\\mu}_{k}^{T} \\mathbf{\\Sigma}_{k}^{-1} \\mathbf{x}=\\mathbf{x}^{T} \\mathbf{\\Sigma}_{\\ell}^{-1} \\mathbf{x}-2 \\boldsymbol{\\mu}_{\\ell}^{T} \\mathbf{\\Sigma}_{\\ell}^{-1} \\mathbf{x}+C_{k, l} \\end{align*}\\] <p>Decision Boundary is quadratic since gaussian is quadratic. When we have to humps that share the same covariance, the decision boundary is linear. </p>"},{"location":"csc311/probabilistic_models.html#properties-of-gaussian-distribution","title":"Properties of Gaussian Distribution","text":"<p>\\(\\mathbf{x} \\sim \\mathcal{N}(\\boldsymbol{\\mu}, \\mathbf{\\Sigma})\\) is defined as </p> \\[p(\\mathbf{x})=\\frac{1}{(2 \\pi)^{d / 2}|\\mathbf{\\Sigma}|^{1 / 2}} \\exp \\left[-\\frac{1}{2}(\\mathbf{x}-\\boldsymbol{\\mu})^{T} \\mathbf{\\Sigma}^{-1}(\\mathbf{x}-\\boldsymbol{\\mu})\\right]\\] <p>Empirical Mean \\(\\hat{\\boldsymbol{\\mu}}=\\frac{1}{N} \\sum_{i=1}^{N} \\mathbf{x}^{(i)}\\) Empirical Covariance \\(\\hat{\\mathbf{\\Sigma}}=\\frac{1}{N} \\sum_{i=1}^{N}\\left(\\mathbf{x}^{(i)}-\\hat{\\boldsymbol{\\mu}}\\right)\\left(\\mathbf{x}^{(i)}-\\hat{\\boldsymbol{\\mu}}\\right)^{\\top}\\)</p>"},{"location":"csc311/probabilistic_models.html#gda-vs-logistic-regression","title":"GDA vs.  Logistic Regression","text":"<ul> <li>GDA is generative while LR is discriminative model. </li> <li>GDA makes stringer modelling assumptions: assumes gaussian distributon. When assumption true, GDA asymptotically efficient. - - LR more robust, less sensitive to incorrect modelling assumptions (LR uses CE, no assumption.) </li> <li>Class-conditional distributions usually lead to logistic classifier. </li> </ul>"},{"location":"csc311/reinforce_learning.html","title":"Reinforcement Learning","text":""},{"location":"csc311/reinforce_learning.html#reinforcement-learning-problem","title":"Reinforcement Learning Problem","text":"<p>An agent continually interacts with the environment. How should it choose its actions so that its long-term rewards are maximized. </p> <p>Reinforcement Learning is challenging in </p> <ul> <li>continuous stream of input information, and we have to choose actions</li> <li>effects of an action depend on the state of the agent in the world </li> <li>obtain reward that depends on the state and actions </li> <li>You know only the reward for your action, not other possible actions. </li> <li>Could be a delay between action and reward. </li> </ul>"},{"location":"csc311/reinforce_learning.html#markov-decision-process-mdp","title":"Markov Decision Process (MDP)","text":"<p>MDP is the mathematical framework to describe RL problems.</p> <p>A discounted MDP is defined by a tuple \\((S, A, P, R, \\gamma)\\) where </p> <ul> <li>\\(S\\) state space, discrete or continuous (most cases discrete)</li> <li>\\(A\\) action space. we only consider finite action apace, i.e. \\(A = \\{a_1,...,a_M\\}\\)</li> <li>\\(P\\) transition probability</li> <li>\\(R\\) immediate reward distribution</li> <li>\\(\\gamma\\) discount factor \\(0\\leq \\gamma \\leq 1\\)</li> </ul> <p>The agent has a state \\(s\\in S\\) in the environment. </p> <p>At every time step \\(t = 0, 1,...\\) the agent is at state \\(S_t\\). </p> <ul> <li>Takes an action \\(A_t\\)</li> <li>Moves into a new state \\(S_{t+1}\\), according to the dynamics of the environment and the selected action, i.e., \\(S_{t+1}\\sim P(\\cdot|S_t, A_t)\\)</li> <li>Receives some reward \\(R_t \\sim R(\\cdot|S_t, A_t, S_{t+1})\\)</li> <li>Alternatively, \\(R_t \\sim R(\\cdot|S_t, A_t)\\) or \\(R_t\\sim R(\\cdot |S_t)\\)</li> </ul> <p>The action selection mechanism is described by a policy \\(\\pi\\), which is a mapping from states to actions, i.e., \\(A_t = \\pi(S_t)\\) (deterministic policy) or \\(A_t\\sim \\pi(\\cdot|S_t)\\) (stochastic policy). </p> <p>The goal is to find a policy \\(\\pi\\) s.t. long term rewards of the agent is maximized. </p> <p>Different notions of long-term reward: </p> <ul> <li>Cumulative/total reward: \\(R_0 + R_1 + R_2 + ...\\)</li> <li>Discounted (cumulative) reward: \\(R_0 + \\gamma R_1 + \\gamma^2 R_2 + ...\\) <ul> <li>The discount factor \\(0\\leq \\gamma \\leq 1\\) determines how myopic the agent is. </li> <li>When $\\gamma &gt;&gt; 0\\Rightarrow $ more myopic, $\\gamma &gt;&gt; 1\\Rightarrow $ less myopic. </li> </ul> </li> </ul>"},{"location":"csc311/reinforce_learning.html#transition-probability-or-dynamics","title":"Transition Probability (or Dynamics)","text":"<p>The transition probability describes the changes in the state of the agent when it chooses actions </p> \\[P(S_{t+1} = s'|S_t = s, A_t = a)\\] <p>This model has Markov property; the future depends on the past only through the current state. </p> <p>A policy is the action-selection mechanism of the agent, and describes its behavior. Policy can be deterministic or stochastic. </p>"},{"location":"csc311/reinforce_learning.html#value-function-based-reinforcement-learning","title":"Value Function based Reinforcement Learning","text":"<p>The expected future reward, and is used to evaluate the desirability of states. State-value function \\(V^\\pi\\) for policy \\(pi\\) is a function defined as </p> \\[V^\\pi(s):= E_\\pi \\bigg[\\sum_{t\\geq 0}\\gamma^t R_t\\mid S_0 = s], R_t\\sim R(\\cdot |S_t, A_t, S_{t+1})\\bigg]\\] <p>describes the expected discounted reward if the agent starts from state \\(s\\) following policy \\(\\pi\\). </p> <p>Action-value function \\(Q^\\pi\\) for policy \\(\\pi\\) is </p> \\[Q^\\pi(s,a) := E_\\pi\\bigg[\\sum_{t\\geq 0}\\gamma^t R_t\\mid S_0 = s, A_0 = a\\bigg]\\] <p>describes the expected discounted reward if the agent starts from state \\(s\\), takes action \\(a\\), and afterwards follows policy \\(\\pi\\). </p> <p>The goal is to find a policy \\(\\pi\\) that maximizes the value function, i.e. the optimal value function </p> \\[Q^*(s,a)=\\sup_\\pi Q^\\pi(s,a)\\] <p>Given \\(Q^*\\), the optimal policy can be obtained as </p> \\[\\pi^*(s) = \\arg\\max_a Q^* (s,a)\\] <p>The goal of an RL agent is to find a policy \\(\\pi\\) that is close to optimal, \\(Q^\\pi \\approx Q^*\\)</p>"},{"location":"csc311/svm.html","title":"Support Vector Machines and Boosting","text":""},{"location":"csc311/svm.html#support-vector-machines","title":"Support Vector Machines","text":""},{"location":"csc311/svm.html#general-setting","title":"General Setting","text":"<p>If we use \\(t\\in\\{-1, 1\\}\\) instead of \\(\\{0,1\\}\\), i.e. the model is </p> \\[z=\\vec w^T\\vec x + b, y = sign(z)\\]"},{"location":"csc311/svm.html#hinge-loss","title":"Hinge Loss","text":"\\[\\mathcal L_H(z,t) =\\max\\{0,1-zt\\}\\] <p>If we use a linear classifier and write \\(z^{(i)}(w,b)=\\vec w^T\\vec x + b\\), then we want to minimize the training loss </p> \\[\\min_{\\vec w, b} \\sum_{i=1}^NL_H(z^{(i)}, t^{(i)}) = \\min_{\\vec w, b} \\sum_{i=1}^N \\max\\{0, 1 - t^(i)z^{(i)}(\\vec w, b)\\}\\] <p>This formulation is called support vector machines.</p> <p>Generally used with \\(L_2\\) regularization</p> \\[\\min_{\\vec w, b} \\sum_{i=1}^N \\max\\{0, 1 - t^(i)z^{(i)}(\\vec w, b)\\} + \\frac{\\lambda\\|\\vec w\\|_2^2}{2}\\]"},{"location":"csc311/svm.html#optimal-separating-hyperplane","title":"Optimal Separating Hyperplane","text":"<p>Suppose we want to find a linear classifier that separates a set of data points. In \\(\\mathbb R^2\\), this will looks like a line \\(y=wx+b\\); in \\(\\mathbb R^{D&gt;2}\\), that will be a hyperplane. </p> <p>Note that there are multiple separating hyperplanes, i.e. different \\(w,b\\)'s</p> <p>A hyperplane that separates two classes and maximizes the distance to the closest point from either class, i.e. maximize the margin of the classifier. </p>"},{"location":"csc311/svm.html#boosting","title":"Boosting","text":"<p>Train classifiers sequentially, each time focusing on training data points that were previously misclassified.</p>"},{"location":"csc311/svm.html#key-idea","title":"Key Idea","text":"<p>learn a classifier using different costs (by applying different weights). Intuitively, \"tries harder\" on examples with higher cost. </p> <p>Let misclassification rate be and change cost function</p> \\[\\frac{1}{N}\\sum_{n=1}^N \\mathbb I[h(x^{(n)})\\neq t^{(n)}] \\Rightarrow \\frac{1}{N}\\sum_{n=1}^N w^{(n)}\\mathbb I[h(x^{(n)})\\neq t^{(n)}]\\] <p>\\(w^{(n)}\\) is normalized so that the overall weights does not change the overall loss. </p> \\[\\forall n. w^{(n)} &gt; 0, \\sum_{n=1}^N w^(n)=1\\]"},{"location":"csc311/svm.html#weak-learner-classifier","title":"Weak Learner / Classifier","text":"<p>Weak learner is a learning algorithm that outputs a hypothesis that performs slightly better than chance i.e. error rate \\(&gt;0.5\\)</p> <p>We want such classifiers for they are computationally efficient. </p> <p>For example, Decision Stump: A decision tree with a single split. </p> <p>One single weak classifier is not capable of making the training error small. so that </p> \\[err = \\sum_{n=1}^N w^{(n)}\\mathbb I[h(x^{(n)})\\neq t^{(n)}] \\leq \\frac{1}{2} -\\gamma, \\gamma &gt; 0 \\text{ is small}\\] <p>Then, we want to combine many weak classifiers to get a better ensemble of classifiers. </p>"},{"location":"csc311/svm.html#adaptive-boosting-adaboost","title":"Adaptive Boosting (AdaBoost)","text":""},{"location":"csc311/svm.html#key-steps","title":"Key steps","text":"<ul> <li>At each iteration, re-weight the training samples by assigning larger weights to samples that were classified incorrectly. </li> <li>We train a new weak classifier based on the re-weighted samples</li> <li>We add this weak classifier to the ensemble of weak classifiers. This ensemble is the new classifier</li> <li>Repeat the process</li> </ul>"},{"location":"csc311/svm.html#boosting-vs-bootstrap","title":"Boosting vs. Bootstrap","text":"<p>Boosting reduces bias by making each classifier focus on previous mistakes. Bootstrap reduces variance, while keeping bias the same </p>"},{"location":"csc311/svm.html#algorithm","title":"Algorithm","text":"<p>data \\(D_N = \\{x^{(n)}, t^{(n)}\\}_{n=1}^N, t^{(n)}\\in\\{-1,1\\}\\) A classifier or hypothesis \\(h: \\vec x \\rightarrow \\{-1, 1\\}\\) 0 - 1 loss: \\(\\mathbb I(h(x^{(n)}\\neq t^{(n)})) = \\frac{(1-h(x^{(n)})t^{(n)})}{2}\\) WeakLearn\\(:D_N\\times \\vec w\\rightarrow h\\) be a function that returns a classifier</p> <p>AdaBoost(\\(D_N\\), WeakLearn):  </p> <ul> <li>init \\(w^{(n)} = N^{-1}, \\forall n\\) </li> <li>For \\(t = 1, ..., T\\) <ul> <li>\\(h_t\\leftarrow \\text{WeakLearn}(D_N, \\vec w)\\)</li> <li>compute weighted error \\(err_t = \\frac{\\sum_1^N w^{(n)\\mathbb I(h_t(x^{(n)})\\neq t^{(n)})}}{\\sum_{n=1}^Nw^{(n)}}\\)</li> <li>compute classifier coefficient \\(\\alpha_t = \\frac{1}{2}\\log(\\frac{1-err_t}{err_t}), a_t \\in (0, \\infty)\\)</li> <li>update data weights \\(w^{(n)}\\leftarrow w^{(n)}\\exp(-\\alpha_tt^{(n)}h_t(\\vec x^{(n)}))\\)</li> </ul> </li> <li>return \\(H(x) = sign(\\sum_{t=1}^T \\alpha_t h_t(\\vec x))\\)</li> </ul>"},{"location":"csc311/svm.html#intuition","title":"Intuition","text":"<p>Weak classifiers which get lower weighted error get more weight in the final classifier. Consider the shape of \\(\\alpha_t\\)  - If \\(err\\approx 0, a_t\\) high so misclassified examples get more attention  - If \\(err\\approx 0.5, a_t\\) low so misclassified examples are not emphasized </p> <p></p>"},{"location":"csc311/svm.html#geometric-converge","title":"Geometric converge","text":"<p>Assume that at each iteration of AdaBoost the WeakLearn returns a hypothesis with error \\(err\\leq \\frac{1}{2} - \\gamma\\) for all \\(t = 1, ..., T\\) with \\(\\gamma &gt; 0\\). The training error of the output hypothesis  \\(H(\\vec x) = sign(\\sum^T a_th_x(\\vec x))\\) is at most </p> \\[L_N(H) = \\frac{1}{N} \\sum^N \\mathbb I\\{H(\\vec x^{(i)}\\neq t^{(i)})\\}\\leq \\exp(-2\\gamma^2T)\\] <p>under the assumption that weak learner is \\(\\gamma\\)-better than a random predictor. </p>"},{"location":"csc311/svm.html#additive-model","title":"Additive Model","text":"<p>With a more complex \\(H\\) by adding more weak classifiers, we expect more complex classifiers overfit. However, this is not often true, sometimes the test error decreases even after the training error is zero.</p>"},{"location":"csc311/svm.html#general-setting_1","title":"General Setting","text":"<p>Consider a hypothesis class \\(\\mathcal H\\) with each \\(h_i\\in\\mathcal H:\\vec x\\rightarrow \\{-1,1\\}\\) being \"weak learners\", or bases. </p> <p>Define an additive model with \\(m\\) terms to be </p> \\[H_m(x) = \\sum_{i=1}^m \\alpha_ih_i(\\vec x)\\] <p>where \\((\\alpha_1, ..., \\alpha_m) \\in \\mathbb R^m\\) and generally \\(a_i\\in\\mathbb R^+, \\sum a_i = 1\\). So that \\(H_m\\) is a linear combination of bases classifiers, like in boosting. </p>"},{"location":"csc311/svm.html#stage-wise-training","title":"Stage-wise Training","text":"<p>Consider a greedy approach to fitting additive models, known as stagewise training</p> <ul> <li>init \\(H_0(x)= 0\\)</li> <li> <p>For \\(m=1,..., T\\)</p> <ul> <li>compute the \\(m\\)-th hypothesis \\(H_m = H_{m-1} + a_mh_m\\) so that the loss is minimized, i.e.</li> </ul> \\[h_m,\\alpha_m \\rightarrow arg\\min_{h\\in\\mathcal H, \\alpha\\in\\mathbb R}\\sum_{i=1}^N \\mathcal L(H_{m-1}(x^{(i)}) + \\alpha h(x^{(i)}, t^{(i)}))\\] <ul> <li>\\(H_m = H_{m-1} + \\alpha_mh_m\\)</li> </ul> </li> </ul> <p>Then consider the exponential loss, as a smooth simulation to Hinge loss. </p> <p></p> \\[\\begin{align*} (h_m,a_m)\\leftarrow arg\\min &amp;\\sum_{i=1}^N \\exp(-\\bigg[H_{m-1}(x^{(i)} + ah(x^{(i)}))\\bigg]t^{(i)})\\\\ = &amp;\\sum_{i=1}^N \\exp(-H_{m-1}(x^{(i)}t^{(i)}))\\exp(-ah(x^{(i)}t^{(i)}))\\\\ = &amp;\\sum_{i=1}^N w_i^{(m)}\\exp(-ah(x^{(i)}t^{(i)})) \\end{align*}\\] <p>where \\(w_i^{(m)}:= \\exp(-H_{m-1}(x^{(i)})t^{(i)})\\)</p> <p>Then, notice that \\(h(x)\\in \\{-1, 1\\}\\) is the classifier so that</p> \\[h(x^{(i)}) = t^{(i)}, \\exp(-ah(x^{(i)})t^{(i)}) = \\exp(-a)\\] \\[h(x^{(i)}) \\neq t^{(i)}, \\exp(-ah(x^{(i)})t^{(i)}) = \\exp(a)\\] <p>Then, our additive model is equivalent to </p> \\[\\begin{align*} \\sum_{i=1}^N w_i^{(m)} \\exp(-ah(x^{(i)})t^{(i)}) &amp;= e^{-a} \\sum^N w_i^{(m)}\\mathbb I\\{h(x^{(i)})=t^{(i)}\\} \\\\ &amp;+e^a \\sum_{i=1}^N w_i^{(m)}\\mathbb I(h(x^{(i)})\\neq t^{(i)})\\\\ &amp;= \\text{correct predictions + incorrect predictions} \\end{align*}\\] <p>Thne, define \\(I_{t}=\\mathbb I(h(x^{(i)}) = t^{(i)}), I_{f}=\\mathbb I(h(x^{(i)}) \\neq t^{(i)})\\), and consider of summation above, we can further group them by </p> \\[\\begin{align*} &amp;e^{-a}\\sum^N w_i^{(m)} I_t + e^a\\sum^N w_i^{(m)}I_f - e^{-a}\\sum^N w_i^{(m)}I_f + e^{-a}\\sum^N w_i^{(m)}I_f\\\\ &amp;= (e^a - e^{-a})\\sum_{i=1}^N w_i^{(m)}I_f + e^{-a}\\sum_{i=1}^N w_i^{(m)}(I_f+I_t)\\\\ &amp;= (e^a - e^{-a})\\sum_{i=1}^N w_i^{(m)}I_f + e^{-a}\\sum_{i=1}^N w_i^{(m)} \\end{align*}\\] <p>To optimize \\(h\\), is equivalent to minimize </p> \\[ (e^a - e^{-a})\\sum_{i=1}^N w_i^{(m)}\\mathbb I(h(x^{(i)})\\neq t^{(i)})\\] <p>To optimize \\(\\alpha\\), define weighted classification error: </p> \\[err_m = \\frac{\\sum^N w_i^{(m)}\\mathbb I(h_m(x^{(i)})\\neq t^{(i)})}{\\sum w_i^{(m)}}\\] <p>and with of minimization on \\(h\\) </p> \\[\\begin{align*} &amp;\\min_a\\min_{h\\in\\mathcal H} \\sum_{i=1}^N w_i^{(m)}\\exp(-ah(x^{(i)})t^{(i)})\\\\\\ =&amp;\\min_a\\bigg\\{(e^a-e^{-a})\\sum^N w_i^{(m)}I_f + e^{-a}\\sum^N w_i^{(m)}\\bigg\\}\\\\ \\mathcal L=&amp;\\min_a\\bigg\\{(e^a-e^{-a})err_m\\sum^N w_i^{(m)} + e^{-a}\\sum^N w_i^{(m)}\\bigg\\} \\end{align*}\\] <p>Then consider the derivative, where \\(W = \\sum^N w_i^{(m)}\\), </p> \\[\\frac{d\\mathcal L}{da} = (e^a + e^{-a})err_m W - e^{-a} W\\] <p>Set to 0</p> \\[\\begin{align*} (e^a + e^{-a})err_m &amp;= e^{-a}\\\\ (e^{2a} + 1)err_m &amp;= 1\\\\ e^{2a} &amp;= \\frac{1-err_m}{err_m}\\\\ a &amp;= \\frac{1}{2}\\log(\\frac{1-err_m}{err_m}) \\end{align*}\\] <p>And for each iteration, </p> \\[\\begin{align*} w_i^{(m+1)} &amp;= \\exp(-H_m(x^{(i)})t^{(i)})\\\\ &amp;= \\exp(-\\big[H_{m-1}(x^{(i)} + a_m h_m(x^{(i)}))\\big]t^{(i)})\\\\ &amp;= \\exp(-H_{m-1}(x^{(i)})t^{(i)})\\exp(-a_mh_m(x^{(i)}t^{(i)}))\\\\ &amp;= w_i^{(m)}\\exp(-a_mh_m(x^{(i)}t^{(i)})) \\end{align*}\\] Source code <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\n\nz = np.arange(-3, 3, 0.01)\nplt.figure(figsize=(6, 4))\nplt.plot(z, np.maximum(0, 1 - z), label=\"y=1\")\nplt.plot(z, np.maximum(0, 1 + z), label=\"y=-1\")\nplt.title(\"Hinge Loss\")\nplt.xlabel(\"z\")\nplt.ylabel(\"Loss\")\nplt.legend()\nplt.tight_layout()\nplt.savefig(\"../assets/svm_hinge_loss.jpg\")\n\nerr = np.arange(0.001, 0.999, 0.001)\na = np.log((1 - err) / err) / 2\nplt.figure(figsize=(6, 4))\nplt.plot(err, a)\nplt.xlabel(\"err, weighted error\")\nplt.ylabel(\"classifier coefficient\")\nplt.tight_layout()\nplt.savefig(\"../assets/svm_error.jpg\")\n\n\nz = np.arange(-2, 3, 0.01)\nplt.figure(figsize=(6, 4))\nplt.plot(z, np.maximum(0, 1-z), label=\"Hinge\")\nplt.plot(z, np.exp(-z), label=\"exponential\")\nplt.title(\"Hinge Loss vs. Exponential Loss\")\nplt.xlabel(\"z\")\nplt.ylabel(\"Loss\")\nplt.ylim(0, 3)\nplt.tight_layout()\nplt.savefig(\"../assets/svm_vs.jpg\")\n</code></pre>"},{"location":"csc320/blending.html","title":"Image Pyramids and Blending","text":""},{"location":"csc320/blending.html#smoothing-filter","title":"Smoothing Filter","text":"<p>Assume the smoothing weights \\(\\hat w\\) is \\(1\\times 5\\). To make such weights a proper smoothing filters. \\(\\hat w\\) is symmetric, sum to \\(1\\), and have equal contribution </p> \\[\\hat w = [c, b, a, b, c], b = 1/4, c = \\frac{1-2a}{4}\\] \\[a+2b+2c = 1\\] \\[a+2c = 2b=1/2\\] <p>then the convolution filter is \\(W = \\hat w\\hat w^T\\) is a \\(5\\times 5\\) filter. </p>"},{"location":"csc320/blending.html#reduce-function","title":"Reduce Function","text":"<p>Define \\(Reduce: g_l\\Rightarrow g_{l+1}\\) where </p> \\[g_{l+1}(i,j) = \\sum_{-2}^2 \\sum_{-2}^2 W(m,n)g_i(2i-m,2j-n)\\] \\[D_l = \\begin{bmatrix} 1&amp;0&amp;0&amp;...&amp;0\\\\ 0&amp;0&amp;1&amp;...&amp;0\\\\ \\vdots&amp;\\vdots&amp;\\vdots&amp;\\ddots&amp;\\vdots\\\\ 0&amp;0&amp;0&amp;...&amp;1 \\end{bmatrix}, C_l = \\begin{bmatrix} c&amp;b&amp;a&amp;b&amp;c&amp;0&amp;...&amp;0\\\\ 0&amp;c&amp;b&amp;a&amp;b&amp;c&amp;...&amp;0\\\\ \\vdots&amp;\\ddots&amp;\\ddots&amp;\\ddots&amp;\\ddots&amp;\\ddots&amp;\\ddots&amp;\\vdots\\\\ 0&amp;0&amp;0&amp;0&amp;0&amp;0&amp;...&amp;c \\end{bmatrix}\\] <p>so that \\(g_{l+1} = D_L \\cdot C_l\\cdot g_l\\) where \\(g\\) is vectorized image. </p>"},{"location":"csc320/blending.html#laplacian-pyramid","title":"Laplacian Pyramid","text":"<p>Let \\(L_i = g_i - expand(g_{i+1})\\) be the difference between levels \\(g_l, g_{l+1}\\), since \\(g_{l+1}\\) has different size. We need to expand.</p>"},{"location":"csc320/blending.html#expand-function","title":"Expand Function","text":"\\[expand(g_l) = 4\\sum^2_{-2}\\sum^2_{-2} W(m,n)g_l(\\frac{i-m}{2}, \\frac{j-n}{2})\\]"},{"location":"csc320/blending.html#blending","title":"Blending","text":"<p>Given source images \\(A,B\\) and binary matte \\(M\\), compute Laplacian Pyramids for \\(A, B\\) i.e. \\(AL_0, ..., AL_{N-1}, Ag_N, BL_0, ..., BL_{N-1}, Bg_N\\), Gaussian pyramid for \\(M\\), i.e. \\(Mg_0, ..., Mg_N\\). Then, using matting equation, using \\(M\\) as alpha channel. </p> \\[SL_i = Mg_l AL_l + (1-Mg_l)BL_l\\] <p>Finally, reconstruct Laplacian pyramid \\(SL\\), which is desired. </p>"},{"location":"csc320/camera.html","title":"Camera","text":""},{"location":"csc320/camera.html#focus","title":"Focus","text":"<p>The focal point \\(f = \\lim_{d\\rightarrow\\infty} u\\) where \\(f\\) is the focal length \\(d\\) is the distance of the object from the lens \\(u\\) is the distance of the image plane</p> <p>Thin lens law \\(f^{-1} = u^{-1} + d^{-1}\\)</p>"},{"location":"csc320/camera.html#depth-of-field","title":"Depth of Field","text":"<p>Range of distances where blur &lt; 1 sensor pixel. </p> <p>\\(DOF \\approx \\frac{2u^2 NC}{f^2}\\) where \\(C=\\) given circle of confusion (pixel size) \\(f=\\) focal length \\(N=\\) f-stop \\(u=\\) subject distance </p> <p>Typically, callphone camera has wider-angle lens (short focal length), hence larger DoF. Therefore, it can capture all objects in different distance, and then fake the blur by algorithm</p>"},{"location":"csc320/camera.html#camera-controls","title":"Camera controls","text":"<p>Aperture expressed as \\(D:=f/N\\),the relative size of the area in which light is collected through the lens</p> <p>Shutter speed \\(\\Delta t\\), the duration of the exposure, often expressed as fractions of a second</p>"},{"location":"csc320/camera.html#dof-vs-aperture-and-shutter-speed","title":"DoF vs. Aperture and Shutter Speed","text":"<p>The capture photons \\(\\propto D^2 \\Delta t\\) The number of photons captured will influence the exposure. </p> <p>Also, consider DoF,  \\(D = f/N\\uparrow \\Rightarrow DOF\\downarrow\\) </p>"},{"location":"csc320/camera.html#iso-film-speed","title":"ISO film speed","text":"<p>The sensitivity of film/sensor to light</p> <p>Given exposure (\\(D^2\\Delta t\\)) being constant, \\(ISO\\uparrow\\Rightarrow brightness\\uparrow\\)</p>"},{"location":"csc320/camera.html#color-image","title":"Color image","text":"<p>All sensor pixels have same response curve, i.e. monochromatic and can only accepts intensity. To obtain a colored image, typically each pixel will be made to be sensitive or one of RGB by filters, typically 25% R, 25% B, 50% G. And full-color images can be obtained by demosaicing each pixel with missing RGB. </p>"},{"location":"csc320/camera.html#steps-of-image-formation","title":"Steps of  image formation","text":""},{"location":"csc320/camera.html#photons-to-digital-numbers-linear-operations","title":"Photons to Digital Numbers (Linear operations)","text":"<p>radiant power from scene Arriving photons causes photo-electrons and the charge accumulates are more photons hits the photo-diode.</p> <p>The radiant power </p> \\[\\Phi = \\int_{q} \\int_\\lambda H(\\bar{q}, \\lambda) S(\\bar{q}) Q(\\lambda)d\\bar{q} d\\lambda\\] <p>where \\(\\bar q =\\) pixel footprint \\(\\lambda =\\) wave length    \\(H=\\) incidcent spectral irrandiance (the flux that can be received per/at that surface per/at that wavelength)   \\(S=\\) spatial response (the sensitivity of the sensor to radiation from different directions) \\(Q=\\) quantum device efficiency (electrons that can be collected per incident photon at given wavelength)</p> <p>Exposure After exposure time, amplifier converts charge to measurable voltage  </p> <ul> <li>With the exposure time \\(\\Delta t\\), now the total illuminance/irradiance is \\(\\Phi\\Delta t\\)</li> <li>At same time, black level, a non-photoelextric current from photo diode, \\(I_0\\) is added, to make the least black level. The result is \\(\\Phi \\Delta t + I_0\\) </li> </ul> <p>Sensor saturation The current cannot exceed saturation current a set-maximum non-discarded current from photodiode     - $I_m = $ saturation current, adjust the voltage by \\(\\min(\\Phi\\Delta t + I_0, I_m)\\) </p> <p>Gain factor Apply an amplifier gain \\(g\\), which is controlled by ISO (the larger \\(g\\) the darker the darker)     - the result will be \\(\\frac{\\min(\\Phi\\Delta t + I_0, I_m)}{g}\\)     - Since digital numbers is discrete, apply a flooring function \\(\\lfloor\\frac{\\min(\\Phi\\Delta t + I_0, I_m)}{g}\\rfloor\\) </p> <p>Therefore, </p> \\[DN = \\lfloor\\frac{\\min(\\Phi\\Delta t + I_0, I_m)}{g}\\rfloor\\] <p>And for the whole process, the relationship is linear</p>"},{"location":"csc320/camera.html#gamma-correction-camera-response-function","title":"Gamma correction (camera response function)","text":"<p>Since human visual system doesn't have a linear response to light, DNs are passed through a gamma function to compensate, such function is \\(f(DN) := \\beta(DN)^{1/\\gamma}\\) where \\(\\beta, \\gamma\\) are constants that varies among different manufactures. </p>"},{"location":"csc320/camera.html#image-noise","title":"Image Noise","text":"<p>Defocus Blur when the scene points of interest are \"out of focus\" and not within the DoF</p> <p>Motion Blur Camera moves significantly during exposure time More like with long exposures and long focal length (zooming in)</p> <p>Pixel noise Incorrect exposure, not enough photons reaching sensor High ISO (gain) causes noise</p> <p>Rolling shutter wiki When captured by scanning across the scene rapidly</p>"},{"location":"csc320/camera.html#sources-of-noise","title":"Sources of Noise","text":"<p>radiant power from scene </p> <p>Dark Current Noise</p> <ul> <li>free electrons due to thermal energy, depends on temperature  </li> <li>\\(\\sim \\text{Poisson}(\\lambda = D\\Delta t), D:=\\)thermal electron rate \\((e^-/sec)\\)</li> </ul> <p>Exposure</p> <p>Photon (Shot) Noise </p> <ul> <li>\\(\\sim \\text{Poisson}(\\lambda = \\Phi\\Delta t )\\) \\(P_\\lambda(\\text{k events in }\\Delta t) = \\frac{\\lambda ^k e^{-\\lambda}}{k!}\\), i.e. \\(P(\\text{\\#received photons}=k)= \\frac{\\Phi\\Delta t ^k e^{-\\Phi\\Delta t}}{k!}\\)</li> <li>Largest source of noise for high exposures</li> <li>For large \\(\\Phi\\Delta t\\), by LLN, we can approximate by \\(\\text{Poisson}(\\lambda)\\approx N(\\lambda, \\sqrt \\lambda)\\)</li> </ul> <p>Sensor saturation </p> <p>Readout Noise</p> <ul> <li>\\(\\sim N(0, \\sigma_r)\\), \\(\\sigma_r\\) depends on characteristics of electronics</li> </ul> <p>Gain factor </p> <p>Amplifier, ADC, and Quantization Noise </p> <ul> <li>\\(\\sim N(0, \\sigma_{ADC})\\) </li> <li>The amplifier noise is dependent on \\(g\\), i.e. gain or ISO. </li> <li>Largest source of noise for low exposures</li> </ul>"},{"location":"csc320/camera.html#put-all-noises-together","title":"Put all Noises Together","text":"<p>Since all four types of noise are independent of each other, </p> \\[\\begin{align*} E(e^-) &amp;=&amp;\\text{Black level} + &amp;\\text{dark current} + &amp;\\text{photon noise}\\\\ &amp;= \\min\\{&amp;I_0 + &amp;D\\Delta t + &amp;\\Phi\\Delta t, I_m\\}\\\\  \\end{align*}\\] \\[\\begin{align*} var(e^-) &amp;= \\text{dark current} &amp;+ \\text{photon noise} &amp;+ \\text{black level} &amp;+ \\text{readout noise} &amp;+ \\text{ADC noise}\\\\ var(e^-) &amp;= D\\Delta t &amp;+ \\phi\\Delta t &amp;+ I_0 &amp;+ \\sigma_r^2 &amp;+ \\sigma^2_{ADC} g^2 \\end{align*}\\] <p>Hence \\(E(DN) = \\min\\{\\frac{I_0 + D\\Delta t + \\Phi\\Delta t}{g}, \\frac{I_m}{g}\\}\\) \\(var(DN) = \\frac{D\\Delta t + \\phi\\Delta t + I_0 + \\sigma_r^2}{g^2} + \\sigma^2_{ADC} g^2\\)</p> <p>Signal-to-noise ratio \\(SNR = 10\\log_{10} \\frac{E(DN)^2}{var(DN)}\\)</p>"},{"location":"csc320/hdr.html","title":"High Dynamic Range","text":""},{"location":"csc320/hdr.html#dynamic-range","title":"Dynamic Range","text":"<p>\\(DR := \\frac{L_{max}}{L_{min}}\\) where \\(L\\) is the the brightness of the visible in the photo Measured in \\(EV := \\lg(DR)\\)</p>"},{"location":"csc320/hdr.html#problem-with-dr","title":"Problem with DR","text":"<p>Human perception can detect \\(\\sim 14 EV\\) stops, while 8-bit image can only represent \\(\\sim 6\\) stops (due to the \\(\\gamma\\)-function), even the best sensor can capture \\(14\\) bits.pixel, and TV can only display \\(6-10\\) stops</p> <p>On the images, bright pixels will saturate the sensor, while the dark pixels are below the the threshold required to be represented</p>"},{"location":"csc320/hdr.html#general-ideas-of-hdr","title":"General Ideas of HDR","text":"<p>Capturing taking photos at multiple EV to capture more Display find a mapping function that can display more tones from HDR images in 8-bit LDR display</p>"},{"location":"csc320/hdr.html#inverse-camera-response-function","title":"Inverse Camera Response Function","text":"<p>Let \\(\\Phi\\) represent the scene irradiance, \\(Z(x,y)\\) be the camera recorded value on \\((x,y)\\).  Consider \\(\\Delta t\\) be he different exposure time. Then, </p> \\[Z = f_{camera}(\\Phi\\Delta_t)\\] <p>where \\(f_{camera}\\) is all the remapping from RAW (i.e. gamma function).  Then if we know how to approximate the inverse of \\(f_{camera}\\), then we can know the original \\(\\Phi\\Delta_t\\), then the algorithm is to</p> Inverse Camere Response<pre><code>for (x, y) in all image pixel:\n    for j in photos with exposure time delta_t(j):\n        estimate Phi_ij by Z(x,y) and delta_t(j)\nestimate Phi_i by Phi_ij's\noutput Phi_i with image response function\n</code></pre>"},{"location":"csc320/hdr.html#log-inverse-response-function","title":"Log-inverse Response Function","text":"\\[g(Z) = log(f^{-1}(Z)) = \\log\\Phi + \\log\\Delta_t\\] <p>i.e. the log of inverse response function Also, note \\(Z\\in \\mathbb N. Z \\leq 255\\), thus we only need to approximate \\(256\\) values</p> <p>Then, note that we have \\(N\\) pixels and \\(P\\) images, i.e. \\(NP\\) equations and \\(N + 256\\) unknowns. i.e.  </p> \\[g(Z_{ij}) - \\log \\Phi_i = \\log \\Delta t_{j}\\] <p>where \\(Z_{ij}, \\log\\Delta t_j\\) are known, \\(g, \\log\\Phi_i\\) are unknown Then, denote \\(g_{ij} = g(Z_{ij}), \\phi_i = \\log \\Phi_i, \\delta_j = \\log \\Delta t_j\\) Let </p> \\[\\vec x = [g(0), g(1), ..., g(255), \\phi_1,...,\\phi_N]^T\\] \\[\\vec b = [\\underset{j\\text{ times}}{\\delta_1,...,\\delta_1},...,\\underset{j\\text{ times}}{\\delta_i,...,\\delta_i}]\\] \\[A_{NP\\times 256+N}\\] <p>where for the \\(ij\\)th row, \\(A[Z_{ij}+1] = 1, A[256+i] = -1\\)</p>"},{"location":"csc320/hdr.html#smoothness-constraints","title":"Smoothness Constraints","text":"<p>Since we know that \\(g\\) is a smoothly increasing function, i.e. </p> \\[g_{z+1} - g_z \\approx g_{z} - g_{z-i}\\Rightarrow 2g_z - g_{z+1} - g_{z-1}\\approx 0\\] <p>so we add the \\(254\\) equations, i.e. \\(254\\) columns to \\(A, x\\). Where</p> \\[A_{NP + k, k-1} = -1, A_{NP+k,k}=2, A_{NP+k, k+1}= -1, b_{NP+1 : NP+254} = 0\\]"},{"location":"csc320/hdr.html#final-equation","title":"Final Equation","text":"\\[\\Phi_i = \\exp(\\frac{\\sum^P w(Z_{ij} \\log \\phi_{ij})}{\\sum^P w(Z_{ij})})\\] <p>\\(w\\) is a weighting factor that depends on the pixel value, i.e. the integer approximation of \\(g\\). \\(w\\) should be lower at \\(0/255\\) for the compensation of black level and saturation. </p>"},{"location":"csc320/matting.html","title":"Image Matting","text":""},{"location":"csc320/matting.html#alpha-channel","title":"Alpha Channel","text":"<p>Alpha channel is pixel \"transparency\" \\(\\alpha\\in [0, 1]\\) </p> <p>When representing RGBA pixel as RGB, we calculate the alpha composite   </p> \\[C = \\alpha_F C_F + (1-\\alpha_F)C_B\\] <p>where \\(F:=\\) foreground, \\(B:=\\)background</p>"},{"location":"csc320/matting.html#matting-problem","title":"Matting Problem","text":"<p>We want to extract all the foreground pixels \\(F = [F_r, F_g, F_b]\\) and matte \\(\\alpha\\), Given \\(B=[B_r, B_g, B_n], C=[C_r, C_g, C_b]\\) Therefore, for each pixel, the equation is </p> \\[C_{r,g,b}=\\alpha F_{r,g,b} + (1-\\alpha)B_{r,g,b}\\] <p>Which are 3 equations and 7 unknowns</p>"},{"location":"csc320/matting.html#methods-to-solve-matting-equation","title":"Methods to solve Matting equation","text":""},{"location":"csc320/matting.html#known-background","title":"Known Background","text":"<p>If \\(B\\) is known, and given there is no semi-transparency, i.e. \\(\\alpha = \\mathbb I(C=B)\\) Therefore, we reduce \\(4\\) unknowns</p>"},{"location":"csc320/matting.html#problems","title":"Problems","text":"<p>Background must be known accurately, and constant Foreground subject cannot be similar to the background \\(\\alpha\\) is either 0 or 1, hence no semi-transparency</p>"},{"location":"csc320/matting.html#blue-screen-matting","title":"Blue Screen Matting","text":"<p>Assume background contains only blue, i.e. \\(B = [0, 0, B_b]\\), then  </p> \\[C_r = aF_r, C_g = aF_g, C_b = (1-a)B_b\\]"},{"location":"csc320/matting.html#problems_1","title":"Problems","text":"<p>You cannot have any blue channel in the foreground, which is almost impossible. Also, \"blue/green spilling\" will have blue light reflected, make components blue</p>"},{"location":"csc320/matting.html#gray-or-skin-colored-foreground","title":"Gray or Skin Colored Foreground","text":"<p>Constant, one-channel color background, and assume foreground color is proportional, such as gray \\(F:= [d, d, d]\\), flesh\\(F:=[d, d/2, d/2]\\). Then, </p> \\[C_r = aF, C_g = aF, C_b = aF + (1-a)B_v\\] <p>The assumption is too strong</p>"},{"location":"csc320/matting.html#triangulation-matting","title":"Triangulation Matting","text":"<p>If there are two different background, with the same lighting and position, let the two backgrounds be \\(B_0, B_1\\), then  </p> \\[C_0 = aF + (1-a)B_0, C_1 = aF + (1-a)B_1\\] <p>We have 6 equations, 4 unknowns</p> <p>Then, to solve such system of equations, we can use a sparse matrix </p> \\[\\begin{bmatrix}C_{0,r}\\\\C_{0,g}\\\\C_{0,b}\\\\C_{1,r}\\\\C_{1,g}\\\\C_{1,b}\\end{bmatrix}  = \\begin{bmatrix}1&amp;0&amp;0&amp;B_{0,r}\\\\0&amp;1&amp;0&amp;B_{0,g}\\\\0&amp;0&amp;1&amp;B_{0,b}\\\\1&amp;0&amp;0&amp;B_{1,r}\\\\0&amp;1&amp;0&amp;B_{1,g}\\\\0&amp;0&amp;1&amp;B_{1,b} \\end{bmatrix}\\begin{bmatrix}F_r\\\\F_g\\\\R_b\\\\\\alpha\\end{bmatrix}\\] \\[b = Ax\\] <p>So that we can approximate using psurdo-inverse, \\(x = (A^TA)^{-1}A^Tb\\)</p>"},{"location":"csc320/poly_fitting.html","title":"Polynomial Fitting","text":"<pre><code>import numpy as np\nimport matplotlib.pyplot as plt\n</code></pre>"},{"location":"csc320/poly_fitting.html#goal","title":"Goal","text":"<p>Since an image is a discrete mapping of pixels, while the real world is \"continuous\", we want to fit the pixels by a continuous functions so that we can extract information. </p>"},{"location":"csc320/poly_fitting.html#sliding-window-algorithm","title":"Sliding Window Algorithm","text":"<ul> <li>Define a \"pixel window\" centered at pixel \\((x, y)\\) so that an 1D patch can be \\((0, y)\\sim (2x,y)\\)</li> <li>Fit a n-degree polynomial to the intensities (commonly \\(n \\leq 2\\))</li> <li>Assign the poly's derivatives at \\(x=0\\) to pixel at windows's center</li> <li>Slide window one pixel over until window reaches border</li> </ul>"},{"location":"csc320/poly_fitting.html#taylor-series-approximation-of-images","title":"Taylor-series Approximation of Images","text":"<p>Consider a 1D patch from \\((0,y)\\) to \\((2x,y)\\) Consider the Taylor expansion of \\(I\\) centered at \\(0\\)</p> \\[I(\\vec x) = \\sum_{i=0}^\\infty \\frac{d^iI}{d\\vec x^i}\\vec x^i I(0) \\approx \\sum_{i=0}^N \\frac{d^iI}{d\\vec x^i}\\vec x^i I(0) + R_{N+1}(x)\\] <p>Thus, we can write it as </p> \\[I(x) = [1, x, x^2/2, x^3/6,...,x^n/n!][I(0), d_xI(0), d^2_{x^2}I(0), ..., d^n_{x^n}I(0)]^T\\] <p>Note that we have \\(2x+1\\) pixels in the patch, ranges from \\(-x\\) to \\(x\\), let their intensities be \\(I_x\\). So that we have</p> \\[ \\begin{bmatrix} I_{-x}\\\\ I_{-x+1}\\\\ ...\\\\ I_{x-1}\\\\ I_{x} \\end{bmatrix}= \\begin{bmatrix} 1 &amp;-x &amp;...&amp;(-x)^n/n!\\\\ 1 &amp;(-x+1) &amp;...&amp;(-x+1)^n/n!\\\\ ... &amp;... &amp;...&amp;...\\\\ 1 &amp;(x-1) &amp;...&amp;(x-1)^n/n!\\\\ 1 &amp;x &amp;...&amp;x^n/n!\\\\ \\end{bmatrix}   \\begin{bmatrix} I(0)\\\\d_xI(0)\\\\...\\\\d^{n-1}_{x^{n-1}}I(0)\\\\d^n_{x^n}I(0)\\end{bmatrix}\\] \\[I_{(2x+1)\\times 1}=X_{(2x+1)\\times n}d_{n\\times 1}\\] <p>where \\(I, X\\) are known and we want to solve \\(d\\)</p> <p>Since this is not always have a solution. We want to minimize the \"fit error\" \\(\\|I-Xd\\|^2 = \\sqrt{\\sum^n I_i^2}\\)</p> <p>When \\(n = 0\\), obviously \\(d\\) is minimized at mean, i.e. \\(\\sum^{2x+1} I_i / (2x+1)\\)</p>"},{"location":"csc320/poly_fitting.html#weighted-least-square-estimation","title":"Weighted least square estimation","text":"<p>If we want to extract more information, or in favor of, the neighboring points, we can have \\(I' = \\Omega I\\) where \\(\\Omega\\) is the weight function. </p> <p>In 1-D case, let \\(\\Omega = diag(\\omega_1, ..., \\omega_{2x+1})\\), we want to solve \\(\\Omega I = \\Omega X d\\) which is to minimize \\(\\|\\Omega(I-Xd)\\|^2\\)</p>"},{"location":"csc320/poly_fitting.html#random-sample-consensus-ransac","title":"Random Sample Consensus (RANSAC)","text":"<p>Suppose we have some outliers, then the line fitting with least square is not a good solution. </p> <pre><code>x = np.arange(-10, 10, 0.5)\ny = x * 0.5 + np.random.normal(0, 1, x.shape)\ny += (np.random.randn(y.shape[0]) &lt; -1.5) * 8\ny -= (np.random.randn(y.shape[0]) &lt; -1.5) * 8\nfit = np.polyfit(x, y, 1)\nf = fit[0] + fit[1] * x\nplt.scatter(x, y)\nplt.plot(x, f);\n</code></pre> <p></p>"},{"location":"csc320/poly_fitting.html#algorithm","title":"Algorithm","text":"<p>Given: \\(n=\\) degree of freedom (unkonwn polynomial coefficients) \\(p=\\) fraction of inliers $t = $ fit threshold $P_s = $ success probability \\(K=\\) max number of iterations</p> RANSAC<pre><code>for _ in K:\n    randomly choose n + 1 pixels\n    fit a n-degree polynomial\n    count pixels whose vertical distance from poly is &lt; t\n    if there are &gt;= (2w+1) * p pixels, mark them as inliers:\n        do n-degree poly fitting on inliers \n        return the fitting\n# if there is insufficient inliers\nreturn None\n</code></pre> <p>Consider the value for \\(K\\):  Suppose inliers are independent distributed, hence </p> \\[P(\\text{choose n+1 inliers}) = p^{n+1}\\] \\[P(\\text{at least 1 outliers}) = 1 - p^{n+1}\\] \\[P(\\text{at least 1 outliers in all K trials}) = (1-p^{n+1})^K\\] \\[P_s = 1 - (1-p^{n+1})^K\\Rightarrow K = \\frac{\\log(1-P_s)}{\\log(1-p^{n+1})}\\]"},{"location":"csc320/poly_fitting.html#2d-image-patch","title":"2D Image Patch","text":"<p>Given an image patch \\(I(1:W, 1:H), I\\in[0,1]\\) is the intensity mapping</p>"},{"location":"csc320/poly_fitting.html#2d-taylor-series-expansion","title":"2D Taylor Series Expansion","text":"<p>Let \\(I(0,0)\\) be the center</p> \\[\\begin{align*} I(x,y) &amp;= I(0,0) + x\\partial_xI(0,0) + y\\partial_yI(0,0) \\\\ &amp; + \\frac{1}{2}(x^2\\frac{\\partial^2I}{\\partial x^2}(0,0) + y^2\\frac{\\partial^2I}{\\partial y^2}(0,0) + xy\\frac{\\partial^2I}{\\partial x\\partial y}(0,0)) + ... \\end{align*}\\]"},{"location":"csc320/poly_fitting.html#directional-derivative","title":"Directional Derivative","text":"<p>Using 1st order Taylor series approximation.  Let \\(\\theta\\) be the direction, \\(v=(\\cos \\theta, \\sin\\theta)\\) be the unit vector in direction \\(\\theta\\). Consider the points \\((t\\cos\\theta, t\\sin\\theta)\\), i.e. \\(t\\) units away from \\((0,0)\\) in the direction of \\(\\theta\\)</p> \\[I(t\\cos\\theta, t\\sin\\theta) = I(0,0) + I(0,0) + t\\cos\\theta\\partial_xI(0,0) + t\\sin\\theta\\partial_yI(0,0)\\] <p>Then consider the directional derivative </p> \\[\\begin{align*}\\partial_v I(0,0) &amp;= \\frac{\\lim_{t\\rightarrow 0}I(t\\cos\\theta, t\\sin\\theta) - I(0,0)}{t}\\\\ &amp;\\approx \\cos\\theta\\partial_xI(0,0) + \\sin\\theta\\partial_yI(0,0)\\\\ &amp;=\\begin{bmatrix}\\partial_xI(0,0)&amp;\\partial_yI(0,0)\\end{bmatrix}\\begin{bmatrix}\\cos\\theta\\\\\\sin\\theta\\end{bmatrix} \\end{align*}\\] <p>Let \\(\\nabla I = (\\partial_xI, \\partial_yI)\\). Note that </p> \\[\\partial_vI(0,0) = \\begin{cases}1 &amp;v\\text{ is parallel to }\\nabla I(0,0) \\\\0 &amp;v\\perp \\nabla I(0,0) (\\text{isophote})\\end{cases}\\] <p>Therefore, the gradient \\(\\nabla I\\) tells the direction and magnitude of max changes in intensity. </p>"},{"location":"csc320/poly_fitting.html#discrete-derivative","title":"Discrete Derivative","text":"<p>Noting that the image are pixels, hence we can approximate the derivative by its limit definition. </p> \\[\\partial_xI(x,y) = \\frac{1}{2}(I(x+1,y) - f(x-1,y))\\] \\[\\partial_yI(x,y) = \\frac{1}{2}(I(x,y+1) - f(x,y-1))\\] <p>Or we can use forward difference \\((x+1, x)\\) or backward difference \\((x,x-1)\\) vs. \\((x-1,x+1)/2\\)</p> <p>Magnitude \\(\\|\\nabla I(x,y)\\| = \\sqrt{\\partial_xI(x,y)^2 + \\partial_yI(x,y)^2}\\) is the \\(L_2\\) norm Direction \\(\\Theta(x,y) = \\arctan(\\partial_xI / \\partial_yI)\\)</p>"},{"location":"csc320/poly_fitting.html#laplacian","title":"Laplacian","text":"<p>Based on central difference, the approximation to the laplacian of \\(I\\) is </p> \\[\\frac{d^2f}{dx^2} \\approx f(x+1)-2f(x) + f(x-1)\\] <p>For 2D functions, </p> \\[\\frac{\\partial^2f}{\\partial x^2} \\approx f(x+1, y)-2f(x,y) + f(x-1,y)\\] <p>Hence </p> \\[\\begin{align*} \\nabla^2 f &amp;= \\frac{\\partial^2f}{\\partial x^2}+\\frac{\\partial^2f}{\\partial y^2}\\\\ &amp;= f(x+1, y)-2f(x,y) + f(x-1,y) + f(x, y+1)-2f(x,y) + f(x,y-1)\\\\ &amp;= f(x+1, y) + f(x-1,y) + f(x, y+1)+ f(x,y-1)-4f(x,y)\\\\ &amp;= \\begin{bmatrix} 0&amp;1&amp;0\\\\1&amp;-4&amp;1\\\\0&amp;1&amp;0 \\end{bmatrix} \\end{align*}\\]"},{"location":"csc320/wavelet.html","title":"Wavelet Transform","text":"<pre><code>import math\nimport numpy as np\nimport matplotlib.pyplot as plt\n</code></pre>"},{"location":"csc320/wavelet.html#properties-of-transformation","title":"Properties of Transformation","text":"<ul> <li>Minimal</li> <li>Multiple scales represented simultaneously </li> <li>Invertible, linear</li> </ul>"},{"location":"csc320/wavelet.html#1d-haar-wavelet-transform","title":"1D Haar Wavelet Transform","text":"<p>Given an array \\(I_n\\) of \\(2^n\\) elements, then \\(I_{n-1}\\) ve two arrays. Average \\(I_{n-1}\\) be the average of every two elements, </p> \\[I_i^j = \\frac{1}{2}(I_{2i}^{j+1} + I_{2i+1}^{j+1})\\] <p>Detail Coef \\(D_{n-1}\\) be the differnce of first pixel from the average. </p> \\[D_i^j = I_{2i}^{j+1} - \\frac{1}{2}(I_{2i}^{j+1} + I_{2i+1}^{j+1}) = \\frac{1}{2}(I_{2i}^{j+1} - I_{2i+1}^{j+1})\\] <pre><code>I = [9., 7., 3., 5., 6., 10., 2., 6.]\ndef wavelet_recursive(I):\n    I_2, D_2 = [], []\n    for i in range(0, len(I) - 1, 2):\n        I_2.append((I[i] + I[i+1]) / 2)\n        D_2.append((I[i] - I[i+1]) / 2)\n    return I_2, D_2\nprint(\"I{}:\".format(int(math.log2(len(I)))), I)\nprint()\nwhile len(I) &gt;= 2:\n    I, D = wavelet_recursive(I)\n    print(\"I{}:\".format(int(math.log2(len(I)))), I)\n    print(\"D{}:\".format(int(math.log2(len(D)))), D)\n    print()\n</code></pre> <pre><code>I3: [9.0, 7.0, 3.0, 5.0, 6.0, 10.0, 2.0, 6.0]\n\nI2: [8.0, 4.0, 8.0, 4.0]\nD2: [1.0, -1.0, -2.0, -2.0]\n\nI1: [6.0, 6.0]\nD1: [2.0, 2.0]\n\nI0: [6.0]\nD0: [0.0]\n</code></pre> <p>Now, consider the vector representation of \\(I\\), i.e. \\(2^n\\times 1\\) vector. Note that if we have all the \\(D_i\\)'s and \\(I_0\\), we can reconstruct the original image.  For example, \\(I_1 = (I_0 - D_0, I_0 + D_0)\\). Also, notice that \\(|I_0| + |D_0| + |D_1| + |D_2| = 1 + 1 + 2 + 4 +... = 1 + 2^n - 1 = 2^n\\), so that if we represent the wavelet transformed image as one vector </p> \\[\\bigg[[I_0]^T, [D_0]^T, [D_1]^T,..., [D_{N-1}]^T \\bigg]^T\\] <p>Also, notice that for any input image \\(I_{N}\\), we can obtain \\(I_{N-1}, D_{N-1}\\) as </p> \\[I_{N-1} = \\underset{2^{N-1}\\times 2^N}{\\frac{1}{2}\\begin{bmatrix} 1&amp;1&amp;0&amp;0&amp;...&amp;0&amp;0\\\\ 0&amp;0&amp;1&amp;1&amp;...&amp;0&amp;0\\\\ \\vdots&amp;\\vdots&amp;\\vdots&amp;\\vdots&amp;\\ddots&amp;\\vdots&amp;\\vdots\\\\ 0&amp;0&amp;0&amp;0&amp;...&amp;1&amp;1 \\end{bmatrix}}I_N\\] \\[D_{N-1} = \\underset{2^{N-1}\\times 2^N}{\\frac{1}{2}\\begin{bmatrix} 1&amp;-1&amp;0&amp;0&amp;...&amp;0&amp;0\\\\ 0&amp;0&amp;1&amp;-1&amp;...&amp;0&amp;0\\\\ \\vdots&amp;\\vdots&amp;\\vdots&amp;\\vdots&amp;\\ddots&amp;\\vdots&amp;\\vdots\\\\ 0&amp;0&amp;0&amp;0&amp;...&amp;1&amp;-1 \\end{bmatrix}}I_N\\] <p>We can then concat them vertically so that compute \\([[I_{N-1}]^T, [D_{N-1}]^T]^T\\), and we can recursively work on the top vector and keep going. </p> <pre><code>def imshow_labelled(m, ax, title=\"\", r=1):\n    ax.imshow(m, cmap=\"bwr\", vmin = -r, vmax = r)\n    for (j,ii),label in np.ndenumerate(np.round(m, 2)):\n        ax.text(ii,j,label,ha='center',va='center', color=\"black\")\n    ax.set_title(title)\n    ax.set_axis_off()\n\nimage = np.array([[9., 7., 3., 5., 6., 10., 2., 6.]]).T\nI = image.copy()\nmatrices = []\nnumber_m = int(math.log2(I.shape[0]))\nfig, axs = plt.subplots(number_m, 3, figsize=(12, 12))\nhaar_wave_matrix = np.identity(I.shape[0])\nfor steps in range(number_m, 0, -1):\n    m = np.zeros((len(I), len(I)))\n    d = 2**(steps - 1)\n    for i in range(d):\n        m[i, 2*i:2*i+2] = 1/2 \n        m[d + i, 2*i] = 1/2 \n        m[d + i, 2*i + 1] = -1/2\n        m[d*2:, d*2:] = np.identity(2 ** number_m - d * 2)\n    haar_wave_matrix = m @ haar_wave_matrix\n    I = m @ I\n    imshow_labelled(m, axs[number_m - steps][0], \"Transform matrix\", 1)\n    imshow_labelled(haar_wave_matrix, axs[number_m - steps][1], \"Product of matrices\", 1)\n    imshow_labelled(I, axs[number_m - steps][2], \"Resulted vector\", 10)\n</code></pre> <p></p>"},{"location":"csc320/wavelet.html#reconstruction-of-wavelet-transform","title":"Reconstruction of wavelet transform","text":"<p>Consider the shape of each row in the wavelet transform matrix, note that the product of two distinct row will always be 0, and the same row will be the square sum of the entries. </p> <p>Further notice that the sum is \\(2^n(2^{-n}2^{-n}) = 2^{-n}\\), which is just the first none zero entry on wavelet transform matrix. So that we have a diagonal matrix, let </p> \\[\\Lambda := WW^T\\] <p>so that let \\(\\mathcal I\\) be the transformed vector, \\(I\\) be the original image</p> \\[\\mathcal I = WW^T(WW^T)^{-1}\\mathcal I = WW^T\\Lambda^{-1}\\mathcal I = WI\\] <p>We need \\(\\Lambda\\) instead of directly finding inverse of \\(W\\) because finding inverse for diagonal matrix is just inverse each diagonal values. we now have the reconstruction matrix being </p> \\[W^T\\Lambda^{-1}\\] <pre><code>fig, axs = plt.subplots(1, 2, figsize=(12, 6))\nlbd = haar_wave_matrix @ haar_wave_matrix.T\nlbd_inv = np.linalg.inv(lbd)\nimshow_labelled(lbd_inv, axs[0], r\"${lambda}^{-1}$\", 8)\nrecons = haar_wave_matrix.T @ lbd_inv\nimshow_labelled(recons, axs[1], \"reconstruction matrix\", 1)\n</code></pre> <p></p> <p>If we consider \\(W\\) as a basis of \\(\\mathbb R^{2^N}\\), so that the transformed image is a decomposition of the basis image decomposed from the original image. </p> <p>We can further use a normalized wavelet transformation by </p> \\[\\tilde W = \\Lambda^{1/2}W\\] <p>where the square root is element wise on the diagonal, so that \\(\\tilde W I\\) is a set of wavelet coefficient \\(c_0^0, d_0^0, ...\\) that express \\(I\\) as a linear combination of the basis. </p> <pre><code>fig, axs = plt.subplots(1, 2, figsize=(12, 6))\nhaar_wave_matrix_normalized = lbd_inv ** 0.5 @ haar_wave_matrix\nimshow_labelled(haar_wave_matrix_normalized, axs[0], \"Normalized W\")\nwavelet_coef_normalized = haar_wave_matrix_normalized @ image \nimshow_labelled(wavelet_coef_normalized, axs[1], \"Normalized Wavelet coefficient\", 10)\n</code></pre> <p></p> <p>Note that we did a change of basis that is optimal to the reconstruction error, just as PCA. We can sort the coefficients by absolute value and zero-out the coefficients other than some \\(k2^N\\) coefficient, \\(0&lt;k&lt;1\\).</p>"},{"location":"csc336/approx_error.html","title":"Approxmation Errors and Conditioning, Stability","text":"<pre><code>import matplotlib.pyplot as plt\nimport numpy as np\n</code></pre> <p>Let \\(A:=\\) approximation, \\(T:=\\) true value</p>"},{"location":"csc336/approx_error.html#absolute-error-and-relative-error","title":"Absolute error and relative error","text":"<p>Absolute error \\(A-T\\) Relative error \\(\\frac{A-T}{T}\\), let \\(A-T = \\delta\\), then \\(A = T(1+\\delta)\\) avoids the issue with \\(A = 0\\)</p> <p>Connect Digits \\(A:= 5.46729 \\times 10^{-12}, T:= 5.46417\\times 10^{-12}\\) Then, \\(\\delta_{absolute}= A-T = 0.000312 \\times 10^{-12}\\) \\(\\delta_{relative} = 0.000312 / 5.46417 = 3.12/5.46417 \\times 10^{-3}\\) Let \\(p\\) be the first non-agreed digit, then the magnitude relative error will be around \\(10^{-p\\pm 1}\\).</p> <p>0.99... = 1 However, consider \\(A= 1.00596\\times 10^{-10}, T = 0.99452 \\times 10^{-10}\\), the relative error is much smaller than the magnitude approximation.  Therefore, we say that \\(0.999...\\) agrees with \\(1.000...\\), hence \\(A,T\\) agrees on 2 sig digits. </p>"},{"location":"csc336/approx_error.html#data-error-and-computational-error","title":"Data error and computational error","text":"<p>Data error let \\(\\hat x:=\\) input value, \\(x\\) be actual value, error = \\(\\hat x - x\\), for example  - we cannot represent \\(A\\) as a terminating floating number \\((3.14\\sim \\pi)\\)  - measurement error </p> <p>Computational error let \\(f\\) be the actual mapping, \\(\\hat f\\) be the approximation function, errors of the computation is \\(\\hat f(x) - f(x)\\), for example  - often \\(\\cos\\) is calculated by its Taylor series, while Taylor series is a infinite sum, and we can only compute by finite approximation. </p>"},{"location":"csc336/approx_error.html#truncation-error-and-rounding-error-subclasses-of-computational-error","title":"Truncation error and rounding error (subclasses of computational error)","text":"<p>Truncation error The difference between the true result \\(f\\) and the result that would be produced by a given (finite approximation) algorithm using exact arithmetic \\(\\hat f\\). </p> <p>rounding error The difference between the result produced by a given algorithm using exact arithmetic, and the result produced by the same algorithm using a finite-precision, rounded arithmetic. </p> <p>Example To approximate \\(f'(x)\\) with known \\(f\\) By definition \\(f'(x) = \\lim_{h\\rightarrow 0} \\frac{f(x+h) - f(x)}{h}\\), hence, choose a small but not necessarily 0 value \\(h\\), we can approximate \\(f'(x)\\) for all \\(x\\).  Assuming \\(f\\) is smooth, then </p> \\[\\begin{align*} &amp;\\quad \\frac{f(x+h)-f(x)}{h}\\\\ &amp;= \\frac{1}{h}(\\sum_{k=0} f^{(k)}(x)\\frac{h^{k}}{k!} -f(x))&amp;\\text{Taylor expansion around }x \\\\  &amp;= \\frac{1}{h}(f(x)+f'(x)h + f''(c)h^2/2 - f(x)) &amp;\\text{Remainder theorem}, c\\in[x, x+h]\\\\ &amp;= f'(x) + \\frac{f''(c)h}{2} \\end{align*}\\] <p>Therefore, the truncation error will be \\(\\frac{f''(c)h}{2}\\), by IVT, such error is bounded since \\(f\\) is continuous.</p> <p>Then, let \\(FL\\) be the mapping to the floating representation result, assume </p> \\[\\begin{align*} FL(f(x)) &amp;= f(x) + \\epsilon_0 \\\\ FL(f(x+h)) &amp;= f(x+h) + \\epsilon_h \\\\ FL(f'(x)) &amp;= f'(x) + \\hat\\epsilon\\\\ \\Rightarrow FL\\bigg(\\frac{f(x+h) - f(x)}{h} -f'(x)\\bigg)&amp;= \\frac{f(x+h) + \\epsilon_h - (f(x) + \\epsilon_0)}{h} -(f'(x) + \\hat\\epsilon) \\\\ &amp;= \\frac{f''(c)h}{2} + \\frac{\\epsilon_h - \\epsilon_0}{h}+ \\hat\\epsilon \\\\ \\text{Total Error}&amp;=\\text{Truncation Error + Rounding Error} \\end{align*}\\] <p>Note that \\(\\epsilon_h\\) is less influenced by \\(h\\) when \\(h\\) is small, then we can looks \\(\\epsilon_h -\\epsilon_0 =:\\epsilon\\) as a constant. Then, let \\(E(h)\\) be the total error, \\(E'(h) = f''(c)/2 - \\frac{\\epsilon}{h^2}\\Rightarrow argmin(E(h))=\\sqrt{\\epsilon_h/M}\\)</p> <p>Claim \\(\\lim_{h\\rightarrow 0} FL(f(x+h)) - FL(f(x)) = 0\\)</p> <p>Notice that when \\(h\\) is particularly smaller than the machine's precision, i.e. \\(x&gt;&gt;h\\), then \\(x + h\\) will be exactly \\(x\\). </p> <p>Therefore, for very small \\(h\\), \\(FL\\bigg(\\frac{f(x+h) - f(x)}{h} -f'(x)\\bigg) = FL(-f'(x))\\)</p>"},{"location":"csc336/approx_error.html#forward-error-and-backward-error","title":"Forward Error and backward error","text":"<p>Let \\(y = f(x)\\) be the true result, \\(\\hat y = COMP(f(x))\\) be the computational result, where \\(COMP\\) is all the possible errors caused in the computation. Then, forward error is \\(\\hat y - y\\). Take \\(\\hat x\\) such that with the true computation, \\(\\hat y = f(\\hat x)\\), then backward error is \\(\\hat x - x\\)</p>"},{"location":"csc336/approx_error.html#conditioning-of-problems","title":"Conditioning of Problems","text":"<p>Let \\(\\hat x \\in (x-\\delta, x+\\delta)\\), i.e. any \\(\\hat x\\) within the neighborhood of some \\(x\\). Consider the relative forward error, assume \\(f\\) is differentiable around \\(x\\)</p> \\[\\begin{align*} \\frac{\\hat y - y}{y} &amp;:= \\frac{f(\\hat x) - f(x)}{f(x)} \\\\ &amp;= \\frac{f'(c)}{f(x)}(\\hat x - x) &amp;\\text{MVT, }c\\in [x, \\hat x]\\\\ &amp;= \\frac{xf'(c)}{f(x)}\\frac{\\hat x - x}{x} \\\\ &amp;\\approx \\frac{xf'(x)}{f(x)}\\frac{\\hat x - x}{x} &amp;c \\text{ is quiet near } x \\\\ &amp;= \\text{conditioning number} \\times \\text{relative backward error} \\end{align*}\\] <p>The problem is well conditioned if \\(\\forall x\\), condition number is small; is ill conditioned if \\(\\exists x\\), condition number is large</p> <p>Connection to Approximation error \\(y = f(x) \\Rightarrow \\hat y = f(\\hat x)\\), \\(\\hat x - x\\) will have different conditioning with \\(\\frac{\\hat x- x}{x}\\), often we measuring the relative errors</p> <p>Example \\(f(x):= \\sqrt x, x\\geq 0\\) \\(\\forall x \\geq 0. c\\approx\\frac{xf'(x)}{f(x)} = \\frac{x}{2\\sqrt x\\sqrt x} = 1/2\\Rightarrow\\) very well-conditioned</p> <p>Example \\(f(x)=e^x\\) </p> <p>\\(c\\approx \\frac{xe^x}{e^x} =x\\Rightarrow\\) less and less well-conditioned when \\(x\\) gets big. However, \\(e^x\\) will overflows or underflows if \\(|x|\\) is large, even before it gets ill-conditioned. </p> <p>Example \\(f(x)=\\sin(x)\\) </p> <p>\\(c\\approx \\frac{x\\cos(x)}{\\sin(x)} = x\\cot(x)\\Rightarrow\\) ill-conditioned near \\(k\\pi, k\\neq 0\\) or \\(|x|\\) is big and \\(\\cos(x)\\neq 0\\)</p> <pre><code>t = np.arange(-5., 5., 0.01)\nplt.figure(figsize=(12, 4))\nplt.plot(t, t * np.cos(t) / np.sin(t))\nplt.title(\"conditioning number of sin(x)\")\nplt.savefig(\"assets/approx_error_1.jpg\")\n</code></pre> <p></p> <pre><code>t = np.arange(-100., 100., 0.2)\nplt.figure(figsize=(12, 4))\nplt.plot(t, t * np.cos(t) / np.sin(t))\nplt.title(\"conditioning number of sin(x)\")\nplt.savefig(\"assets/approx_error_2.jpg\")\n</code></pre> <p></p>"},{"location":"csc336/approx_error.html#stability-of-algorithms","title":"Stability of Algorithms","text":"<p>An algorithm is stable if small change to the input results in small change in the output</p>"},{"location":"csc336/approx_error.html#stable-good","title":"stable \u2260 good","text":"<p>Consider \\(\\hat f(x) := 0\\: vs. f(x):= \\sin(x)\\), it is very stable, while it does not accomplish a good approximation as wanted. </p>"},{"location":"csc336/arithmetic.html","title":"Computer Arithmetic","text":""},{"location":"csc336/arithmetic.html#integers","title":"Integers","text":"<p>The only issue is overflow, and the issue with division (Euclidean division or floating depends on algorithms)</p>"},{"location":"csc336/arithmetic.html#floating-point-numbers","title":"floating-point numbers","text":"<p>Often approximates reals</p>"},{"location":"csc336/arithmetic.html#representation-of-floating-numbers","title":"Representation of floating numbers","text":"\\[\\pm d_1.d_2d_3d_4...d_p \\times \\beta^n \\equiv\\] <p>where \\(\\beta\\): base, \\(p\\): procession, \\(0\\leq d_i &lt; \\beta\\) If normalized, \\(d_1 \\neq 0\\) \\(L \\leq n \\leq U\\)</p> <p>Underflow limit (UFL) the smallest positive number before getting a underflow is \\(1.00..0 \\times \\beta^L\\) Overflow limit (OFL) the largest positive number before getting an overflow is \\([\\beta-1].[\\beta-1]...[\\beta-1] \\times \\beta^U = (\\beta - \\beta^{1-p})\\times \\beta^U = (1-\\beta^{-p})\\beta^{U+1}\\).</p>"},{"location":"csc336/arithmetic.html#ieee-floating-point-standard","title":"IEEE Floating Point Standard","text":"p L U decimal numbers exponent range in base 10 single precision 24 -126 127 6-7 -37 ~ +37 double precision 53 -1022 1023 16 -308 ~ +308"},{"location":"csc336/arithmetic.html#rounding","title":"Rounding","text":"<p>Most of the time, we can round to the nearest, while when the rounding is exactly at the middle, round to the nearest even least-significant-digit (round to even). In binary, such case will always round to 0 (as 0 is \"even\")</p> <p>IEEE has 4 rounding modes, but we will only encounter rounding to the nearest. </p> <p>Example Consider 3 decimal digit numbers, i.e. \\((p = 3, \\beta = 10)\\), assume \\(L,U = [-10, 10]\\). Then </p> \\[1.54\\times 10^1 + 2.56\\times 10^{-1} = 15.4 + 0.256 = 15.656 \\rightarrow 1.57 \\times 10^1\\]"},{"location":"csc336/arithmetic.html#distance-between-two-adjacent-floating-number-machine-epsilon","title":"Distance between two adjacent floating number =&gt; Machine epsilon","text":"<p>The distance from 1 to the next bigger floating-point number will be \\(1.00...0 \\times \\beta^0\\), the next number will be \\(1.00...01\\times \\beta^0\\), \\(d=\\beta^{1-p}=:\\epsilon_{mach}\\), which is the machine epsilon. </p> <p>For each adjacent pair of numbers \\(a, b\\) in \\([\\beta^i, \\beta_{i+1}), d(a, b) = \\beta^i \\epsilon_{mach}\\), a.k.a. numbers are evenly spread with tte distance apart \\(=\\beta^i \\epsilon_{mach}\\)</p> <p>For some real number \\(a\\), considering the rounding to the nearest floating point number, the absolute error \\(FP(a)\\), \\(|fl(a) - a| \\leq \\frac{\\beta^i\\epsilon_{mach}}{2}\\) and the relative error \\(\\frac{|fp(a)-a|}{|a|}\\leq \\frac{\\beta^i\\epsilon_{mach}}{2\\beta^i} = \\epsilon_{mach}/2\\)</p>"},{"location":"csc336/arithmetic.html#ieee-rule-of-operations","title":"IEEE Rule of operations","text":"<p>IEEE has 5 Basic operations: \\(+,-,\\times, /, \\sqrt{x}\\) and guarantees that  </p> <p>\\(fl(a \\: op\\: b):=\\) the floating point number closest to \\(a\\: op\\:b\\), assuming using rounding to nearest mode and not exceeding UFL or OFL</p> <p>\\(\\Rightarrow |\\frac{fl(a\\: op\\: b) - (a\\: op\\: b)}{a\\: op\\: b}| \\leq \\frac{\\epsilon_{mach}}{2}\\)</p> <p>If encounters UFL or OFL, this property may not be guaranteed  </p> \\[fl(2.02\\times 10^{-16}\\times 1.11\\times 10^{-6}) \\rightarrow 0.02\\times 10^{-10}\\] \\[\\frac{|2.00\\times 10^{-12} - 2.2422\\times 10^{-12}|}{2.2422\\times 10^{-12}} \\approx 0.108 &gt; \\frac{1}{2}10^{-2}=\\epsilon_{mach}/2\\]"},{"location":"csc336/arithmetic.html#problems-with-2-numbers","title":"Problems with \u22652 numbers","text":"<p>Consider \\(fl(1.00\\times 10^3 + 1.00\\times 10^7 - 1.00\\times 10^7)\\), for exact arithmetic the result will be \\(1.0001\\times 10^7\\),  while if we do the calculate in the left-right order \\(fl(*) = fl(1.00\\times 10^7 - 1.00\\times 10^7) = 0\\) if we do the subtraction first, the result will be \\(fl(1.00\\times 10^3 + 0) = 1.00\\times 10^3\\)</p> <p>Catastrophic cancellation If you compute a sum and some of the intermediate values are much larger in magnitude than the final result, then the relative error in the computed sum may be very large. (Consider the example given, $\\frac{|0 - T|}{T} = 1\\Rightarrow $ no accuracy at all)</p> <p>Consider \\(fl((a*b)*c)\\), assuming no OFL/UFL, since \\(fl(a*b) = (a*b)(1+\\delta), |\\delta| \\leq \\epsilon_{mach}/2\\). Then, </p> \\[\\begin{align*} fl(a*b*c) &amp;= fl((a*b)(1+\\delta)*c) \\\\ &amp;= [(ab)(1+\\delta_1)]c(1+\\delta_2)\\\\ &amp;= (abc)(1+\\delta_1+\\delta_2 + \\delta_1\\delta_2)\\\\ &amp;\\leq (abc)(1+\\epsilon_{mach} + \\epsilon^2_{mach}/4)\\\\ \\frac{fl(a*b*c) - abc}{abc}&amp;= \\epsilon + \\epsilon^2/4 \\end{align*}\\] <p>Therefore, multiplications often have fewer errors</p> <p>Suppose \\(a,b,c\\geq 0\\) (also works on \\(a,b,c\\leq 0\\)), assume no OFL/UFL  </p> \\[\\begin{align*} fl(a+b+c) &amp;= fl((a+b)(1+\\delta)+c)\\\\ &amp;= ((a+b)(1+\\delta_1) + c)(1+\\delta_2)\\\\ &amp;= ((a+b+c)(1+\\hat\\delta_1))(1+\\delta_2)&amp;\\text{take }\\hat\\delta_1 *\\\\ &amp;= (a+b+c)(1+\\hat\\delta_1)(1+\\delta_2)\\\\ &amp;= (a+b+c)(1+\\tilde \\delta) \\end{align*}\\] <p>Claim \\(*\\: |\\hat\\delta_1|\\leq \\epsilon/2\\) </p> \\[\\begin{align*}(a+b)(1+\\delta_1)+c &amp;\\leq (a+b)(1+\\epsilon/2) + c&amp;a,b,c\\geq 0\\\\ &amp;\\leq (a+b+c)(1+\\epsilon/2)\\\\ (a+b)(1-\\delta_1)+c&amp;\\geq (a+b)(1-\\epsilon/2)+c \\\\&amp;\\geq (a+b+c)(1-\\epsilon/2) \\end{align*}\\] <p>Note \\(a,b,c\\) must have the same sign to make this true</p>"},{"location":"csc336/arithmetic.html#computing-infinite-sum","title":"Computing infinite sum","text":"\\[fl(\\sum^\\infty n^{-1}) = fl(\\sum^N \\frac{1}{n} + \\frac{1}{N+1})\\] <p>However, \\(\\frac{1}{N+1}\\) will cause a UFL, hence \\(fl(*)\\) gives a finite result instead of \\(\\infty\\) </p> <p>Example \\(\\beta = 10, p =3, L= -10, U = 10\\) IEEE guarantees that we can always get the exact value, then do the rounding</p> \\[fl(3.67\\times 10 + 4.56 \\times 10^2) = fl(36.7 + 456) = fl(492.7) = 4.93\\times 10^2\\] <p>Could have underflow</p> \\[fl(2.02\\times 10^{-6} \\times 1.01\\times 10^{-5}) = fl(2.0402\\times 10^{-11})=UFL\\]"},{"location":"csc336/arithmetic.html#subnormal-denormal-numbers-and-gradual-underflow","title":"Subnormal (denormal) numbers and Gradual Underflow","text":"<p>Subnormal numbers have \\(d_1 = 0\\). Benefit: We filled in the gap between \\([0, \\beta^L)\\) Penalty: The machine epsilon number rule does not hold, i.e. \\(\\exists a. |fl(a)-a|/|a| &gt; \\epsilon_{mach}/2\\)</p> <p>Example with \\(\\beta = 10, p = 3, L = -10, U = 10\\) </p> \\[\\begin{align*} 1.01 \\times 10^{-5} \\cdot 2.02 \\times 10^{-6} &amp;= 2.0402 \\times 10^{-11}\\\\ &amp;= 0.20402\\times 10^{-10} &amp;\\text{using subnormal}\\\\ &amp;\\rightarrow 0.20\\times 10^{-10} &amp;\\text{still need leading 0 to tell this is subnormal} \\\\ 1.01 \\times 10^{-6} \\cdot 2.02 \\times 10^{-7} &amp;= 2.0402 \\times 10^{-13}\\\\ &amp;= 0.0020402\\times 10^{-10} \\\\ &amp;\\rightarrow 0.00\\times 10^{-10} &amp;\\text{underflow}\\\\ &amp;\\rightarrow 0 \\end{align*}\\] <p>The second case will be the underflow to a subnormal number</p> \\[\\begin{align*} 3.56 \\times 10^6 \\cdot 5.41 \\times 10^6 &amp;= 1.92596 \\times 10^{11}\\\\ &amp;\\rightarrow +Inf &amp;\\text{overflow happens} \\end{align*}\\]"},{"location":"csc336/arithmetic.html#inf-and-nan","title":"Inf and NaN","text":"<p>When overflow happens, IEEE rules it as \\(\\pm Inf\\) (it just means greater than the computer's capacity, not actually infinity)  </p> <pre><code>Inf  + Inf =&gt; +Inf\nInf - Inf =&gt; NaN\nInf/Inf =&gt; NaN\n0/0 =&gt; NaN\n</code></pre> <p><code>NaN</code> better understands as I don't know what it is</p>"},{"location":"csc336/arithmetic_examples.html","title":"Computer Arithmetic Examples","text":"<pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nimport math\n</code></pre>"},{"location":"csc336/arithmetic_examples.html#example-1","title":"Example 1","text":"<p>Is \\(y=\\sqrt{1+x^2} - 1=:f(x)\\) well-conditioned?  </p> \\[\\begin{align*} C.N &amp;= \\frac{xf'(x)}{f(x)}\\\\ &amp;= \\frac{x(x(1+x^2)^{-1/2}}{(1+x^2)^{1/2}-1}\\\\ &amp;= \\frac{x(x(1+x^2)^{-1/2}}{(1+x^2)^{1/2}-1} \\frac{\\sqrt{1+x^2}+1}{\\sqrt{1+x^2}+1}\\\\ &amp;= \\frac{x^2\\sqrt{1+x^2}+1}{\\sqrt{1+x^2}(1+x^2-1)}\\\\ &amp;= \\frac{\\sqrt{1+x^2} + 1}{\\sqrt{1+x^2}}\\\\ C.N. &amp;\\in [1,2] \\end{align*}\\] <pre><code>a = np.arange(-100, 100, 1)\nb = ((a ** 2 + 1) ** 0.5 + 1) / ((a ** 2 + 1) ** 0.5)\nplt.plot(a, b)\nplt.savefig(\"assets/arithmetic_examples_1.jpg\")\n</code></pre> <p></p> <p>Does \\(fl(\\sqrt{1+x^2}-1)\\) always give an accurate result</p> <p>When \\(x\\) is small enough so that \\(fl(\\sqrt{1+x^2}) = 1\\), then \\(\\hat y = 0, y \\neq 0\\Rightarrow \\frac{\\hat y - y}{y}=\\infty\\) \\(fl(\\sqrt{1+x^2})=1\\Rightarrow x^2 &lt; \\frac{\\epsilon}{2}\\Rightarrow |x|\\leq \\sqrt{\\epsilon/2}\\) </p> <p>When \\(|x|\\leq \\sqrt{\\epsilon/2}, x\\neq 0\\), this operation gives an inaccurate result, and for moderately small \\(x\\) this is still not so accurate</p> <p>Can we change \\(\\sqrt{1+x^2}-1\\) to a mathematically equivalent that has a much smaller f.p. error. </p> \\[(*)=\\sqrt{1+x^2}-1\\frac{\\sqrt{1+x^2}+1}{\\sqrt{1+x^2}+1}= \\frac{x^2}{\\sqrt{1+x^2}+1}\\] <p>lemma 1 \\(\\sqrt{1+\\delta} = 1+\\hat \\delta, |\\hat\\delta| &lt; \\epsilon/2\\) lemma 2 \\((1+\\delta)^{-1} = 1+\\hat\\delta, |\\hat\\delta| &lt; \\frac{1.01\\epsilon}{2}\\)</p> <p>Claim accuracy   </p> \\[\\begin{align*} fl\\bigg(\\frac{x^2}{\\sqrt{1+x^2}+1}\\bigg) &amp;= (1+\\delta_5)\\bigg(\\frac{x^2(1+\\delta_1)}{(\\sqrt{(1+x^2(1+\\delta_1))(1+\\delta_2)}(1+\\delta_3)+1)(1+\\delta_4)}\\bigg)\\\\ &amp;= \\frac{(1+\\delta_5)x^2(1+\\delta_1)}{[\\sqrt{(1+x^2)(1+\\hat\\delta_1)(1+\\delta_2)}(1+\\delta_3)+1](1+\\delta_4)}\\\\ &amp;= \\frac{(1+\\delta_5)x^2(1+\\delta_1)}{[\\sqrt{(1+x^2)}\\sqrt{(1+\\hat\\delta_1)}\\sqrt{(1+\\delta_2)}(1+\\delta_3)+1](1+\\delta_4)}\\\\ &amp;= \\frac{(1+\\delta_5)x^2(1+\\delta_1)}{[\\sqrt{(1+x^2)}(1+\\hat\\delta_1)(1+\\hat\\delta_2)(1+\\delta_3)+1](1+\\delta_4)}\\\\ &amp;= \\frac{(1+\\delta_5)x^2(1+\\delta_1)}{[\\sqrt{(1+x^2)}(1+\\tilde\\delta_1)(1+\\tilde\\delta_2)(1+\\delta_3)+1](1+\\delta_4)}\\\\ &amp;= \\frac{x^2}{\\sqrt{(1+x^2)}+1}\\frac{(1+\\delta_1)(1+\\delta_5)}{(1+\\delta_1^*)(1+\\delta_2^*)(1+\\delta_3^*)(1+\\delta_4)}\\\\ &amp;= \\frac{x^2}{\\sqrt{(1+x^2)}+1}(1+\\delta_1)(1+\\delta_5)(1+\\delta_1^{**})(1+\\delta_2^{**})(1+\\delta_3^{**})(1+\\delta_4^{**}) \\end{align*}\\] <p>Note that \\(\\delta^{**} \\leq 1.01\\epsilon/2\\), let the product of all \\((1+\\delta)\\)'s, i.e. \\(1 + \\tilde\\delta \\leq \\frac{7(1.01)\\epsilon}{2}\\)</p> <p>\\(2.56248\\times 10^4, 2.56125\\times 10^4\\) agrees to 3 sig-dig (to say agree to n sig dit, the exponent must match) \\(relative. error = \\frac{2.56248 - 2.56125}{2.56125} = \\frac{.00123}{2.56125}=1.23/2.56125\\times 10^{-3}\\) \\(p\\) sig-dig agree \\(\\Rightarrow 10^{-p\\pm 1}\\) relateive error</p>"},{"location":"csc336/arithmetic_examples.html#example-2","title":"Example 2","text":"<p>Let \\(x = 1\\times 10^{-13}, y = 1 + x - 1, z = 4 + x - 4\\) \\(r_y = (y-x)/x, r_z = (z-x)/x\\) will have large relative errors</p> <pre><code>x = 1.000000000000000000000e-13\ny = 1 + x - 1\nz = 4 + x - 4\nry = (y - x )/x\nrz = (z - x)/x\nprint(\"y =\", y)\n#&gt;&gt; y = 9.992007221626409e-14\nprint(\"z =\", z)\n#&gt;&gt; z = 1.0036416142611415e-13\nprint(\"ry =\", ry)\n#&gt;&gt; ry = -0.000799277837359144\nprint(\"rz =\", rz)\n#&gt;&gt; rz = 0.003641614261141482\n</code></pre> <p>Consider a system of \\(\\beta=10, p = 5\\) </p> \\[x = 1.0000 \\times 10^{-3}, y = 1 + x - 1 = 1.0010 - 1 = 0.001 = 1.0000\\times 10^{-3}\\] <p>will have no error at all </p> <p>However, if \\(\\beta = 2\\) </p> \\[\\begin{align*} fl_2(1+x) &amp;= &amp;1.0000000 \\times 2^0 \\\\ &amp; &amp;+ 0.00..0b_1b_2...b_k...b_{53}\\times 2^0 \\\\ &amp;= 1.0...0b_1b_2...b_k... b_{53} \\times 2^0 \\\\ &amp;\\rightarrow 1.0...0b_1b_2...b_k\\\\ fl_2(1+x - 1) &amp;= 1.0...0b_1b_2...b_k - 1.0000\\times 2^0\\\\ &amp;= 0.0...0b_1b_2...b_k \\times 2^0\\\\ &amp;= b_1.b_2...b_k00...0\\times 2^{-n}\\\\ \\text{Note } x &amp;= b_1.b_2b_3...b_kb_{k+1}...b_{53}\\times 2^{-n} \\end{align*}\\] <p>We lost the last cut of numbers, and its about 3 significant digits</p>"},{"location":"csc336/arithmetic_examples.html#example-3","title":"Example 3","text":"<p>Consider the integral \\(I_n = \\int_0^1 x^n e^{x-1}dx\\), \\(n\\in \\mathbb N\\) Note tat \\(x^n e^{x-1} &gt; 0, \\forall x \\in (0, 1), I_n &gt; 0\\) Also, \\(x^ne^{x-1} &gt; x^{n+1}e^{x-1}\\) therefore this is a positive monotonic decreasing sequence.</p> <p>Also, note that </p> \\[\\begin{cases}I_0 = [e^{x-1}]^1_0 = 1/e\\\\ I_n = [x^ne^{x-1}]^1_0 - n\\int_0^1 x^{n-1}e^{x-1}dx = 1-nI_{n-1}\\end{cases}\\] <p>Therefore we can calculate this recursively</p> <pre><code>l = [1 - 1/math.e]\nfor n in range(1, 24):\n    l.append(1 - n * l[-1])\nfig, axs = plt.subplots(1, 2, figsize=(12, 6))\naxs[0].plot(range(1, 20), l[:19])\naxs[1].plot(range(1, 25), l);\nfig.savefig(\"assets/arithmetic_examples_2.jpg\")\n</code></pre> <p></p> \\[\\begin{align*} \\tilde I_0 &amp;= I_0 + \\epsilon_0, |\\epsilon_0| \\approx 10^{-16}\\\\ \\tilde I_1 &amp;= 1 - 1 \\tilde I_0 = 1 - (I_0 + \\epsilon_0) = I_1 - \\epsilon_0  \\\\  \\tilde I_2 &amp;= 1 - 2(I_1 - \\epsilon_0) = 1 - 2I_1 + 2\\epsilon_0 = I_2 + 2\\epsilon_0\\\\  &amp;...\\\\  \\tilde I_n &amp;= I_n + n! \\epsilon_0 \\end{align*}\\] <pre><code>print(\"n = 10 =&gt;\", 1e-16 * math.factorial(10))\n#&gt;&gt; n = 10 =&gt; 3.6288e-10\n\nprint(\"n = 18 =&gt;\", 1e-16 * math.factorial(18))\n#&gt;&gt; n = 18 =&gt; 0.6402373705728\n</code></pre> <p>Consider </p> \\[I_n = 1-nI_{n-1}\\equiv I_{n-1} = \\frac{1 - I_n}{n}\\] \\[\\tilde I_{n-1} = \\frac{1}{N}(1 - \\tilde I_N) = \\frac{1 - I_N}{n} - \\frac{\\epsilon_N}{N}\\] <p>Note that </p> \\[\\tilde I_{n-k} = I_{N-k} + (-1)^n \\frac{\\epsilon_N}{N! / (N-k+1)!}\\] <p>Also, note that \\(e^{-1} \\leq e^{x-1}\\leq e^0 = 1\\) </p> \\[\\frac{e^{-1}}{n+1}=e^{-1}\\int_0^1 x^n dx\\leq \\int_0^1 x^ne^{x-1}dx \\leq \\int_0^1 x^ndx = (n+1)^{-1}\\] <p>So that </p> \\[\\int_0^1 x^n e^{n-1}dx \\approx \\frac{1}{2}\\frac{1 - e^{-1}}{n+1}\\] \\[|I_n - A|\\leq \\frac{1}{2}(\\frac{1 - e^{-1}}{n+1}) = \\frac{1}{n+1}\\frac{1-e^{-1}}{2}\\]"},{"location":"csc336/interpolations.html","title":"Interpolations","text":"<pre><code>import numpy as np\nimport matplotlib.pyplot as plt\n</code></pre>"},{"location":"csc336/interpolations.html#introduction","title":"Introduction","text":"<p>For a given sequence of observations \\(\\{(x_i, y_i)\\}^n_{i=1}\\), we want to find some \\(f:\\mathbb R\\rightarrow \\mathbb R\\) s.t. \\(f(x_i) = y_i\\) or in some cases \\(f(x_i) \\approx y_i\\). The approximation can be defined as Least square approximation \\(\\arg\\min_f\\|f(x_i) - y_i\\|\\), where \\(f\\) belongs to some group of functions (say we want to fit a line). </p>"},{"location":"csc336/interpolations.html#usages-in-numerical-methods","title":"Usages in Numerical Methods","text":"<ul> <li>Newton's method</li> <li>Integrals: for some \\(f\\) that is hard to find integral, find some polynomial \\(p\\rightarrow^{p.w.} f\\). </li> </ul>"},{"location":"csc336/interpolations.html#polynomial-interpolations","title":"Polynomial Interpolations","text":"<p>Given \\(\\{(x_i,y_i)\\}_{i=1}^n\\), WTF \\(f(x) = \\sum_{i=1}^n c_i \\phi_i(x)\\), i.e. </p> \\[ \\underset{A}{\\begin{bmatrix} \\phi_1(x_i)&amp;...&amp;\\phi_n(x_1)\\\\ \\vdots &amp;\\ddots&amp;\\vdots\\\\ \\phi_1(x_n)&amp;...&amp;\\phi_n(x_n) \\end{bmatrix}} \\underset{c}{\\begin{bmatrix} c_1\\\\\\vdots\\\\c_n\\end{bmatrix}}=  \\underset{y}{\\begin{bmatrix} y_1\\\\\\vdots\\\\y_n\\end{bmatrix}} \\] <p>If \\(A\\) is nonsingular, then always a unique solution to \\(f(x_i) = y_i\\)</p>"},{"location":"csc336/interpolations.html#monomial-basis","title":"Monomial Basis","text":"<p>If we let \\(\\phi_i(x) = x^{i-1}\\), then \\(f(x) = p(x) = \\sum_{i=0}^N c_i x^i\\) s.t. \\(p(x_i) = y_i\\) and we have </p> \\[ A = \\begin{bmatrix} 1&amp;x_1&amp;...&amp;x_1^{n-1}\\\\ \\vdots&amp;\\vdots &amp;\\ddots&amp;\\vdots\\\\ 1&amp;x_n&amp;...&amp;x_n^{n-1} \\end{bmatrix}=:\\text{Vandermonde Matrix} \\] <p>Note that \\(A\\) is non-sigular if \\(x_i\\)'s are distinct. </p> <p>Proposition Given \\(A\\) is non-singular, then \\(Ac =0\\) IFF \\(c=0\\) </p> <p>proof. Define \\(p(x_j) := \\sum_{i=1}^n c_i x_j^{i-1} = 0\\) for \\(j = 1,...,n\\). \\(p(x)\\) is a polynomial of degree \\(n-1\\). By fundamental theorem of algebra, if at least one \\(c_i \\neq 0\\) then \\(p(x)\\) has at most \\(n-1\\) roots. </p>"},{"location":"csc336/interpolations.html#example","title":"Example","text":"<p>Find a quadratic \\(p(x) = c_1 + c_2 x + c_3 x^2\\) s.t. \\(p(-2) = -27, p(0) = -1, p(1) = 0\\) Then we construct \\(A = \\begin{bmatrix} 1&amp;(-2)^1&amp;(-2)^2\\\\1&amp;0&amp;0^2\\\\1&amp;1&amp;1^2\\end{bmatrix} = \\begin{bmatrix} 1&amp;-2&amp;4\\\\1&amp;0&amp;0\\\\1&amp;1&amp;1\\end{bmatrix}, y = \\begin{bmatrix}-27\\\\-1\\\\0\\end{bmatrix}\\)</p> <pre><code>x = np.array([-2, 0, 1])\ny = np.array([-27, -1, 0])\n\ndef find_fit(x, y):\n    A = np.empty((x.shape[0], x.shape[0]))\n    for i in range(A.shape[0]):\n        A[:, i] = x ** i\n    c = np.linalg.solve(A, y)\n    return c\nc = find_fit(x, y)\nprint(\"y = \" + \" + \".join([str(int(round(c[i]))) + \"x^\" + str(i) for i in range(c.shape[0])]))\n#&gt;&gt; y = -1x^0 + 5x^1 + -4x^2\n</code></pre> <pre><code>xl = np.arange(-2.5, 2.5, 0.01)\nyl = sum([c[i] * (xl ** i) for i in range(c.shape[0])])\nplt.plot(xl, yl)\nplt.scatter(x, y)\nplt.savefig(\"assets/interpolations_1.jpg\")\n</code></pre> <p></p>"},{"location":"csc336/interpolations.html#conditioning","title":"Conditioning","text":"<p>Often Vandermonde Matrices are very badly conditioned. When \\(x_1\\neq x_2\\) but \\(x_1\\approx x_2\\), then first two rows of \\(A\\) are almost identical, resulting close to singular and bad conditioning. </p>"},{"location":"csc336/interpolations.html#properties-of-polynomials","title":"Properties of Polynomials","text":"<ul> <li>easy differentiation and integral</li> <li>efficient evaluation</li> </ul> <p>We can evaluate by </p> <pre><code>n=2\nx_k = 1\nfor k in range(1, n):\n    x_k *= x\n    p += c[k] * x_k\n</code></pre> <p>Or we can even improve by noticing that \\(p(x) = c_0 + c_1x + c_2x^2 = c_0 + (c_1 + x(c_2 + x))\\)</p> <pre><code>p = c[n]\nfor k in range(n-1, 0, -1):\n    p = c[k] + x * p\n</code></pre>"},{"location":"csc336/interpolations.html#lagrange-form-of-the-interpolation-polynomial","title":"Lagrange Form of the Interpolation Polynomial","text":"<p>WTF \\(p(x_i) = y_i\\) for \\(i = 1, 2,...,n\\) and \\(p\\) has \\(n-1\\) degree. \\(x_i\\) are distinct. </p> <p>Define the Lagrange basis functions </p> \\[l_j(x) = \\prod_{i=0, i\\neq j}^n \\frac{x-x_i}{x_j-x_i}\\] <p>Note that \\(l_j(x_i) = \\mathbb I(i=j)\\) Then \\(p(x) = \\sum_{j=1}^n l_j(x)y_j\\)</p> <p>Note that for each \\(l(x)\\), there are at most \\(n-1\\) times of \\(x\\) multiplied together, hence at most \\(n-1\\) degree.</p> <p>Consider \\(p(x_i) = \\sum l_j(x)y_j\\), because the indicator property of \\(l\\), only \\(i_i(x_i) = 1\\) and others are all 0, therefore, the result is \\(l_i(x) y_i + \\sum_{j\\neq i} l_j(x)x_j = y_i + \\sum 0 =  y_i\\)</p> <p>Given the same example \\(p(-2) = -27, p(0) = -1, p(1) = 0\\)</p> \\[\\begin{align*} l_1(x) &amp;= \\frac{(x)(x-1)}{(-2)(-3)} = \\frac{x(x-1)}{6}\\\\ l_2(x) &amp;= \\frac{(x+2)(x-1)}{(2)(-1)} = \\frac{-(x+2)(x-1)}{2}\\\\ l_3(x) &amp;= \\frac{(x+2)(x)}{(3)(1)} = \\frac{(x+2)x}{3}\\\\ p(x) &amp;= \\frac{x(x-1)}{6}(-27) + -\\frac{(x+2)(x-1)}{2}(-1) + \\frac{(x+2)x}{3}(0) = -4x^2 +5x - 1 \\end{align*}\\] <pre><code>def indicator(x, j, x_array):\n    prod = 1\n    for i in range(len(x_array)):\n        if i != j:\n            prod *= ((x - x_array[i]) / (x_array[j] - x_array[i]))\n    return prod\n\ndef p_lagrange(x, x_array, y_array):\n    ret = 0\n    for i in range(len(x_array)):\n        ret += indicator(x, i, x_array) * y_array[i]\n    return ret\n\nyl = []\nfor e in xl:\n    yl.append(p_lagrange(e, x, y))\n\nplt.plot(xl, yl)\nplt.scatter(x, y)\nplt.savefig(\"assets/interpolations_2.jpg\")\n</code></pre> <p></p> <p>Consider the equation \\(p(x) = \\sum l_i(x)c_i\\), which the problem becomes the system of equations</p> \\[ \\underset{A}{\\begin{bmatrix} l_1(x_1)&amp;...&amp;l_n(x_1)\\\\ \\vdots &amp;\\ddots&amp;\\vdots\\\\ l_1(x_n)&amp;...&amp;l_n(x_n) \\end{bmatrix}} \\underset{c}{\\begin{bmatrix} c_1\\\\\\vdots\\\\c_n\\end{bmatrix}}  =  \\underset{y}{\\begin{bmatrix} y_1\\\\\\vdots\\\\y_n\\end{bmatrix}} \\] <p>Now, consider the property of \\(l\\), only the diagonal \\(l_i(x_i) = 1\\), i.e. \\(A = I, c_i = y_i\\). </p>"},{"location":"csc336/interpolations.html#computation","title":"Computation","text":"<p>Need almost no time to construct, while it's hard to use. </p>"},{"location":"csc336/interpolations.html#newton-form-of-interpolation-polynomials","title":"Newton Form of Interpolation Polynomials","text":""},{"location":"csc336/interpolations.html#basis-function","title":"Basis Function","text":"<p>Let \\(\\pi_1(x) = 0, \\pi_j(x) = \\prod_{i=1}^{j-1}(x-x_i)\\) for \\(j \\geq 2\\)</p> <p>Let \\(p_j(x) = \\sum_{i=1}^j c_i \\pi_i(x)\\). </p> <p>Assuming we have found \\(c_j\\)'s s.t. \\(p_j(x_i) = y_i\\) for \\(i\\in\\{1,...,j\\}\\) and we want to find \\(c_{j+1}\\) so that \\(p_{j+1}(x) := p_t(x) + c_{j+1}\\pi_{j+1}(x)\\) and \\(p_{j+1}(x_i) = y_i\\) for \\(i\\in \\{1,...,j+1\\}\\)</p> <p>Consider such recurrence relationship, \\(\\forall i &lt; j+1, \\pi_{j+1}(t_i) = 0\\) (since one of \\((x-x_i)\\) in \\(\\pi\\) must be 0). So that </p> \\[y_{j+1} = p_{j+1}(x_{j+1}) = p_j(x_{j+1}) + c_{j+1}\\pi_{j+1}(x_{j+1})\\Rightarrow c_{j+1} = \\frac{y_{j+1} - p_j(x_{j+1})}{\\pi_{j+1}(x_{j+1})}\\] <p>Define some \\(y\\) such that the following relationship holds</p> \\[c_j = y\\{t_1,...,t_j\\} = \\frac{y\\{t_2,...,t_j\\} - y\\{t_1,...,t_{j-1}\\}}{t_j - t_1}\\] <p>Define the divided difference table </p> \\(y[t_i]\\) \\(y[t_i, t_{i+1}]\\) \\(y[t_i, t_{i+1}, t_{i+2}]\\) \\(y[t_i, t_{i+1}, t_{i+2}, t_{i+3}]\\) \\(t_1\\) \\(y[t_1] = y_1\\) \\(y[t_1, t_2] = \\frac{y[t_2] - y[t_1]}{t_2 - t_1}\\) \\(y[t_1, t_2, t_3] = \\frac{y[t_2, t_3] - y[t_1, t_2]}{t_3 - t_1}\\) \\(y[t_1, t_2, t_3, t_4] = \\frac{y[t_2, t_3, t_4] - y[t_1, t_2, t_3]}{t_4 - t_1}\\) \\(t_2\\) \\(y[t_2] = y_2\\) \\(y[t_2, t_3] = \\frac{y[t_3] - y[t_2]}{t_3 - t_2}\\) \\(y[t_2, t_3, t_4] = \\frac{y[t_3, t_4] - y[t_2, t_3]}{t_4 - t_2}\\) \\(t_3\\) \\(y[t_3] = y_3\\) \\(y[t_3, t_4] = \\frac{y[t_4] - y[t_3]}{t_4 - t_3}\\) \\(t_4\\) \\(y[t_4] = y_4\\) <p>And we will use the first row to compute \\(c_j\\)'s. </p> <p>Such table needs around \\(n^2/2\\) operations</p> <p>Such divided difference table actually approximates the \\((n+1)\\)th derivative. </p> \\[y[t_1,...,t_n] = \\frac{y^{(n+1)}(\\epsilon)}{(n+1)!}, \\epsilon \\text{ belongs to some interval contains all }t\\]"},{"location":"csc336/interpolations.html#computation_1","title":"Computation","text":"\\[p(x) = \\sum_{i=1}^4 c_i \\bigg[\\prod_{j=1}^{i-1}(x-x_j)\\bigg] = c_1 + (t-t_1)(c_2 + (t-t2)(c_3 + c_4(t-t_3)))\\] <p>Therefore, for each iteration, \\(1\\) adds and \\(2\\) multiplications are performed. Where a total of \\(n-1\\) iterations are needed. </p>"},{"location":"csc336/interpolations.html#error-of-interpolations","title":"Error of Interpolations","text":"<p>Suppose \\(y_i = y(t_i)\\) for some smooth function \\(y\\), i.e. \\(y_i\\) are coming out of some underlying function model \\(y\\). We aim to measure \\(\\|y-p\\|_\\infty\\)</p> <p>Claim </p> \\[(t)-p(t) = \\frac{y^{(n)}(\\epsilon_t)}{n!}\\prod_{i=1}^n (t-t_i)\\] <p>where \\(\\epsilon_t\\) is in the smallest interval containing \\(t_1,...,t_n\\)</p> <p>Recall Rolle's Theorem Given continuous \\(\\phi'(t). \\phi(t_1) = \\phi(t_2) = 0\\Rightarrow \\alpha \\in (t_1, t_2). \\phi'(\\alpha) = 0\\)</p> <p>proof. Fix \\(t\\). Define \\(w(t) = \\prod (t-t_i)\\) Suppose \\(t\\in \\{t_i\\}\\). Then \\(y_{t_i} - p(t_i) = 0\\land \\prod (t - t_i) = 0\\) (one of it is 0). Suppose \\(t\\not\\in \\{t_i\\}\\), define function </p> \\[\\phi(s) := y(s)-p(s) - w(s)\\frac{y(t)-p(t)}{w_t}\\] <p>Then \\(\\phi(t)=0\\) and we have \\(n+1\\) distinct places where \\(\\phi(s)=0\\), hence \\(n\\) places where \\(\\phi'(s) = 0\\), ... \\(1\\) place where \\(\\phi^{(n)}(s) = 0\\) Notice that </p> \\[\\phi^{(n)}(s) = y^{(n)}(s) - p^{(n)}(s) - \\frac{y(t)-p(t)}{w(t)}w^{(n)}(s)\\] <p>However, \\(p\\) is a polynomial of degree \\(n-1, p^{(n)}(s) = 0\\) \\(w(s) = \\prod^n (s-t_i) = s^n + o(s^{n-1})\\Rightarrow w^{(n)}(s) = n!\\)  Therefore </p> \\[\\phi^{(n)}(s) = y^{(n)}(s) - \\frac{y(t)-p(t)}{w(t)}n!\\] <p>Let \\(\\phi^{(n)}(s) = 0, y(t)-p(t) = \\frac{w(t)y^{(n)}(s)}{n!}\\) as required </p> <p>Consider the Taylor expansion of \\(y\\), with Taylor's remainder and centered at \\(t_1\\), i.e. \\(y(t) = \\sum^n \\frac{y^{(i)}(t) (t-t_1)}{(n-1)!} +  \\frac{y^{(i)}(\\epsilon) (t-t_1)}{(n-1)!}\\) and \\(p\\) is now equal to the Taylor polynomial. </p>"},{"location":"csc336/interpolations.html#polynomial-interpolations-does-not-always-converge","title":"Polynomial Interpolations does not always converge","text":"<p>Consider Runge's Function \\(y(t) = (1+25 t^2)\\). Taken #n# evenly spaced points on \\([-1,1]\\) and interpolate \\(y(t)\\) by a polynomial. However, \\(y(t)-p(t)\\) does not converges. </p> <pre><code>x = np.arange(-1, 1, 0.01)\ny = 1/(1 + 25 * (x**2))\nplt.plot(x, y)\nplt.savefig(\"assets/interpolations_3.jpg\")\n</code></pre> <p></p>"},{"location":"csc336/interpolations.html#piece-wise-polynomial-interpolation","title":"Piece-wise Polynomial Interpolation","text":"<p>If we take several polynomials and assign each to a domain.</p>"},{"location":"csc336/interpolations.html#piece-wise-linear-polynomials","title":"Piece-wise linear polynomials","text":"<p>Suppose we want to interpolate on \\([a,b]\\), partition \\([a,b]\\) into \\(x_0, ..., x_n\\). Then, on each interval \\([x_i, x_{i+1}]\\), we have a linear polynomial \\(p_i(x) = \\frac{y_{i+1} - y_i}{x_{i+1}-x_i}(x-x_i) + y_i, x\\in[x_i, x_{i+1}]\\)</p> <p>Define \\(y(x)\\) be the underlying function, \\(s\\) be the collection of all \\(p_i\\), connected in order. Then </p> \\[\\|y-s\\|_\\infty \\leq \\frac{h^2}{4} \\max_{\\epsilon\\in[a,b]} \\frac{|y''(\\epsilon)|}{2} = \\frac{h^2}{8} \\max_{\\epsilon\\in[a,b]}|y''(\\epsilon)|, h = |x_{i+1}-x_i|\\]"},{"location":"csc336/lu_decomposition.html","title":"LU Decomposition","text":"<pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sn\n</code></pre> <p>Note that </p> \\[Ax = b \\Rightarrow M_{n-1}...M_2M_1Ax = M_{n-1}...M_2M_1b = \\hat b\\] <p>\\(M_i\\) are \\(n\\times n\\) matrices, </p> <p>Let \\(U = M_{n-1}...M_1A\\) where \\(U\\) is upper triangular. Then \\(Ux = \\hat b \\Leftrightarrow Ax = b\\) and since \\(U\\) is triangular, it's very easy to solve \\(Ux = \\hat b\\) (bottom up). Note that \\(U\\) must have all its diagonal being non-zero. </p> <p>Note that for each row, it takes \\(1\\) division and \\(n\\) multiplications and \\(n\\) additions. There's a total of </p> \\[n\\cdot \\text{DIV} + \\sum^n (i - 1)(\\text{ADD} + \\text{MULT}) = n\\cdot \\text{DIV} + \\frac{n(n-1)}{2}(\\text{ADD}+\\text{MULT})\\]"},{"location":"csc336/lu_decomposition.html#gaussian-elimination","title":"Gaussian Elimination","text":"<p>Denote the entries of all matrices in such</p> \\[A = \\begin{bmatrix}a_{11}&amp;...&amp;a_{1n}\\\\...&amp;...&amp;...\\\\a_{n1}&amp;...&amp;a_{nn}\\end{bmatrix}\\] <p>Consider \\(M_1\\) so that \\(M_1A\\) have the first column be \\((a_{11}, 0, ..., 0)^T\\). Let the diagonal of \\(M_1\\) be all \\(1\\), and the first column be \\(m_{i1}=-\\frac{a_{i1}}{a_{11}}, i = 1, 2,..,n\\) (note that \\(m_{11}=a_{11}/a_{11}=1\\)).</p> \\[M_1 = I - \\vec m_1\\vec e_1^T, \\vec m_1 = (0, \\frac{a_{21}}{a_{11}}, ..., \\frac{a_{n1}}{a_{11}})\\] <p>Then, for \\(M_2\\), \\(M_2A\\) have the first two columns be \\((a_{12}, a_{22}, 0, ..., 0)^T\\). Let \\(M_2\\) be the identity with the second column be \\(m_{i1}=-\\frac{\\hat a_{i2}}{a_{22}}, i = 2, 3,...,n\\)</p> \\[M_2 = I - \\vec m_2\\vec e_2^T, \\vec m_2 = (0, 0, \\frac{a_{32}}{a_{22}}, ..., \\frac{a_{n2}}{a_{22}})\\] <p>Therefore, we are recursively doing Gaussian elimination, and each time, we reduced one column and shrink the dimension by one. </p> <pre><code>D = 5\nM = []\nA = np.random.randint(-10, 10, (D, D))\nfig, ax = plt.subplots(figsize=(4, 4))\nax.imshow(A,cmap=\"bwr\", vmin=-10, vmax=10)\nfor (j,ii),label in np.ndenumerate(np.round(A, 2)):\n    ax.text(ii,j,label,ha='center',va='center', color=\"black\")\nplt.tight_layout()\nfig.savefig(\"assets/lu_decomposition_1.jpg\")\n</code></pre> <p></p> <pre><code>fig, axs = plt.subplots(D - 1, 2, figsize=(6, D*2.4))\nfor i in range(D - 1):\n    m = np.identity(D)\n    m[i+1:, i] -= A[i+1:, i] / A[i, i]\n    A = m @ A\n    axs[i][0].imshow(m, cmap=\"bwr\", vmin=-1, vmax=1)\n    for (j,ii),label in np.ndenumerate(np.round(m, 2)):\n        axs[i][0].text(ii,j,label,ha='center',va='center', color=\"black\")\n    axs[i][1].imshow(A, cmap=\"bwr\", vmin=-10, vmax=10)\n    for (j,ii),label in np.ndenumerate(np.round(A, 2)):\n        axs[i][1].text(ii,j,label,ha='center',va='center', color=\"black\")\nplt.tight_layout()\nfig.savefig(\"assets/lu_decomposition_2.jpg\")\n</code></pre> <p></p>"},{"location":"csc336/lu_decomposition.html#total-work","title":"Total work","text":"<p>Division: \\((n-1) + (n-2) + ... + 1 = \\frac{n(n-1)}{2}\\) Multi and Add: \\((n-1)^2 + (n-2)^2 + ... + 1 = \\frac{n(2n-1)(n-1)}{6}\\) Total: \\(\\frac{n^3}{3} + O(n^2)\\)</p>"},{"location":"csc336/lu_decomposition.html#compute-b-hat","title":"Compute b hat","text":"\\[\\hat b = M_{n-1}...M_{1}b\\] <p>Note that matrix matrix multiplication \\(M_iM_j\\) takes \\(O(n^3)\\) operations while matrix vector multiplication \\(M_ib\\) takes \\(O(n^2)\\) operations.</p> <p>Also, note that \\(M_1 = I - m_1e_1^T\\), </p> \\[M_1b = b - (\\vec m_1e_1^Tb) = b - \\vec m_1(e_1^Tb) = b - \\vec m_1b_1\\] <p>Only takes \\(n-1\\) additions and multiplications</p> <p>Similarly, </p> \\[M_2M_1b = M_2\\tilde b = \\tilde b - m_2\\tilde b_2\\] <p>Takes \\(n-2\\) additions and multiplications</p> <p>The total steps are \\(\\frac{n(n-1)}{2} = \\frac{n^2}{2} + O(n)\\) addition and multiplications</p> <p>If we want to solve \\(Ax = b, Ay = c\\), we can store \\(M_{n-1}...M_1A\\) so that we only have to compute \\(\\hat b, \\hat c\\) which takes much less time. (Since the most expensive work is to get \\(M_k\\)'s). </p>"},{"location":"csc336/lu_decomposition.html#properties-of-ms","title":"Properties of M's","text":"<p>Note that determinant of triangular matrices are the product of the diagonal See Proof</p> <p>Therefore, \\(M\\) are non-singular with \\(det(M)=1\\)</p> <p>Theorem \\(M_1^{-1} = (I + m_1e_1^T)\\)</p> <p>proof. </p> \\[\\begin{align*} MM_1^{-1} &amp;= (I-m_1e_1^T)(I+m_1e_1^T) \\\\ &amp;= I - Im_1e_1^T + Im_1e_1^T + (m_1e_1^T)(m_1e_1^T)\\\\ &amp;= I - m_1(e_1^Tm_1)e_1^T\\\\ &amp;= I - m_1\\quad [1,0,...,0]\\begin{bmatrix}0\\\\m_{21}\\\\...\\\\m_{n1}\\end{bmatrix}\\quad e_1^T\\\\ &amp;= I - m_10e_1^T\\\\ &amp;= I \\end{align*}\\]"},{"location":"csc336/lu_decomposition.html#lu-factorization","title":"LU Factorization","text":"<p>Want to find \\(L\\) being lower triangular and \\(U\\) upper triangular so that \\(A = LU\\). </p> <p>By Gaussian elimination we already have \\(U\\) so that we can have \\(A = M_1^{-1}...M_{n-1}^{-1}U\\), so that \\(L = M_1^{-1}...M_{n-1}^{-1}\\)</p> <p>By theorem above, we know \\(M_k = I-m_ke_k^T\\)</p> <p>Note that \\(M_1^{-1}...M_{n-1}^{-1} = I + \\sum_{i=1}^N m_ie_i^T\\)</p> <p>If you have a LU factorization of \\(A\\), then can easily solve \\(Ax=b\\Rightarrow LUx = b \\Rightarrow Ly = b\\) So that we can solve \\(Ly=b\\) then \\(Ux=y\\)</p>"},{"location":"csc336/lu_decomposition.html#runtime","title":"Runtime","text":"<p>Since \\(L\\) is lower triangular. we'll solve top down, and takes a total of \\(\\frac{n(n-1)}{2}\\) addition and multiplication. </p>"},{"location":"csc336/lu_decomposition.html#computing-determinant","title":"Computing Determinant","text":"<p>Consider \\(A_{kk}\\) be \\(A_{k:n \\times k:n}\\) so that the classical way of solving determinant will be expanding the determinant along a row/column</p> \\[\\det(A) = \\sum_{i=1}^n (-1)^{i-1}{a_{1i}} \\det(A_{ii})\\] <p>Let \\(t(n)\\) be the number of multiplications to compute \\(A_{n\\times n}\\) then  \\(f(n)=nf(n-1) +n \\geq n!\\)</p> <p>Theorem \\(\\det(AB) =\\det(A)\\det(B)\\)</p> <p>So that \\(\\det(A) = \\det(L)\\det(U) = 1 \\times diag(U)\\) Remember that computing \\(U\\) takes \\(\\frac{n^3}{3} + O(n^2)\\) times</p>"},{"location":"csc336/matrix_conditioning.html","title":"Conditioning of Linear Equation Systems","text":""},{"location":"csc336/matrix_conditioning.html#conditioning-for-ax-b","title":"Conditioning for Ax = b","text":"<p>Input \\(A,b\\), output \\(x\\)</p> <p>Assume \\(A_{n\\times n}\\) is non-singular (square matrix), suppose changing \\(b\\rightarrow \\hat b\\), denote \\(\\Delta b = b - \\hat b\\).  Consider \\(\\Delta x = x - \\hat x\\), where \\(A\\hat x = \\hat b\\). Having \\(\\frac{\\|\\Delta b\\|}{\\|b\\|}\\), WTF \\(\\frac{\\|\\Delta x\\|}{\\|x\\|}\\)</p> \\[\\|Ax\\| = \\frac{\\|Ax\\|\\|x\\|}{\\|x\\|} \\leq \\max_{x\\neq 0}\\frac{\\|Ax\\|\\|x\\|}{\\|x\\|} = \\|A\\|\\|x\\|\\] <p>So that</p> \\[\\|b\\| = \\|Ax\\| \\leq \\|A\\|\\|x\\|\\] \\[\\|\\Delta x\\| = \\|A^{-1}\\Delta b\\| \\leq \\|A^{-1}\\|\\|\\Delta b\\|\\] <p>Then, using the two inequalities</p> \\[\\frac{\\|\\Delta x\\|}{\\|A\\|\\|x\\|} \\leq \\frac{\\|A^{-1}\\|\\|\\Delta b\\|}{\\|A\\|\\|x\\|} \\leq \\frac{\\|A^{-1}\\|\\|\\Delta b\\|}{\\|b\\|}\\] \\[\\frac{\\|\\Delta x\\|}{\\|x\\|} \\leq \\|A\\|\\|A^{-1}\\|\\frac{\\|\\Delta b\\|}{\\|b\\|}\\] <p>Define \\(\\text{Cond}(A) = \\|A\\|\\|A^{-1}\\|\\)</p> <p>Also, note that \\(\\text{Cond}(A)^{-1} \\frac{\\|\\Delta b\\|}{\\|b\\|} &lt; \\frac{\\|\\Delta\\|}{\\|x\\|} \\leq \\text{Cond}(A)\\frac{\\Delta b\\|}{\\|b\\|}\\) And \\(\\text{Cond}(A)^{-1} \\leq \\text{Cond}(A)\\implies \\text{Cond}(A)^2 \\geq 1\\implies \\text{Cond}(A) \\geq 1\\)</p> <p>Also, noting that the lemma is only true if the norm induced by vector norm </p> <p>Theorem \\(\\forall A\\in \\mathbb R^n \\times \\mathbb R^n. \\text{Cond}(A) \\geq 1\\) (Another proof)</p> <p>proof. \\(1 = \\|I\\| = \\|A^{-1}A\\| \\leq \\|A^{-1}\\|\\|A\\| = \\text{Cond}(A)\\) by triangular inequality of the norm</p> <p>Theorem \\(\\frac{\\|\\Delta x\\|}{\\|x\\|} \\leq \\text{Cond}(A)\\big(\\frac{\\|\\Delta b\\|}{\\|b\\|} + \\frac{\\|\\Delta A\\|}{\\|A\\|}\\big)\\)</p> <p>For \\(2\\times 2\\) matrices </p> \\[A = \\begin{bmatrix} a&amp;b\\\\c&amp;d\\end{bmatrix}, A^{-1} = \\det(A)^{-1}\\begin{bmatrix}d&amp;-b\\\\-c&amp;a\\end{bmatrix}\\]"},{"location":"csc336/matrix_conditioning.html#example","title":"Example","text":"<p>Given \\(A = \\begin{bmatrix}3&amp;-1\\\\-2&amp;1\\end{bmatrix}\\) with \\(\\|\\cdot\\|_\\infty\\)</p> <p>Then, \\(A^{-1} = \\begin{bmatrix}1&amp;1\\\\2&amp;3\\end{bmatrix}\\), \\(\\|A\\|_\\infty = \\max(3 + 1, 2 + 1)=4, \\|A^{-1}\\| = 5\\) Therefore, \\(\\text{Cond}(A) = 4\\times 5 = 20\\), which is good-conditioning. </p> <p>Define \\(y:= A^{-1}x\\) so that \\(x\\neq 0 \\Leftrightarrow y\\neq 0\\) </p> \\[\\|A^{-1}\\| = \\max \\frac{\\|A^{-1}x\\|}{\\|x\\|} = \\max\\frac{\\|y\\|}{\\|Ay\\|} = \\max (\\frac{\\|Ay\\|}{\\|y\\|}^{-1}) = \\min(\\frac{\\|Ax\\|}{\\|x\\|})^{-1}\\] <p>So that </p> \\[\\|A\\|\\|A^{-1}\\| = \\frac{\\max(\\frac{\\|Ax\\|}{\\|x\\|})}{\\min(\\frac{\\|Ax\\|}{\\|x\\|})} = \\frac{\\max_{\\|x\\|=1}\\|Ax\\|}{\\min_{\\|x\\|=1}\\|Ax\\|}\\] <p>Is the ratio between how much \\(A\\) expands \\(x\\) and how much \\(A\\) contracts \\(x\\)</p>"},{"location":"csc336/matrix_conditioning.html#properties-of-conditioning-numbers","title":"Properties of Conditioning Numbers","text":"<p>\\(\\text{Cond}(A)\\geq 1\\)</p> <p>\\(\\text{Cond}(I) = \\|I\\|\\|I^{-1}\\| = 1\\)</p> <p>\\(\\text{Cond}(\\gamma A) = |\\gamma|\\|A\\||\\gamma^{-1}|\\|A^{-1}\\| = \\text{Cond}(A)\\)</p>"},{"location":"csc336/matrix_conditioning.html#diagonal-matrices","title":"Diagonal matrices","text":"<p>If \\(D\\) is a diagonal matrix of \\(diag = \\{d_1, d_2,...,d_n\\}, di\\neq 0\\), then </p> \\[\\text{Cond}_\\infty(D) = \\frac{\\max(diag)}{\\max(diag^{-1})} = \\frac{\\max(diag)}{\\min(diag)}\\]"},{"location":"csc336/matrix_conditioning.html#near-singular","title":"Near singular","text":"<p>If changing some elements of \\(A\\) by \\(A+E\\), then it will be singular.   </p> \\[\\frac{\\|E\\|}{\\|A\\|}\\propto \\text{Cond}(A)^{-1}\\] <p>So that the larger conditioning number indicates that \\(A\\) is closer to be near singular</p> <p>\\(\\det(A) = 0 \\iff A\\) is singular, If the \\(\\det\\) is small, then it may not be near singular (\\(\\det(\\gamma I) = \\gamma\\) can be arbitrarily small while still away from singular)</p>"},{"location":"csc336/matrix_conditioning.html#estimate-matrix-inverse","title":"Estimate matrix inverse","text":"<p>Note that computing \\(A^{-1}\\) requires \\(\\sim n^3\\) operations (by Gaussian elimination). To estimate \\(\\|A^{-1}\\|\\) for computing \\(\\text{Cond}(A)\\)</p> \\[y=Az\\implies z = A^{-1}y \\implies \\|z\\| = \\|A^{-1}y\\| \\leq \\|A^{-1}\\|\\|y\\|\\implies \\frac{\\|z\\|}{\\|y\\|} \\leq \\|A^{-1}\\|\\] <p>Therefore, choose a sequence of \\(z\\)'s to try to make \\(\\frac{\\|z\\|}{\\|y\\|}\\) as large as possible (iterative estimating)</p>"},{"location":"csc336/matrix_conditioning.html#rounding-errors","title":"Rounding errors","text":"<p>Define \\(\\Delta A = A - \\hat A, \\Delta b = b - \\hat b\\), then \\(\\frac{\\|\\Delta A\\|}{\\|A\\|}, \\|\\Delta b\\| \\approx \\epsilon_{mach}\\). Then </p> \\[\\frac{\\|x-\\hat x\\|}{\\|x\\|} \\approx \\text{Cond}(A)(2\\epsilon_{mach})\\] <p>Consider \\(Ax=b\\) computed to get \\(\\hat x\\), define residual \\(r := b - A\\hat x = b - \\hat b = \\Delta b\\) Define \\(\\Delta x= x - \\hat x\\), then, </p> \\[\\frac{\\|\\Delta x\\|}{\\|x\\|} \\leq \\text{Cond}(A)\\frac{\\|r\\|}{\\|b\\|}\\] <p>Nearly all good linear equation solvers have small \\(\\|r\\|\\) so it's only a matter of the conditioning number</p>"},{"location":"csc336/non_linear_equations.html","title":"Solve Non-linear equations","text":"<pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n</code></pre>"},{"location":"csc336/non_linear_equations.html#example","title":"Example","text":"<p>solve \\(f(x) = x^2 - 4\\sin(x)= 0\\), i.e. \\(x^2 = 4\\sin x\\)</p> <pre><code>plt.figure(figsize=(4, 3))\nx = np.arange(-4, 4, 0.01)\nsq = x ** 2\nsin = 4 * np.sin(x)\nplt.plot(x, sq, label=r\"$x^2$\")\nplt.plot(x, sin, label=r\"$4\\sin(x)$\")\nplt.legend()\nplt.tight_layout()\nplt.savefig(\"assets/non_linear_equations_1.jpg\")\n</code></pre> <p></p> <p>Noting that \\(f(\\pi/2) = (\\pi/2)^2 - 4(1) &lt; 0\\) and \\(f(2) = 2^2 - 4\\sin(2)&gt;0\\), by IVT, \\(\\exists x\\in(\\pi/2, 2), f(x)=0\\)</p>"},{"location":"csc336/non_linear_equations.html#solving-non-linear-equations","title":"Solving non-linear  Equations","text":"<p>For linear functions, it's easy to determine the number of roots, while for non-linear equations, this is harder. </p>"},{"location":"csc336/non_linear_equations.html#bisection-method","title":"Bisection Method","text":"<p>For continuous functions, using IVT, we can determine the minimum number of roots. Then, using this fact, we define a recursive algorithm </p> <ul> <li>find \\(a, b\\) s.t. \\(f(a)f(b)\\leq 0\\)</li> <li>take \\(m = (a+b)/2\\), if \\(f(a)f(m)\\leq 0\\), then the root \\(x\\in [a,m]\\), otherwise \\(f(b)f(m)\\leq 0, x\\in[m,b]\\)</li> </ul> bisection method<pre><code>init a, b &lt;= f(a)f(b) &lt;= 0\nwhile b - a &gt; tolerance:\n    m = (a + b) / 2\n    if f(a)f(m) &lt;= 0:\n        a = m\n    else:\n        b = m\n</code></pre> <p>By nested interval theorem, we are guaranteed to find one solution. </p>"},{"location":"csc336/non_linear_equations.html#fixed-point-method","title":"Fixed Point Method","text":"<p>A point \\(x^*\\) is a fixed point of a function \\(g\\) if \\(x^*=g(x^*)\\) </p>"},{"location":"csc336/non_linear_equations.html#contraction-mapping-theorem","title":"Contraction Mapping Theorem","text":"<p>A function \\(f:\\mathbb R^n \\rightarrow \\mathbb R^n\\) is a contraction if \\(\\exists \\gamma &lt; 1, \\|f(x)-f(y)\\| &lt; \\gamma \\|x-y\\|\\), then \\(\\exists x^*, x^* = g(x^*)\\), i.e. exists a fixed point. </p> <p>proof (MAT337 Banach Contraction Principle</p> <p>Therefore, fixed point method gives the algorithm</p> fixed point method<pre><code>init x[0] from domain\nx[1] = f(x[0])\ni = 1\nwhile abs( x[i+1] - x[i] ) &gt;= tolerance:\n    x[i+1] = f(x[i])\n    i += 1\n</code></pre>"},{"location":"csc336/non_linear_equations.html#rate-of-convergence","title":"Rate of Convergence","text":"<p>Consider any iterative method for computing non-linear function solution. \\(x^*\\) being the exact solution (fixed point in FPM), then define \\(e_k = x^* - x_k\\) assuming \\(x_k\\rightarrow x^*\\). Suppose </p> \\[\\lim_{k\\rightarrow\\infty} \\frac{|e^{k+1}|}{|e_k|^r} = c, c\\neq 0\\Rightarrow |e_{k+1}|\\approx c|e_k|^r\\] <p>Then the rate of convergence is defined as \\(r\\)</p> <ul> <li>\\(r=1: c&lt;1\\) - linear convergence  </li> <li>\\(r&gt;1\\):  super linear convergence</li> <li>\\(r=2\\):  quadratic convergence</li> </ul>"},{"location":"csc336/non_linear_equations.html#example_1","title":"Example","text":"<p>Suppose we start with \\(e_0 = 0.1\\)</p> <pre><code>k = np.arange(5)\ne = np.zeros((3, 5))\ne[:, 0] = 0.1\n\nfor i in range(1, 5):\n    e[0, i] = e[0, i-1] ** 1 * 1/2\n    e[1, i] = e[1, i-1] ** 1.5\n    e[2, i] = e[2, i-1] ** 2\nplt.figure(figsize=(4, 3))\nplt.plot(k, np.log10(e[0]), label=\"r=1, c = 0.5\");\nplt.plot(k, np.log10(e[1]), label=\"r=1.5, c = 1\");\nplt.plot(k, np.log10(e[2]), label=\"r=2, c = 1\");\nplt.legend()\nplt.title(r\"$\\log10(e_k)$\")\nplt.tight_layout()\nplt.savefig(\"assets/non_linear_equations_2.jpg\")\n</code></pre> <p></p>"},{"location":"csc336/non_linear_equations.html#newtons-method","title":"Newton's Method","text":"<p>Replacing the function \\(f(x)\\) by its tangent line at the current point of approximation \\(x^{(k)}\\), assuming the function is differentiable at \\(x^{(k)}\\), i.e. </p> \\[y = f(x^{(k)}) + f'(x^{(k)})(x-x^{(k)})\\] <p>Then approximate the root of \\(f(x)=0\\), i.e. the point where the graph of \\(f(x)\\) hits the \\(x\\)-axis, by the point where the graph of the tangent hits the \\(x\\)-axis. We are looking for the point \\(x\\) s.t. \\(y = 0\\). This will be the new approximation \\(x^{(k+1)}\\) to the root of \\(f\\). Thus</p> \\[0 = f(x^{(k)}) + f'(x^{(k)})(x^{(k+1)}-x^{(k)})\\Rightarrow x^{(k+1)} = x^{(k)}-\\frac{f(x^{(k)})}{f'(x^{(k)})}\\]"},{"location":"csc336/non_linear_equations.html#algorithm","title":"Algorithm","text":"<p>Newton's Method<pre><code>init x[0]\nfor k in range(n):\n    x[k] = x[k-1] - f(x[k-1]) / df(x[k-1])\n    if abs(f(x[k])) &lt; tolarence:\n        return x[k]      \n</code></pre> Note we don't use <code>while</code> loop because Newton's method does not always converge. However, it always converges if \\(f\\) is twice differentiable and \\(x^{(0)}\\) is chosen close enough to the root. Also, \\(f(x^{(k)}) / f'(x^{(k)})\\) should not be \\(0\\). </p>"},{"location":"csc336/non_linear_equations.html#rate-of-convergence_1","title":"Rate of Convergence","text":"<p>Note that Newton's Method can be seen as a fixed point method. </p> <p>Assume \\(f'(x^*)\\neq 0, f(x^*) = 0\\), let \\(g(x) = x - \\frac{f(x)}{f'(x)}\\Rightarrow \\frac{d}{dx}g = 1 - \\frac{f(x)f''(x)}{(f(x))^2}\\). Because \\(g'(x^*)=0\\) and \\(g''(x^*) \\neq 0\\) in most cases, \\(x_n\\rightarrow x^*\\Rightarrow \\lim_{n\\rightarrow\\infty} \\frac{x_{n+1}-x^*}{(xn-x^*)^2} = C\\neq 0\\) This is approximated as </p> \\[x_{n+1}-x^* \\approx C(x_n-x^*)^2 = (c(x_n-x^*))^2\\] <p>Theorem If \\(f^{(3)}(x)\\) exists and is continuous, \\(f(x^*) = 0, f'(x^*)\\neq 0\\), then Newton's method will converge quadratically to \\(x^*\\) provided close start. </p> <p>proof. </p> \\[\\begin{align*} x_{n+1} - x^* &amp;= g(x_n) - g(x^*)\\\\ &amp;= g'(x^*)(x_n - x^*) + \\frac1 2 g''(\\xi_n)(x_n-x^*)^2 &amp;\\xi_n\\in(x_n, x^*)\\\\ &amp;= \\frac1 2 g''(\\xi_n)(x_n-x^*)^2 &amp;g'(x^*) = 0 \\end{align*}\\] <p>Suppose \\(|x_0 - x^*| \\leq \\epsilon\\) for some \\(\\epsilon &gt; 0\\), let \\(a = \\max_{\\xi\\in[x^*-\\epsilon, x^*+\\epsilon]} |f''(\\xi)|/2\\), we need to have \\(a\\epsilon &lt; 1\\) (that's what close enough mean in the claim).  </p> <p>Then, </p> \\[|x_1 - x^*| \\leq \\frac{|g''(\\xi_0)|}{2}|x_0 - x^*|^2 \\leq a|x_0 - x^*|^2 \\leq a\\epsilon^2 = (a\\epsilon)\\epsilon &lt; \\epsilon\\] <p>Therefore, by induction, all points through Newton's method will converge quadratically. </p> <p>Note that close enough is a very important condition for Newton's method. Consider \\(f(x) = 1 + xe^{-x}\\)</p> <pre><code>def f(x):\n    return 1 + x * np.exp(-x)\ndef df(x):\n    return (1 - x) * np.exp(-x)\ndef Newton(x):\n    return x - f(x) / df(x)\n</code></pre> <pre><code>iter_ = 5\nn1 = [0.2] \nn2 = [1.1]\nfor i in range(iter_):\n    n1.append(Newton(n1[-1]))\n    n2.append(Newton(n2[-1]))\npd.DataFrame({\n    'n': np.arange(iter_ + 1),\n    'start = ' + str(n1[0]): n1,\n    'start = '+ str(n2[0]): n2\n})\n\n#&gt;&gt;   n  start = 0.2   start = 1.1\n#&gt;&gt;   0     0.200000  1.100000e+00\n#&gt;&gt;   1    -1.576753  4.214166e+01\n#&gt;&gt;   2    -1.045035  4.870894e+16\n#&gt;&gt;   3    -0.705991           inf\n#&gt;&gt;   4    -0.581505           NaN\n#&gt;&gt;   5    -0.567311           NaN\n</code></pre> <pre><code>x = np.arange(-2, 5, 0.01)\ny = f(x)\nplt.figure(figsize=(4, 3))\nplt.plot(x, y)\nn1 = np.array(n1)\nplt.scatter(n1, f(n1))\nplt.plot(n1, f(n1), ls=\":\")\nplt.axhline(y=0, ls=\":\")\nfor i in range(iter_):\n    plt.annotate(str(i), (n1[i], f(n1)[i]), size=14);\nplt.xlim(-2, 1)\nplt.tight_layout()\nplt.savefig(\"assets/non_linear_equations_3.jpg\")\n</code></pre> <p></p>"},{"location":"csc336/non_linear_equations.html#secant-method","title":"Secant Method","text":"<p>Note that when \\(f'(x) = 0\\) and \\(f(x)\\neq 0\\), things get somewhat messy with Newton's method. Also, computing derivative is not convenient/possible in all cases.</p> <p>Consider the approximation of \\(f'(x_n)\\) by \\(\\frac{f(x_n) - f(x_{n-1})}{x_n - x_{n-1}}\\), then we have </p> \\[x_{n+1} = x_n - \\frac{f(x_n)(x_n - x_{n-1})}{f(x_n) - f(x_{n-1})}= \\frac{f(x_n)x_{n-1} - f(x_{n-1}x_n)}{f(x_n) - f(x_{n-1})}\\] <p>Note that secant method will have the same issue as Newton's method for the convergence</p> <pre><code>def secant(l, f):\n    nomi = f(l[-1]) * (l[-1] - l[-2])\n    denom = f(l[-1]) - f(l[-2])\n    l.append(l[-1] - nomi / denom)\n</code></pre> <pre><code>l = [-1.5, 0.2]\nfor i in range(5):\n    secant(l, f)\nl = np.array(l)\nplt.figure(figsize=(4, 3))\nplt.plot(x, f(x))\nplt.scatter(l, f(l))\nplt.plot(l, f(l), ls=\":\")\nplt.axhline(y=0, ls=\":\")\nplt.xlim(-2, 1)\nfor i in range(7):\n    plt.annotate(str(i), (l[i], f(l)[i]), size=14)\nplt.tight_layout()\nplt.savefig(\"assets/non_linear_equations_4.jpg\")\n</code></pre> <p></p> <p>It is proven that the rate of convergence is \\(r = \\frac{1+\\sqrt 5}{2}\\approx 1.618\\), so that it is slower than Newton's method. </p>"},{"location":"csc336/norm.html","title":"Norms of vectors and matrices","text":""},{"location":"csc336/norm.html#norms","title":"Norms","text":"<p>Consider a linear system \\(Ax = b\\) where \\(A,b\\) are input.  </p> <p>To measure the conditioning we measure the norm of output </p> \\[\\begin{cases} |x|&amp;\\in\\mathbb R\\\\ \\sqrt{a^2+b^2} &amp;\\in \\mathbb C, a+bi\\\\ \\sqrt{\\sum^n x_i^2} = \\|x\\|_2 &amp;x\\in\\mathbb R^n \\end{cases}\\] <p>Claim \\(\\|x\\|_\\infty \\leq \\|x\\|_2\\leq \\sqrt n \\|x\\|_\\infty\\) proof. </p> \\[\\|x\\|_\\infty = \\sup|x_i| = |x_k| = \\sqrt{x_k^2} \\leq \\sqrt{\\sum x_i^2} = \\|x\\|_2\\] \\[\\|x\\|_2 =\\sqrt{\\sum x_i^2} \\leq \\sqrt{\\sum x_k^2} = \\sqrt n \\sqrt{x_k^2} = \\sqrt n k = \\sqrt n \\|x\\|_\\infty\\] <p>Claim reverse triangular inequality \\(|\\|x\\| - \\|y\\|| \\leq \\|x-y\\|\\) </p> \\[\\|x\\| - \\|y\\| = \\|y + x -y\\| - \\|y\\|\\leq \\|y\\| + \\|x - y\\| - \\|y\\| = \\|x-y\\|\\]"},{"location":"csc336/norm.html#norm-for-matrices","title":"Norm for matrices","text":"<p>One common way to get a matrix norm is from a vector norm \\(\\|A\\| = \\max_{x\\neq 0} \\frac{\\|A\\vec x\\|}{\\|\\vec x\\|}\\), i.e. how much \\(A\\) can expand \\(\\vec x\\).   </p> <p>Theorem \\(\\|A\\| = \\max_{x\\neq 0} \\frac{\\|Ax\\|}{\\|x\\|} = \\max_{\\|x\\|=1} \\|Ax\\|\\)</p> <p>proof. </p> \\[\\max_{x\\neq 0} \\frac{\\|Ax\\|}{\\|x\\|} = \\max\\frac{\\|A(x/\\|x\\|)\\|}{\\|x\\|/\\|x\\|} = \\max_{\\|x\\|=1}\\|Ax\\|\\] <p>Matrix norm is induced by / subordinate to the vector norm, </p> <p>Examples Given \\(A_{n\\times n}\\) </p> \\[\\|A\\|_1 = \\max \\frac{\\|Ax\\|_1}{\\|x\\|_1} = \\max_{j = 1...n} \\sum_{i=1}^n |A_{ij}| = \\text{max column sum}\\] \\[\\|A\\|_\\infty = \\max_{i = 1 ... n} \\sum_{j=1}^n |A_{ij}| = \\text{max row sum}\\] \\[\\|A\\|_2 = \\max\\{\\sqrt \\lambda\\| \\lambda := \\text{eigenvalues of }A^TA\\}\\] \\[\\|A\\|_F = \\sqrt {\\sum_i\\sum_j |a_{ij}|^2}\\] <p>Note that F-norm cannot be induced by vector norm. For example $|I|_F = \\sqrt n $   </p> <p>If a norm is subordinated from a vector norm, then   - it follows triangular inequality  - \\(\\|I\\| = \\max \\frac{\\|Ix\\|}{\\|x\\|} = 1\\)</p>"},{"location":"csc336/norm.html#norm-equivalence","title":"Norm Equivalence","text":"<p>For any vector and matrix norms \\(\\|\\cdot\\|_a, \\|\\cdot\\|_b\\),  </p> \\[\\forall n\\in\\mathbb Z^+. \\exists c_{1, n}, c_{2, n} &gt; 0. \\forall x\\in \\mathbb R^n . c_{1, n}\\|x\\|_a \\leq \\|x\\|_b \\leq c_{2, n} \\|x\\|_a\\]"},{"location":"csc336/pivoting.html","title":"Pivoting","text":""},{"location":"csc336/pivoting.html#permutation-matrices","title":"Permutation Matrices","text":"<p>Has exactly one \\(1\\) in each row and all others are \\(0\\) and the same for each column. i.e. a permutation of rows/columns of \\(I\\). </p> <p>If we want to interchange column 1 and column J, then let \\(E_{ij}\\) denote the zero matrix with the \\(ij\\)'s entry be \\(1\\). </p> \\[\\begin{align*} P &amp;= I - (e_i - e_j)(e_i-e_j)^T\\\\ &amp;= I - (e_ie_i^T - e_ie_j^T - e_je_i^T + e_je_j^T)\\\\ &amp;= I - E_{11} + E_{1j} + E_{j1} - E_{jj} \\end{align*}\\] <p>Is a identity matrix that interchange rows \\(i,j\\). </p> <p>So that \\(PA\\) interchanges row of \\(A\\) and \\(AP\\) interchanges column of \\(A\\) </p> <p>A permutation matrix is non-singular and \\(P^{-1} = P^T\\)</p> <p>\\(P\\) is closed under multiplications: By definition, there is only one \\(1\\) on each row and column of \\(P_1\\), so that consider \\(P_2\\), which only interchanges rows, and the resulted matrix will still have only one \\(1\\) on each row, and since columns are not interchanged, the property of permutation is maintained. </p>"},{"location":"csc336/pivoting.html#pivoting-row-partial-pivoting","title":"Pivoting (Row partial pivoting)","text":""},{"location":"csc336/pivoting.html#intuition","title":"Intuition","text":"<p>Consider \\(A =\\begin{bmatrix}\\epsilon&amp;1\\\\1&amp;1\\end{bmatrix}, \\epsilon\\) is small, \\(b = \\begin{bmatrix}1\\\\2\\end{bmatrix}\\). Then the multiplier \\(m_{21} = \\epsilon^{-1}\\). The decomposition \\(L = \\begin{bmatrix}1&amp;0\\\\\\epsilon^{-1}&amp;1\\end{bmatrix}, U = \\begin{bmatrix}\\epsilon&amp;1\\\\0&amp;1-\\epsilon^{-1}\\end{bmatrix}\\).  However, if \\(\\epsilon\\) is very small, \\(\\epsilon^{-1}\\) is extremely large and \\(U\\rightarrow \\tilde U =\\begin{bmatrix}\\epsilon&amp;1\\\\0&amp;-\\epsilon^{-1}\\end{bmatrix}\\), and \\(L\\tilde U = \\begin{bmatrix}\\epsilon&amp;1\\\\1&amp;0\\end{bmatrix}\\)</p> <p>Note that \\(A^{-1} = (-1+\\epsilon)^{-1} \\begin{bmatrix}1&amp;-1\\\\-1&amp;\\epsilon\\end{bmatrix}, \\tilde A = L\\tilde U = -1\\begin{bmatrix}0&amp;-1\\\\-1&amp;\\epsilon\\end{bmatrix}\\). If we want to solve \\(Ax = [1,2]^T, x = A^{-1}b\\approx [1,1]^T, \\tilde x = \\tilde A^{-1}b \\approx [2, 1]^T\\)</p> <p>However, if \\(A =  \\begin{bmatrix} 1&amp;1\\\\ \\epsilon&amp;1 \\end{bmatrix}, b = \\begin{bmatrix}2\\\\1\\end{bmatrix}\\Rightarrow m_{21} =\\epsilon, M_1 = \\begin{bmatrix} 1&amp;0\\\\ -\\epsilon&amp;1 \\end{bmatrix} \\rightarrow U = \\begin{bmatrix} 1&amp;1\\\\ 0&amp;1 \\end{bmatrix}, M_1 b = \\begin{bmatrix} 2\\\\1 \\end{bmatrix}, x = [1,1]^{-1}\\)</p>"},{"location":"csc336/pivoting.html#algorithm","title":"Algorithm","text":"<p>Searching first column for element of largest magnitude</p> \\[A_{J\\cdot}, J = arg\\max_{1\\leq j \\leq n}\\{|a_{j1}|\\}\\] <p>Then use a permutation matrix \\(P_1\\) to interchange \\(A_J\\cdot\\) with \\(A_1\\cdot\\) and calculate \\(M_1\\) so that \\(|m_{i1}|\\leq 1\\) So that we can do such recursively on \\(A_{1:n\\times 1:n}\\) and so on</p> <p>Therefore, we have \\(M_{n-1}P_{n-1}...M_1P_1A\\)</p>"},{"location":"csc336/pivoting.html#decompose-pivoted-matrix","title":"Decompose pivoted matrix","text":"<p>Consider \\(MPA = U\\Rightarrow PA = M^{-1}U\\), since the pivot, all \\(m_{ij}\\leq 1\\) so that \\(L\\) has all the lower triangular entries be \\(|L_{ij}|\\leq 1\\)</p>"},{"location":"csc336/pivoting.html#solving-linear-systems","title":"Solving linear systems","text":"<p>Given \\(Ax =b\\), since \\(P\\) is non-singular \\(PAx = Pb\\), Let \\(\\hat b = Pb, LU x = \\hat b\\), and we solve it as before. </p>"},{"location":"csc336/pivoting.html#example","title":"Example","text":"\\[A = \\begin{bmatrix}-2&amp;10&amp;1\\\\1&amp;-4&amp;2\\\\4&amp;-8&amp;4\\end{bmatrix}, b = \\begin{bmatrix}4\\\\3\\\\1\\end{bmatrix}\\] <p>Let \\(P_1 = \\begin{bmatrix}0&amp;0&amp;1\\\\0&amp;1&amp;0\\\\1&amp;0&amp;0\\end{bmatrix}, P_1A = \\begin{bmatrix}4&amp;-8&amp;4\\\\1&amp;-4&amp;2\\\\-2&amp;10&amp;1\\end{bmatrix}, M_1 = \\begin{bmatrix}1&amp;0&amp;0\\\\-1/4&amp;1&amp;0\\\\1/2&amp;0&amp;1\\end{bmatrix}, M_1P_1A = \\begin{bmatrix}4&amp;-8&amp;4\\\\0&amp;-2&amp;1\\\\0&amp;6&amp;3\\end{bmatrix}\\) \\(P_2 = \\begin{bmatrix}0&amp;1\\\\1&amp;0\\end{bmatrix}, M_2=\\begin{bmatrix}1&amp;0\\\\1/3&amp;1\\end{bmatrix}, M_2P_2M_1P_1A = \\begin{bmatrix}4&amp;-8&amp;4\\\\0&amp;6&amp;3\\\\0&amp;0&amp;2\\end{bmatrix} = U\\)</p>"},{"location":"csc336/pivoting.html#finding-pa-lu","title":"Finding PA = LU","text":"<p>Noting that \\(P_{k+1}M_k \\neq M_kP_{k+1}\\), but suppose we have \\(P_{k+1}M_k = \\hat M_k P_{k+1}\\), then we can have </p> \\[M_nP_n...M_1P_1A = b \\Rightarrow P_n...P_1A = \\hat M_1^{-1}...\\hat M^{-1}_n b\\] <p>Since \\(P^T = P^{-1}, P_2M_1P_2^T = \\hat M_1 P_2P_2^T = \\hat M_1P_2P_2^{-1} = \\hat M_1\\) </p> \\[\\hat M_1 = P_2(I-m_1e_1^T)P_2^T = P_2P_2^T - P_2m_1e_1^TP_2^T = I - (P_2m_1)(P_2e_1)^T\\] <p>By the design of the algorithm, \\(P_2\\) will not interchange the first columns, hence \\(P_2e_1 = e_1\\), but \\(P_2m_1\\) will interchange the multipliers, let \\(\\hat m_1 = P_2m_1\\).  </p> <p>Also, consider the intuition behind: \\(M_1P_2^T\\) only change \\([2:n]\\) columns, which does not touch the first column (\\(m\\)'s), \\(P_2M_1\\) cannot change the first row, but the order of \\(m\\)'s. Then consider \\(M_1[2:n, 2:n] = I_{(n-1)\\times (n-1)}\\), there's only \\(1\\) on the diagonal,  interchanging \\(i,j\\) row plus interchanging \\(i,j\\) column will just swap them back. Therefore, the only change thing is the order of \\(m\\)'s, i.e. \\(m_i, m_j\\) are swapped. </p> <p>Therefore, because \\(m_1e_1^Tm_2e_2^T = 0\\) (see notes on LU decomposition)</p> \\[\\hat M_1^{-1} = I + \\hat m_1e_1^T, M_1^{-1}M_2^{-1} = (I + \\hat m_1e_1^T)(I + \\hat m_2e_2^T) = I + \\hat m_1e_1^T + \\hat m_2e_2^T\\] <p>Therefore, \\(L = I + \\sum \\hat m_k e_k^T, P = \\prod P_k\\)</p>"},{"location":"csc336/pivoting.html#solving-system-of-linear-equations","title":"Solving system of linear equations","text":"<p>Let \\(\\hat b = Pb\\), since we have computed \\(PA=LU, PAx = Pb\\Rightarrow LUx = \\hat b\\Rightarrow Ly = \\hat b\\)</p> <p>You can just store \\(P\\) as a vector form, and we know we only need to interchange \\((n-1)\\) times. </p>"},{"location":"csc336/pivoting.html#find-determinant","title":"Find Determinant","text":"<p>Note that \\(P_k\\) is either identity or interchange two rows, \\(\\det(I)=1\\), but if we interchange two rows \\(\\det(P_k)=-1\\).   Considered the matrix where the interchanged 1's are on the corner, for such matrix, determinant is \\(-1\\) and for the other parts, it's \\(1\\). </p> \\[\\begin{align*} \\det(PA) &amp;= \\det(LU)\\\\ \\det(P)\\det(A)&amp;=\\det(L)\\det(U)\\\\ \\det(A) &amp;= \\det(P)^{-1}{\\det(L)\\det(U)}\\\\ &amp;=\\det(P)^{-1}[1][\\prod_i^n u_{kk}]\\\\ &amp;= \\prod_i^n u_{kk} * (-1)^{\\text{number of interchanges} \\% 2} \\end{align*}\\]"},{"location":"csc336/pivoting.html#computation-time","title":"Computation Time","text":"<p>Consider \\(A_{n\\times n}\\) </p> <ul> <li>Find the largest magnitude in the first column takes \\(n\\) comparisons  </li> <li>Swap the two rows takes \\(n\\) swaps. </li> <li>Then we are doing the normal Gaussian elimination steps, which takes \\(n-1\\) divisions and \\((n-1)^2\\) addition and multiplications. </li> </ul> <p>Notice that the next stage will just work recursively on \\(A_{n-1\\times n-1}\\).  Therefore, the total comparisons and swaps will be \\(\\frac{n(n-1)}{2}\\), Gaussian elimination takes \\(\\frac{n^3}{3} + O(n^2)\\)</p>"},{"location":"csc336/pivoting.html#solving-x","title":"Solving x","text":"<p>Then consider \\(Ux = Lb\\)  - Since \\(P\\) matrix only swaps, each \\(P_i\\) takes 1 swap  - \\(M_i\\) takes \\((n-i)\\) adds and multiplications</p> <p>Therefore, the total computation takes \\(\\frac{n^2 - n}{2}\\) adds and multiplications and \\((n-1)\\) swaps. </p>"},{"location":"csc343/fd.html","title":"Functional Dependency","text":"<p>Let \\(R\\) be a relation, \\(X,Y\\) subset of attributes of \\(R\\)</p> <p>X determines Y assets that if two tuples agree on all the attributes in set \\(X\\), they much also agree on all the attributes in set \\(Y\\). \\(X\\rightarrow Y: \\forall t_1,t_2. t_1[A]=t_2[A]\\rightarrow t_1[B]=t_2[B]\\)</p> <p>\\(A_1...A_n \\rightarrow B_1...B_m: \\forall t_1,t_2.t_1[A_1]=t_2[A_1]\\land ...\\land t_1[A_n]=t_2[A_n]\\rightarrow t_1[B_1]=t_2[B_1]\\land ...\\land t_1[B_m]=t_2[B_m]\\) </p> <p>We can't break the LHS and get multiple FDs, but we can break RHS.</p> <p>FD are closely related to keys Suppose K is a set of attributes for relation R K is a superkey for R IFF K functionally determines all of R</p> <p>Inferring FDs  Given a set of FDs we can often infer further FDs Example: \\(A\\rightarrow B \\land B\\rightarrow C \\Rightarrow A\\rightarrow C\\)</p>"},{"location":"csc343/fd.html#methods-for-inferring-fds","title":"Methods for Inferring FDs","text":"<p>Proven an FD follows using the Closure Test - Assume your know the values of the LHS attributes, and figure out everything else that is determined Attribute-closure(Y, S)<pre><code>\"\"\" Y: set of attributes\n    S: set of FDs\n\"\"\"\nY+ = Y\nwhile more changes occur:\n    if exists FD LHS-&gt;RHS in S s.t. LHS in Y+:\n        Y+ += YHS\nreturn Y+\n</code></pre></p> <p>Follow Algorithm FD_follows(S, LHS-&gt;RHS)<pre><code>Y += Attribute-closure(LHS, S)\nreturn RHS is in Y+\n</code></pre></p> <p>Projection algorithm project(S, L)<pre><code>T = []\nfor each subset X of L:\n    Compute X+\n    for attribute A in X+:\n        if A in L: \n            T += [X-&gt; A]\nreturn T\n</code></pre> (Too expansive)</p> <p>Minimal Basis: to minimize FDs minimal_basis(S)<pre><code>split the RHS of each FD\nfor X-&gt;Y in FD where |X|&gt;=2:\n    remove any attribute from X that get an FD that follows from S\nfor f in FD:\n    if S-{f} implies f:\n        remove f from S\n</code></pre></p>"},{"location":"csc343/fd.html#fd-on-database-design","title":"FD on Database Design","text":""},{"location":"csc343/fd.html#decomposition","title":"Decomposition","text":"<p>decompose \\(R(A_1,...,A_n)\\) to two relations \\(R_1, R_2\\) where \\(R_1, R_2\\) are projections of \\(R\\) and \\(R_1\\bowtie R_2=R\\)</p> <p>Boyce-Codd Normal Form \\(\\forall X\\rightarrow Y\\in R\\) be nontrivial FD (\\(Y\\not\\subseteq X\\)) with \\(X\\) being the superkey (equv \\(X^+\\) contains all attributes). </p> <p>To find BCNF</p> <p>BCNF_decompose(R, F)<pre><code>\"\"\" R: relation\n    F: sets of FDs\n\"\"\"\nif X-&gt;Y in F violates BCNF:\n    compute X+\n    replace R by two relations:\n        R1 = X+ \n        R2 = R-((X+)-X)\n    project the FD's F onto R1 and R2\n    BCNF_decompose(R1, F)\n    BCNF_decompose(R2, F)\n</code></pre> NOTE</p> <p>If there are \\(\\geq 1\\) FD violates BCNF, there will probably be multiple results.</p>"},{"location":"csc343/fd.html#speed-ups","title":"Speed-ups","text":"<p>Only need to check whether the LHS of each FD is a superkey using the closure test</p> <p>When project FDs onto the new relation, check if the new relation violate BCNF because of this FD, abort the projection. </p>"},{"location":"csc343/fd.html#properties-of-decomposition","title":"Properties of decomposition","text":"<p>We want: 1. No anomalies 2. Lossless join, which be able to  - project the original relations onto the decomposed schema  - then reconstruct the original by joining, we should get back exactly the original tuples 3. Dependency preserved</p> <p>A lossy join may create new tuples </p> <p>BCNF guarantees 1 and 2, not 3. 3NF guarantees 2 and 3, not 1. However, 3NF guarantees minimal bases</p>"},{"location":"csc343/fd.html#third-normal-form-3nf","title":"Third Normal Form (3NF)","text":"<p>An attribute is prime if it is a member of any key</p> <p>\\(X\\rightarrow A\\) violates 3NF IFF X is not a superkey and A is not prime</p> 3NF_synthesis(F, L)<pre><code>\"\"\" F: set of FDs\n    L: set of attributes\n\"\"\"\n    construct a minimal basis M for F\n    for X-&gt;Y in M:\n        define a new relation with schema union(X,Y)\n        if no relation is a superkey for L:\n            add a relation whose schema is some key\n</code></pre> <p>Example R(A,B,C,D), \\(A\\rightarrow B, A\\rightarrow C\\).  - FD set is already a minimal basis - R1(A,B), R2(A,C) - No relation is a superkey for L: R3(A,D)</p>"},{"location":"csc343/fd.html#chase-test","title":"Chase Test","text":"<p>When a new set of relations is not generated from BCNF or 3NF, to check lossless condition.</p>"},{"location":"csc343/ra.html","title":"Relational Algebra","text":"<p>Example schema will be used for examples </p> <ul> <li>Relations: Movies(mID, title, director, year, length); Artists(aID, aName, nationality); Roles(mID, aID, character)</li> <li>Foreign key constraints: Roles[mID]\\(\\subseteq\\) Movies[mID]; Roles[aID]\\(\\subseteq\\) Artists[aID]</li> </ul>"},{"location":"csc343/ra.html#relational-algebra_1","title":"Relational algebra","text":"<ul> <li>The value of any expression is a relation</li> <li>Assumptions<ul> <li>Relations as sets (without duplicated rows)</li> <li>Every cell has a value</li> </ul> </li> <li>Operands: tables</li> <li>Operator  examples:<ul> <li>Choose only the rows wanted</li> <li>Choose only the columns wanted</li> <li>combine tables</li> </ul> </li> </ul>"},{"location":"csc343/ra.html#operators","title":"Operators","text":""},{"location":"csc343/ra.html#select-rows","title":"Select Rows","text":"<p>\\(\\sigma_c(R)\\): \\(R\\) table, \\(c\\) boolean expression</p> <ul> <li>The result is a relation with the same schema but with only the tuples satisfy \\(c\\)</li> <li>Select all British actors  \\(\\sigma_{\\text{nationality = 'British'}}(Actors)\\) </li> <li>Select all movies from 1970s \\(\\sigma_{1970\\leq year\\leq 1979}(Movies)\\)</li> </ul>"},{"location":"csc343/ra.html#project","title":"Project","text":"<p>\\(\\Pi_c(R)\\) slice vertically</p> <ul> <li>onto fewer attributes can remove key that makes duplicates possible, whenever duplicates happens, only one copy of each is kept  </li> <li>To perform multiple query together Example: find the names of all directors of movies from the 1970s \\(\\pi_{director}(\\sigma_{1970 &lt;year&lt;=1979}(Movies))\\)</li> </ul>"},{"location":"csc343/ra.html#cartesian-product","title":"Cartesian Product","text":"<p>\\(R_1\\times R_2\\) map two relations to a new relation with every combination of a tuple from \\(R_1\\) concatenated to a tuple from \\(R_2\\)</p> <ul> <li>Resulted schema is every attribute from \\(R_1\\) followed by \\(R_2\\) in order</li> <li>The resulted relation have \\(R_1.cardinality|\\times R_2.cardinality\\) tuples</li> </ul>"},{"location":"csc343/ra.html#natural-join","title":"Natural join","text":"<p>\\(R_1\\bowtie R_2\\) take the Cartesian product and select rows with the same attribute and value  that are in both relation to ensure equality on attributes, then project to remove duplicate attributes</p> <ul> <li>Natural join is commutative and associative</li> </ul>"},{"location":"csc343/ra.html#theta-join","title":"Theta Join","text":"<p>\\(R_1\\bowtie_{c} R_2:= \\sigma_c{R\\times S}\\)</p>"},{"location":"csc343/ra.html#assignment","title":"Assignment","text":"<p>\\(R:= Expression\\) or \\(R(A_1,...,A_n):=Expression\\), the second way allows to rename all the attributes</p> <ul> <li>\\(R\\) must be temporary and not one of the relations in the schema, it should not be updated</li> </ul>"},{"location":"csc343/ra.html#rename","title":"Rename","text":"<p>\\(\\rho_{R_1}(R_2)\\) or \\(\\rho_{R_1(A_1,...,A_n)}(R_2)\\) renames the relation. Note that \\(R_1:=\\rho_{R_1(A_1,...,A_n)}(R_2)\\) is equivalent to \\(R_1(A_1,...,A_n):=R_2\\)</p>"},{"location":"csc343/ra.html#division","title":"Division","text":"<p>\\(R/S:=\\) the largest relation \\(Q\\) s.t. \\(Q\\times S\\subseteq R\\). the operation will return a relation will all the attributes in \\(R\\) that's not in \\(S\\) and all tuples in \\(R\\) that match every tuple in \\(S\\)</p>"},{"location":"csc343/rm.html","title":"Relational Model","text":""},{"location":"csc343/rm.html#relation","title":"Relation","text":"<p>Based on the concept of a relation or table. A mathematical relation is a subset of a Cartesian product</p> <ul> <li>Schema definition of the structure of the relation, name and constraint<ul> <li>Notation Example: Team have name, home field, coach, then the schema is <code>Team(Name, Home Field, Coach)</code></li> </ul> </li> <li>Instance particular data in the relation</li> <li>Conventional databases stores the current version of the data, temporal databases record the history</li> <li>Terminology: relation = table, attribute = column, tuple = row, arity = number of attributes, cardinality = number of rows</li> <li>Relations are sets (no duplication, order does not matter), while most commercial DBMS uses multi-sets (bag) rather than sets</li> </ul>"},{"location":"csc343/rm.html#constraint","title":"Constraint","text":"<ul> <li>Example: \\(\\forall\\) (<code>t1</code>, <code>t2</code>). <code>t1.name</code> \\(\\neq\\) <code>t2.name</code> or <code>t1.homefield</code> \\(\\neq\\) <code>t2.homefield</code></li> <li>Solution: set {home, homefield} as a superkey</li> <li>Superkey a set of &gt;=1 attributes whose combined values are unique</li> <li>Commonly, all database relations must have a defined super key, otherwise assign all attributes as the super key</li> <li>We want the minimal set of attributes as the superkey</li> <li>Primary Key if we choose one attribute to be the key, then in DBMS we can define it to the be primary key</li> <li>Notation \\(R[A]\\) the set of all tuples from \\(R\\), but with only the attributes in list \\(A\\)</li> <li>\\(R\\) relation</li> <li>\\(A\\) the list of attributes in R</li> </ul>"},{"location":"csc343/rm.html#reference","title":"Reference","text":"<ul> <li>foreign key refers to an attribute that is a key in another table.</li> <li>a way to refer to a single tuple in another relation.</li> <li>A foreign key may need to have several attributes</li> <li>Declare the foreign key constraints by \\(R_1[X]\\subseteq R_2[Y]\\)</li> <li>X, Y may be lists of the same arity</li> <li>These \\(R_1[X]\\subseteq R_2[Y]\\) relationships are a kind of referential integrity constraint or inclusion dependency.</li> <li>A foreign key must refer to a unique tuple, hence Y must be a key in \\(R_2\\)</li> </ul>"},{"location":"csc343/rm.html#designing-a-schema","title":"Designing a schema","text":"<ul> <li>Important goals: Represent the data well, avoid redundancy</li> </ul>"},{"location":"csc343/sql_def.html","title":"SQL Definitions Syntax","text":""},{"location":"csc343/sql_def.html#build-in-data-types","title":"Build in data types","text":"<p><code>CHAR(n)</code> fixed-length string of n char. Padd with blanks if necessary <code>VARCHAR(n)</code> variable length string of up to n characters <code>TEXT</code> variable-length, unlimited (not SQL standard) <code>INT</code> <code>FLOAT</code> <code>BOOLEAN</code> <code>DATA; TIME; TIMESTAMP</code></p> <p><code>'string'</code> must be surround by single quotes</p>"},{"location":"csc343/sql_def.html#user-defined-types","title":"User-defined types","text":"<pre><code>create domain Grade AS int\n    default null\n    check (value &gt;= 0 and value &lt;= 100);\n\ncreate domain Campus as varchar(4)\n    default 'StG'\n    check (value in ('StG', 'UTM', 'UTSC'))\n</code></pre> <p>Constraints on a type are checked every time a value is assigned to an attribute of that type.</p> <p>Fault value when no value has been specified. We can run a query and insert the resulting tuples into a relation even if the query does not give values for all the attributes in the relation if the types of the missing attributes have default values.</p> <p>Table attributes can also have default values.</p> <p>The difference  - attribute default: for that one attribute in that one table  - type default: for every attribute defined to be of that type</p>"},{"location":"csc343/sql_def.html#keys-and-foreign-keys","title":"Keys and Foreign Keys","text":""},{"location":"csc343/sql_def.html#primary-key","title":"Primary Key","text":"<p><code>PRIMARY KEY</code> for one or more attributes in a relation means  - the attributes form a key  - their values will never be null</p> <p>Every table must have 0 or 1 primary key</p> <p>example <pre><code>create table T1 (\n    ID integer primary key,\n    name varchar(25)\n);\n\n\n-- only way for multi-attribute keys\ncreate table Blah(\n    ID integer,\n    name varchar(25),\n    primary key(ID)\n);\n</code></pre></p>"},{"location":"csc343/sql_def.html#unique-key","title":"Unique Key","text":"<p><code>unique</code> for one or more attributes  - form a key - value can be null (different from <code>primary key</code>)</p> <p>Can declare more than one set of attributes to be <code>unique</code></p> <pre><code>create table T1 (\n    ID integer unique,\n    name varchar(25)\n);\n\n\n-- only way for multi-attribute keys\ncreate table Blah(\n    ID integer,\n    name varchar(25),\n    unique (ID)\n);\n</code></pre> <p>For uniqueness constraints, no two nulls are considered equal</p> <ul> <li>Set the attributes by unique, then you can't insert two <code>('A', 'B')</code>, but you can insert two <code>(null, 'B')</code></li> </ul>"},{"location":"csc343/sql_def.html#foreign-key","title":"Foreign key","text":"<pre><code>foreign key (sID) references Student\n</code></pre> <p>every value for sID in this table must actually occur in the Student table and sID must be <code>primary key</code> or <code>unique</code></p> <p>Can be declare with the attribute or as a separate table element. Can reference attributes that are not the primary key as long as they are unique; just name them</p> <pre><code>create table People (\n    SIN int primary key,\n    name text, \n    OHIP text unique\n);\n\ncreate table Volunteers (\n    email text primary key,\n    OHIPnum text references People(OHIP)\n);\n</code></pre>"},{"location":"csc343/sql_def.html#enforce-foreign-key-constraints","title":"Enforce foreign-key constraints","text":""},{"location":"csc343/sql_def.html#check-constraints","title":"\"check\" constraints","text":"<pre><code>create domain Grade as smallint\n    default null\n    check (value &gt;= 0 and value &lt;= 100)\n</code></pre> <p>define on attribute, tuples of a relation, and across relations.</p>"},{"location":"csc343/sql_def.html#attribute-based","title":"Attribute-based","text":"<ul> <li>Defined with a single attributes. constrains its value in every tuple</li> <li>can only refer to that attribute</li> <li>can include a subquery</li> </ul> <pre><code>create table Student (\n    sin int,\n    program varchar(5) check (program in (select post from O)),\n    firstName varchar(15) not null\n);\n</code></pre> <p>checked only when insert a tuple, or update attribute value</p> <p><code>not null</code> is very commonly used</p>"},{"location":"csc343/sql_def.html#tuple-based","title":"Tuple-based","text":"<ul> <li>defined as a separate element of the table schema, so can refer to any attributes of the table</li> <li>The condition to be checked can be anything that could go in a <code>WHERE</code>, and can include a subquery</li> </ul> <pre><code>create table Student (\n    sID int,\n    age int, \n    year int,\n    college varchar(4),\n\n    check (year = age - 18),\n    check college in (select name from Collges)\n);\n</code></pre> <p>Only when a tuple is inserted, or it updated </p> <p><code>check</code> only fails if it evaluates to <code>false</code>, (different from <code>where</code> which only evaluates to <code>true</code>)</p> <p>Problem with <code>null</code>, the only way to prevent <code>null</code> is <code>not null</code></p> <p>name constraint <pre><code>constraint XXX check (...);\nconstraint XXX foreign key (cNum, dept) references Course\n</code></pre></p>"},{"location":"csc343/sql_def.html#assertions","title":"Assertions","text":"<p><code>check</code> constraints apply to an attribute or table but they can't express constraints across tables  </p> <p><code>assertions</code> can express cross-table constraints <code>create assertion (&lt;name&gt;) check (&lt;predicate&gt;);</code></p> <p><code>assertions</code> are costly because they have to be checked upon every database update and each check can be expensive</p>"},{"location":"csc343/sql_def.html#triggers","title":"Triggers","text":"<p>cross-table constraints, as powerful as assertions, but can control the cost by having control over when they are applied</p> <p>specify - event: some type of database action  <pre><code>after delete on Courses \n-- or\nbefore update of grade on Took\n</code></pre> - condition: bool-valued expression <pre><code>when grade &gt; 95\n</code></pre></p> <ul> <li>action: any SQL statements <pre><code>insert into Winners values (sID)\n</code></pre></li> </ul>"},{"location":"csc343/sql_def.html#reaction-policies","title":"Reaction Policies","text":"<p><code>cascade</code> propagate the change to the referring table <code>set null</code> set the referring attribute(s) to null</p> <p>Suppose table R refers to table S We can define \"fixes\" that propagate changes backwards from S to R We cannot define fixes that propagate forward from R to S</p> <p>Add your reaction policy where you specify the foreign key constraint</p> <pre><code>create table Took (\n    ...\n    foreign key (sID) references Student on delete cascade\n    ...\n);\n</code></pre> <p>You can react to ... - <code>on delete</code> - <code>on update</code> - or both</p> <p>Policy can specify  - <code>restrict</code>: Don't allow deletion/update - <code>cascade</code>: make the same deletion/update in the referring tuple - <code>set null</code>: Set the corresponding value in the referring tuple to <code>null</code></p>"},{"location":"csc343/sql_def.html#update-schema","title":"Update Schema","text":"<ul> <li>Alter: alter a domain or table</li> </ul> <pre><code>alter table Course\n    add column numSections int;\n\nalter table Course\n    drop column breadth;\n</code></pre> <ul> <li>Drop: remove a domain, table, or whole schema <pre><code>drop table Course;\n-- Course still exists, but no content in it\n</code></pre></li> </ul> <p>-Delete: <pre><code>delete from Course\n-- Course does not exist\n</code></pre></p>"},{"location":"csc343/sql_def.html#schema","title":"Schema","text":"<p>Schemas let you create different name spaces. For logical organization and for avoiding name clashes</p> <p>A default schema <code>public</code> is available</p> <p>To create additional schema:</p> <pre><code>CREATE SCHEMA University;\n</code></pre> <p>To refer to things inside a schema</p> <pre><code>CREATE SCHEMA University.Student (...);\nSELECT * FROM University.student\n</code></pre>"},{"location":"csc343/sql_man.html","title":"SQL Data Manipulation Syntax","text":""},{"location":"csc343/sql_man.html#insert","title":"Insert","text":"<pre><code>INSERT INTO &lt;&lt;table&gt;&gt; VALUES &lt;&lt;list of rows&gt;&gt;\nINSERT INTO &lt;&lt;table&gt;&gt; (&lt;&lt;subquery&gt;&gt;)\n</code></pre> <p>If we name the attributes we are providing values for, the system will use <code>NULL</code> or a default for the rest</p> <p>Create a table before insert</p> <pre><code>CREATE TABLE Invite (\n    name TEXT,\n    campus TEXT DEFAULT 'StG',\n    email TEST,\n    age INT\n);\n\nINSERT INTO Invite(name, email)(\n   SELECT first, email\n   FROM Student\n   WHERE cgpa &gt; 3.4\n);\n</code></pre> <p>the query have values for name and email, campus gets the default value, age gets <code>NULL</code></p>"},{"location":"csc343/sql_man.html#delete","title":"Delete","text":"<pre><code>-- delete tuples satisfying WHERE clause\nDELETE FROM &lt;&lt;relation&gt;&gt;\nWHERE &lt;&lt;condition&gt;&gt;;\n\n-- delete all tuples\nDELETE FROM &lt;&lt;relation&gt;&gt;\n\n--   alternatively can use DROP, which destroy the table\nDROP TABLE &lt;&lt;relation&gt;&gt;\n</code></pre> <p>Example <pre><code>DELETE FROM Course\nWHERE NOT EXISTS (\n    SELECT *\n    FROM Took JOIN Offering ON Took.oid = Offering.oid\n    WHERE grade &gt; 50 AND\n    Offering.dept = Course.dept AND\n    Offering.num = Course.num\n);\n</code></pre></p>"},{"location":"csc343/sql_man.html#update","title":"Update","text":"<pre><code>UPDATE &lt;&lt;relation&gt;&gt;\nSET &lt;&lt;list of attribute assignments&gt;&gt;\nWHERE &lt;&lt;condition on tuples&gt;&gt;;\n</code></pre> <p>Updating one tuple <pre><code>UPDATE Student\nSET campus = 'UTM'\nWHERE sid = 99999;\n</code></pre> Update several tuple <pre><code>UPDATE Took\nSET grade = 50\nWHERE grade &gt;= 47 AND grade &lt; 50;\n</code></pre></p>"},{"location":"csc343/sql_man.html#manipulating-views","title":"Manipulating Views","text":"<p>Generally, it is impossible to modify a virtual view. </p> <p>We don't often (most systems prohibit) translate updates on views into equivalent updates on base tables.</p>"},{"location":"csc343/sql_man.html#materialized-views","title":"Materialized Views","text":"<p>Problem: each time a base table changes, the materialized view may change. Since cannot afford to recompute the view with each change</p> <p>Solution: periodic reconstruction of the materialized view, which is otherwise \"out of date\". </p>"},{"location":"csc343/sql_query.html","title":"SQL Query Syntax","text":""},{"location":"csc343/sql_query.html#basic-query","title":"Basic Query","text":"<p>Any query need SELECT FROM WHERE <pre><code>SELECT column names\nFROM Relation\nWHERE condition;\n</code></pre> With Multiple relationsA $\\times $ B <pre><code>From A, B\n</code></pre></p> <p>Temporarily renaming <pre><code>SELECT e.name, d.name\nFROM Employee e, department d\nWHERE d.name = 'marketing' AND e.name = 'Horton';\n</code></pre></p> <p>Self joins requires renaming <pre><code>SELECT e1.name, e2.name\nFROM Employee e1, Employee e2\nWHERE e1.salary &lt; e2.salary;\n</code></pre></p> <p>* in SELECT means all attributes of this relation <pre><code>SELECT *\n</code></pre></p> <p>Rename attributes  <pre><code>SELECT name AS new_name\n</code></pre></p> <p>Conditions allowed, note != is \\&lt;&gt;  <pre><code>| = | &lt;&gt; | &lt; | &gt; | &lt;= | &gt;= |\nAND | OR | NOT\n</code></pre></p> <p>Put tuples in order, add this as the final clause, default in ascending order, add DESC to force descending <pre><code>ORDER BY &lt;&lt;attribute list&gt;&gt; ;\nORDER BY &lt;&lt;attribute list&gt;&gt; [DESC];\n</code></pre> The attribute list can include expressions: <pre><code>ORDER BY attribute_a + attribute_b;\n</code></pre></p> <p>Keywords and identifiers are not case-sensitive and white space are ignored</p> <p>Expressions can be used in SELECT clauses,  operands: attributes, constants operators: arithmetic ops, string ops <pre><code>SELECT, sid, grade+10 as adjusted\nFROM Took;\n\nSELECT dept|| cNum\nFrom Course;\n</code></pre></p> <p>Expressions that are a constant <pre><code>SELECT, dept, cNum, 'satisfies' AS breadthRequirement\nFROM Course\nWHERE breadth;\n\nPattern operators\n```sql\n&lt;&lt;attribute&gt;&gt; LIKE &lt;&lt;pattern&gt;&gt;\n&lt;&lt;attribute&gt;&gt; NOT LIKE &lt;&lt;pattern&gt;&gt;\n</code></pre> <code>%</code> any string  <code>_</code> any string char <pre><code>SELECT *\nFROM Course\nWhere name LIKE '%Comp%';\n</code></pre></p>"},{"location":"csc343/sql_query.html#aggregation","title":"Aggregation","text":"<p><code>SUM, AVG, COUNT, MIN, MAX</code> can be applied to a column in a <code>SELECT</code></p> <p><code>COUNT(*)</code> counts the number of tuples </p> <p><code>DISTINCT</code> inside the brackets can stop duplicates from contributing to the aggregation. </p> <p><code>GROUP BY &lt;attributes&gt;</code> The tuples are grouped according to the values of those attributes and any aggregation give us a single value per group</p> <p>If any aggregation is used, then each element of the SELECT list must be aggregated or an attribute on the GROUP BY list. </p> <p><code>HAVING</code> decide which groups to keep.  <pre><code>...\nGROUP BY &lt;&lt;attributes&gt;&gt;\nHAVING &lt;&lt;condition&gt;&gt;\n</code></pre></p> <p>Outside subqueries, HAVING may refer to attributes only if they are either aggregated or an attribute on the GROUP BY list. </p> <p>Order of execution of a query  Writing query (execution order):  <pre><code>SELECT (5)\nFROM (1)\nWHERE (2)\nGROUP BY (3)\nHAVING (4)\nORDER BY (6)\n</code></pre></p>"},{"location":"csc343/sql_query.html#set-operations","title":"Set operations","text":"<p>A table can have duplicate tuples, unless it violate an integrity constraint. </p> <p>Set operations with Bags  Using Multiset operations rather than set operations <pre><code>(&lt;subquery&gt;)\nUNION / INTERSECT / EXCEPT\n(&lt;subquery&gt;)\n</code></pre> brackets are mandatory</p> <p>To force the result of a query to be a set, use <code>SELECT DISTINCT</code></p> <p>To force the set operations to be a bag, use <code>UNION/INTERSECT/EXCEPT ALL</code></p>"},{"location":"csc343/sql_query.html#views","title":"Views","text":"<p>Use virtual views to temporarily refer to the result of a query</p> <pre><code>CREATE VIEW xxx AS\nSELECT ...\nFROM ...\nWHERE ...\n</code></pre> <p>Break down a large query</p>"},{"location":"csc343/sql_query.html#joins","title":"Joins","text":"<p>Joins allowed in <code>FROM</code> <pre><code>R, S  == R CROSS JOIN \nR NATURAL JOIN\nR JOIN S ON &lt;CONDITION&gt;\n</code></pre></p> <p>Inner join is not the best practice since a working query can be broken by adding a column to a schema.</p>"},{"location":"csc343/sql_query.html#outer-join","title":"Outer Join","text":"<p>preserves dangling tuples by padding them with <code>NULL</code> in the other relation <pre><code>R LEFT OUTER JOIN S\nR RIGHT OUTER JOIN S\nR FULL OUTER JOIN S\n</code></pre></p> <p>When use <code>LEFT, RIGHT, FULL</code> it will be outer join, if not added, then inner</p>"},{"location":"csc343/sql_query.html#null","title":"NULL","text":"<p>Used for indicating missing value or inapplicable attribute</p> <p>Cant be checked by <code>IS NULL</code> and <code>IS NOT NULL</code>, e.x. <pre><code>SELECT *\nFROM Course\nWHERE br IS NULL;\n</code></pre></p> <p>Also, if one or both operands to a comparison is <code>NULL</code>, the truth value will always be <code>UNKNOWN</code></p> <p>NULL can be imagined to have value 0.5, compared to TRUE = 1, FALSE = 0 when consider logical operations</p> <p>Notice that <code>WHERE</code> will only pick rows with <code>TRUE</code></p>"},{"location":"csc343/sql_query.html#subqueries","title":"Subqueries","text":"<p><code>FROM</code> clause, but you must name the result and parenthesized. Hence can refer to it in the outer query</p> <pre><code>SELECT ...\nFROM A, \n   (SELECT *\n    FROM Offering\n    WHERE instructor='Horton') B\nWHERE A.oid = B.oid;\n</code></pre> <p><code>WHERE</code> clause if the subquery guaranteed to produce exactly one tuple. </p> <p>However, we can also use keywords <code>ANY/SOME</code> for existence, <code>ALL</code> for for all</p> <pre><code>x &lt;&lt;comparison&gt;&gt; ANY|ALL (&lt;&lt;subquery&gt;&gt;)\n</code></pre> <p><code>IN, NOT IN, EXISTS</code> is some syntax sugar</p> <pre><code>x IN (&lt;&lt;subquery&gt;&gt;) === x = SOME (&lt;&lt;subquery&gt;&gt;)\nx NOT IN (&lt;&lt;subquery&gt;&gt;) === x &lt;&gt; ALL (&lt;&lt;subquery&gt;&gt;)\nEXISTS (&lt;&lt;subquery&gt;&gt;) \\exists y\\in (&lt;&lt;subquery&gt;&gt;)\n</code></pre>"},{"location":"csc367/cuda.html","title":"CUDA Programming - Basics","text":""},{"location":"csc367/cuda.html#gpu-architecture-based-on-nvidia","title":"GPU Architecture (based on NVIDIA)","text":"<p>A GPU has a massive number of ALU (or other computation cores such as CUDA cores, Tensor cores, RT cores) and a few CU. </p> <p>Each core on GPU makes one thread, even though much slower than a CPU core. but GPU can have thousands of cores (3584 on RTX3060 vs. 8 on Ryzen5)</p> <p>The GPU (device) is connected to the CPU (host) through PCIe bus, GPU has its own memory system, which have significantly larger bandwidth (~300GB/sec GDDR6 vs. ~25GB/sec DDR4 3200), but smaller (12GB on RTX3060).</p> <p></p>"},{"location":"csc367/cuda.html#sm-streaming-multiprocessor","title":"SM: Streaming Multiprocessor","text":"<p>The core of one GPU is further divided into SMs (28 SMs for RTX3060). SM is the basic unit for arithmetic operations. The architecture design helps to help latency and manages the high throughput. Each SM has</p> <ul> <li>128 computing cores</li> <li>thousands of registers (or even more)</li> <li>cache system: <ul> <li>shared memory: fast interchange within SM</li> <li>constant cache: fast broadcast read-only constant memory</li> <li>texture cache: cache designed specifically for texture files (remember GPU is originally designed for graphics)</li> <li>L1 cache: cache for local to global memory</li> </ul> </li> <li>Warp scheduler: quickly switch between thread contexts, and issue instructions to groups of threads (warps)</li> <li>FP32 cores, Int cores, or even Tensor cores and RT cores</li> </ul>"},{"location":"csc367/cuda.html#cuda-basics","title":"CUDA Basics","text":"<p>Download CUDA Toolkit, which includes the compiler <code>nvcc</code>, profiling tool <code>nvprof</code>, debugger <code>cuda-gdb</code>. and all libraries for arithmetic applications (cuBLAS, cuFFT, cuRAND, cuSOLVER, cuSPARSE, etc.)</p>"},{"location":"csc367/cuda.html#cuda-c-typical-program","title":"CUDA C (Typical) Program","text":"<ol> <li>allocate memory on CPU (host)</li> <li>allocate memory on GPU (device)</li> <li>copy data from host to device</li> <li>launch kernels</li> <li>wait/sync for all threads</li> <li>copy data from device to host (not always necessary, eg. graphics output)</li> </ol>"},{"location":"csc367/cuda.html#communicate-between-host-and-device","title":"Communicate between host and device","text":"<p>The program need to transform data between host and device, since they have different memory system. </p> <pre><code>// allocate memory on the device, similar to how malloc is used\n__host__ __device__\ncudaError_t cudaMalloc(\n    void** devPtr, \n    size_t size\n);\n\n// Copies data between host and device.\n__host__ \n\u200bcudaError_t cudaMemcpy(\n    void* dst, \n    const void* src, \n    size_t count, \n    cudaMemcpyKind kind\n);\n// cudaMemcpyKind includes\ncudaMemcpyHostToDevice; \ncudaMemcpyDeviceToHost;\ncudaMemcpyDefault; // inferred from src and dst pointer\n</code></pre> <p>The most basic routine is similar to how we use heap memory. <pre><code>int input_cpu[N];\nint output_cpu[N];\n// init variable for GPU\nint32_t *input_gpu = NULL;\nint32_t *output_gpu = NULL;\n// allocate memory for GPU\ncudaMalloc(&amp;input_gpu, size_of(int32_t) * N);\ncudaMalloc(&amp;output, size_of(int32_t) * N);\n\n// copy from host to device\ncudaMemcpy(input_cpu, input_gpu, sizeof(int32_t) * N, cudaMemcpyHostToDevice);\n\n/* ... do computations on kernel ... */\n\n// copy from device to host\ncudaMemcpy(output_gpu, output_cpu, sizeof(int32_t) * N, cudaMemcpyDeviceToHost);\n\n// free the device memory to avoid mem leak\ncudaFree(&amp;input_gpu);\ncudaFree(&amp;output_gpu);\n</code></pre></p>"},{"location":"csc367/cuda.html#function-scope-and-kernels","title":"Function scope and kernels","text":"<p>If a program runs on CPU (as normal program), its scope is on host, if on GPU, its scope is on device.</p> <p>We use qualifier before the function signature to specify its scope</p> qualifier executed on callable from example <code>__device__</code> device device <code>__device__ float add(float a, float b);</code> <code>__global__</code> device host/device <code>__global__ void render();</code> <code>__host__</code> or no qualifier host host <code>int main()</code>; <p>For <code>__global__</code>, we call it a kernel, it is called by the CPU. </p>"},{"location":"csc367/cuda.html#grids-blocks-and-threads","title":"Grids, blocks, and threads","text":"<p>kernels are called with  <pre><code>kernel_name&lt;&lt;&lt; n_blocks, n_threads[, shared_mem_size] &gt;&gt;&gt;(args);\n</code></pre> <code>n_blocks, n_threads</code> refers to a hierarchy of threads in software which mimics how thread processors are grouped on the GPU. A kernel is launched with a <code>grid</code> of <code>thread_blocks</code>. </p> <p><code>thread</code> is the basic computation unit.   </p> <p><code>block</code> is a collection of threads, which can be organized in 1D, 2D, 3D. Threads in the block is scheduled in wraps, so <code>block</code> should have multiple of 32 threads.  </p> <ul> <li><code>block</code> will be assigned to a SM (when available, otherwise queued), and each SM can have multiple blocks' context. </li> <li>Since <code>block</code> resides in one SM, the threads in the same block can communicate via shared memory, while thread in different cannot.</li> </ul> <p><code>grid</code> (<code>&lt;&lt;&lt;&gt;&gt;&gt;</code> notation) is the collection of blocks, which can be organized in 1D, 2D, 3D. </p>"},{"location":"csc367/cuda.html#starting-a-kernel-given-grid-layout","title":"Starting a kernel given grid layout","text":"<pre><code>/* for 1D, use integer type */\n// starts a kernel with 32 threads per block, \n// 64 blocks per grid\n// total of 64 * 32 = 2048 threads\nkernel_name&lt;&lt;&lt;64, 32&gt;&gt;&gt;(args);\n\n/* for 2D and 3D, use dim3 */\ndim3 threadsPerBlock(16, 8); // 2D block with 16 * 8 = 128 threads\ndim3 blocks(2, 2, 4); // 3D grid with 2 * 2 * 4 = 16 blocks\n// total of 128 * 16 = 2048 threads\n\nthreadsPerBlock.x; // 16\nthreadsPerBlock.y; // 8\nthreadsPerBlock.z; // 1 since 2D\nblocks.z // 4\n\n// start the kernel with given grid layout\nkernel_name&lt;&lt;&lt;blocks, threadsPerBlock&gt;&gt;&gt;(args);\n</code></pre>"},{"location":"csc367/cuda.html#indexing-a-grid","title":"Indexing a grid","text":"<p>Within kernels, we can use build-in notations to get the index  <pre><code>dim3 threadIdx; // thread idx within its block\ndim3 blockDim; // size of a block\ndim3 blockIdx; // block Index in the grid\ndim3 gridDim; // size of the grid, how many blocks\n\n// whether starting with 1D, 2D, 3D\n// they are all dim3, thus access with\nthreadIdx.x; \nthreadIdx.y;\nthreadIdx.z;\n</code></pre></p>"},{"location":"csc367/cuda.html#divergent-warp","title":"Divergent Warp","text":"<p>Note that GPU has much smaller CU, hence less emphasis on the control flow. The warp scheduler is used to hide memory access latency and control flow execution (<code>if/else</code> clauses). </p> <p>All threads in a warp (controlled by the warp scheduler on the hardware) will execute the same instruction. Therefore, consider the <code>if test-stmt then-stmt else-stmt</code> clauses, if all of the warp runs on the <code>then-stmt</code>, then it is coherent execution. Otherwise, it is called divergent execution. </p> <p>In the case of divergent execution, a warp will take 2x time steps. First run only the <code>then-stmt</code> threads by masking out <code>else-stmt</code> threads; and then run <code>else-stmt</code> masking out the <code>then-stmt</code> threads. </p>"},{"location":"csc367/cuda.html#basic-example","title":"Basic Example","text":"<p>GPU implementation to fill an array with <code>range(N)</code>; <pre><code>__global__ void fill_range(int32_t *arr, int32_t N) {\n    int32_t i = blockIdx.x*blockDim.x + threadIdx.x;\n    if (i &lt; N) arr[i] = i;\n}\n\nint main() {\n    int32_t *arr_gpu = NULL;\n    cudaMalloc(&amp;arr_gpu, N * sizeof(int32_t));\n\n    fill_range&lt;&lt;&lt;N / 128, 128&gt;&gt;&gt;(arr_gpu, N);\n\n    int arr[N];\n    cudaMemcpy(arr_gpu, arr, N * sizeof(int32_t), cudaMemcpyDeviceToHost);\n    cudaFree(&amp;arr_gpu);\n    return 0;\n}\n</code></pre></p>"},{"location":"csc367/cuda.html#memory-types","title":"Memory Types","text":"type scope access speed syntax global device and host 1x <code>[__global__] int32_t* arr;</code> local per thread ~1x within kernel, <code>int arr[N];</code> shared per block ~10-100x <code>__shared__ int arr[N];</code> constant device and host, read-only &gt;1x, more to consider <code>__constant__ int32_t arr[N];</code> texture device and host, read-only &gt;1x <code>texture&lt;float, cudaTextureType2DLayered&gt; tex;</code>"},{"location":"csc367/cuda.html#local-and-global-memory","title":"Local and Global Memory","text":"<p>Global memory are the largest memory on the device. It is allocated and managed by the host. Its memory addresses are accessible from both host and device (hence <code>__global__</code>). Global is allocated via <code>cudaMalloc</code> (and all <code>malloc</code>-like APIs). As in the basic example, <code>arr_gpu</code> is in the global memory</p> <p>Local memory are memory that statically allocated within a kernel. It is only visible per thread. The access to local memory is coalesced. As in the basic example, <code>i</code> is in the global memory.</p> <p>The latency and bandwidth for global and local memory are similar. An (not very accurate) analogy can be the stack memory and heap memory. </p>"},{"location":"csc367/cuda.html#shared-memory","title":"Shared Memory","text":"<p>Shared memory can be thought as a controllable L1 cache. It is declared using <code>__shared__</code> keyword, allocated per block. The memory is \"shared\" per block. </p>"},{"location":"csc367/cuda.html#bank-conflict","title":"Bank Conflict","text":"<p>The shared memory, in order to minimize latency and maximize throughput, is organized in banks (equally sized memory modules). Each bank is 4 bytes. Thus, any memory ops of <code>N</code> addresses spanning <code>N</code> distinct memory banks can be served simultaneously.  </p> <p>Thus, a bank conflict happens when multiple threads request the same memory bank. The only exception is when all threads address the same shared memory address, resulting in a broadcast. </p> <p>For example,  <pre><code>int block_size = blockDim.x;\nint tid = threadIdx.x;\n\n__shared__ int32_t shmem[128];\n/*  bank0   0 32 64 96\n    bank1   1 33 65 97\n    ...\n    bank31 31 63 95 127\n*/\n\n// thread i access bank i + offset, no conflict\nint32_t a = shmem[offset + tid];\n\n// thread i access bank 2 * i + offset, 2x conflict\nint32_t b = shmem[(offset + 2 * tid)];\n</code></pre></p> <p>Also note that per-bank is 32 bits (one <code>float</code> or <code>int</code>), thus if we use <code>short, double, long, struct</code> we must care about bank conflict.  <pre><code>__shared__ long shmem[64];\n/*  bank0   0 16 32 48\n    bank1   1 17 33 49\n    ...\n    bank31 15 31 47 63\n*/\n// 2x conflict, the same effect as of 2 * i + offset\nlong a = shmem[offset + tid];\n\n__shared__ short shmem[256];\n/*  bank0   0  64 128 192\n    bank1   2  66 130 194\n    ...\n    bank31 62 126 190 255\n*/\n// 2x conflict, adjacent tids are in the same bank \nshort a = shmem[offset + tid];\n</code></pre></p>"},{"location":"csc367/cuda.html#synchronizations","title":"Synchronizations","text":"<p>Note that shared memory is shared within a block, whiles threads in one block won't necessarily run code simultaneously. Therefore, we need to sync the threads block-wise using  <pre><code>__syncthreads();\n</code></pre></p> <p>shared memory also introduces race conditions. Thus, CUDA supports several atomic ops. In the style of  <pre><code>// read a from address, compute a + b, stores in address, return a\nNumber atomicOp(Number *address, Number b);\n\nint a = 2, b = 1, c;\nc = atomicAdd(&amp;a, b); \nprintf(\"%d %d %d\\n\", a, b, c); // 3 1 2\n</code></pre></p>"},{"location":"csc367/cuda.html#constant-memory","title":"Constant Memory","text":"<p>Read-only small memory (~64KB). Since it is read only, it is broadcast to a half-warp, and is cached. </p>"},{"location":"csc367/cuda.html#texture-memory","title":"Texture Memory","text":"<p>It is designed for graphics purposes (for mapping 2D data (textures) onto 3D objects). While it can also be used in CUDA for general purposes. </p> <p>It is a special structured data. For example.  <pre><code>texture&lt;float, cudaTextureType2DLayered&gt; tex;\n</code></pre></p> <p>The usage and characteristic is similar to constant memory. However, since it is specially purposed, it can be useful when memory accesses have good spatial locality. </p>"},{"location":"csc367/cuda_opt.html","title":"CUDA Programming - Parallel Reduction","text":"<p>To parallelize a reduction, obviously we will do binary operations pair-wise in each time step, collect the results, and then reduce pair-wise until obtaining the final results. For a \\(N\\) elements array, such reduction will take <code>lg(n)</code> time steps. </p> <p>Note that GPU can only synchronize block-wise, global synchronization can only be done implicitly when kernel finishes.</p> <p>We will use the sum example</p>"},{"location":"csc367/cuda_opt.html#interleave-addressing","title":"Interleave Addressing","text":"<p>The basic idea is to  1. load one element from global memory to shared memory 2. each thread reduce two elements and so on, until one thread left. 3. write the shared memory back to global memory.</p> <pre><code>__global__ void dot_product0(float *g_idata, float *g_odata) {\n    // for dynamic sized shared memory\n    extern __shared__ int sdata[];\n    unsigned int tid = threadIdx.x;\n    unsigned int i = blockIdx.x * blockDim.x + threadIdx.x;\n\n    sdata[tid] = g_idata1[i];\n    __syncthreads();\n\n    for(unsigned int s = 1; s &lt; blockDim.x; s*=2){\n        if (tid % (2*s) == 0)  sdata[tid] += sdata[tid + s];\n        __syncthreads();\n    }\n\n    if (tid == 0) { g_odata[blockIdx.x] = sdata[0]; }\n}\n\n#define M (1024*1024)\n\nint main() {\n    /* all GPU arrays pointers are already allocated \n        d_idata: M array, alreay copied input data\n        s: float, 1024\n    */\n    int block_num = M;\n    while (block_num &gt; 1) {\n        dot_product0&lt;&lt;&lt;block_num, 128, 128 * sizeof(float)&gt;&gt;&gt;(d_idata, d_idata);\n        block_num /= 128;\n    }\n    /* d_idata[0] is now the sum */\n}\n</code></pre>"},{"location":"csc367/cuda_opt.html#divergence-execution","title":"Divergence Execution","text":"<p>Adjacent threads will run different if-branch, causing divergent execution. Although we don't have the else branch, the threads running the else branch, since within the same warp, are waiting for the then branch. Instead of scheduling new work. </p> <p><pre><code>/* \nif (tid % (2*s) == 0)  sdata[tid] += sdata[tid + s]; \n*/\n\n// Instead, we write\nint idx = 2 * s * tid;\nif (idx &lt; blockDim.x) sdata[idx] += sdata[idx + s];\n</code></pre> Thus, we are doing the same interleave addressing, but the adjacent threads are computing the same branch. For the latter half, they can be scheduled with new works.  </p>"},{"location":"csc367/cuda_opt.html#bank-conflict","title":"Bank Conflict","text":"<p>Note that shared memory are managed in 4 Byte banks. Thus in each time step, the same bank (eg. <code>shmem[0], shmem[32], ...</code>) are accessed by different threads in the same warp, causing bank conflict. </p> <p>To avoid bank conflict, reverse fop loop and threadID-based indexing as  <pre><code>/*\nfor(unsigned int s = 1; s &lt; blockDim.x; s*=2){\n     int idx = 2 * s * tid;\n        if (idx &lt; blockDim.x) sdata[idx] += sdata[idx + s];\n   __syncthreads();\n}\n*/\n\n// Instead, we write\nfor (unsigned int s = blockDim.x/2; s &gt; 0; s &gt;&gt;= 1) { \n    if (tid &lt; s) {  \n        sdata[tid] += sdata[tid + s];\n    }\n    __syncthreads();\n}\n</code></pre></p>"},{"location":"csc367/cuda_opt.html#unroll-first-iteration","title":"Unroll First Iteration","text":"<p>Consider the kernel, note that half of the thread will only execute the read part and idle. Thus, we can unroll the first iteration of the for loop to reduce idle time. </p> <p><pre><code>/*\nsdata[tid] = g_idata1[i];\n*/\n\n// gridSize need to be halved since now each thread will read 2 elements\nsdata[tid] = g_idata[i] + g_idata1[i+blockDim.x];\n</code></pre> Note that we can even take steps further by unroll first few iterations, so that fewer threads get idled. </p> <pre><code>/*\nsdata[tid] = g_idata1[i];\n*/\n\n// blockSize can be provided using template\n// gridSize need to be tuned since now each thread will read lg n elements\nunsigned int gridSize = blockSize * 2 * gridDim.x;\nsdata[tid] = 0;\nwhile (i &lt; n) {\n    sdata[tid] += g_idata[i] + g_idata[i + blockSize];\n    i += gridSize;\n}\n</code></pre>"},{"location":"csc367/cuda_opt.html#unroll-the-warp","title":"Unroll the Warp","text":"<p>Using the property that instructions are synced within a warp. We can save more instructions and <code>__syncthreads()</code>. Now, we can end the loop at 32 for all threads, and only one warp will run the unrolled warp code.  <pre><code>/*\nfor (unsigned int s = blockDim.x/2; s &gt; 0; s &gt;&gt;= 1) { ... }\n*/\n\nfor (unsigned int s = blockDim.x/2; s &gt; 32; s &gt;&gt;= 1) { ... } \n\nif (tid &lt; 32) {\n    // volatile enforce warp sync execution\n    // by preventing compiler optimzations on the shared mem\n    volatile int* smem = sdata;\n    smem[tid] += smem[tid + 32];\n    smem[tid] += smem[tid + 16];\n    smem[tid] += smem[tid + 8];\n    smem[tid] += smem[tid + 4];\n    smem[tid] += smem[tid + 2];\n    smem[tid] += smem[tid + 1];\n}\n</code></pre></p>"},{"location":"csc367/cuda_opt.html#complete-unrolling-using-template","title":"Complete Unrolling using template","text":"<p>Note that we call the kernel with a given <code>blockSize</code>, and we know that each block can have a max of 1024 threads. Thus we know the block size at compile time. We can use C++ template to complete unroll the for loop, when we call the kernel, we can save the <code>for</code> loop test conditions and indexer. </p> <pre><code>template &lt;unsigned int blockSize&gt;\n__global__ void reduceSum(float *g_data1, float *g_odata)\n{\n    extern __shared__ int sdata[];\n\n    unsigned int tid = threadIdx.x;\n    unsigned int i = blockIdx.x * (blockDim.x*2) + threadIdx.x;\n\n    unsigned int gridSize = blockSize * 2 * gridDim.x;\n    sdata[tid] = 0;\n    while (i &lt; n) {\n        sdata[tid] += g_idata[i] + g_idata[i + blockSize];\n        i += gridSize;\n    }\n    __syncthreads();\n\n    if (blockSize &gt;= 512) { \n        if (tid &lt; 256) sdata[tid] += sdata[tid + 256];\n        __syncthreads();\n    }\n    if (blockSize &gt;= 256) { \n        if (tid &lt; 128) sdata[tid] += sdata[tid + 128]; \n        __syncthreads();\n    }\n    if (blockSize &gt;= 128) { \n        if (tid &lt; 64) sdata[tid] += sdata[tid + 64];\n        __syncthreads();\n    }\n\n    if (tid &lt; 32) {\n        volatile int* smem = sdata;\n        if (blockSize &gt;= 64) smem[tid] += smem[tid + 32];\n        if (blockSize &gt;= 32) smem[tid] += smem[tid + 16];\n        if (blockSize &gt;= 16) smem[tid] += smem[tid + 8];\n        if (blockSize &gt;= 8) smem[tid] += smem[tid + 4];\n        if (blockSize &gt;= 4) smem[tid] += smem[tid + 2];\n        if (blockSize &gt;= 2) smem[tid] += smem[tid + 1];\n    }\n\n    if (tid == 0) { g_odata[blockIdx.x] = sdata[0]; }\n}\n</code></pre>"},{"location":"csc367/hie.html","title":"Single Processor Machines","text":""},{"location":"csc367/hie.html#program-compiler-and-uni-processor-model","title":"Program, Compiler, and Uni-processor model","text":"<p>We know that the processor executes machine codes (e.x. <code>Assembly</code>) and a compiler compiles a piece of code (say <code>C/C++/Fortan</code>) into machine code. </p> Variables Operations Control flow program typed variables (<code>int, float, pointer, array, struct</code>) arithmetic ops, logical ops, etc. if, for, function calls Assembly (Processor) move bytes between memory and register arithmetic instructions (ALU) jump/branch instructions (CU) <p>Ideally, assuming control, save, load are free, arithmetic operations all have the same cost.</p>"},{"location":"csc367/hie.html#compiler-optimizations","title":"Compiler Optimizations","text":"<p>Compiler optimizations aim to reduce number of instructions by</p> <ul> <li>Improve register reuse (fewer save/load instruction)<ul> <li>interchange nested loops</li> <li>reorder instructions, ex. <code>e = a + b; f = e - 1</code> becomes <code>R1 = a + b; R1 = R1 - 1</code></li> </ul> </li> <li>Eliminate unnecessary control flows (fewer branch/jump)<ul> <li>unroll small loops </li> <li>fuse loops (merge nested loops into one)</li> <li>eliminate dead code branch</li> </ul> </li> <li>Strength reduction (turn expensive instruction into cheaper ones)<ul> <li>multiply by 2 becomes shift right</li> </ul> </li> </ul> <p>However, most optimizations are performed locally. They often give up optimizations on complex code to preserve correctness. </p>"},{"location":"csc367/hie.html#memory-hierarchy","title":"Memory Hierarchy","text":"<p>The memory system can contain multiple levels. For example, a personal computer system (Ryzen5-2700) should have </p> memory device latency size bandwidth register per-code 0 bytes L1 cache per-core &gt;1ns 768KB 1TB/sec L2 cache per-processor ~3ns 4MB 1TB/sec L3 cache (or SRAM) per-CPU (or motherboard) ~6ns 16MB ~500GB/sec Main memory DDR4 RAM ~20ns 8-64GB ~25GB/sec Hard drive SSD ~10ms 1TB ~5GB"},{"location":"csc367/hie.html#caches-locality","title":"Caches Locality","text":"<p>A cache line is the contiguous block that read/write in between memory and cache per instruction. It's typically a few memory addresses. </p> <p>Since cache has significantly smaller latency, we want to store previously accessed cache lines so that references to memory is reduced. A cache hit if the data required by the processor is already in cache, otherwise a cache miss, which resulting a load from main memory. </p> <ul> <li>Temporal locality: reuse data already in cache</li> <li>Spatial locality: Operate on data that are stored close in memory, so that the data can be brought into cache all together. </li> </ul>"},{"location":"csc367/hie.html#single-processor-parallelism","title":"Single Processor Parallelism","text":""},{"location":"csc367/hie.html#single-instruction-multiple-data-simd","title":"Single Instruction Multiple Data (SIMD)","text":"<p>SIMD (single instruction, multiple data) can execute one arithmetic instruction (one clock cycle) on multiple scalars. </p> <p>For example <code>AVX-512</code> do arithmetic operations on 512 bits (aka 8 double precision / 16 single precision operations) at a time.  </p> <p>Some restriction on SIMD includes: expose parallelism to the compiler (flags/pragma to the compiler), data needs to be contiguous in memory and cache aligned. </p>"},{"location":"csc367/hie.html#memory-alignment-and-strides","title":"Memory Alignment and Strides","text":"<p>Note that cache line is loaded from memory to cache at one time, which means contiguous memory accesses cannot be done in parallel (otherwise very easy to cause data conflicts). However, non-contiguous memory access can be done in parallel. </p> <p>Memory alignment is a common technique, instead of writing data in contiguous blocks, align them on cache line boundaries. For example, if the cache line is 32 bytes, we can store data in </p> <pre><code>double x[]; // each double is 8 bytes\nx[0], x[4], x[8] // stride for 32 / 8 = 4 doubles \n</code></pre>"},{"location":"csc367/hie.html#fused-multiply-add-fma","title":"Fused Multiply Add (FMA)","text":"<p>FMA refers to operations like \\(x = y + c \\cdot z\\), which is very common for matrix multiplication. With FMA support, such operation can be done as one instruction instead of two. Also, only one rounding is performed, hence rounding error won't accumulate and is smaller. </p>"},{"location":"csc367/hie.html#roofline-model","title":"Roofline Model","text":"<p>Roofline: an insightful visual performance model for multicore architectures</p> <p>The idea is that applications are measured by </p> factor unit level arithmetic performance FLOPs/s machine memory bandwidth bytes/s machine computational intensity FLOPs/byte application <p>Then, consider a task of \\(N\\) FLOPs or arithmetic operations and \\(M\\) bytes of data movement. Assuming a cold start (all data initially resides in DRAM) and each byte of data will be moved exactly once. Then we have that </p> <pre><code>time = max(N / (Peak FLOPS per sec), M / Peak Bandwidth)\nN / time = min(Peak FLOPS per sec, (N / M) * Peak Bandwidth)\n</code></pre> <p>Thus, we can get the roofline performance model, where x-axis is the FLOPs / data ratio (N/M) and y-axis is the speed FLOPs / sec. </p>"},{"location":"csc367/matrix.html","title":"Single Processor Matrix Multiplications","text":""},{"location":"csc367/matrix.html#performance-model","title":"Performance Model","text":"<p>Consider a simplified memory model, where we only have the cache (fast memory) and slow memory. All data are initially in slow memory, and the fast memory is empty. </p> <p>Let \\(m\\) be the number of elements moved between fast and slow, \\(t_m\\) be the time per memory save/load.  Let \\(f\\) be the number of arithmetic ops (flops), \\(t_f\\) be the time per flop. Let \\(q = f/m\\) be the average #flops per slow memory access.  Assuming that \\(t_f &lt;&lt; t_m\\). </p> <p>The actual time taken is </p> \\[f \\cdot t_f + m \\cdot t_m = f \\cdot t_f \\cdot (1 + \\frac{t_m}{t_f} \\times \\frac{1}{q})\\] <p>\\(t_m/t_f\\) is called the machine balance, which is hardware specific and constant (to us programmer). \\(q\\geq t_m/t_f\\) means that we can get at least half of peak speed (\\(f\\cdot t_f\\), or the time taken if all data in cache). </p>"},{"location":"csc367/matrix.html#matrix-multiplication","title":"Matrix Multiplication","text":"<p>For the following examples, all matrices are \\(n\\times n\\), and vectors are \\(n\\times 1\\). Matrices and vectors are stored in contiguous row-major arrays (of arrays) of floats. Thus <code>A[i][j]</code> is the ith row, jth col. </p>"},{"location":"csc367/matrix.html#naive-matrix-matrix-multiplication","title":"Naive Matrix-Matrix Multiplication","text":"<p>Consider the following implementation for matrix-vector mult. <code>y += A * x</code></p> <pre><code>// assume cache size = cn, 3 &lt; c &lt;&lt; n\n__load(x, n);\n__load(y, n);\nfor (int i = 0; i &lt; n; i++) {\n    __load(A[i], n);\n    for (int j = 0; i &lt; n; j++) {\n        y[i] += A[i][j] * x[j];\n    }\n}\n__save(y, n);\n</code></pre> <p>\\(m = 3n + n^2, f = 2n^2, q \\approx 2\\)</p> <p>Similarly, consider matrix-matrix multiplication <code>C += A * B</code>. The most naive matrix-matrix multi. will be \\(n\\) matrix-vector multiplication so that \\(m = 3n^2+n^3, f = 2n^3\\). Then, we will still have \\(q\\approx 2\\). </p>"},{"location":"csc367/matrix.html#blocked-tiled-matrix-multiplication","title":"Blocked (Tiled) Matrix Multiplication","text":"<p>Suppose that the cache is large enough to fit all 3 matrices. Then, we only need \\(3n^2\\) loads and \\(n^2\\) saves so that \\(m = 4n^2, f = 3n^3\\). In this case, \\(q \\in O(n)\\). </p> <p>Thus, if we subdivide a matrices into smaller blocks, each of size \\(b\\times b\\) and load each block at a time, we can have \\(q\\in O(b)\\) performance. More specifically,</p> blocked matrix multiplication<pre><code>// assume that n divides b for simplicity\n// assume __block_load/save loads/saves the b * b sub-matrices \n// given the top-left corner\nfor (int i = 0; i &lt; n; i += b) {\n    for (int j = 0; j &lt; n; j += b) {\n        __block_load(C[i][j], b);\n        for (int k = 0; k &lt; n; k += b) {\n            __block_load(A[i, k], b);\n            __block_load(B[k, j], b);\n            // assume this computes C += A*B given the top-left corner\n            // and size of matrix\n            matrix_matrix_multiplication(\n                C[i][j], \n                A[i][k], \n                B[k][j],\n                b\n            )\n        }\n        __block_save(C[i][j], b);\n    }\n}\n</code></pre> <p>Each for loop will execute \\(N = n/b\\) times so that </p> \\[ m = 2N^2b^2 + 2N^3b^2 = 2n^2 + 2n^3/b, q = f/m = \\frac{2n^3}{2n^2 + 2n^3/b} \\approx b \\] <p>Knowing the cache size \\(M\\), we can then find calculate \\(b \\leq \\sqrt{M / 3}\\). </p> <p>Also, in practice a memory hierarchy system has more than two levels of cache, and the algorithm is possible for further subdividing into smaller blocks according to cache levels.</p>"},{"location":"csc367/matrix.html#recursive-matrix-multiplication","title":"Recursive Matrix Multiplication","text":"<p>Another approach is to do blocking recursively, instead of using a fixed block size. </p> Recursive matrix multiplication<pre><code>RMM(A, B, n) {\n    // base case: when n is sufficiently small\n    // for naive MM\n    if (n == 1) {\n        return A * B;\n    }\n    // recursive steps\n    else {\n        A00, A01, A10, A11 = subdivide(A);\n        B00, B01, B10, B11 = subdivide(B);\n        C00 = RMM(A00, B00, n/2) + RMM(A01, B10, n/2);\n        C01 = RMM(A00, B01, n/2) + RMM(A01, B11, n/2);\n        C10 = RMM(A11, B00, n/2) + RMM(A11, B10, n/2);\n        C11 = RMM(A11, B01, n/2) + RMM(A11, B11, n/2);\n        C = assemble(C00, C01, C10, C11);\n        return C;\n    }\n}\n</code></pre> <p>arithmetic computational cost is </p> \\[T(n) = 8T(\\frac n2) + 4(\\frac{n}{2})^2\\] <p>By master's theorem, we get \\(2n^3 - n^2 \\in O(n^3)\\)</p> <p>data movement cost is similar</p> \\[T_m(n) = \\begin{cases}8T_m(\\frac n2) + 4\\times 3(\\frac{n}{2})^2&amp;3n^2 &gt; M\\\\3n^2\\end{cases}\\] <p>RMM has the same asymptotic bound as BMM, but does not need to know the cache sizes. However, because of the cost of function stack allocation, RMM is generally a bit smaller than BMM. </p>"},{"location":"csc367/matrix.html#data-layouts","title":"Data Layouts","text":"<p>Consider the existence of cache line, to maximize spatial locality, the data layout can be further optimized according to the algorithm. </p> <p>For BMM, instead of save the whole matrices in row-major or col-major. We can do blocked-row major/col-major. Therefore, each block will be in contiguous memory. </p> <p>For RMM, we can use Z-Morton order, i.e. the matrix is recursively subdivided into 4 blocks, until the matrix is sufficiently small. However, Z-Morton order can be very hard for matrix indexing. </p>"},{"location":"csc367/mpi.html","title":"Distributed Memory Model and MPI Introduction","text":""},{"location":"csc367/mpi.html#distributed-memory-architecture","title":"Distributed Memory Architecture","text":"<p>Commonly, a distributed memory system refers to a multiprocessor computer system in which each processor has its own private memory, and communicate messages through a network(interconnections). </p> <p>The network refers to the the collection of processors and their communication interface . It can be seen as a graph where nodes are the processors and edges are the interconnections (for communication). The topology of such graph is a key factor of the network performance (latency, bandwidth, scalability). </p> <p>Compare to a shared memory model, distributed memory model can scale to much larger computations as we consider the hardware limit on shared memory. </p>"},{"location":"csc367/mpi.html#message-passing-model","title":"Message Passing Model","text":"<p>Although distributed memory architecture supports different programs on each host, but the common usage (and the purpose of designing such distributed memory system), is to have all processes execute the same code, and communicate via messages, or Single Program Multiple Data (SPMD). </p> <p>The message passing model is primarily designed for distributed memory even though it can still apply to shared memory system  - Nodes are connected with a network - Partition the data across the nodes, computation in parallel - If local data is needed on remote node, send it over the interconnect - Computation is done collectively - Can always add more and more nodes, bottleneck is not the number of cores or memory on a single node - Scale out instead of scale up. Can increase the data size while increasing processors</p> <p>Programming philosophy is different from OpenMP/Pthreads  - communication overhead is more significant, let the processes compute independently as much as possible (sometimes replicate data is better).  - need to consider the message passing operations (since each process now have different data)  - data partition based, since SPMD.</p>"},{"location":"csc367/mpi.html#mpi-message-passing-protocol","title":"MPI: Message Passing Protocol","text":"<p>The standard library for message passing. </p> <p>In the C/C++ files  <pre><code>#include &lt;mpi.h&gt;\n</code></pre></p> <p>Run the program  <pre><code>mpirun -np 8 ./myapp arg1 arg2\n</code></pre></p> <p>However, since MPI is mostly used in large scale systems for scientific computations. There are other toolchains to compile, run, and manage them (commonly Slurm, command <code>srun</code>). </p>"},{"location":"csc367/mpi.html#basics","title":"Basics","text":""},{"location":"csc367/mpi.html#mpi_init-mpi_finalize","title":"<code>MPI_init, MPI_finalize</code>","text":"<p><code>MPI_init</code> the MPI environment, extract and remove MPI parts of the command line args</p> <pre><code>// init the MPI env\n// MPI_SUCCESS or err code\nint MPI_init(int *argc, char ***argv);\n</code></pre> <p><code>MPI_finalize</code> terminate the env, and no MPI calls are allowed afterwards. <pre><code>// terminate MPI environment\n// MPI_SUCCESS or err code\nint MPI_Finalize();\n</code></pre></p>"},{"location":"csc367/mpi.html#mpi_comm-mpi_comm_world","title":"<code>MPI_Comm, MPI_COMM_WORLD</code>","text":"<p><code>MPI_Comm</code> the struct (communicators) that store info. about communication domains (set of processes that are allowed to communicate). By default, stored in <code>MPI_COMM_WORLD</code></p> <p>Communicator size and id of current process can be obtained <pre><code>int MPI_Comm_size(MPI_Comm comm, int *size);\nint MPI_Comm_rank(MPI_Comm comm, int *rank);\n</code></pre></p>"},{"location":"csc367/mpi.html#mpi_wtime","title":"<code>MPI_Wtime</code>","text":"<p>Get the elapsed time on the processor, in secs <pre><code>double MPI_Wtime();\n</code></pre></p>"},{"location":"csc367/mpi.html#basic-example","title":"Basic Example","text":"<pre><code>int main(int argc, char **argv) {\n    // first thing to do, init MPI env, get #processes and id\n    MPI_Init( &amp;argc, &amp;argv );\n    double start, end;\n    start = MPI_Wtime();\n    int n_proc, rank;\n    // use default communicator MPI_COMM_WORLD \n    MPI_Comm_size(MPI_COMM_WORLD, &amp;n_proc);\n    MPI_Comm_rank(MPI_COMM_WORLD, &amp;rank);\n\n    /* read your program args and run program in MPI world*/\n\n    end = MPI_Wtime();\n    printf(\"Elapsed time: %f\\n\", t2 - t1); \n    // last thing, terminate MPI env\n    MPI_Finalize();\n    return 0;\n}\n</code></pre>"},{"location":"csc367/mpi.html#data-types","title":"Data types","text":"<p>data types are used to determine what kind of data is to communicate (so that we can use typed variables as buffer to send and receive them).</p> <p>Basic data types equivalent to built-in C types, including <pre><code>MPI_CHAR, MPI_UNSIGNED_CHAR // 1\nMPI_SHORT, MPI_UNSIGNED_SHORT // 2\nMPI_INT, MPI_UNSIGNED // 4\nMPI_LONG, MPI_UNSIGNED_LONG // 8\nMPI_FLOAT // 4\nMPI_DOUBLE // 8\n</code></pre> In addition  <pre><code>MPI_BYTE // used like MPI_UNSIGNED_CHAR\nMPI_PACKED // used with pack and unpack \nMPI_Datatype // used to define new data type\n</code></pre></p>"},{"location":"csc367/mpi.html#mpi_datatype","title":"<code>MPI_Datatype</code>","text":"<p>The super class of all C equivalent data types, and it can be used to create new data type (for example sending <code>struct</code>). </p> <pre><code>MPI_Type_contiguous(int size, MPI_Datatype oldtype, MPI_Datatype *newtype);\nMPI_Type_commit(MPI_Datatype *newtype);\n\n\n// example\ntypedef struct {\n  double x;\n  double y;\n  double z;\n} vec3d;\n\nMPI_Datatype MPI_VEC3D;\n\n// define the MPI version of vec3d\n// struct is just a contiguous memory\nMPI_Type_contiguous(3, MPI_DOUBLE, &amp;MPI_VEC3D);\n// commit it so that it can be used in communications\nMPI_Type_commit(&amp;MPI_VEC3D);\n</code></pre>"},{"location":"csc367/mpi.html#pack-and-unpack","title":"Pack and Unpack","text":"<p>Pack/unpack several variables (with different data type) together for communication.  <pre><code>int MPI_Pack(\n    const void *inbuf, // pointer to data to be packed\n    int incount, // count of to be packed data\n    MPI_Datatype datatype, // dtype of to be packed data\n    void *outbuf, // pointer to the \"pack\" buffer\n    int outsize, // size of the \"pack\" buffer in Bytes\n    int *position, // current position in buffer, in bytes\n    MPI_Comm comm \n);\n\nint MPI_Unpack(\n    const void *inbuf,\n    int insize,\n    int *position,\n    void *outbuf,\n    int outcount,\n    MPI_Datatype datatype,\n    MPI_Comm comm\n);\n\n\n// example\nshort c[50];\nint i;\nchar buffer[110];\nint position = 0;\n\nMPI_Pack(c, 50, MPI_SHORT, buffer, 110, &amp;position, MPI_COMM_WORLD);\nMPI_Pack(i, 1, MPI_INT, buffer, 110, &amp;position, MPI_COMM_WORLD);\n\n// communication, more on later\nMPI_Send(buffer, position, MPI_PACKED, 1, 0, MPI_COMM_WORLD);\nMPI_Recv(buffer, 110, MPI_PACKED, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n\nMPI_Unpack(buffer, 110, &amp;position, &amp;i, 1, MPI_INT, MPI_COMM_WORLD);\nMPI_Unpack(buffer, 110, &amp;position, c, 50, MPI_SHORT, MPI_COMM_WORLD);\n</code></pre></p>"},{"location":"csc367/mpi_comm.html","title":"MPI Communications","text":"<p>There are two flavors of communication</p> <ul> <li>Point-to-point operations: one processor send, another receive</li> <li>Collective operations: all processors participate in communications. With some common patterns. </li> </ul>"},{"location":"csc367/mpi_comm.html#point-point-communication","title":"Point-Point Communication","text":"<p>A processor communicates with another with sending and receiving messages. </p>"},{"location":"csc367/mpi_comm.html#mpi_send-mpi_recv-basic-messaging","title":"<code>MPI_Send, MPI_Recv</code> Basic messaging","text":"<pre><code>// send a message to the dest process\nint MPI_Send(\n    void *buf, // pointer to the message to be sent\n    int count, // count of message\n    MPI_Datatype datatype, // datatype of message\n    int dest, // destination process\n    int tag, // tag to distinguish it from other messages\n    MPI_Comm comm\n);\n// receive a message from the source process\n// and save the status\nint MPI_Recv(\n    void *buf, // pointer to the receiving buffer\n    int count, // count of message \n    MPI_Datatype datatype, // datatype of message\n    int source,  // source process\n    int tag, // tag to distinguish it from other messages\n    MPI_Comm comm, \n    MPI_Status *status\n);\n\n// the status struct\ntypedef struct MPI_Status {\n    int MPI_SOURCE; // source of the received message\n    int MPI_TAG; // tag of the received message\n    int MPI_ERROR; // a potential error code\n};\n// to retrive the length of received message\nint MPI_Get_count(MPI_Status *status, MPI_Datatype datatype, int *count);\n</code></pre>"},{"location":"csc367/mpi_comm.html#message-blocking","title":"Message Blocking","text":"<p><code>MPI_Recv</code> is blocking, which means it will wait to return until message is received and put into <code>buf</code>. </p> <p><code>MPI_Send</code> is not necessarily blocking, depends on the implementation, it </p> <ul> <li>either blocks until the message is received, </li> <li>or put the msg into another buffer and returns without waiting. </li> </ul> <p><code>buf</code>, in both send and receive, are safe to reuse after the function returns. </p>"},{"location":"csc367/mpi_comm.html#deadlock","title":"Deadlock","text":"<p>WARNING Due to the unspecified behavior of <code>MPI_Send</code>, it can be either blocking or non-blocking. Consider the circular chain (each process send to the next one, and receive from previous one) example <pre><code>// rank is the current process id\n// np is the number of processes\nMPI_Send(a,20, MPI_INT, (rank+1)%np, 1, MPI_COMM_WORLD);\nMPI_Recv(b,20, MPI_INT, (rank-1+np)%np, 1, MPI_COMM_WORLD, &amp;status);\n</code></pre> If the implementation is blocking, then it will cause a deadlock, as all processes waiting for the blocked send to receive. </p> <p>Solution <pre><code>if (rank % 2) {\n    MPI_Send(a,20, MPI_INT, (rank+1)%np, 1, MPI_COMM_WORLD);\n    MPI_Recv(b,20, MPI_INT, (rank-1+np)%np, 1, MPI_COMM_WORLD, &amp;status);\n}\nelse {\n    MPI_Recv(b,20, MPI_INT, (rank-1+np)%np, 1, MPI_COMM_WORLD, &amp;status);\n    MPI_Send(a,20, MPI_INT, (rank+1)%np, 1, MPI_COMM_WORLD);\n}\n</code></pre></p>"},{"location":"csc367/mpi_comm.html#mpi_sendrecv-sendrecv-simultaneously","title":"<code>MPI_Sendrecv</code> Send/recv simultaneously","text":"<p>The deadlock problem is a common pattern, while handling such deadlocks is time consuming and annoying. Thus, a more elegant solution is to use <code>MPI_Sendrecv</code>.</p> <p>Its API is self-explanatory  <pre><code>int MPI_Sendrecv(\n    void *sendbuf, int sendcount, MPI_Datatype sendtype, int dest, int sendtag,\n    void *recvbuf, int recvcount, MPI_Datatype recvtype, int source, int recvtag,\n    MPI_Comm comm, \n    MPI_Status *status\n);\n\n// circular chain\nMPI_Sendrecv(\n    a, 20, MPI_INT, (rank+1)%np, 1,\n    b, 20, MPI_INT, (rank-1+np)%np, 1,\n    MPI_COMM_WORLD, &amp;status\n);\n</code></pre> Note that in this case, <code>send</code> and <code>recv</code> may happen simultaneously (depends on implementation). Therefore, <code>sendbuf, recvbuf</code> must be different. </p> <p>Another API is to use single buffer so that received message replaces sent message in the same buffer. In this case, the send and recv message must be the same size and type. <pre><code>int MPI_Sendrecv_replace(\n    void *buf, int count, MPI_Datatype datatype, \n    int dest, int sendtag,\n    int source, int recvtag,\n    MPI_Comm comm, MPI_Status *status\n);\n</code></pre></p>"},{"location":"csc367/mpi_comm.html#mpi_isend-mpi_irecv-non-blocking-sendreceive","title":"<code>MPI_Isend, MPI_Irecv</code>, Non-blocking send/receive","text":"<p>Non-blocking send and recv will immediately return. </p> <p>WARNING it unsafe to reuse after the function returns. The copying to communication buffer may not be finished as function returns.</p> <pre><code>// API, almost the same as send and recv\nint MPI_Isend(\n    const void *buf, int count, MPI_Datatype datatype, int dest, int tag,\n    MPI_Comm comm, \n    MPI_Request *request\n);\nint MPI_Irecv(\n    void *buf, int count, MPI_Datatype datatype, int source, int tag, \n    MPI_Comm comm, \n    MPI_Request * request\n);\n</code></pre> <p>The <code>MPI_Request</code> will record the process status, and it is used with <code>MPI_Test</code> and <code>MPI_Wait</code> to determine whether the communication has finished. </p> <pre><code>/* test whether finish\n   if finished then return non-zero, and set flag to non-zero, \n   and deallocate request, set to MPI_REQUEST_NULL,\n   and set approriate status as blocking send/recv\n   if not finished then return zero\n*/\nint MPI_Test(MPI_Request *request, int *flag, MPI_Status *status);\n\n/* block until request finishes, \n   and deallocate request, set to MPI_REQUEST_NULL\n   and set approriate status as blocking send/recv\n*/\nint MPI_Wait(MPI_Request *request, MPI_Status *status)\n</code></pre>"},{"location":"csc367/mpi_comm.html#collective-operations","title":"Collective Operations","text":"<p>Collective operations are common computing patterns where all processes communicate. </p> <p>All collective operations have implicit barrier that blocks till finish. </p>"},{"location":"csc367/mpi_comm.html#mpi_barrier","title":"<code>MPI_Barrier</code>","text":"<p>As all other barrier, blocks until ALL processes is here. </p> <p>WARNING barrier must be  set in all processes. Barrier, as OpenMP, is implemented on a count base. It releases all processes when counts to <code>n_processes</code>.</p> <pre><code>int MPI_Barrier(MPI_Comm comm);\n\n/* Deadlock example\nif (rank % 2) {\n    ...\n    MPI_Barrier(MPI_Comm comm);\n}\n*/\n</code></pre>"},{"location":"csc367/mpi_comm.html#mpi_bcast-broadcast","title":"<code>MPI_Bcast</code> Broadcast","text":"<p>Broadcasts a message from the process with rank root to all other processes <pre><code>int MPI_Bcast(\n    void *buffer, int count, MPI_Datatype datatype, int root, \n    MPI_Comm comm\n);\n</code></pre> For example, root process will generate the data (with some randomness) and broadcast to all processes so that all processes get the same data. <pre><code>int data[100];\nint root = 0, rank;\nMPI_Comm_rank(MPI_COMM_WORLD, &amp;rank);\n\n// root process generate data\nif (rank == root) random_init(data);\n\n/* \n    \"-\" can be 0 or any garbage in the memory\n    P0: 1 4 3 5 2 4 ... 3\n    P1: - - - - - - ... -\n    P1: - - - - - - ... -\n    P1: - - - - - - ... -\n*/\nMPI_Bcast(data, 100, MPI_INT, root, MPI_COMM_WORLD);\n/* \n    P0: 1 4 3 5 2 4 ... 3\n    P1: 1 4 3 5 2 4 ... 3\n    P2: 1 4 3 5 2 4 ... 3\n    P3: 1 4 3 5 2 4 ... 3\n*/\n</code></pre></p> <p>Broadcast is more efficient then let the root send <code>np-1</code> messages out, since it is optimized to send message in a hierarchical way (eg. <code>root -&gt; 2 processes -&gt; 4 processes -&gt; ... -&gt; all processes</code>). </p>"},{"location":"csc367/mpi_comm.html#mpi_reduce-reduction","title":"<code>MPI_Reduce</code> Reduction","text":"<p>Reduces values on all processes to a single value, stored in the <code>root</code> process.  </p> <p>Or <code>Allreduce</code>, stored (broadcast) in all processes.</p> <p>Note that it is an element-wise reduction so that send buffer and recv buffer must be of the same count and dtype. </p> <p><pre><code>int MPI_Reduce(\n    const void *sendbuf, void *recvbuf, \n    int count, MPI_Datatype datatype,\n    MPI_Op op, \n    int root, \n    MPI_Comm comm\n);\n\nint MPI_Allreduce(\n    const void *sendbuf, void *recvbuf, \n    int count, MPI_Datatype datatype, \n    MPI_Op op, \n    MPI_Comm comm\n);\n\n/* MPI_Op is the reduction operation */\n// max min argmax argmin\nMPI_MAX, MPI_MIN, MPI_MINLOC, MPI_MAXLOC\n// sum product\nMPI_SUM, MPI_PROD\n// logical and or xor\nMPI_LAND, MPI_LOR, MPI_LXOR\n// bitwise and or xor\nMPI_BAND, MPI_BOR, MPI_BXOR\n// or user defined op\n</code></pre> Intuitive example of reduction with sum <pre><code>int data[6], sum[6];\ninit_darandom_init(data);ta\n/* data:\n    P0: 1 3 2 5 2 4\n    P1: 4 5 2 6 2 5\n    P2: 2 5 3 6 1 4\n    P3: 5 6 1 0 6 2\n*/\n\nMPI_Reduce(data, sum, 6,  MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n/* sum:\n    P0: 12 19  8 17 11 15\n    P1:  -  -  -  -  -  -\n    P2:  -  -  -  -  -  -\n    P3:  -  -  -  -  -  -\n*/\nMPI_Allreduce(data, sum, 6,  MPI_INT, MPI_SUM, MPI_COMM_WORLD);\n/* sum:\n    P0: 12 19  8 17 11 15\n    P1: 12 19  8 17 11 15\n    P2: 12 19  8 17 11 15\n    P3: 12 19  8 17 11 15\n*/\n</code></pre></p>"},{"location":"csc367/mpi_comm.html#mpi_scatter-mpi_gather","title":"<code>MPI_Scatter, MPI_Gather</code>","text":"<p>Scatters a buffer to all processes,  <code>Scatterv</code> version can specify different number of elements to be scattered <pre><code>int MPI_Scatter(\n    const void *sendbuf, int sendcount, MPI_Datatype sendtype,\n    void *recvbuf, int recvcount, MPI_Datatype recvtype, \n    int root,\n    MPI_Comm comm\n);\n\nint MPI_Scatterv(\n    const void *sendbuf, \n    // array of length n_process, \n    // specifying the number of elements to send to each processor\n    const int *sendcounts, \n    // array of length n_process, \n    // specifying the displacement (relative to sendbuf)\n    const int *displs,\n    MPI_Datatype sendtype,\n    void *recvbuf, int recvcount, MPI_Datatype recvtype,\n    int root, \n    MPI_Comm comm\n);\n</code></pre> Gathers together values from a group of processes. <code>Gatherv</code> version can specify different number of elements to be gathered. <code>Allgather, Allgatherv</code> is similar to all reduce, where the gather results are broadcasted to all processes. <pre><code>int MPI_Gather(\n    const void *sendbuf, int sendcount, MPI_Datatype sendtype,\n    // recv args only apply to the root process, \n    // can be invalid for other processes\n    void *recvbuf, int recvcount, MPI_Datatype recvtype, \n    int root, \n    MPI_Comm comm\n);\n\nint MPI_Gatherv(\n    const void *sendbuf, int sendcount, MPI_Datatype sendtype,\n    // recv args only apply to the root process, \n    // can be invalid for other processes\n    void *recvbuf, \n    const int *recvcounts, \n    const int *displs,\n    MPI_Datatype recvtype, \n    int root, \n    MPI_Comm comm\n);\n\nint MPI_Allather(\n    const void *sendbuf, int sendcount, MPI_Datatype sendtype,\n    void *recvbuf, int recvcount, MPI_Datatype recvtype,\n    MPI_Comm comm\n);\n\nint MPI_Allgatherv(\n    const void *sendbuf, int sendcount, MPI_Datatype sendtype,\n    void *recvbuf, \n    const int *recvcounts, \n    const int *displs,\n    MPI_Datatype recvtype, \n    MPI_Comm comm\n);\n</code></pre></p> <p>The self-explanatory example</p> <pre><code>int a[8], b[4];\nif (rank == root) init_data(a);\n\n/* a\n    P0: 1 2 3 4 5 6 7 8\n    P1: - - - - - - - -\n    P2: - - - - - - - -\n    P3: - - - - - - - -\n*/\nMPI_Scatter(a, 2, MPI_INT, b, 2, MPI_INI, root, MPI_COMM_WORLD);\n/* b\n    P0: 1 2 - -\n    P1: 3 4 - -\n    P2: 5 6 - -\n    P3: 7 8 - -\n*/\n\n(*b) *= -1;\n/* b\n    P0: -1 2 - -\n    P1: -3 4 - -\n    P2: -5 6 - -\n    P3: -7 8 - -\n*/\nMPI_Gather(b, 2, MPI_INT, a, 2, MPI_INT, root, MPI_COMM_WORLD);\n/* a\n    P0: -1  2 -3  4 -5  6 -7  8\n    P1:  -  -  -  -  -  -  -  -\n    P2:  -  -  -  -  -  -  -  -\n    P3:  -  -  -  -  -  -  -  -\n*/\nMPI_Allather(b, 2, MPI_INT, a, 2, MPI_INT, MPI_COMM_WORLD);\n/* a\n    P0: -1  2 -3  4 -5  6 -7  8\n    P1: -1  2 -3  4 -5  6 -7  8\n    P2: -1  2 -3  4 -5  6 -7  8\n    P3: -1  2 -3  4 -5  6 -7  8\n*/\n</code></pre>"},{"location":"csc367/mpi_comm.html#mpi_alltoall","title":"<code>MPI_Alltoall</code>","text":"<p>Sends data from all to all processes (better explained with example).</p> <p>If <code>sendcount, recvcount</code> are both <code>1</code>, then it can be understood as a transpose.  <pre><code>int MPI_Alltoall(\n    const void *sendbuf, int sendcount, MPI_Datatype sendtype,\n    void *recvbuf, int recvcount, MPI_Datatype recvtype,\n    MPI_Comm comm\n);\n\n// example\nint a[8], b[8];\nif (rank == root) init_data(a);\n/* a\n    P0:  0  1  2  3  4  5  6  7\n    P1: 10 11 12 13 14 15 16 17\n    P2: 20 21 22 23 24 25 26 27\n    P3: 30 31 32 33 34 35 36 37\n*/\nMPI_Alltoall(a, 2, MPI_INT, b, 2, MPI_INT, MPI_COMM_WORLD);\n/* b\n    P0:  0  1 10 11 20 21 30 31\n    P1:  2  3 12 13 22 23 32 33\n    P2:  4  5 14 15 24 25 34 35\n    P3:  6  7 16 17 26 27 36 37\n*/\n</code></pre></p>"},{"location":"csc367/omp.html","title":"OpenMP - Introduction of Parallelism","text":"<p>PThreads are based on OS features. It has high overhead for thread creation. Also, it is quite low level, which means more and harder-found data race bugs. Also, deadlocks are usually easier. </p>"},{"location":"csc367/omp.html#introduction","title":"Introduction","text":"<p>As almost all machines today are using multi-core CPU, we need simpler, \"lighter\" syntax way to write multithreading code.  An alternative is OpenMP. </p> <p>Using PThreads with  <pre><code>#include &lt;omp.h&gt;\n</code></pre> compile (using <code>gcc</code>) with  <pre><code>gcc -fopenmp *.c \n</code></pre></p>"},{"location":"csc367/omp.html#basic-directives","title":"Basic directives","text":"<p>OpenMP uses \"hints\" or compiler \"directives\" as to what intended to parallelize <pre><code>#pragma omp directivename [clause list]\n</code></pre></p>"},{"location":"csc367/omp.html#library-functions","title":"Library functions","text":"<p>Also, library functions included in <code>omp.h</code> for providing information about currently running program <pre><code>/* used within parallel */\nint omp_get_num_threads(); // return #threads running to the closest block\nint omp_get_thread_num(); // get thread id\nint omp_in_parallel(); // return non-zero if within a parallel region\n\n/* used anywhere */\nint omp_get_max_threads() // max #threads can be created;\nint omp_get_num_procs(); // #processors available \nvoid omp_set_num_threads(int n); // set #threads for the next parallel section \n</code></pre></p>"},{"location":"csc367/omp.html#basic-example","title":"Basic Example","text":"<pre><code>int main() {\n    omp_set_num_threads(8);\n    #pragma omp parallel {\n        omp_get_thread_num(); // 1 ... 8, no guarantees on order,\n        omp_get_num_threads(); // 8;\n    }\n    omp_get_num_threads(); // 1\n    return 0;\n}\n</code></pre>"},{"location":"csc367/omp.html#directive-clauses","title":"Directive Clauses","text":"<pre><code>// only parallel if expr holds\n#pragma omp parallel if(expr) \n\n// #threads, overrides omp_set_num_threads or env variable OMP_NUM_THREADS\n#pragma omp parallel num_threads(8)\n</code></pre>"},{"location":"csc367/omp.html#nested-parallelism","title":"Nested parallelism","text":"<p>OpenMP supported arbitrarily deep nesting of <code>omp parallel</code>, whenever there are enough threads. However, <code>omp_get_num_threads, omp_get_thread_num</code> will only get number of threads within its group/parallel region.  <pre><code>#pragma omp parallel num_threads(2) {\n    omp_get_num_threads(); // 2\n    #pragma omp parallel num_threads(2) {\n        // there are 4 threads/cores running\n        omp_get_num_threads(); // 2\n    }\n    omp_get_num_threads(); // 2\n}\n</code></pre></p>"},{"location":"csc367/omp.html#variable-semantics","title":"Variable Semantics","text":""},{"location":"csc367/omp.html#data-sharing-scope-of-variables","title":"Data sharing / scope of variables","text":"<p>-<code>private</code>: each thread get a local copy -<code>shared</code>: all threads share the same copy -<code>firstprivate</code>: like private, but init to the value before the parallel directive <pre><code>int main() {\n    int A = 0, B = 1, C = 2, D = 3, E = 4;\n    #pragma omp parallel private(A, B) shared(C) firstprivate(D) {\n        printf(\"%d\\n\", A); // undeclared \n        B = 5;\n        printf(\"%d\\n\", B); // 5\n\n        printf(\"%d\\n\", C); // 2\n        C = -2;\n        printf(\"%d\\n\", C); // -2\n\n        printf(\"%d\\n\", D); // 3\n        D = -3;\n        printf(\"%d\\n\", D); // -3\n\n        printf(\"%d\\n\", E); // 4\n        E = -4;\n        printf(\"%d\\n\", E); // -4\n    }\n    printf(\"%d\\n\", A); // 0\n    printf(\"%d\\n\", B); // 1\n    printf(\"%d\\n\", C); // -2\n    printf(\"%d\\n\", D); // 3\n    printf(\"%d\\n\", E); // -4\n</code></pre></p>"},{"location":"csc367/omp.html#default-state","title":"<code>default</code> State","text":"<p>use default clause to set default state <pre><code>#pragma omp parallel default(shared | none)\n</code></pre></p> <p><code>shared</code>: unless specified using private/firstprivate, all variables are shared even variables init inside the parallel block <pre><code>#pragma omp parallel default(shared)\n</code></pre> <code>none</code>: must specify every variable used in parallel region otherwise compile error <pre><code>#pragma omp parallel default(none)\n</code></pre> no <code>default</code> clause: variable declared outside a parallel block is <code>shared</code> (with some exceptions like loop counters). inside is implicitly <code>private</code>. <pre><code>#pragma omp parallel\n</code></pre></p>"},{"location":"csc367/omp.html#reduction","title":"Reduction","text":"<p>Reduction refers to the operations where multiple local copies of a variable are combined into a single copy at the master when the parallel block ends. </p> <p>Syntax <pre><code>#pragma omp parallel reduction(op: var)\n</code></pre></p> <p>For example,  <pre><code>int s = 0;\n#pragma omp parallel reduction(+: s) {\n    s += 10;\n}\n\n/*  reduction can be considered as the following \n    but it guarantees no sync bugs and no contentions\n*/\nint s = 0; \n#pragma omp parallel {\n    int s_local = s; // get a private copy\n    s_local += 10; // do operations\n    atomic_add(s, s_local); // atomic add as specified reduction ops\n}\n</code></pre> The supported op includes  <code>+, -, *, &amp;, |, ^, &amp;&amp;, ||</code></p>"},{"location":"csc367/omp.html#example-dot-product","title":"Example: dot product","text":"<pre><code>// compute the dot product of vec a * b\nfloat dot(float *a, float *b, size_t n) {\n    int s;\n    #pragma omp parallel reduction(+: s) {\n        int nthreads = omp_get_num_threads();\n        int tid = omp_get_thread_num();\n        int chunk = (n + nthreads - 1) / nthreads;\n\n        for (int i = tid*chunk; i &lt; tid*chunk + chunk &amp;&amp; i &lt; n; i++) {\n            s += a[i] * b[i];\n        }\n    }\n}\n</code></pre>"},{"location":"csc367/omp_for.html","title":"OpenMP - For Loop, Sections, Tasks","text":""},{"location":"csc367/omp_for.html#for-loops-in-concurrent-tasks","title":"For Loops in Concurrent Tasks","text":"<p>From the dot product example, we see that we manually partition the <code>for</code> loop using thread number and thread id. </p> <p>Such partitioning can be done by  <pre><code>/* pragma omp for must be immediately before the for loop\n*/\n#pragma omp parallel [clause list] {\n    /* ... */\n    #pragma omp for [clause list]\n    for (init; test; update) {\n        /* loop body */\n    }\n    /* ... */\n}\n</code></pre></p>"},{"location":"csc367/omp_for.html#restrictions-on-for-loop","title":"Restrictions on <code>for</code> loop","text":"<p>Therefore several restrictions on the for loop to be parallelized by <code>omp</code> <pre><code>// for (init_expr, test_expr, update_expr)\nfor (int i = 0; i &lt; n; i++) {\n    /* body */\n}\n</code></pre></p> <ul> <li><code>init_expr</code> must be integer type and must be an integer assignment</li> <li><code>test_expr</code> must be a <code>&lt;, &gt;, &lt;=, &gt;=</code> expr</li> <li><code>update_expr</code> must be integer increments</li> <li><code>body</code> cannot have <code>break</code></li> </ul>"},{"location":"csc367/omp_for.html#for-loop-clauses","title":"For loop Clauses","text":"<p>The <code>clause list</code> includes</p> <ul> <li><code>private(var), firstprivate(var)</code> as parallel directive</li> <li><code>lastprivate(var)</code> only in <code>for, sections</code>, set variable to the thread that execute the last iteration or last section</li> <li><code>reduction(op: var)</code>  as parallel directive</li> <li><code>schedule(cls[, param])</code> specify how the for loop should be divided, more on <code>schedule</code> section</li> <li><code>nowait</code> only in <code>for, sections</code>, no barrier/sync after the for loop</li> <li><code>ordered</code> only in <code>for</code>, block of code that must be executed in sequential order <pre><code>// https://stackoverflow.com/a/13230816\ntid  List of     Timeline\n     iterations\n0    0,1,2       ==o==o==o\n1    3,4,5       ==.......o==o==o\n2    6,7,8       ==..............o==o==o\n</code></pre></li> </ul>"},{"location":"csc367/omp_for.html#parallel-for","title":"Parallel For","text":"<p>We can combine <code>#pragma omp parallel</code> with <code>#pragma omp for</code> as  <pre><code>#pragma omp parallel for [clause list]\n</code></pre> where <code>clause list</code> is the union of <code>parallel</code> and <code>for</code>. Note that the scope clauses <code>private, default(none), default(shared)</code> will not impact the behavior of loop counter. </p>"},{"location":"csc367/omp_for.html#loop-counter-and-loop-carried-dependence","title":"Loop counter and loop carried dependence","text":"<p>Note that <code>#pragma omp for</code> will make loop index private by default, and assign new values and conditions. Thus, for correctness, there should be no loop carried dependence in loop body. For example,</p> <pre><code>int j = 5;\nfor (int i = 0; i &lt; n; i++) {\n    // j is increment in each iteration\n    j += 2;\n    A[i] = b[j];\n}\n\n/* j is loop dependent, \n   adding omp for directly will be incorrect\n*/\n#pragma omp parallel for\nfor (int i = 0; i &lt; n; i++) {\n    // j is increment in each iteration\n    int j = 5 + 2 * (i + 1);\n    A[i] = b[j];\n}\n</code></pre>"},{"location":"csc367/omp_for.html#schedule-ways-to-assign-iterations","title":"<code>schedule</code>: Ways to assign iterations","text":"<p>The schedule clause is of the form <code>schedule(cls[, param])</code></p> <p>where <code>cls</code> is of <code>static, dynamic, guided, runtime</code></p>"},{"location":"csc367/omp_for.html#static-schedulestatic-s","title":"Static <code>schedule(static, S)</code>","text":"<p>Split iterations into equal chunks of size <code>S</code>, assign to thread round-robin. If <code>S</code> is not specified, <code>S = N / nthreads</code> (uniform data partition model) <pre><code>/* the following 2 are the same, each thread gets\nT0: [0: 256]  T1: [256: 512]  T2: [512: 768]  T3: [768: 1024]\n*/\n#pragma omp parallel for schedule(static) num_threads(4)\nfor (int i = 0; i &lt; 1024; i++)\n#pragma omp parallel for schedule(static, 256) num_threads(4)\nfor (int i = 0; i &lt; 1024; i++)\n\n/* If chunk size is smaller \nT0: [0:128], [512:640]\nT1: [128:256], [640:768]\nT2: [256:384], [768:896]\nT3: [384:512], [896:1024]\n*/\n#pragma omp parallel for schedule(static, 128) num_threads(4)\nfor (int i = 0; i &lt; 1024; i++)\n</code></pre></p>"},{"location":"csc367/omp_for.html#dynamic-scheduledynamic-s","title":"Dynamic <code>schedule(dynamic, S)</code>","text":"<p>Split iterations into equal chunks of size <code>S</code>, assign to thread when it is idle. If <code>S</code> is not specified, <code>S = 1</code>. (Work pool model)</p> <p>Often has better load balance if the amount of work is uneven among iterations. However, it has more overhead due to dynamic nature. </p> <p>Also, tradeoffs between chunk size and dynamic scheduling overhead.  Larger chunk size may cause huge load imbalance (think of 20 chunks, 16 threads). </p>"},{"location":"csc367/omp_for.html#guided-scheduleguided-s","title":"Guided <code>schedule(guided, S)</code>","text":"<p>Dynamic scheduling, but instead of constant chunk size, it starts with a large chunk size and gets smaller as computation progresses, if loads gets imbalanced. </p> <p><code>S</code> is the minimum size, if not specified, then <code>S = 1</code>. </p>"},{"location":"csc367/omp_for.html#runtime-scheduleruntime","title":"Runtime <code>schedule(runtime)</code>","text":"<p>Delay until runtime, by passing env variable <code>OMP_SHEDULE</code> to the program</p>"},{"location":"csc367/omp_for.html#auto","title":"Auto","text":"<p>If no schedule type is identified (either as clause or at runtime), the runtime system will choose the most appropriate schedule (depends on OS/hardware).</p>"},{"location":"csc367/omp_for.html#nowait-clause","title":"<code>nowait</code> clause","text":"<p>Each <code>omp for</code> will have a implicit barrier, using <code>nowait</code> can cancel the wait behavior. </p>"},{"location":"csc367/omp_for.html#sections-and-tasks","title":"Sections and Tasks","text":"<p>In the for loop, each iteration of chunk of iterations have the same work. We can have similar kinds of parallelism with different tasks, using <code>sections</code>. </p>"},{"location":"csc367/omp_for.html#sections","title":"Sections","text":"<p><pre><code>#pragma omp parallel {\n    #pragma omp sections [clauses] {\n        #pragma omp section {\n            // task 1\n        }\n        #pragma omp section {\n            // task 2\n        }\n        /* ... */\n        #pragma omp section {\n            // task n\n        }\n    }\n}\n\n// similar to for, it can be merged into\n#pragma omp parallel sections [clauses]\n</code></pre> <code>clauses list</code> includes <code>private, firstprivate, lastprivate, reduction, nowait</code>. <code>sections</code> behaves similar to <code>for</code>, it will have an implicit barrier (hence <code>nowait</code>). </p>"},{"location":"csc367/omp_for.html#tasks","title":"Tasks","text":"<p>Another way is to use <code>task</code>, it is similar to <code>section</code>, but won't put barrier. For sync/barrier, we use <code>taskwait</code>.  <pre><code>#pragma omp parallel {\n    #pragma omp task [clauses] {\n      // task 1\n    }\n    #pragma omp task [clauses] {\n      // task 2\n    }\n    /* ... */\n    #pragma omp task [clauses] {\n      // task n\n    }\n}\n</code></pre></p> <p>Note that <code>omp task</code> generates a task whenever a thread encounters it, and the task can be executed immediate or deferred. A deferred task is not necessarily executed by the thread that creates it. And<code>taskwait</code> will wait for all <code>task</code> generated by the thread. </p> <p>With such properties, task are designed and mostly used for recursive decompositions. Where a single thread enters the function, generates new tasks when encounter the <code>omp task</code> clause, and then <code>taskwait</code> all generated <code>task</code> to be finished.  <pre><code>// parallel recursive fibonacci implementation\nint fib(int n) {\n    int x, y;\n    if (n &lt; 2) return n;\n\n    // x, y are local to the thread executing fib\n    // but they have to be shared on recursive child tasks\n    // further generated\n    #pragma omp task shared(x)\n    x = fib(n-1);\n    #pragma omp task shared(y)\n    y = fib(n-2);\n    // wait for recursive calls and collect results\n    #pragma omp taskwait\n\n    return x + y;\n}\n\nint main() {\n    int M = 5000;\n    #pragma omp parallel {\n        // single thread to enter the function call\n        // other threads created by parallel will \n        // execute the tasks generated\n        #pragma omp single\n        fib(M);\n    }\n}\n</code></pre></p>"},{"location":"csc367/omp_sync.html","title":"OpenMP - Synchronizations and Performance Profiling","text":""},{"location":"csc367/omp_sync.html#synchronization","title":"Synchronization","text":""},{"location":"csc367/omp_sync.html#barrier","title":"<code>barrier</code>","text":"<p>Wait for all threads spawned by closest enclosing <code>parallel</code>.  <pre><code>#pragma omp barrier\n</code></pre></p> <p>Note that <code>for</code> and <code>sections</code> will set implicit barriers. </p>"},{"location":"csc367/omp_sync.html#single-and-master","title":"<code>single</code> and <code>master</code>","text":"<p><pre><code>#pragma omp single [private | firstprivate | nowait] {\n    /* ... */\n}\n</code></pre> Only one thread will execute the code block, and set a implicit barrier unless <code>nowait</code></p> <p><pre><code>#pragma omp single [private | firstprivate]{\n    /* ... */\n}\n</code></pre> Only the master thread executes the code block, no barrier will be set. </p>"},{"location":"csc367/omp_sync.html#critical-sections","title":"<code>critical</code> sections","text":"<p><code>critical [(name)]</code> Set up a critical section (in <code>section</code>) where threads must serialize to avoid race conditions.</p> <pre><code>int data = 100;\n#pragma omp parallel sections {\n    #pragma omp section {\n        #pragma omp critical(cs1) {\n        data += 42;\n    }\n}\n    #pragma omp section {\n        #pragma omp critical(cs1) {\n            data += 5;\n        }\n    }\n}\nprintf(\"Data=%d\\n\", data); // 149\n</code></pre>"},{"location":"csc367/omp_sync.html#atomic","title":"<code>atomic</code>","text":"<p>If the critical section is just an update to a single memory location, and it's a supported atomic operation <pre><code>#pragma omp atomic [ read | write | update | capture ]\nvar op= expr\n\n// for example\n#pragma omp atomic update\nsum_global += sum_local;\n</code></pre></p> <p>Such operations still have overhead (varying based on hardware support), but smaller than <code>critical</code>. </p>"},{"location":"csc367/omp_sync.html#explicit-lock","title":"Explicit <code>lock</code>","text":"<p>The same way as of pthread <code>mutex</code> <pre><code>void omp_init_lock(omp_lock_t *lock); // pthread_mutex_init\nvoid omp_destroy_lock(omp_lock_t *lock);// pthread_mutex_destroy\nvoid omp_set_lock(omp_lock_t *lock); // pthread_mutex_lock\nvoid omp_unset_lock(omp_lock_t *lock); // pthread_mutex_unlock\nint omp_test_lock(omp_lock_t *lock); // pthread_mutex_trylock\n\n// nested locks\n// can be locked multiple times by same thread\n// unlocked only once it's been unset the same number of times\nvoid omp_init_nest_lock(omp_lock_t *lock);\nvoid omp_destroy_nest_lock(omp_lock_t *lock);\nvoid omp_set_nest_lock(omp_lock_t *lock);\nvoid omp_unset_nest_lock(omp_lock_t *lock);\nint omp_test_nest_lock(omp_lock_t *lock);\n</code></pre></p>"},{"location":"csc367/omp_sync.html#performance-profiling","title":"Performance Profiling","text":"<p><code>omp</code> has its built-in timing functions <pre><code>// returns elapsed wall clock time in seconds\ndouble omp_get_wtime();\n\ndouble start = omp_get_wtime();\n#pragma omp parallel {\n    /* ... */\n}\ndouble time = omp_get_wtime() - start;\n\n// returns the number of seconds between two successive clock ticks\ndouble omp_get_wtick();\n\nomp_get_wtick();\n#pragma omp parallel {\n    /* ... */\n}\ndouble time = omp_get_wtick();\n</code></pre></p>"},{"location":"csc367/omp_sync.html#false-sharing-and-cache-coherence","title":"False Sharing and Cache Coherence","text":"<p>Consider our shared memory system, note that each thread (physically the core) gets its own L1 cache (and probably L2 depends on arch), while share the L3 cache and memory. </p> <p>Note that the smallest unit for moving between the cache and the memory is the cache line, typically 64 bytes. It is larger than a <code>float</code> (4 Bytes) or <code>double</code> (8 Bytes). If two (or more) threads are modifying close data, the cache line is not coherent among all threads, and the hardware (processor) need time to sync them up, which is called false sharing. </p> <p>When a system participant attempts to periodically access data that is not being altered by another party, but that data shares a cache block with data that is being altered, the caching protocol may force the first participant to reload the whole cache block despite a lack of logical necessity.</p> <p>A typical example is that each thread is given some iterative work, and at each iteration we update them into a global array <pre><code>double result[NUM_THREADS] = {0};\n\n#pragma omp parallel {\n    int tid =  omp_get_thread_num();\n    /* ... */\n    for (int i = 0; i &lt; N; i++) {\n        result[tid] += ...;\n    }\n}\n</code></pre> Solution: if you know the cache line size of the machine, then pad the array to avoid false sharing.  <pre><code>/* cache line size / sizeof(double) */\n# define PAD 8 \ndouble result[NUM_THREADS][0] = {0};\n\n#pragma omp parallel {\n    int tid =  omp_get_thread_num();\n    /* ... */\n    for (int i = 0; i &lt; N; i++) {\n        result[tid][0] += ...;\n    }\n}\n</code></pre></p>"},{"location":"csc367/parallel.html","title":"Parallel Algorithm Design","text":""},{"location":"csc367/parallel.html#general-guidelines","title":"General Guidelines","text":"<p>Given a program, we are interested in  - which tasks can be performed concurrently  - how to map concurrent tasks (ops) onto multiple threads/processes/devices  - how to partition data and assign to processes  - how to handle concurrent accesses to shared data  - synchronizations to make sure data integrity (correctness)</p>"},{"location":"csc367/parallel.html#task-decomposition","title":"Task Decomposition","text":"<p>A task is a unit of computation taht can be extracted from the main program and assigned to a process, and which can be run concurrently with other tasks. </p> <p>For example, for matrix-vector multiplication \\(y = Ax\\), it can be seen as \\(n\\) tasks <pre><code>y = [A[i, :] * x[:] for i in range(n)]\n</code></pre></p>"},{"location":"csc367/parallel.html#task-dependencies","title":"Task Dependencies","text":"<p>If a task A requires the results/data from task B, then A depends on B since A has to wait for B is ready. </p> <p>Thus, we can represent the dependency as a directed acyclic graph where tasks are nodes and dependencies are edges. A node is a start node if no incoming edges, and finish node is no outgoing edges. </p> <p>Suppose that each task takes approximate the same time and there are as many processes as possible, then the longest path will dominate the running time. Otherwise, if we assign each node with the time for executing its task, then the most costly path will dominate. </p>"},{"location":"csc367/parallel.html#degree-of-concurrency","title":"Degree of Concurrency","text":"<p>Max degree of concurrency is the max #tasks that can be executed simultaneously at any given time</p> <p>Average degree of concurrency is the average #tasks during the program's execution.</p> <p>Consider the task dependencies graph. Since it is a DAG, each node can be assigned with a level as the number of nodes of the longest path to a starting node. Degree of concurrency is computed as the sum of all nodes at the same level.   Critical path is the path with the largest sum of all nodes. Max deg of concurrency is computed as the max of degree of concurrency at each level.  Average degree of concurrency is the sum of all tasks, divided by the sum on the critical path. </p>"},{"location":"csc367/parallel.html#granularity","title":"Granularity","text":"<p>determined by how many tasks what the workload/size of each task. Often described as coarse or fine. </p> <p>For matrix-vector multiplication example. A fine-grained decomposition will be \\(n\\) tasks  </p> <pre><code>y = [A[i, :] * x[:] for i in range(n)]\n</code></pre> <p>A coarse-grained will be 2 tasks</p> <pre><code>y = A[:n/2, :] * x[:] , A[n/2:, :] * x[:] \n</code></pre> <p>With higher granularity, it looks like more concurrency is possible. However, it is more difficult to properly divide tasks, and it will take more time for computing the partitioning. </p>"},{"location":"csc367/parallel.html#task-interactions","title":"Task Interactions","text":"<p>Task dependency only consider producer-consumer interactions, a.k.a. input/output. However, there are other kinds of interactions. For example, exchange data, synchronize.</p> <p>Read-only interactions only need to read data shared among tasks. For example, all tasks in vec-mat multi. \\(y=Ax\\) need to access vector x. </p> <p>Read-write interactions task can read or write data shared. For example, several threads read the same array and sum them up to a number. </p> <p>Typically, read-only is safer to divide, while read-write should be kept on the same process as much as possible. </p>"},{"location":"csc367/parallel.html#decomposition-techs","title":"Decomposition Techs","text":""},{"location":"csc367/parallel.html#recursive-decomposition","title":"Recursive Decomposition","text":"<p>Typically useful for problems with divide-and-conquer strategy, where each subproblem can be solved concurrently. </p> <p>The program will recusively span new processes/threads, and recollect them. </p>"},{"location":"csc367/parallel.html#data-decomposition","title":"Data Decomposition","text":"<p>Partition the data on which computations are performed. </p> <p>For example, for mat-vec multiplication, we partition on the output vector \\(y\\), each process compute the dot-product of two vectors. Or we can partition on the input data, we partion \\(A\\) into rows and \\(B\\) into scalars, each process compute part of \\(b\\) and sum them together. </p>"},{"location":"csc367/parallel.html#mapping-to-processes","title":"Mapping to Processes","text":"<p>Given a task graph with different time for each task. We want to map the tasks onto \\(p\\) processes. The goal is to minimize completion time by  - load balances, maximize the use of concurrency (processes don't get idle) - minimize interactions among processes</p>"},{"location":"csc367/parallel.html#static-vs-dynamic-mapping","title":"Static vs. Dynamic Mapping","text":"static dynamic how assign tasks before execution starts assign tasks during execution example parallel_for Python Pool pros less overhead on partitioning, if the task size is well known, can be balanced dynamic load balancing, usually more effective than static if size unknown cons If the task size is unknown, a naive assignment may result in severe load imbalances. overhead on dynamically assigning work, overheads for data movement"},{"location":"csc367/parallel.html#data-interaction-overheads","title":"Data Interaction Overheads","text":"<p>Processes share data and/or may require data generated by other processes. We need to  = minimize the volume of interaction overheads (use local data as much as possible) - minimize interactions frequency (use large chunks of shared data to reduce number of interactions)</p>"},{"location":"csc367/parallel.html#contention-and-hotspots","title":"Contention and hotspots","text":"<p>Contention happens if accessing shared data concurrently. For example, several processes try to access the same memory block, or interact with one specific process with messages. </p> <p>One common category of problems is reduction. Which an array is reduced to a specific value, such as <code>sum</code>, <code>prod</code>. A naive implementation of <code>sum</code> will use \\(p\\) processes to compute sum of its array partition and add to one variable, causing high contention. A more appropriate way is to  decentralize the shared data by pair-wise summing processes in \\(\\lg(n)\\) steps. </p>"},{"location":"csc367/parallel.html#overlap-computations-with-interactions","title":"Overlap computations with interactions","text":"<p>Process may idle waiting for shared data before sync happens. We can assign more tasks before current task is completed (e.g. context switch) or initiate an interaction earlier than necessary. </p> <p>Need more hardware/OS support. Commonly seen in OS kernel, the OS is switching among program processes, while waiting for kernel mode execution or disk IO. </p>"},{"location":"csc367/parallel.html#replicate-data-or-computations","title":"Replicate Data or Computations","text":"<p>Instead of interaction overheads, simply replicate the necessary data for each process. </p> <p>Benefiticial for read-only interactions and the shared data is not too large. If the interactions has write, then maintaining the coherent copies will have more overheads since you need to broadcast the changes to all processes. </p>"},{"location":"csc367/parallel.html#common-parallel-algorithm-models","title":"Common Parallel Algorithm Models","text":"data parallel work pool master slave description each process works on one data partition processes take work from a pool, take another when finish current work a master process generates work and allocates to worker processes decomposition static and uniform data partitioning depends on the problem depends on the problem mapping (mostly) static dynamic Often dynamic strategies for interactions locality-preserving decomposition, overlap computation with interaction adjust granularity, tradeoff between load imbalance and overhead for managing pool Choose granularity for master, overlap computation typical use case problems with known size non-fixed and/or imbalanced task size distributed parallel arch (eg. web server)"},{"location":"csc367/parallel.html#performance-model","title":"Performance Model","text":"<p>Note that often times \\(N\\) processes does not results in \\(N\\) times performance due to overheads. </p> <p>Overheads include inter-process communication, idling, and excess computation (sometimes necessary for reducing communications). </p> <p>Given the resources (#processes), the execution time is obviously <code>end_time - start_time</code>, which is determined by slowest process. </p>"},{"location":"csc367/parallel.html#speedup","title":"Speedup","text":"<p>One measure for performance gain is speedup \\(S=T_s/T_p\\), i.e. the time ratio between serial execution and parallel execution with \\(p\\) processors.  </p> <p>Note that \\(T_s\\) must be fully optimized serial implementation, otherwise this measurement is not useful. </p>"},{"location":"csc367/parallel.html#considerations","title":"Considerations","text":"<p>Also, it is very clear that \\(S \\leq p\\), i.e. Maximum achievable speedup as linear speedup. </p> <p>If \\(S &gt; p\\), i.e. superlinear speedup can happen if sequential algorithm is at a disadvantage compared to parallel version. For example, data too large to fit into L1 cache (since it is per-core) so that data accesses are slower. However, in most cases, espeically for shared memory models, this rarely happens.  </p>"},{"location":"csc367/parallel.html#amdahls-law","title":"Amdahl's Law","text":"<p>Let the total time for a squential execution be \\(1\\), </p> \\[ T_s = T_{1} + T_{2} = 1 \\] <p>where \\(T_1\\) is the fraction done sequentially and \\(T_2\\) be the fraction done parallelizable (with 1 process).  Then consider \\(p\\) processes, \\(T_1\\) is not parallelizable so that unchanged, \\(T_2\\) can have at most a linear speedup</p> \\[ T_p = T_{1} + T_{2}' \\leq T_{1} + T_{2}/p \\] <p>Thus, the speedup is </p> \\[S(p) = \\frac{1}{T_1+T_{2}/p}\\leq T_1^{-1}\\] <p>Thus, the speedup is upper bounded by the fraction of sequencial work. </p>"},{"location":"csc367/parallel.html#efficiency","title":"Efficiency","text":"<p>Efficiency is defined as </p> \\[E = S / p\\] <p>Since \\(1 \\leq S \\leq p\\), \\(0 \\leq E \\leq 1\\) where \\(1\\) is the ideal efficiency. </p>"},{"location":"csc367/pthread.html","title":"Shared Memory Model with PThread","text":""},{"location":"csc367/pthread.html#shared-memory-model","title":"Shared Memory Model","text":"<p>A parallel programming model is the languages/libraries that abstract a machine system (with multiple cores/processors/servers). </p> <p>It helps us to identify controls (how to start parallelism, the ordering of operations), data (private vs. shared, what kind of interactions), synchronization (when to communicate, atomic ops).</p> <p>Shared memory model is the most common model for personal computers, where each program is a collection of threads of control, and can be created mid-execution. </p> <p>For shared memory model, each worker is a thread. Each threads has a set of private variables and all together a set of shared variables. Communications are implicitly done by reading/writing the shard variables. </p>"},{"location":"csc367/pthread.html#posix-threads-pthreads","title":"POSIX Threads (PThreads)","text":"<p>Using PThreads with </p> <p><pre><code>#include &lt;pthread.h&gt;\n</code></pre> compile (using <code>gcc</code>) with  <pre><code>gcc -lpthread *.c \n</code></pre></p>"},{"location":"csc367/pthread.html#fork","title":"Fork","text":"<p><code>pthread_create</code> starts a new thread in the calling processing, return <code>0</code> on success, error code on error.  <pre><code>int pthread_create(\n    pthread_t *thread,\n    const pthread_attr_t *attr,\n    void *(*func)(void *),\n    void *arg\n);\n</code></pre> <code>pthread_join</code> waits for the <code>thread</code> to terminate. return <code>0</code> on success, error code on error.  <pre><code>int pthread_join(\n    pthread_t thread, \n    void **retval\n);\n</code></pre></p> <p>A simple code example (this is a toy example and poorly performed) <pre><code>#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n#include &lt;pthread.h&gt;\n\n// the program fills the arr with 1's\n\nvoid* fill_one(void *data) {\n    int *arr = (int *) data;\n    *arr = 1;\n}\n\nint main() {\n\n    // the shared variable\n    int arr[5] = {0};\n\n    pthread_t threads[5];\n    int i;\n    for (i = 0; i &lt; 5; i++) {\n        pthread_create(&amp;threads[i], NULL, fill_one, &amp;(arr[i]));\n    }\n    for (i = 0; i &lt; 5; i++) {\n        pthread_join(threads[i], NULL);\n    }\n    for (i = 0; i &lt; 5; i++) {\n        printf(\"%d \", arr[i]);\n    }\n}\n</code></pre></p>"},{"location":"csc367/pthread.html#mutex","title":"Mutex","text":"<p>Because the code does not run sequentially, we don't know when a thread switches to another.  <pre><code>// the program aims to make a into 1\n\nvoid* make_true(void *data) {\n    int *a = (int *) data;\n    if (*a == 0) (*a)++;\n}\n\nint main() {\n    int a = 0;\n    pthread_t threads[20];\n    int i;\n    for (i = 0; i &lt; 20; i++) {\n        pthread_create(&amp;threads[i], NULL, make_true, &amp;a);\n    }\n    for (i = 0; i &lt; 20; i++) {\n        pthread_join(threads[i], NULL);\n    }\n    printf(\"%d\", a);\n}\n// output can be any number from 1 to 20\n</code></pre></p>"},{"location":"csc367/pthread.html#mutual-exclusion","title":"Mutual Exclusion","text":"<p>A race condition happens when 2 or more threads manipulate a shared resource. The segment of code accessing the shared resources is the critical section. </p> <p>Mutual exclusion means that only one thread can enter the critical section, and others must wait BEFORE entry, and another thread enters CS when the thread leaves. </p>"},{"location":"csc367/pthread.html#mutex-api","title":"Mutex API","text":"<pre><code>pthread_mutex_t lock;\n\n// init a mutex\nint pthread_mutex_init(\n    pthread_mutex_t *mutex,\n    const pthread_mutexattr_t *attr\n);\n// lock the mutex, if already locked, then blocked until obtain the lock\nint pthread_mutex_lock(pthread_mutex_t *mutex); \n// lock the mutex, if already locked, then return EBUSY\nint pthread_mutex_trylock(pthread_mutex_t *mutex);\n// unlock the mutex\nint pthread_mutex_unlock(pthread_mutex_t *mutex);\n</code></pre> <p>Then, we can make a correct example </p> <pre><code>// need a struct to pass lock and data into the func\ntypedef struct {\n    int num;\n    pthread_mutex_t lock;\n} int_thread;\n\nvoid* make_true(void *data) {\n    int_thread *a = (int_thread *) data;\n    pthread_mutex_lock(a-&gt;lock);\n    if (a-&gt;num == 0) a-&gt;num ++;\n    pthread_mutex_unlock(a-&gt;lock);\n}\n\nint main() {\n    // allocate and init the lock\n    pthread_mutex_t lock;\n    pthread_mutex_init(&amp;lock, NULL);\n\n    int_thread a = {0, lock};\n\n    pthread_t threads[20];\n    int i;\n    for (i = 0; i &lt; 20; i++) {\n        pthread_create(&amp;threads[i], NULL, make_true, &amp;a);\n    }\n    for (i = 0; i &lt; 20; i++) {\n        pthread_join(threads[i], NULL);\n    }\n    printf(\"%d\", a);\n}\n// output 1\n</code></pre> <p>Note that the mutex lock is not unique in your program. You can have several locks for higher degree of concurrency and have more fine-grained controls (with the risk of more sync bugs).</p>"},{"location":"csc367/pthread.html#correctness-and-deadlock","title":"Correctness and Deadlock","text":"<p>Note that mutex does not solve sync issues automatically. It's just a \"lock\".</p> <p>Deadlock is another typical bug introduced by the lock, deadlock happens when a thread never releases a lock. </p> <p>A typical (but does not happen often) example  <pre><code>pthread_mutex_lock(lock);\n/*...*/\nif (cond) return false;\n/*...*/\npthread_mutex_unlock(lock);\nreturn true;\n</code></pre></p> <p>Deadlock happens mostly when we have multiple locks, a typical example</p> <pre><code>// thread 1\npthread_mutex_lock(A);\n/*...*/\npthread_mutex_lock(B);\n/*...*/\npthread_mutex_unlock(A);\n/*...*/\npthread_mutex_unlock(B);\n\n\n// thread 2\npthread_mutex_lock(B);\n/*...*/\npthread_mutex_lock(A);\n/*...*/\npthread_mutex_unlock(B);\n/*...*/\npthread_mutex_unlock(A);\n</code></pre> <p>Simplest way to avoid is to enforce same ordering of locks in every piece of code. </p>"},{"location":"csc367/pthread.html#overhead","title":"Overhead","text":"<p>Note that the waiting causes lock contention. Even without waiting, mutex acquiring has overhead. </p> <p>Solution: localize computations are much as possible, make fewer global updates.  For reducing lock overhead on smaller operations, we can also use <code>atomic</code> operations. </p>"},{"location":"csc367/pthread.html#barriers","title":"Barriers","text":"<p>Another way to do synchronization is to use barrier. It works similar to <code>pthread_join</code>  in the master thread, which waits for all forked threads to finish. Threads that reach the barrier stop until other <code>n-1</code> threads (<code>n</code> often defined as #threads)  have reached it as well. </p>"},{"location":"csc367/pthread.html#barrier-api","title":"Barrier API","text":"<pre><code>// init the barrier, with the number of threads to wait\nint pthread_barrier_init(\n    pthread_barrier_t *barrier,\n    const pthread_barrierattr_t *attr,\n    unsigned int count\n);\n\n// wait till all threads reach this point\nint pthread_barrier_wait(pthread_barrier_t *barrier);\n// destroy the barrier data\nint pthread_barrier_destroy(pthread_barrier_t *barrier);\n\n// Common usage\npthread_barrier_t barrier;\npthread_barrier_init(&amp;barrier, NULL, num_threads);\n</code></pre>"},{"location":"csc367/simd.html","title":"SIMD Parallelism","text":"<p>Modern x86 CPUs support single instruction multiple data instructions that operate on special registers of holding 128, 256, or even 512 bits of data in one clock cycle. The specific support is different for manufactures (for example, AMD does not support <code>avx512</code>). The supported instruction sets can be found by </p> <pre><code>cat /proc/cpuinfo | grep flags\n</code></pre> <p>To find whether the current CPU support the instruction set</p> <pre><code>#include &lt;iostream&gt;\nusing namespace std;\n\nint main() {\n    // advanced vector extensions\n    cout &lt;&lt; __builtin_cpu_supports(\"avx\") &lt;&lt; endl; \n    cout &lt;&lt; __builtin_cpu_supports(\"avx2\") &lt;&lt; endl; \n    cout &lt;&lt; __builtin_cpu_supports(\"avx512f\") &lt;&lt; endl;\n    return 0;\n}\n</code></pre> <p>To use these instructions, we need to tell the compiler to enable the instruction extensions. Also, include <code>&lt;immintrin.h&gt;</code> to use vector intrinsic. For example,</p> <pre><code>#pragma GCC target(\"avx2\")\n#pragma GCC optimize(\"O3\")\n\n#include &lt;immintrin.h&gt;\n</code></pre> <p>or compile the code with <code>gcc</code> with <code>O3</code> flag and provide the CPU architecture as <code>-march cpu-type</code> ( List of CPU options)</p>"},{"location":"csc367/simd.html#registers-and-types","title":"Registers and types","text":"<p>SIMD extensions supports special sets of registers </p> <ul> <li><code>SSE</code> supports 16 <code>128-bit</code> registers named <code>xmm0</code> through <code>xmm15</code>. </li> <li><code>AVX</code> supports 16 <code>256-bit</code> registers named <code>ymm0</code> through <code>ymm15</code>.</li> <li><code>AVX512</code> supports 16 <code>512-bit</code> registers named <code>zmm0</code> through <code>zmm15</code>.</li> </ul> <p>For C/C++, <code>imminstin.h</code> supports special vector types for data in these registers as</p> <pre><code>__m{size}{dtype}\n// For example\n\n// 128-bit can do 4 floats or 2 doubles or 4 integers at the same time\n__m128  // 128-bit single precision floats\n\n// 256-bit = 8 floats, 4 doubles, 8 integers\n__m256d // 256-bit double precision floats\n\n// 512-bit = 16 floats, 8 doubles, 16 integers\n__m512i // 512-bit integers\n</code></pre> <p>For <code>gcc</code>, we can also define your own vector types and then use most C/C++ math operations with them. </p> <pre><code>typedef int   v8si __attribute__ (( vector_size(32) ));\n//     dtype  name                   size in bytes\n</code></pre>"},{"location":"csc367/simd.html#simd-intrinsics","title":"SIMD Intrinsics","text":"<p>Intrinsics are C-style functions that do something with these vector data types, usually by simply calling the associated assembly instruction.</p> <p>In general, the naming convention is <code>_mm{size}_{action}_{dtype}</code>.</p>"},{"location":"csc367/simd.html#moving-data","title":"Moving data","text":"<p>To move data between SIMD registers and memory, the actions are <code>load, loadu, store, storeu</code>. The <code>u</code> stands for unaligned. <code>load</code> and <code>store</code> work correctly when read/write fits into a single cache line. In other words, the data in the memory should be aligned to 32 bytes for <code>_m256</code> instructions. </p> <p>Note that unaligned version will be slower. </p> <p>For loading constants to the registers, use </p> <pre><code>// Initializes 256-bit vector with float64 values.\nextern __m256d _mm256_set_pd(double, double, double, double);\n// Initializes 256-bit vector in reversed order.\nextern __m256d _mm256_setr_pd(double, double, double, double);\n// Initializes 256-bit vector filled with one scalar\nextern __m256d _mm256_set1_pd(double);\n</code></pre>"},{"location":"csc367/simd.html#performance-considerations","title":"Performance considerations","text":"<p>Note that there are only 16 SIMD registers, and intrinsic simply maps the variables to the registers. When implementing a microkernel, we should fully use 16 registers and, at the same time, make sure there no register misses.</p> <p>As an example, we can write a \\(4\\times 4\\) matrix multiplication microkernel for \\(C = C + A \\cdot B\\) where \\(A, B, C\\) are stored in column-major as </p> <pre><code>// C[i, j] can be defined from macro as C + i + j * dim\n// compute C[i:i + 4, j:j + 4]\nvoid micro_kernel4x4(int i, int j, double *A, double *B, double *C)\n{\n    __m256d c00 = _mm256_loadu_pd(C[i, j    ]);\n    __m256d c10 = _mm256_loadu_pd(C[i, j + 1]);\n    __m256d c20 = _mm256_loadu_pd(C[i, j + 2]);\n    __m256d c30 = _mm256_loadu_pd(C[i, j + 3]);\n\n    __m256d a0, b0, b1, b2, b3;\n    for (int k = 0; k &lt; K; k++)\n    {\n        a0 = _mm256_load_pd(A[i, k]);\n        b0 = _mm256_set1_pd(B[k, j    ]);\n        b1 = _mm256_set1_pd(B[k, j + 1]);\n        b2 = _mm256_set1_pd(B[k, j + 2]);\n        b3 = _mm256_set1_pd(B[k, j + 3]);\n\n        c00 = _mm256_fmadd_pd(a0, b0, c00);\n        c10 = _mm256_fmadd_pd(a0, b1, c10);\n        c20 = _mm256_fmadd_pd(a0, b2, c20);\n        c30 = _mm256_fmadd_pd(a0, b3, c30);\n    }\n    _mm256_storeu_pd(C[i, j    ], c00);\n    _mm256_storeu_pd(C[i, j + 1], c10);\n    _mm256_storeu_pd(C[i, j + 2], c20);\n    _mm256_storeu_pd(C[i, j + 3], c30);\n}\n</code></pre>"},{"location":"csc367/slurm.html","title":"SLURM Job Scheduling","text":"<p>Simple Linux Utility for Resource Management, used to manage resources (CPU cores/threads, memory, interconnect (MPI), GPUs, etc.) and job scheduling (different scheduler such as for different users, groups, or priority based). </p>"},{"location":"csc367/slurm.html#job-allocation","title":"Job Allocation","text":"<pre><code>sbatch # submit a script for later execution (batch mode)\nsalloc # create job allocation and start a shell (interactive mode)\nsrun # create job allocation and launch a job step\nsattach # attach stdio to an existing job\n</code></pre>"},{"location":"csc367/slurm.html#common-argument-options","title":"Common Argument Options","text":"<p>Some commonly used argument for <code>sbatch</code> and <code>srun</code></p> <pre><code>-J, --job-name=&lt;jobname&gt; \n# Specify a name for the job\n\n-n, --ntasks=&lt;number&gt;\n# number of tasks will run, so that slurm allocates\n# enough resources for all the tasks. \n\n-c, --cpus-per-task=&lt;n&gt; \n# number of CPUs per process, default is 1\n\n--mem=&lt;size&gt;[K|M|G|T] \n# the size of memory required per node, default unit in Mb\n\n-q, --qos=&lt;qos&gt; \n# request a quality of service for the job.\n# use the following command to show all qos set by admin\nsacctmgr show qos format=name,priority\n\n-t, --time=&lt;time&gt; # set a limit to the total running time \n# by SIGTERM and SIGKILL. Acceptable time formats include \n# \"mm\", \"mm:ss\", \"hh:mm:ss\", \n# \"days-hh\", \"days-hh:mm\" and \"days-hh:mm:ss\".\n\n-p, --partition=&lt;partition_names&gt; \n# request a specific partition for the resource allocation. \n\n-i, --input=&lt;filename_pattern&gt;\n-o, --output=&lt;filename_pattern&gt;\n-e, --error=&lt;filename_pattern&gt;\n# Specify the \"filename pattern\" for stdin/out/err redirection. \n\n-G, --gpus=[type:]&lt;number&gt;\n# the total number of GPUs required for the job. \n# An optional GPU type specification can be supplied. \n</code></pre>"},{"location":"csc367/slurm.html#srun","title":"<code>srun</code>","text":"<pre><code>srun [OPTIONS ... [executable [args ...]]]\n</code></pre> <p>Run a parallel job on cluster managed by Slurm. If necessary, srun will first create a resource allocation in which to run the parallel job.</p> <p><code>srun</code> is interactive and blocking</p> <p>Compared to <code>sbatch</code>, often consider adding</p> <pre><code>--pty # Execute task zero in pseudo terminal mode. \n</code></pre> <p>Simple example <pre><code>$&gt; srun -mem 20M echo helloworld\nhelloworld\n\n$&gt; srun -mem 20M --pty bash\ncpu0 $&gt; # running bash on the allocated process\n</code></pre></p>"},{"location":"csc367/slurm.html#sbatch-and-sbatch-script","title":"<code>sbatch</code> and sbatch script","text":"<p><pre><code>sbatch [OPTIONS(0)...] [ : [OPTIONS(N)...]] script [args(0)...]\n</code></pre> sbatch submits a batch script to slurm and returns after the task script is submitted to slurm. </p> <p>The batch script may be given to <code>sbatch</code> through a file name on the command line, or if no file name is specified, <code>sbatch</code> will read in a script from standard input. </p> <p>The batch script may contain options preceded with <code>#SBATCH</code> before any executable commands in the script. <code>sbatch</code> will stop processing further #SBATCH directives once the first non-comment non-whitespace line has been reached in the script.</p> <p>Simple example <pre><code>$&gt; cat script.slrm\n#!/bin/bash\n#SBATCH --job-name=train\n#SBATCH --output=output.log\n#SBATCH -n 1\necho helloworld\n\n$&gt; sbatch script.slrm\nSubmitted batch job XXXXXXX\n\n$&gt; cat output.log\nhelloworld\n</code></pre></p>"},{"location":"csc367/slurm.html#filename-pattern","title":"Filename Pattern","text":"<p>For redirected out/err files, we could use the following pattern replacement.</p> <pre><code>%j # jobid of the running job.\n%u # User name.\n%x # Job name.\n</code></pre>"},{"location":"csc367/slurm.html#examples","title":"Examples","text":"<p>Submitting a NN training job using python and pytorch <pre><code>#!/bin/bash\n#SBATCH --job-name=train\n#SBATCH -c 4\n#SBATCH --mem=16G\n#SBATCH --time=24:00:00\n#SBATCH --output=result_%j.log\npwd; hostname; date\npython train.py\ndate\n</code></pre></p> <p>Submitting an OMP job <pre><code>#!/bin/bash\n#SBATCH --ntasks=1\n#SBATCH --cpus-per-task=8\n#SBATCH --time=00:10:00\n#SBATCH --job-name=omp\n#SBATCH --output=omp_%j.out\n\nexecutable\n</code></pre></p> <p>Submitting a MPI job</p> <pre><code>#!/bin/bash\n#SBATCH --nodes=1\n#SBATCH --ntasks=16\n#SBATCH --cpus-per-task=1\n#SBATCH --time=1:00:00\n#SBATCH --job-name=mpi\n#SBATCH --output=mpi_%j.out\n\nexecutable\n</code></pre>"},{"location":"csc367/slurm.html#sinfo","title":"<code>sinfo</code>","text":"<p><pre><code>sinfo [OPTIONS...]  \n</code></pre> view partition and node information for a system running Slurm.</p> <p>Some commonly used options </p> <pre><code>-l, --long\n# Print more detailed information.\n\n-N, --Node\n# Print information in a node-oriented format \n# with one line per node and partition. \n\n-e, --exact\n# If set, do not group node information on multiple nodes \n# unless their configurations to be reported are identical. \n\n-n, --nodes=&lt;nodes&gt;\n# Print information about the specified node(s). \n# Multiple nodes may be comma separated or expressed using a node range \n# expression (e.g. \"linux[00-17]\").\n\n-p, --partition=&lt;partition&gt;\n# Print information only about the specified partition(s). \n# Multiple partitions are separated by commas.\n</code></pre> <p>For Example <pre><code>$&gt; sinfo -lNe -p p100\nNODELIST   NODES PARTITION       STATE CPUS    S:C:T MEMORY TMP_DISK WEIGHT AVAIL_FE REASON\ngpu001         1      p100       mixed 32      2:8:2 172000        0      1     P100 none\ngpu002         1      p100    drained* 32      2:8:2 172000        0      1     P100 not powering\ngpu003         1      p100   draining@ 32      2:8:2 172000        0      1     P100 Kill task failed\ngpu004         1      p100       mixed 32      2:8:2 172000        0      1     P100 none\ngpu005         1      p100   draining@ 32      2:8:2 172000        0      1     P100 Kill task failed\ngpu006         1      p100       mixed 32      2:8:2 172000        0      1     P100 none\ngpu007         1      p100   draining@ 32      2:8:2 172000        0      1     P100 Kill task failed\n</code></pre></p>"},{"location":"csc367/slurm.html#squeue-scancel","title":"<code>squeue, scancel</code>","text":"<p>View the queue for sbatch jobs and cancel job given job ID</p> <pre><code>squeue [OPTIONS ...]\nscancel [job_id]\n</code></pre> <p><code>squeue</code> takes options to limit the display range, useful for large clusters</p> <pre><code>-p, --partition=&lt;part_list&gt;\n# limit to specific partion or a comma separated list of partition names.\n\n-q, --qos=&lt;qos_list&gt;\n# limit to the qos(s) or a comma separated list of qos's.\n\n-u, --user=&lt;user_list&gt;\n# limit to the user or a comma separated list of users\n\n-t, --states=&lt;state_list&gt;\n# limit to the state of a comma separated list of states, case insensitive\n</code></pre> <p>Example <pre><code>$&gt; squeue -u user\n  JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n8426702      t4v2    train     user  R    1:18:26      1 gpu063\n\n$&gt; scancel 8426702\n$&gt; squeue -u user\n  JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n</code></pre></p>"},{"location":"csc367/slurm.html#squeue-state-codes","title":"<code>squeue</code> State Codes","text":"<p>A subset of the commonly seen state codes <pre><code>BF BOOT_FAIL\n# Job terminated due to launch failure, typically due to a hardware failure\n\nCA CANCELLED\n# Job was explicitly cancelled by the user or system administrator. \n# The job may or may not have been initiated.\n\nCD COMPLETED\n# Job has terminated all processes on all nodes with an exit code of zero.\n\nF FAILED\n# Job terminated with non-zero exit code or other failure condition.\n\nOOM OUT_OF_MEMORY\n# Job experienced out of memory error.\n\nPD PENDING\n# Job is awaiting resource allocation.\n\nPR PREEMPTED\n# Job terminated due to preemption.\n\nST STOPPED\n# Job has an allocation, but execution has been stopped with SIGSTOP signal. \n# CPUS have been retained by this job.\n\nS SUSPENDED\n# Job has an allocation, but execution has been suspended and CPUs have been\n# released for other jobs.\n\nTO TIMEOUT\n# Job terminated upon reaching its time limit.\n</code></pre></p>"},{"location":"csc373/dp.html","title":"Dynamic Programming","text":""},{"location":"csc373/dp.html#activity-scheduling-with-profits","title":"Activity Scheduling with Profits","text":"<ul> <li>Input: \\(A =\\{A_1,...,A_n\\}\\) where \\(A_i=(s_i,f_i,w_i)\\) where \\(w_i\\in\\mathbb{N}\\) is the profit</li> <li>Output: \\(S\\subseteq A, S\\) has no conflict and \\(w(S)=\\sum_{A_i\\in S} w_i\\) is maximized</li> </ul>"},{"location":"csc373/dp.html#discussion","title":"Discussion","text":"<ul> <li>Note that Greedy won't work (finish time, profit, unit-profit)</li> <li>Assume the activity are sorted by finish time. Consider \\(S^*\\) optimal, consider \\(A_n\\) which is the activity with the largest finishing time. </li> <li>If \\(A_n\\in S^*\\), then \\(S^*-A_n\\) must be optimal for \\(A'=\\{A_1,...,A_k\\}\\) where \\(k\\) is the last activity that does not overlap with \\(A_n\\), a.k.a. \\(\\{A_{k+1},...,A_{n-1}\\}\\) each overlap with \\(A_n\\)</li> <li>If \\(A_n\\not\\in S^*\\), then \\(S^*\\) is optimal for \\(A-\\{A_n\\}\\)</li> <li>Recursive substructure when an optimal solution of a problem is made up of optimal solutions of the sub-problems.</li> </ul>"},{"location":"csc373/dp.html#recursive-implementation","title":"Recursive Implementation","text":"<p><code>initialization(A)</code> compute \\(P = p[1],...,p[n]\\) where \\(p[i]=\\)index of the last activity that does not overlap with \\(A_i\\), note \\(p[i]\\leq i-1\\)</p> solution(A)<pre><code>P = initialization(A)\nreturn RecOpt(n, P)\n</code></pre> <p>RecOpt(n, P)<pre><code>if n = 0;\n    return 0\nreturn max(RecOpt(n-1), w_n + RecOpt(p[n]))\n</code></pre> Note that the runtime has combinatorial explosion due to the repeated calls of <code>RecOpt(i)</code> for small <code>i</code> for exponentially many times. THe runtime is exponential only because it is recursive, To solve it  - memorization: memorize the output of RecOpt(i) and record for later usage  - Instead, compute values bottom-up</p> IterOpt(A)<pre><code>initialization(A)\nOPT = []\nOPT[0] = 0\nfor i in range(1, n):\n    OPT[i] = max(OPT[i-1], w_i + OPT[p[i]])\nreturn OPT # note that OPT is useful to get S\n</code></pre>"},{"location":"csc373/dp.html#dynamic-programming_1","title":"Dynamic Programming","text":"<ul> <li>For optimization problems where global optimum is obtained from optimum to subproblems</li> <li>The subproblems need to be simple Each subproblem can be characterized using a fixed number of indexes or other information</li> <li>Subproblem overlap: smaller subproblems repeated in solutions to larger problems</li> </ul>"},{"location":"csc373/dp.html#process","title":"\"Process\"","text":"<ol> <li>Understand the recursive structure of the optimum solutions</li> <li>Define a table (iterative data structure) to store the optimum value for all subproblems</li> <li>Give a recurrence for the table values, with justification</li> <li>Write iterative algorithm to compute the value bottom-up (solve the recurrence)</li> <li>Use the table values to compute an actual solution</li> </ol>"},{"location":"csc373/dp.html#matrix-chain-multiplication","title":"Matrix Chain Multiplication","text":"<p>Input: \\((d_0,...,d_n), d_i\\in\\mathbb{Z}^+\\) representing matrix dimensions, since the inner dimension are same, we don't store it twice, a.k.a. \\(A_0=[d_0\\times d_1], A_1 = [d_1\\times d_2],..., A_{n-1}=[d_{n-1}\\times d_n]\\)  Output: fully parenthesized expression for \\(\\prod_1^{n-1} A_i\\) that minimize total #flips to computer the product</p>"},{"location":"csc373/dp.html#example","title":"Example","text":"<p>\\(A_{1\\times 10}, B_{10\\times 10}, C_{10\\times 100}\\) are matrices. We can compute it by \\(A(BC)\\) or \\((AB)C\\), note that multiplication are associative.  - \\(A(BC)\\): Consider \\(BC\\), #flips = \\(10^2 \\times 100\\), then \\(A(BC)\\) #flips = \\(1\\times 10\\times 100\\). sum: \\(11,000\\)  - \\((AB)C\\): \\(AB\\): \\(1\\times 10\\times 10 = 100\\), \\((AB)C=1\\times 10\\times 100 = 1000\\), sum: \\(1100\\)</p>"},{"location":"csc373/dp.html#discussion_1","title":"Discussion","text":"<ul> <li>Brutal force # ways to parenthesize is called a Catalan number \\(\\in\\Omega(4^n)\\)</li> <li>Greedy ? consider input \\((10, 1, 10, 100), (1, 10, 100, 1000)\\), try greedy strategies and consider the counter example above. </li> </ul>"},{"location":"csc373/dp.html#process_1","title":"\"Process\"","text":"<ol> <li>Recursive Structure: </li> <li>imagine OPT, the last product will be like \\((A_1\\times A_{k-1})\\times (A_k\\times A_{n-1})\\), note the inner product won't influence the number of operations of this product, hence to minimize, the inner product need to be optimal. </li> <li>Subproblems will all have the form \"find the best parenthesizing of \\(A_i\\times ...\\times A_j, i\\leq j\\)\". Then let \\(N[i,j]=\\) min #flips required to multiple \\(A_i\\times ...\\times A_j, 0\\leq i\\leq j\\leq n-1\\)</li> <li>For \\(i&lt;j\\), for any one pair of parenthesis, the number of operations is \\(N[i,k-1] + N[k,j] + d_id_kd_{j+1}\\) where \\(N(p,q)\\) is the min number of operations taken for \\(A_p\\times...\\times A_q\\). \\(N[i,j]=\\min\\{N[i,k-1] + N[k,j] + d_id_kd_{j+1}\\mid i+1\\leq k\\leq j\\}\\)</li> <li> <p>Table: Consider the table with entry \\(N[i,j]\\), the matrix is strictly upper triangular (since we can interchange \\(i,j\\), the lower triangle is not needed), and the diagonal \\(N[i,i]\\) is filled with \\(0\\). </p> </li> <li> <p>Table Implementation</p> </li> </ol> table<pre><code>N = empty (n-1)*(n-1) matrix\nfor i in range(m-1, 0, -1):\n    N[i,i] = 0\n    (B[i,i] = -1)\n    for j in range(i+1, n-1):\n        N[i,j] = Infinity\n        (B[i,j] = -1)\n    for k in range(i+1, j):\n        if N[i,k-1] + N[k,j] + d_i*d_k*d_j &lt; N[i,j]:\n            N[i,j] = N[i,k-1] + N[k,j] + d_i*d_k*d_j\n            (B[i,j] = k)\nreturn N[0, n-1]\n</code></pre> <ul> <li>Reconstruct solution:<ul> <li>Use \\(B[i,j] =\\) value of \\(k\\) that makes \\(N[i,j] = N[i,k+1] + N[k,j] + d_i d_k d_{j+1}\\) and add into the table</li> <li>For parenthesis</li> </ul> </li> </ul> paren(B,i,j)<pre><code>if i == j:\n    print 'A_i'\nelse:\n    print '(' + paren(B, i, B[i,j]-1) + '*' + paren(B,B[i,j], j) + ')'\n</code></pre>"},{"location":"csc373/dp.html#shortest-paths-all-pairs","title":"Shortest Paths - All-Pairs","text":"<ul> <li>Input: directed \\(G=(V,E), w(v)\\in \\mathbb{Z}\\), no cycle with negative weight.  </li> <li>Output: \\(\\forall u,v \\in V, \\min\\{w(u\\sim v)\\mid u\\sim v\\text{ is a path}\\}\\)</li> </ul>"},{"location":"csc373/dp.html#process_2","title":"\"Process\"","text":""},{"location":"csc373/dp.html#0-recursive-structure","title":"0. Recursive Structure","text":"<p>Consider a shortest \\(u\\sim v\\) path,let \\(k=\\) max index of vertices on \\(u\\sim v\\) excluding \\(u,v\\). Claim: both \\(u\\sim k, k\\sim v\\) are shortest paths with all intermediate vertices \\(&lt;k\\) Proof: by contradiction, if \\(P_1\\) is a shorter path from \\(u\\) to \\(k\\), then \\(P_1 + k\\sim v\\) would be shorter than \\(u \\sim v\\), even if \\(P_1\\) had vertices in common with \\(k\\sim v\\), say vertex \\(j\\), then \\(P_1+ k \\sim v\\) contains a cycle, by assumption, the cycle must have weight \\(\\geq 0\\), then we have a better path by removing the cycle. </p>"},{"location":"csc373/dp.html#1-table","title":"1. Table","text":"<p>\\(A[k,u,v]=\\) min weight of all \\(u\\sim v\\) using only vertices from \\(1,...,k\\) as intermediate for \\(u,v\\in \\{1,...,n\\}. k\\in\\{0,...,n\\}\\)</p>"},{"location":"csc373/dp.html#2-recurrence","title":"2. Recurrence","text":"recurrence relationship<pre><code>A[0,u,v]= { \n    0         if u = v, \n    w(u,v)    if u != v (u,v) in E,\n    Infinity  if u != v, (u,v) not in E\n}\n\nfor k &gt; 0\uff1a\n    A[k,u,v] = min(\n        A[k-1,u,v], # don't use k\n        A[k-1,u,k]+A[k-1,k,v] # use k\n)\n</code></pre>"},{"location":"csc373/dp.html#3-iterative-floyd-warshall","title":"3. Iterative (Floyd-Warshall)","text":"DP construction<pre><code>for u in range(n):\n    for v in range(n):\n        if u == v: \n            A[0,u,v] = 0\n            B[0,u,v] = -1 # not exist\n        elif (u,v) in E:\n            A[0,u,v] = w(u,v)\n            B[0,u,v] = 0 # directly\n        else:\n            A[0,u,v] = Infinity\n            B[0,u,v] = -1\n\nfor k in range(n):\n    for u in range(n):\n        for v in range(n):\n            if A[k-1, u, k] + A[k-1, k, v] &lt; A[k-1, u, v] \n                A[k, u, v] = A[k-1, u, k] + A[k-1, k, v] \n                B[k, u, v] = k\n            else:\n                A[k, u, v] = A[k-1, u, v]\n                B[k, u, v] = B[k-1, u, v]\n</code></pre>"},{"location":"csc373/dp.html#4-reconstruct-the-actual-solution","title":"4. Reconstruct the actual solution","text":"<p>Use \\(B[k, u, v]\\) to track max vertex on any \\(u\\sim v\\) path that has total weight \\(A[k, u, b]\\)</p> reconstruction<pre><code>Path(u,v, B, n):\n    find the intermediate index\n    recursively find the weights\n</code></pre>"},{"location":"csc373/dp.html#observation","title":"Observation","text":"<p>In practice, runtime \\(\\in \\Theta(n^3)\\), space \\(\\in\\Theta(n^3)\\). To improve the space, notice that \\(\\forall k, u, v. A[k, u, k] = A[k-1, u, k], A[k ,k, v] = A[k-1, k, v]\\). Therefore, we don't need a 3-D array, we can keep updating on a 2-D array. We can simply get rid of all index \\(k\\), then the body of the triple for loop becomes: </p> <p><pre><code>if A[u, k] + A[k, v] &lt; A[u, v]:\n    A[u, v] = A[u, k] + A[k, v] \n    B[u, v] = k\n# omit the else since nothing changed\n</code></pre> The space then is \\(O(n^2)\\)</p>"},{"location":"csc373/dp.html#transite-closure","title":"Transite Closure","text":"<p>\\(G\\) directed, \\(\\forall u,v\\in V\\) is there a path \\(u\\sim v\\)?</p>"},{"location":"csc373/dp.html#discussion_2","title":"Discussion","text":"<p>Use adjacency matrix  Example let \\(A =\\)</p> 1 2 3 4 1 1 0 0 1 2 1 1 0 0 3 1 1 1 0 4 0 0 0 1 <p>Note: \\(\\pi(u,v) = 1\\), u is the row index, v is the column index, if \\(u\\rightarrow v\\)</p> <p>Notice that \\(A^2 =\\)</p> 1 2 3 4 1 1 0 0 2 2 2 1 0 1 3 3 2 1 1 4 0 0 0 1 <p>Notice that </p> \\[A^2[2,4]=\\sum_{i=1}^4 A[2,i]\\times A[i,4]\\] <p>Each term of the summation above represents one possible path of length \\(\\leq 2\\) between 2 and 4</p>"},{"location":"csc373/dp.html#trick-1","title":"Trick 1","text":"<p>switch \\(\\times\\rightarrow\\land, +\\rightarrow\\lor\\). Generally, \\(A^k[u,v] = \\mathbb{I}\\) there is some path of length \\(\\leq k\\) from \\(u\\) to \\(v\\), wanted \\(A^n\\) using boolean ops. </p>"},{"location":"csc373/dp.html#trick-2","title":"Trick 2","text":"<p>square the resulted matrix for each matrix multiplication. \\(A^1, A^2, A^4, ..., A^{\\lfloor \\lg(n) \\rfloor^2}\\)</p>"},{"location":"csc373/dp.html#recursive","title":"Recursive","text":"<p>pow(A, n)<pre><code>if n = 1: \n    return A\nelif n is odd:\n    return pow(A,floor(n,2))*pow(A,floor(n,2))* A\nelse:\n    return pow(A,floor(n,2))*pow(A,floor(n,2))\n</code></pre> Runtime \\(O(n^3\\log n)\\)</p>"},{"location":"csc373/dp.html#trick-3","title":"Trick 3","text":"<p>divide and conquer algorithm for matrix product in \\(O(n^{\\lg 7})\\), then the runtime is \\(O(n^{\\lg 7}\\log n)\\in O(n^3)\\)</p>"},{"location":"csc373/dp.html#example-question-1-knapsack","title":"Example Question 1: KnapSack","text":"<p>Given \\(W\\in\\mathbb{Z}^+, I = \\{(w_1, v_1),...,(w_n,v_n)\\}\\), \\(w_i,v_i\\in\\mathbb{Z}^+\\). Maximize for set \\(S\\) s.t. \\(\\sum_{i\\in S} w_i \\leq W \\land \\sum_{i\\in S} v_i\\) maximized. </p>"},{"location":"csc373/dp.html#recurrence","title":"Recurrence","text":"<p>Let \\(v_k\\in S\\) having the maximum value, then \\(S-\\{k\\}\\) must be the optimal set for \\(W-w_k\\), otherwise will cause contradiction. </p>"},{"location":"csc373/dp.html#table","title":"Table","text":"<p>\\(T(k, i) =\\) maximum value of items with weight \\(i\\) and item \\(1,...,k\\). </p> <pre><code>for i in range(W):\n    T[0, i] = 0\nfor i in range(W):\n    for k in range(n):\n        T[i, k] = max(T[i - 1, k], \n                      T[i, k - 1])\n</code></pre>"},{"location":"csc373/dp.html#example-question-2","title":"Example Question 2","text":"<p>Input a list of strictly increasing positive integers \\(D=\\{d_1,...,d_m\\}\\), a positive integer \\(A\\). Output a count set \\(C\\) of \\(D\\) s.t. \\(\\sum_C = A\\), or Null if cannot solve</p>"},{"location":"csc373/dp.html#recurrence_1","title":"Recurrence","text":"<p>The optimal set either</p> <ul> <li>includes \\(d_m\\), then \\(T(A, m) = T(A-d_m, m) \\cup\\{d_m\\}\\)</li> <li>not includes \\(d_m\\), then \\(T(A, m) = T(A, m-1)\\)</li> </ul> <p>The relation  - \\(T(a, 0) = None\\) since there's no coin  - \\(T(0, m) = 0\\) since there's no value  - \\(T(a, m) = T(a, m-1)\\) if \\(d[m]&gt;a\\)  - \\(T(a, m) = \\min\\{T(a, m -1), 1 + T(a-d[m], m)\\})\\)</p>"},{"location":"csc373/greedy_algorithm.html","title":"Greedy Algorithm","text":""},{"location":"csc373/greedy_algorithm.html#activity-schedule","title":"Activity Schedule","text":"<p>\\(A = \\{a_1,...,a_n\\}\\) set of tasks, \\(|A|=n\\), \\(a_i = (s_1,f_i), s_i=\\) start time, \\(f_i=\\) finish time. \\(S \\subseteq A. S=\\) set of tasks without overlap. Maximize \\(|S|\\).  </p>"},{"location":"csc373/greedy_algorithm.html#implementation-greedy-based-on-finish-time","title":"Implementation: Greedy based on finish time","text":"<pre><code>sort A by the finishing time\nf = 0 # finishing time\nS = []\nfor ai in A: \n    if f &lt;= ai.s: # the start time of the ith activity\n        S.append(si)\n        f = ai.f\nreturn S\n</code></pre>"},{"location":"csc373/greedy_algorithm.html#correctness-thrm-161-clrs","title":"Correctness  (Thrm 16.1 CLRS)","text":"<p>Define \\(OPT\\subseteq A\\) be an optimal solution of the problem, in which \\(|OPT|\\) is maximized and there is no overlapping in \\(OPT\\).  Define \\(S_k\\) be the state of \\(S\\) after the \\(k\\)th iteration of the for loop (4-7), \\(S_0\\) be the state before entering the loop.  Claim \\(\\exists OPT. \\forall k\\in\\mathbb{N}.S_k\\subseteq OPT\\subseteq S_k\\cup\\{A_{k+1},...,A_n\\}\\). (loop invariant) Proof prove by induction Base case \\(S_0=\\emptyset\\subseteq OPT\\subseteq \\{A_1,...,A_n\\}\\) Inductive step Take \\(OPT\\) be some optimal solution,assume the inductive hypothesis. Consider the two cases</p> <ul> <li> <p>\\(S_{i+1} = S_i\\), a.k.a. \\(A_{k+1}\\) overlaps with some activity in \\(S_{i}\\). By induction hypothesis, \\(S_{i+1}\\subseteq OPT\\subseteq S_k \\cup \\{A_{k+1},..., A_n\\}=S_{k+1}\\cup \\{A_{k+1},..., A_n\\}\\). Moreover, \\(A_{k+1}\\not\\in OPT\\) because \\(A_{k+1}\\) would overlap with some activity in \\(OPT\\), hence \\(S_{i+1}\\subseteq OPT\\subseteq S_{k+1}\\cup \\{A_{k+1},..., A_n\\} - \\{A_{k+1}\\} = S_{k+1}\\cup \\{A_{k+2},..., A_n\\}\\). </p> </li> <li> <p>\\(S_{i+1} = S_{i}\\cup \\{A_{k+1}\\}\\).   If \\(A_{k+1}\\in OPT\\), then the claim is proven.   If \\(A_{k+1}\\not\\in OPT\\), then consider \\(A_m\\in OPT, m &gt; i+1\\) is the smallest index that is greater than \\(i+1\\), then \\(A_m\\) has greater finishing time than \\(A_{k+1}\\), let \\(OPT' = OPT - {A_m} \\cup \\{A_{i+1}\\}, |OPT'|=|OPT|\\), \\(A_{k+1}\\) will not overlap with any other activities in \\(OPT'\\) since it will not overlap with any activities before \\(A_m\\) by its starting time property, and it will not overlap with any activity after \\(A_m\\) since it even finishes earlier than \\(A_m\\), then \\(OPT'\\) is a optimal solution, and \\(S_{k+1}\\subseteq OPT'\\) since \\(m &gt; i+1, A_m\\not\\in S_{k+1}\\), and \\(OPT' \\subseteq S_{k+1}\\cup\\{A_{k+2},...,A_n\\}\\) since \\(A_{k+1}\\in S_{k+1}\\)</p> </li> </ul>"},{"location":"csc373/greedy_algorithm.html#general-strategy-for-proving-greedy-correctness","title":"General strategy for proving Greedy correctness","text":"<p>every partial solution generated can be extended to an optimal solution </p> <p>Proof of sub-case 2.2 is called exchange lemma</p>"},{"location":"csc373/greedy_algorithm.html#minimal-spanning-tree","title":"Minimal Spanning Tree","text":"<p>Let \\(G=(V,E)\\) be a connected graph with weight function \\(w(e)\\in\\mathbb{N}. \\forall e\\in E\\). A Spanning tree \\(S = (V,T)\\) is an acyclic, connected subgraph (tree) of \\(G\\).  minimize \\(w(T)=\\sum_{e\\in T}w(e)\\) </p>"},{"location":"csc373/greedy_algorithm.html#implementation","title":"Implementation","text":"<ul> <li>Prim's Algorithm Start with an arbitrary vertex \\(v\\), let \\(C = \\{v\\}\\) be the connected component. For each iteration, add the shortest edge attaches to \\(C\\) and some \\(u\\not\\in C\\), until \\(C = V\\). </li> <li>Kruskal's Algorithm Start with \\(T = \\emptyset\\), let \\(E'\\) be \\(E\\) sorted in non-increasing order of weight. Iterate over \\(E'\\), if \\(e\\) connects two different connected component (implement with disjoint set), adds to \\(T\\). </li> </ul> <p>krustal_mst()<pre><code>sort E by non-increasing order\nT = [] #empty disjoint set\nfor e in range(1,m):\n    if e connects two disjoint sets:\n        T.append(e) \n</code></pre>  - Reverse remove Algorithm Starts with \\(T = E\\), let \\(E'\\) be \\(E\\) sorted in non-decreasing order of weight, iterate over \\(E'\\), remove \\(e\\) from \\(T\\) if the two vertices are still connected. </p>"},{"location":"csc373/greedy_algorithm.html#correctness-krustals","title":"Correctness (Krustal's)","text":"<p>Define \\(T_i\\) be the state of \\(T\\) after the \\(i\\)th iteration of the for loop (line 3-5), \\(T_0=\\emptyset\\). Define \\(T^*\\) be the optimal solution.  Claim \\(\\forall n\\in\\mathbb{N}. T_n\\subseteq T^*\\subseteq T_n\\cup\\{e_{n+1},...,e_{m}\\}\\) Proof Let \\(n\\in\\mathbb{N}\\).  Suppose \\(n = 0\\), \\(\\emptyset\\subseteq T^* \\subseteq E\\). Suppose \\(n &gt; 0\\), assume \\(P(n)\\).  </p> <ul> <li>Suppose \\(T_{n+1} = T_{n}\\), then by the if condition (line 4), \\(e_{n+1}\\) will create a cycle in \\(T_n\\), hence \\(e_{n+1}\\not\\in T^*\\). Therefore, \\(T_{n+1} = T_n \\subseteq T^* \\subseteq T_{n+1}\\cup\\{e_{n+2},...,e_m\\}\\)</li> <li>Suppose \\(T_{n+1} = T_{n}\\cup\\{e_{n+1}\\}\\)   If \\(e_{n+1} \\in T^*\\), then proven   If \\(e_{n+1}\\not\\in T^*\\), say \\(e_{n+1} = (u, v),u\\in T_n, v\\not\\in T_n\\), then consider some \\(e_i\\in T^*\\) with one endpoint being \\(v\\), \\(e_i\\not\\in T_n\\) since \\(T_n\\) does not connect \\(v\\). Then, \\(w(e_i)\\geq w(e_{n+1})\\) by the sorting property. Then consider \\(T^{**} = T^* - \\{e_i\\}\\cup\\{e_{k+1}\\}, w(T^{**})\\leq w(T^*)\\). Also, \\(T^{**}\\) is connected, acyclic by the MST property of \\(T^*\\). Hence \\(T^{**}\\) is a MST. \\(T_{n+1}\\subseteq T^{**}\\subseteq T^{**}\\cup \\{e_{n+2},...,e_m\\}\\). </li> </ul>"},{"location":"csc373/greedy_algorithm.html#shortest-path","title":"Shortest Path","text":"<p>Precondition \\(G = (V,E)\\) connected, with \\(w:E\\rightarrow \\mathbb{Z}^+\\). \\(s,t\\in V\\) Postcondition return \\(u\\sim v\\) with minimized \\(w(u\\sim v) = \\sum_{e\\in u \\sim v} w(e)\\)</p>"},{"location":"csc373/greedy_algorithm.html#implementations","title":"Implementations","text":"<ul> <li>Brute force: worst case \\(O(c^n)\\)</li> <li>Special Case: \\(\\forall e\\in E. w(e) = 1\\Rightarrow\\) BFS \\(\\in O(|E|)\\)</li> <li>Dijkstra's Algorithm dijkstra(G, w, s, t)<pre><code>P = [] # list of edges of the shortest paths tree\nQ = empty min-priority queue of vertices v, prioritize on v.d\nfor all v in V:\n    v.parent = Nil\n    v.d = Infinity\n    Q.enqueue(v)\ns.d = 0\nQ.update()\nwhile Q is not empty:\n    v = Q.dequeue()\n    P.append((v,v.parent))\n    for (u,v) in all paths containing v:\n        if u in Q and v.d + w(v,u) &lt; u.d:\n            u.parent = v\n            u.d = v.d + w(u,v)\n            Q.update()\nP.remove((Nil, s))\nreturn p\n</code></pre></li> <li>Similar to BFS, use a priority queue (prioritized by best distance so far) instead of an array</li> <li>Similar to Prim's, but with different priorities</li> </ul>"},{"location":"csc373/greedy_algorithm.html#runtime","title":"Runtime","text":"<p>\\(O((m+n)\\log n) = O(m \\log n)\\) since connected Initialization takes (1-8) \\(O(n)\\) time Consider the while loop (9-16), in each iteration, \\(v\\) is dequeued from \\(Q\\) and no nodes are enqueued, hence the while loop will execute \\(O(n)\\) time. Consider the inner for loop, consider \\(G\\) implemented as an adjacency list. Over all iterations, each edge generates at most one queue update and are examined at most twice. For each queue update, it takes \\(O(\\log n)\\) time. Therefore, the total is \\(O(m\\log n)\\) Since the graph is connected, the total time is \\(O(m \\log n)\\)</p>"},{"location":"csc373/greedy_algorithm.html#properties","title":"Properties","text":"<p>Define \\(\\forall v\\in V. \\delta (s,v)=\\min\\{w(s\\sim v)\\mid s\\sim v\\text{ is a path in }G\\}\\) </p> <p>Lemma 1 \\(\\forall v\\in V\\). in any iteration, \\(v.d\\geq \\delta(s,v)\\). Proof by induction on the number of iteration, based on the algorithm, \\(u.d=\\infty\\) or weight of some particular path. </p> <p>Lemma 2 (Triangular property) \\(\\forall u,v,w\\in V. \\delta(u,v)\\leq \\delta(u,w)+\\delta(w,v)\\). Proof Otherwise \\(u\\sim w\\sim v\\) is the shortest path. </p> <p>Lemma 3 (sub-path property) Any sub-path of a shortest path is shortest.  Proof prove by contradiction, a shorter sub-path will shorten the path. </p>"},{"location":"csc373/greedy_algorithm.html#discussion","title":"Discussion","text":"<p>Show \\(P_k\\) the value of \\(P\\) after the \\(k\\)th iteration of the loop, can be extend to some optimal shortest path tree.  - Core of inductive step: show that \\(\\forall v\\in P_k. v.d=\\delta(s,v)\\)  - Consider one iteration \\(P_{k+1}=P_k\\cup\\{v'.parent, v'\\}\\) which \\(v'\\) is just being connected. Then it follows \\(v'.d=\\delta(s,v')\\).  By lemma 1, we only need to prove \\(u.d \\leq \\delta(s,v)\\) by contradiction.  Assume \\(v.d&gt;\\delta(s,v)\\). Consider \\(s\\sim v\\in P_{k+1}\\), consider \\((s\\sim v)^*\\) be the shortest path, let \\((x,y)\\in(s\\sim v)^*\\) be the first edge such that \\(x\\in P_k, y\\not\\in P_k\\). If \\(y\\neq v\\), then  \\(y.d \\leq x.d + w(x,y)\\leq \\delta(s,x)+w(x,y)&lt;\\delta(s,x)+w(x,y)+\\delta(y,v)=\\delta(s,v)&lt;v.d\\), contradict to \\(v.d\\) is the min in \\(Q\\)  If \\(y=v,v.d\\leq x.d+w(x,v)=\\delta(s,v)&lt;v.d\\)</p>"},{"location":"csc373/greedy_algorithm.html#correctness","title":"Correctness","text":"<p>Let \\(k\\in\\mathbb{N}. k &lt; n\\). Let \\(P(k)\\) be such that \"\\(\\exists P^*\\) be the optimal solution s.t. \\(P_k\\subseteq P^*\\) and \\(P^*-P_k\\) contains only edges without both endpoints in \\(P_k\\) and \\(\\forall u\\in P_k.\\forall v\\not\\in P_k. u.d=\\delta(s,u)\\leq \\delta(s,v)\\leq v.d\\)\".</p> <p>Suppose \\(k = 0\\), \\(P_0=\\emptyset\\subseteq P^*\\).</p> <p>Suppose \\(k &gt; 0\\), assume \\(P(k)\\). consider \\(P_{k+1} = P_k \\cup \\{(u,v)\\}, u\\in P_k, v\\not\\in P_k\\).  - Suppose \\((u,v)\\in P^*\\). Then \\(P_{k+1}\\subseteq P^*\\). Also \\(\\delta(s,v)=\\delta(s,u)+w(u,v)=\\delta(s,v)\\). By the priority of \\(Q\\), \\(v.d\\) is the smallest among vertices not connected by \\(P_k\\), hence \\(\\forall x\\in P_{k+1}.\\forall w\\not\\in P_{k+1}. x.d\\leq \\delta(s,x)\\leq \\delta(s,w)\\leq w.d\\).  - Suppose \\((u,v)\\not\\in P^*\\). Then take \\((s\\sim v)^* \\in P^*\\), \\((w,v)\\) be the last edge in the path \\((s\\sim v)^*\\).  I claim that \\(w\\in P_k\\).   - Let \\((x,y)\\in(s\\sim v)^*\\) be the first edge such that \\(x\\in P_k, y\\not\\in P_k\\). Then \\(y.d\\le \\delta(s,v) \\leq v.d\\) because \\(w((s,v)^*)=\\delta(s,v)\\) and \\(w(s\\sim y) &lt; w((s,v)^*)\\) since \\(y\\) is on the path. However this contradict with the fact that \\(v.d\\) has the minimum priority.   - Therefore, \\(\\delta(s,v) = w.d + w(w,v)\\). Then, since \\(v.d\\) is the shortest distance, \\(v.d \\leq w.d + w(w,v) = \\delta(s,v)\\). By Lemma 1, \\(v.d = \\delta(s,v)\\)  .  Take \\(P^{**} = P^* - \\{(w,v)\\}\\cup\\{(u,v)\\}\\) is a shortest path tree and \\(\\forall x\\in P_k. \\forall y\\not\\in P_k. x.d = \\delta(s,x)\\leq \\delta(s,y) \\leq y.d\\)</p>"},{"location":"csc373/greedy_algorithm.html#example-question-1","title":"Example Question 1","text":"<p>\\(D = \\{1, 5, 10, 25\\}. \\forall n \\in \\mathbb{Z}^+\\). Let \\(S\\) be a multi-set of elements from \\(D\\) such that \\(\\sum_{s\\in S} s = n\\), minimize \\(|S|\\) </p>"},{"location":"csc373/greedy_algorithm.html#implementation_1","title":"Implementation","text":"sequence(n)<pre><code>S = []\nwhile n &gt; 0:\n    if n &gt;= 25:\n        x = 25\n    else if n &gt;= 10:\n        x = 10\n    else if n &gt;= 5:\n        x = 5\n    else:\n        x = 1\nS.append(x)\nn -= x\nreturn S\n</code></pre>"},{"location":"csc373/greedy_algorithm.html#correctness-discussion","title":"Correctness Discussion","text":"<p>Let \\(S^* = \\{s_1^*,...,s_j^*\\}\\) be the optimal sequence, let \\(S_i = \\{s_1,...,s_k\\}\\) be the sequence of \\(S\\) after \\(i\\)th iteration of the while loop. \\(S_0=\\emptyset\\).</p> <p>Consider the case when \\(s_m \\neq s^*_m\\) for some \\(m\\), then we will show that there are coins in \\(S^* - S_m\\) and not the same value as \\(s_m\\) that makes up to \\(s_m\\), where it will use more coins. </p> <p>Notice that this algorithm does not work for a general \\(D = \\{1, d_1,d_2,...,d_k\\}\\) where \\(D\\) is strictly increasing. For example, \\(D=\\{1,3,4\\}. n = 6\\). \\(S = \\{4, 1, 1\\}\\) while \\(S^* = \\{3, 3\\}\\)</p>"},{"location":"csc373/greedy_algorithm.html#example-question-2","title":"Example Question 2","text":""},{"location":"csc373/greedy_algorithm.html#part-a","title":"Part (a)","text":"<p>Prove \\(\\forall G\\) connected, \\(|E| \\geq |V|\\). if \\(e\\) is the unique minimum cost edge, then \\(e\\) must be in every MST of \\(G\\).</p> <p>Proof. Let \\(T\\) be a MST of \\(G\\) that does not contain \\(e=(u,v)\\), Consider \\(u\\sim v \\in T\\), then \\(e\\cup u\\sim v\\) is a cycle, since \\(e\\) has the unique minimum weight, take out any edge \\(w \\in u\\sim v\\), \\(w(T-\\{w|\\cup\\{e\\})&lt;w(T)\\). By contradiction, claim is proven. </p>"},{"location":"csc373/greedy_algorithm.html#part-b","title":"Part (b)","text":"<p>Prove \\(\\forall G\\) connected, \\(|E| \\geq |V|\\). if \\(e\\) is the unique maximum cost edge, then \\(e\\) must not be in any MST of \\(G\\). Or Disprove by counter-example. </p> <p>Proof. I'll disprove this claim, take \\(G\\) such that \\(G'=(V,E-\\{e\\})\\) is disconnected, then \\(e\\) is essential to make a spanning tree for \\(G\\).</p>"},{"location":"csc373/lp.html","title":"Linear Programming","text":""},{"location":"csc373/lp.html#intuitive-example","title":"Intuitive Example","text":"<p>determine type of ridings difference Input platforms issues to emphasize. Market research gives following table of voters (lost/grand for each $1 spending)</p> urban sub rural roads -2 5 3 gun control 8 2 -5 farms 0 0 10 government 10 0 -2 <p>Goal Achieve gains of at least 50k urban, 100 sub-urban, 25k rural while spending as little as possible, identify unknowns \\(x_1,x_2,x_3,x_4\\in\\mathbb{R^{\\geq 0}}\\) is the spend on roads, gun control, farms, and government in advertising (unit is 1k), respectively, where \\(x_1+x_2+x_3+x_4\\) minimized.  Formalized: </p> \\[\\begin{bmatrix}         -2 &amp; 8 &amp; 0 &amp; 10 \\\\         5 &amp; 2 &amp; 0 &amp; 0 \\\\         3 &amp; -5 &amp; 10 &amp; -2 \\\\         1 &amp; 0 &amp; 0 &amp; 0 \\\\         0 &amp; 1 &amp; 0 &amp; 0 \\\\         0 &amp; 0 &amp; 1 &amp; 0 \\\\         0 &amp; 0 &amp; 0 &amp; 1 \\end{bmatrix} \\begin{bmatrix}         x_1 \\\\         x_2 \\\\         x_3 \\\\         x_4 \\end{bmatrix} \\geq  \\begin{bmatrix}         50 \\\\         100 \\\\         25 \\\\         0 \\\\ 0 \\\\ 0 \\\\ 0 \\end{bmatrix}\\] <p>minimize \\(\\sum_1^4 x_i\\)</p>"},{"location":"csc373/lp.html#problem-definition","title":"Problem Definition","text":"<p>a linear program consists of   - variables \\(\\vec{X}\\in\\mathbb{R}^n\\)  - objective function: min/maximization \\(\\vec{c}\\vec{X},\\vec{c}\\in\\mathbb{R^n}\\)  - constraint \\(\\vec{a_j}\\vec{X} \\le | = |\\ge b_j, \\vec{a_j}\\in\\mathbb{R}^n, 1\\leq j\\leq m\\). Note that the constraint are not strict (otherwise it cannot be solved as real number for open set). Matrix notation is used when all the constraints comparison relations are the same</p> <p>Any inequality can be translated as cutting on an infinite plane. Each constraint eliminates one-half plane. </p> <p>Feasible region is the collection of values that satisfy every constraint.  - \\(\\emptyset\\) - No solution   - unbounded - No extremum, no solution  - bounded - either exactly one solution or infinitely many solutions</p>"},{"location":"csc373/lp.html#algorithm","title":"Algorithm","text":"<ul> <li>Simplex algorithm worst-case exponential time (However, the edge case is very rare, commonly, it runs in polynomial with small constant)</li> <li>Interior point methods (worst-case polynomial but has large constant)</li> </ul>"},{"location":"csc373/lp.html#applications","title":"Applications","text":""},{"location":"csc373/lp.html#network-flow-problem","title":"Network Flow Problem","text":"<ul> <li>variables: \\(x(u,v) = f(u,v).\\forall (u,v)\\in E\\)</li> <li>objective function: maximize \\(\\sum_{(s,v)\\in E} x(s,v)\\)</li> <li>constraint: \\(0\\leq x(u,v)\\leq c(u,v)\\land \\sum_{(u,v)\\in E}x(u,v)=\\sum_{(v,u)\\in E} x(v,u). \\forall u\\in V-\\{s,t\\}\\)</li> </ul> <p>clearly, solutions to network flow problem corresponds to solutions to the linear program.   Note that the method does not guarantee integer-valued solution, even the actual solution is integer</p>"},{"location":"csc373/lp.html#shortest-path-with-weinmathbbz","title":"Shortest Path (with \\(w(e)\\in\\mathbb{Z}^+\\))","text":"<ul> <li>variables: \\(\\forall v\\in V. d_v=\\min\\{w(s\\sim v)\\}\\)</li> <li>objective function: \\(\\max d_t\\)</li> <li>constraints: \\(d_v\\geq 0. \\forall v\\in V\\) \\(d_s=0\\) \\(d_v\\leq d_u+w(u,v) \\land d_u\\leq d_v+w(u,v) \\forall (u,v)\\in E\\)</li> </ul>"},{"location":"csc373/lp.html#vertex-cover","title":"Vertex Cover","text":"<p>given \\(G=(V,E)\\) undirected, want \\(S\\subseteq V\\) that \\(S\\) covers all edges a.k.a. \\(\\forall (u,v)\\in E. u\\in S\\lor v\\in S\\), \\(|S|\\) minimized  - variables: \\(\\mathbb{I}_v:=\\mathbb{I}(v\\in S). \\forall v\\in V.\\)  - objective function: \\(\\min\\sum_{v\\in V}\\mathbb{I}_v\\)  - constraint: \\(\\mathbb{I}_u + \\mathbb{I}_v \\geq 1 \\forall (u,v)\\in E\\)  hidden constraint: since \\(\\mathbb{I}_v\\) is an indicator \\(\\mathbb{I}_v\\in\\{0,1\\}\\), while in this case, this is not a linear program constraint. The program becomes an integer program. However, integer program is not in polynomial time!</p>"},{"location":"csc373/lp.html#example-problem-1","title":"Example Problem 1","text":"<p>\"Duckwheat\" is produced in Kansas and Mexico and consumed in New York    and California. Each month, Kansas produces 15 \"shnupells\" of duckwheat    and Mexico, 8, while New York consumes 10 shnupells and California, 13.    The monthly transportation costs per shnupell are 4 from Mexico to New York, 1 from Mexico to California, 2 from Kansas to New York, and 3 from Kansas to California.</p>"},{"location":"csc373/lp.html#variables","title":"Variables","text":"<p>\\(x_1,x_2,x_3,x_4\\) be the amount of Duckwheat from </p> <ul> <li>\\(x_1\\) K to N</li> <li>\\(x_2\\) K to C</li> <li>\\(x_3\\) M to N</li> <li>\\(x_4\\) M to C  </li> </ul>"},{"location":"csc373/lp.html#objective","title":"Objective","text":"<p>minimize \\(2x_1 + 3x_2 + 4x_3 + x_4\\)</p>"},{"location":"csc373/lp.html#constraint","title":"Constraint","text":"<ul> <li>\\(x_1 + x_2 = 15\\)</li> <li>\\(x_3+x_4 = 8\\)</li> <li>\\(x_1 + x_3 = 10\\)</li> <li>\\(x_2+ x_4 = 13\\)</li> <li>\\(x_1,x_2,x_3,x_4 \\geq 0\\)</li> </ul> <p>Then it equals to maximize</p> \\[2x + 3(15-x)+4(10-x)+(x-2)=-4x+93\\] <p>where \\(2\\leq x\\leq 10\\), hence \\(x=2\\)</p>"},{"location":"csc373/lp.html#example-problem-2","title":"Example Problem 2","text":"<p>Consider a set of mobile computing clients in a certain town who each need to be connected to one of several possible \"base stations\". We'll suppose there are \\(n\\) clients, with the position of each client specified by its \\((x,y)\\) coordinates in the plane. There are also m base stations, each of whose position is specified by \\((x,y)\\) coordinates as well. We wish to connect each client to exactly one base station. Our choice of connections is constrained in the following ways. There is a \"range parameter\" r \u2212\u2212 a client can only be connected to a base station that is    within distance r. There is also a \"load parameter\" L \u2212\u2212 no more than L clients can be connected to any single base station.  Show how to represent this problem as a linear program, and how to solve    it using linear programming algorithms. Justify carefully that your    solution is correct. Can you guarantee that your algorithm runs in    polytime?</p>"},{"location":"csc373/lp.html#variables_1","title":"Variables","text":"<p>\\(I_{11},...,I_{nn}\\) where \\(I_{ij}\\) is the indicator that there is a connection between client \\(i\\) and base station \\(j\\).  </p>"},{"location":"csc373/lp.html#objective_1","title":"Objective","text":"<p>minimize \\(\\sum_{i=1}^n\\sum_{j=1}^n d_{ij}I_{ij}\\) where \\(d_{ij}\\) is the distance between client \\(i\\) and base station \\(j\\).  </p>"},{"location":"csc373/lp.html#constraint_1","title":"Constraint","text":"<ul> <li>\\(\\forall i\\in\\{1,2,...,n\\}. \\sum_{j=1}^n I_{ij} = 1\\)</li> <li>\\(\\forall i,j. d_{ij}I_{ij}\\leq r_{ij}\\)</li> <li>\\(\\forall j. \\sum_{i=1}^n I_{ij}\\leq L_j\\)</li> </ul> <p>Since \\(I_{ij}\\) is a indicator function, this is an integer problem</p>"},{"location":"csc373/network_flow.html","title":"Network Flow","text":""},{"location":"csc373/network_flow.html#problem-definition","title":"Problem Definition","text":"<p>network \\(N=(V,E)\\) is a directed, connected with:</p> <ul> <li>source \\(s\\in V\\) no incoming edge</li> <li>sink \\(t\\in V\\) no outgoing edge</li> <li>capacity \\(c:E\\rightarrow \\mathbb{N}\\)</li> </ul>"},{"location":"csc373/network_flow.html#question","title":"Question","text":"<p>Assign flow \\(f(e)\\in\\mathbb{R}^{\\geq 0}\\) s.t.   - \\(\\forall e\\in E. 0\\leq f(e)\\leq c(e)\\)  - \\(\\forall v\\in V-\\{s,t\\}, f^{in}(v)=f^{out}(v)\\), where \\(f^{in}(v)=\\sum_{(u,v)\\in E}f(u,v), f^{out}(v)=\\sum_{(u,v)\\in E}f(v,u)\\)  - Maximize flow in \\(N\\): \\(|f|=f^{out}(s)=f^{in}(t)\\)</p>"},{"location":"csc373/network_flow.html#discussion","title":"Discussion","text":"<p>Start with any feasible solution (one that satisfies the basic constraints)</p> <p>Make incremental improvements, until this is no longer possible</p>"},{"location":"csc373/network_flow.html#ford-fulkerson-algorithm","title":"Ford-Fulkerson Algorithm","text":"<p>start with any valid flow \\(f\\), e.g. \\(f(e)=0\\)   while there is some augmenting path \\(P\\) in \\(N\\): augment \\(f\\) along \\(P\\)</p> <p>path \\(P=s\\rightarrow v_1\\sim v_k\\rightarrow t\\) is augmenting IFF \\(f(e)&lt;c(e), \\forall e\\in P\\)</p> <p>First step  For every edge on \\(P\\), define residual capacity of the edge \\(\\Delta(u,v)=c(u,v)-f(u,v)\\), define residual capacity of the path \\(\\Delta(P)=\\min\\{\\Delta(u,v)\\mid (u,v)\\in P\\}\\), then augmenting \\(f\\) along \\(P\\) simply add \\(\\Delta(P)\\) to \\(f(e)\\) for each \\(e\\in P\\)</p> <p>After the augmentation, the graph will end up with non-maximum </p> <p>Second step Extend the notion of augmenting path. For each path, regardless direction. - send flow on a \"backward edge\" to somewhere else. Call its residual capacity \\(\\Delta(e)=f(e)\\) - All other edges are \"forward\" remains unchanged.  - To augment \\(f\\), increase \\(f(e)\\) for forward edges, decrease \\(f(e)\\) for backward edges. </p>"},{"location":"csc373/network_flow.html#residual-network","title":"Residual Network","text":"<p>For network \\(N\\), flow \\(f\\), create a residual network \\(N'=(V,E')\\). For each edge, change it to dual directions with weights \\(w(u,v)=\\Delta(u,v)\\) on the unchanged direction, \\(w(v,u)=f(u,v)\\) on the reversed direction.</p> <p>Therefore, augment path in \\(N\\) is equivalent to augment path in \\(N'\\)</p>"},{"location":"csc373/network_flow.html#correctness-of-the-strategy","title":"Correctness of the strategy","text":"<p>Cut a partition of \\(V=\\{V_s,V_t\\},s\\in V_s, t\\in V_t\\) \\(\\forall X=\\{V_s,V_t\\}\\) cut.   Define capacity \\(c(X)=\\sum_{u\\in V_s, v\\in V_t\\\\(u,v)\\in E} c(u,v)\\), we say such path is forward in respect to \\(X\\).  Define flow \\(f(X)=\\sum_{u\\in V_s, v\\in V_t\\\\(u,v)\\in E} f(u,v)-\\sum_{u\\in V_t, v\\in V_s\\\\(u,v)\\in E} f(u,v)\\)</p> <p>Observations - \\(\\forall N, f \\text{ valid}, X. f(X)\\leq c(X)\\) - \\(\\forall N,f,X. f(X)=|f|\\)</p> <p>Theorem \\(\\forall N, f \\text{ valid}. |f|\\) is maximized IFF \\(N\\) has no augmenting path. </p> <p>Proof \\(\\Rightarrow\\) Suppose \\(N\\) contains an augmenting path. then current \\(|f|\\) is no maximized. Proven by contradiction. </p> <p>\\(\\Leftarrow\\) Assume \\(N\\) does not contain an augmenting path, construct cut \\(X\\) as: </p> <ul> <li>start with \\(V_s=\\{s\\}, V_t=V-V_s\\) </li> <li>for any edge \\((u,v), u\\in V_s, v\\in V_t, f(u,v)&lt;c(u,v), V_t = V_t-\\{v\\}, V_s = V_s\\cup \\{v\\}\\)</li> <li>for any \\((u,v), u\\in V_t, v\\in V_s, f(u,v)&gt;0, V_t=V_t - \\{u\\}, V_s=V_s\\cup \\{u\\}\\)</li> </ul> <p>repeat as long as possible. Equivalently, \\(V_s =\\) all vertices reachable from \\(s\\) in \\(N'\\), \\(V_t=V-V_s\\)</p> <p>Then, \\(t\\not\\in V_s\\), \\(X=(V_s,V_t)\\) is valid and \\(\\forall (u,v)\\in E, u\\in V_s, v\\in V_t. f(u,v)=c(u,v). \\forall (u,v)\\in E, u\\in V_t, v\\in V_s, f(u,v)=0\\).</p> <p>Therefore \\(|f|=f(X)=c(X)\\)</p>"},{"location":"csc373/network_flow.html#complexity","title":"Complexity","text":"<p>possible for runtime depend on the smallest capacity \\(V=\\{s,t,a,b\\}, E=\\{(s,a,10^{100}), (a,t,10^{100}), (a,b,1), (s,b,10^{100}, (b,c,10^{100})\\}\\) Consider the loop \\(s\\rightarrow a\\rightarrow b\\rightarrow t,s\\rightarrow a\\leftarrow b\\rightarrow t\\)</p> <p>To improve 1. use BFS to find augmented paths: \\(O(|V||E|)\\) augmentations required. Since BFS runs in \\(O(|E|)\\) the total is \\(O(|V||E|^2)\\) 2. since we will rerun BFS sometimes, with each execution of BFS, use all augmented path found before running BFS again. Possible to improve to \\(O(|V|^2|E|)\\) 3. \\(O(n^3)\\) possible using a different tech called preflow algorithms. </p>"},{"location":"csc373/network_flow.html#reduction","title":"Reduction","text":"<p>In practice, solving problems through Network flow involves transforming the original problem into network flow. </p> <pre><code>1  input \n        Describe how to create N_X\n2. network N_X\n        Use F-F algorithm ...\n3. find max-flow f or min-cut X\n        Describe how to construct a solution from the N_X result\n4. construct solution to original problem\n</code></pre>"},{"location":"csc373/network_flow.html#correctness","title":"Correctness","text":"<ul> <li>Argue that every solution to original problem corresponds to some valid flow/cut in \\(N_X\\)</li> <li>Argue that every flow in \\(N_X\\) (or every cut) corresponds to some solution for original problem</li> </ul>"},{"location":"csc373/network_flow.html#max-bipartite-matching-problem","title":"Max bipartite matching Problem","text":"<p>Input: \\(G=(V_1,V_2, E)\\) undirected, bipartite (\\(\\forall (u,v)\\in E. u\\in V_1, v\\in V_2\\))</p> <p>Output: a matching \\(M\\subseteq E\\) s.t. no two edges in \\(M\\) shares endpoints. max \\(|M|\\)</p>"},{"location":"csc373/network_flow.html#solution","title":"Solution","text":"<p>add \\(s\\) to \\(V_1\\), add all edges from \\(s\\) to all vertices in \\(V_1\\), make all edges in \\(E\\) directed from \\(V_1\\) to \\(V_2\\). Add \\(t\\) to \\(V_2\\), add all edges from all vetices in \\(V_2\\) to \\(t\\). Then do network flow (integer). output \\(M=\\{(u,v)\\in E\\mid f(u,v)=1\\}\\)</p>"},{"location":"csc373/network_flow.html#correctness_1","title":"Correctness","text":"<p>for any matching \\(M\\) in \\(G\\), \\(\\exists\\) a corresponding flow \\(f(u,v)=\\mathbb{I}\\)((u,v) matching),\\(f(s,u)=\\mathbb{I}\\)(u matched), \\(f(v,t)=\\mathbb{I}\\)(if v matched). Then, $\\max|f|\\geq $max matching size</p> <p>for any valid flow \\(f\\), \\(\\exists M = \\{(u,v)\\in E\\mid f(u,v)=1\\}\\) because of the capacity constraint, no edges in \\(M\\) can share an endpoint. Then max matching size \\(\\geq \\max|f|\\)</p>"},{"location":"csc373/network_flow.html#max-independent-set-for-bipartite-graphs","title":"Max independent set for bipartite graphs","text":"<p>Input: \\(G=(V_1,V_2,E)\\) bipartite </p> <p>Output: an independent set \\(S\\subseteq V_1\\cup V_2\\) with no edge having both endpoints in \\(S\\). max \\(|S|\\).</p>"},{"location":"csc373/network_flow.html#solution_1","title":"Solution","text":"<ol> <li>add \\(s\\) to \\(V_1\\), add all edges from \\(s\\) to all vertices in \\(V_1\\) and has capacity 1, make all edges in \\(E\\) directed from \\(V_1\\) to \\(V_2\\) and have capacity \\(\\infty\\). Add \\(t\\) to \\(V_2\\), add all edges from all vetices in \\(V_2\\) to \\(t\\) have capacity 1.</li> <li>Find cut \\(X\\) in \\(N\\) with min capacity (first, find max flow, then let \\(X=\\)connected component of \\(s\\) in \\(N'\\))</li> <li>Let \\(X=(V_s,V_t)\\), )Output \\(S=(V_1\\cap V_s) \\cup (V_2\\cap V_t)\\)</li> </ol>"},{"location":"csc373/network_flow.html#correctness_2","title":"Correctness","text":"<ul> <li> <p>Given any independent set \\(S\\subseteq V_1\\cup V_2\\), consider the cut \\(X=(V_s,V_t)\\) where \\(V_s=\\{s\\}\\cup (V_1\\cap S)\\cup(V_2-S)\\). Then, \\(c(X)=\\sum_{u\\in V_1,u\\not\\in S} c(s,u) + \\sum_{v\\in V_2, v\\not\\in S}c(v,t) = |V_1-S|+|V_2-S|=|V_1\\cup V_2|-|S|\\). So min cut capacity \\(\\geq |V_1|+|V_2|\\) (max independent set size)</p> </li> <li> <p>For any cut \\(X=(V_s,V_t)\\) with \\(c(X)&lt;\\infty\\), then \\(S=(V_1\\cap V_s)\\cup(V_2\\cap V_t)\\) is an independent set since the cut has a finite capacity (else, there would be an edge with infinity capacity), then similarly \\(c(X)|=|V_1\\cup V_2|-|S|\\)</p> </li> </ul> <p>Therefore, the cuts is equivalent to the independent set. Hence, the max independent set size \\(=|V_1+V_2|-\\text{min cup capacity}\\)</p>"},{"location":"csc373/npc.html","title":"NP-complete","text":""},{"location":"csc373/npc.html#definitions","title":"Definitions","text":"<p>Theorem \\(\\forall D_1, D_2. D_1\\rightarrow_p D_2, D_2\\in P \\Rightarrow D_1\\in P\\). (also for \\(NP\\) and \\(coNP\\)) </p> <p>Proof \\(f\\) can compute \\(D_1\\)'s input in polynomial time, solving \\(D_2\\) by \\(A(i)\\), solving \\(D_1\\) by \\(A(f(i))\\) is in polynomial time. </p> <p>Corollary \\(D_1\\rightarrow_p D_2, D_1\\not\\in P\\Rightarrow D_2\\not\\in P\\)</p>"},{"location":"csc373/npc.html#np-complete_1","title":"NP-Complete","text":"<p>A decision problem \\(D\\) is NP-complete IFF \\(D\\in NP\\land D\\in NP hard\\)</p>"},{"location":"csc373/npc.html#np-hard","title":"NP-hard","text":"<p>\\(D\\) is NP-hard IFF \\(\\forall D'\\in NP, D'\\rightarrow_p D\\)</p> <p>Theorem If \\(D\\) is \\(NP-complete\\) then \\(D\\in P\\) IFF \\(P=NP\\)</p> <p>Corollary (If \\(P\\neq NP\\), then \\(D\\) is NP-complete) implies \\(D\\not\\in P\\). </p>"},{"location":"csc373/npc.html#sat","title":"SAT","text":"<p>Input: propositional formula \\(\\phi\\) Output: is there a setting to variables \\(\\phi\\) that makes it true</p>"},{"location":"csc373/npc.html#cnf-sat","title":"CNF-SAT","text":"<p>Input: a conjunctive normal propositional formula \\(\\phi'=C_1\\land ...\\land C_m\\) where each clause \\(C_i\\) is a disjunction \\(l_{i,1}\\lor ...\\lor l_{i,k}\\) where each literal \\(l_{i,j}\\) is a variable or variable negation Output: is there a setting to variables \\(\\phi\\) that makes it true</p>"},{"location":"csc373/npc.html#cook-levin-theorem","title":"Cook-Levin Theorem","text":"<p>SAT is NP-complete brief explanation - \\(SAT\\in NP\\) (given a certificate (values for all variables), you can evaluate the formula in poly-time, if output True for some \\(c\\) IFF \\(\\phi\\in SAT\\)) - \\(SAT\\in NPhard\\) for any \\(D\\in NP\\), show \\(D\\rightarrow_p SAT\\) in poly time. intuition: \\(D\\in NP\\Rightarrow \\exists verifier\\), for any \\(x\\), turn the verifier algorithm into a circuit, then into \\(\\phi_x\\) (turns out to be polynomial). Then \\(x\\in D\\) IFF \\(\\phi_x\\) is satisfiable.</p> <p>Note: original proof actually construct \\(\\phi_x\\) in CNF, then CNF-SAT is NP-hard, then SAT is NP-hard. </p>"},{"location":"csc373/npc.html#show-a-problem-is-np-hard","title":"Show a problem is NP-hard","text":"<p>Note that reduction is transitive  </p> <p>Given a problem \\(D\\) - Find \\(D'\\) known to be NP-hard - Show \\(D'\\rightarrow_p D\\) (then \\(\\forall D''\\in NP, D''\\rightarrow_p D'\\rightarrow_p D\\))</p> <p>Corollary SAT, CND-SAT, 3SAT (all clause have exactly 3 literals) are all NP-complete.</p>"},{"location":"csc373/npc.html#example-subset-sum","title":"Example: subset sum","text":"<p>Show the problem (SS) is NP-complete Input \\(S=\\{x_1,...,x_n\\}. x_i\\in \\mathbb{Z}^+, t\\in\\mathbb{Z}^+\\) Output is there \\(S'\\subseteq S, \\sum_{x\\in S'}x=t\\). 1. SS \\(\\in\\) NP given certificate \\(c\\subseteq S\\) (checking c is a subset, addition) 2. WTS 3SAT\\(\\rightarrow_p\\) SS. Want: algorithm runs in polytime that input: \\(\\psi\\) is a propositional formula in 3SAT output: \\(S = \\{y_1,...,y_k\\},t\\) s.t. \\(\\psi\\) is satisfiable iff \\(\\exists S'\\subseteq S, \\sum_{S'}y_i = t\\).  </p> <p>Let \\(\\cdot\\) denote concatenation. Construct function \\(g\\) such that for each variable and each clause</p> \\[g(a,b) = \\mathbb{I}((a=x_i \\land b=x_i)\\lor(a=x_i\\land b=\\bar{x_i})\\lor(a=x_i\\land b= C_j\\land x_i \\in C_j)\\lor(a=\\bar{x_i}\\land b= C_j\\land x_i \\in C_j))\\] <p>Then let \\(k\\leq 2n\\) \\(y_k = g(x_1,x_{\\lceil k/2\\rceil})\\cdot ...\\cdot g(x_n,x_{\\lceil k/2\\rceil})\\cdot g(C_1,x_{\\lceil k/2\\rceil})\\cdot...\\cdot g(C_m,x_{\\lceil k/2\\rceil})\\) if \\(k\\) is odd</p> <p>\\(y_k = g(x_1,\\bar{x}_{\\lceil k/2\\rceil})\\cdot ...\\cdot g(x_n,\\bar{x}_{\\lceil k/2\\rceil})\\cdot g(C_1,\\bar{x}_{\\lceil k/2\\rceil})\\cdot ...\\cdot g(C_m,\\bar{x}_{\\lceil k/2\\rceil})\\) is \\(k\\) is \\(k\\) is even</p> <p>Then let \\(2n+1 \\leq k\\leq 2(m+n)\\) \\(y_k = 100...000\\) if \\(k\\) is odd and \\(1\\) is located at \\(\\lfloor 2(m+n)-k\\rfloor + 1\\)'s position. \\(y_k = 200...000\\) if \\(k\\) is even and \\(2\\) is located at \\(\\lfloor 2(m+n)-k\\rfloor + 1\\)'s position. Let \\(t = 1\\cdot ...\\cdot 1\\cdot 4\\cdot ...\\cdot 4\\) where there are \\(n\\) 1's and \\(m\\) 4's. </p>"},{"location":"csc373/npc.html#argument-of-correctness","title":"Argument of Correctness","text":"<p>Suppose \\(\\psi\\) is satisfiable, then there is some setting of variables such that every clause contains at least one True literal.  Then start with \\(S'=\\) the set where every \\(y\\) corresponding to a true literal. Then, \\(\\sum S' = 11..1\\{1,2,3\\}...\\{1,2,3\\}\\) where there are \\(n\\) 1's and \\(m\\) \\(\\{1,2,3\\}\\)'s. Then, we can add from the number that corresponds to clauses to obtain \\(t\\).  </p> <p>Suppose \\(S\\) contains some subset \\(S', \\sum S'=t=11...144...4\\), then 1. None of the digits can adds to next digit 2. \\(S'\\) must contain exactly one of the numbers corresponding to each \\(\\{x_i,\\bar{x_i}\\}\\) for all \\(i\\in\\{1,2,...,n\\}\\). </p> <p>Set variables to make the literals corresponding to \\(S'\\) true. Then for the last \\(m\\) digits of \\(t\\), since numbers in the second half of \\(S\\) adds up to at most three in each digit, there has to be some number corresponding to \\(\\{x_i,\\bar{x_i}\\}\\) in \\(S'\\) must have a 1 in that digit. So each clause of \\(\\psi\\) contains at least one true literal.</p>"},{"location":"csc373/npc.html#example-vertex-cover","title":"Example: Vertex Cover","text":"<p>Input \\(G=(V,E), k\\in\\mathbb{Z}^+\\). Output \\(G\\) contains a vertex cover \\(C\\) of size \\(k\\) where \\(C\\subseteq V\\) s.t. \\(\\forall e\\in E\\). have at least one endpoint in \\(C\\). </p> <p>Claim. \\(3SAT\\rightarrow_p VC\\) Example \\((x_i + \\bar{x_2} + \\bar{x_4})(x_2+\\bar{x_3}+x_1)(\\bar{x_3}+x_4+\\bar{x_2})\\), label each literal by its position, where the \\(i\\)th clause is \\((a_i,b_i,c_i)\\).</p> <p>Construct \\(G\\) where each clause forms a triangle, for each variable makes \\(x_1-\\bar{x_1}\\), then add edges for each label to its corresponding variable. Construct \\(k=n+2m\\)</p> <p>Suppose \\(\\psi\\) is satisfiable, then  \\(C_1 =\\)all true literals and \\(C_2 =\\) two labels from each triangle where the one left out is connected to \\(C_1\\), \\(C=C_1\\cup C_2\\). </p> <p>Suppose \\(C\\) is a vertex cover of size \\(n+2m\\). Then, for each triangle, at least 2 labels must be picked, otherwise can't cover the triangle, at least 1 variable must be picked, otherwise can't cover the edge between the variable and its negation. Set all picked variables being true, to cover edge between variables and labels, each must contain one literal in \\(C\\). then in each triangle, at least one label is true, hence all clause evaluate true and \\(\\psi\\) is true. </p>"},{"location":"csc373/npc.html#show-decision-problem-is-np-complete","title":"Show decision problem is NP-complete","text":"<ol> <li>describe verifier that input \\(x\\) and certificate \\(c\\), verify in polytime as function of size(x) and show \\(x\\in D\\) IFF \\(\\exists c\\) output True.</li> <li>Identify some \\(D'\\in NPhard\\), show \\(D'\\rightarrow_p D\\) the reduction algorithm takes arbitrary input \\(x\\) for \\(D'\\), generate input \\(y_x\\) for \\(D\\) s.t. \\(x\\in D'\\Rightarrow y_x \\in D'\\) and \\(x\\not\\in D'\\rightarrow y_x\\not\\in D\\) (usually contrapositive by \\(y_x\\in D\\Rightarrow x\\in D'\\)) Notice that algorithm only gets \\(x\\), not \\(c\\) and backward argument starts from \\(y_x\\)</li> </ol>"},{"location":"csc373/optimization_approximation.html","title":"Optimization Approxiamation","text":"<p>Note: definition of NP-hardness based on worst-case analysis. Hence look for special characteristics o f\"real life\" inputs.  </p> <ul> <li>2SAT \\(\\in P\\), </li> <li>many graph problems are easier for restricted graph families <ul> <li>max independent set on trees</li> <li>planar graphs (graph without edge intersection)</li> <li>degree-restricted </li> </ul> </li> </ul> <p>NP-hard: no exact efficient solution for all inputs. </p>"},{"location":"csc373/optimization_approximation.html#approximation-algorithm","title":"Approximation Algorithm","text":"<p>In general, for minimization problem. Let \\(opt(x):=\\) best (minimum) value of any solution for input \\(x\\). For any algorithm, let \\(A(x)\\) be the value of the solution generated. \\(A(x)\\geq opt(x)\\). Approximation ratio for an minimization algorithm, \\(r(n)\\) s.t. all inputs \\(x\\) of size \\(n, A(x)\\leq r(n)opt(x)\\), for maximization algorithm, \\(r(n)A(x)\\geq opt(x)\\). By the definition, \\(r(n)\\geq 1\\)</p>"},{"location":"csc373/optimization_approximation.html#example-min-vertex-cover","title":"Example: Min vertex cover","text":"<p>Idea 1: Pick vertices greedily by non-increasing degree. (pick one, remove edges connected to the removed vertex). \\(r(n)\\in O(\\log n)\\). Idea2: repeatedly pick an edge \\((u,v)\\), remove \\(u,v\\),\\(C=C\\cup\\{u,v\\}\\) and remove all edges attaches to them. Correctness: all edges are removed as they are covered and algo ends with no edge remaining Runtime: \\(O(|E|^2)\\) Claim \\(r(n)=2\\) proof i. \\(r(n)\\geq 2\\): Consider a graph where each edge is matched with a pair of vertices. \\(2opt(G)=A(G)\\) ii. \\(r(n)\\leq 2\\): consider \\(C=\\) <code>approx_cover(G)</code> for any \\(G\\). For each pair of vertices in \\(C\\), each pair are non-adjacent does not share any endpoint. Implies that \\(G\\) contains at least \\(|C/2|\\) edges that an disjoint from each other. Therefore, every vertex cover of \\(G\\) must contain at least one endpoint from each edge in \\(C\\).  For all vertex cover \\(C', |C'|\\geq |C|/2\\)</p> <p>How to find \\(r(n)\\) when \\(opt\\) is unknown. For minimization problems, find a lower bound \\(L(x)\\leq opt(x)\\), find some other quantities, show that \\(A(x)\\leq r(n)L(x)\\)</p> <p>Idea3:  - Given \\(G\\), create a linear program:   - variables: \\(v_1,...,v_n\\) for each vertex  - objective function: \\(\\min{\\sum V}\\)  - constraint: \\(\\forall i,j. v_i + v_j \\geq 1; \\forall i. 0\\leq v_i\\leq 1\\) Notice that this is not the integer programming, we are allowed to give \\(v_i\\) real number - Solve over \\(\\mathbb{R}\\) so that \\(v_1^*,...,v_n^*\\) is the output of linear programming - output \\(C=\\{v_i\\in V\\mid v_i^*\\geq 0.5\\}\\)</p> <p>Correctness For each pair of vertices, one of the vertices is greater than \\(0.5\\) (otherwise they can't resolve the constraint \\(v_i + v_j \\geq 1\\)). Hence the output is a vertex cover</p> <p>Runtime poly time since linear programming</p> <p>Approx. Ratio \\(r(n)\\geq 1.5\\): Consider a triangle, the linear programming output \\((1/2, 1/2, 1/2)\\) \\(r(n)\\leq 2\\): Consider min vertex cover \\(C'\\), \\(v_1', ...,v_n'\\) is the solution of the integer programming of vertex cover problem. Then \\(\\sum v_i'=|C'|=opt(x)\\geq \\sum v_i^*\\) Note that for all vertex cover solutions, \\(v_i=\\mathbb{I}(x^*\\geq 1/2)\\), then \\(\\sum v_i \\leq 2\\sum x_1^*\\leq 2\\sum v_i'=2|C'|\\)</p> <p>Generally,  - VC=2 is the known best), some has non-constant  - set-cover (\\(\\lg n\\)).  - Knapsack \\(1+\\epsilon.\\) run time in \\(O(n^3/\\epsilon)\\) - Traveling Salesman Problem (TSP): no finite ratio. </p>"},{"location":"csc373/optimization_approximation.html#tsp","title":"TSP","text":"<p>Input: \\(G=(V,E)\\) undirected complete with \\(w(e)\\in\\mathbb{R}^+\\), some edge has \\(\\infty\\) weight Output: tour: A Hamiltonian cycle with minimized total weight.</p> <p>Claim NP-hard to approximate TSP with any finite approx ratio. proof  Assume A solves TSP with approx ratio \\(C\\). Consider input \\(G=(V,E)\\) for the Ham. cycle problem, create input \\(G'=(V,E')\\) for TSP, where \\(\\forall e\\in E. w(e)=1. \\forall e\\not\\in E. w(e)=Cn + 1\\). Rum A on this input, if \\(G\\) contains a Ham. cycle, then the same cycle is the output for TSP, and has weight \\(n\\). If \\(G\\) has no Ham cycle, then every tour in \\(G'\\) contains at least one edge with weight \\(Cn+1\\), so all tours have total weight \\(&gt; Cn+1 &gt; Cn\\) The output has weight \\(\\leq Cn\\) (since it is an approximation) if \\(G\\) has a Ham cycle and \\(&gt;Cn\\) if G does not have a Ham. cycle. Therefore, no polytime algo for Ham cycle IMPLIES no poly time approx. algo for TSP. </p>"},{"location":"csc373/optimization_approximation.html#tsp-with-triangle-inequality","title":"TSP with triangle inequality","text":"<p>approx_tsp(G, w)<pre><code>T = MST in G\n# Eulerian tour: go through each edge in M twice, goes around M\ntour = construct Eulerian tour of T\n# pre-order traversal goes along the tour, \n# add each vertex that is not traversed before into the return set\nreturn C:=pre-order_traversal(tour)\n</code></pre> Claim \\(\\forall G\\) follows the pre-condition. \\(r(n)\\leq 2\\) proof Let \\(C^*\\) be the optimum tour. Since \\(T\\) is a MST, it has the least total weight. \\(w(C^*)\\geq w(T)\\). Then, consider \\(C\\) and <code>tour</code>. \\(C\\) replaces some paths in <code>tour</code> with a shorter edge, this replacement is always shorter since the triangle inequality property. Therefore, \\(w(C)\\leq w(tour)=2w(T)\\leq 2w(C^*)\\)</p> <p>Note that the lower bound on approx ratio is open - need examples of inputs where \\(w(C)=2w(C^*)\\) or close to it. </p> <p>A similar idea, but starting from a perfect matching in \\(G\\). yields to \\(r(n)\\leq 1.5\\)</p>"},{"location":"csc373/optimization_approximation.html#knapsack-0-1-natural-number-inputs","title":"KnapSack (0-1, natural number inputs)","text":"<p>Algorithm use dynamic programming to find min weight required to achieve total value \\(v\\). =&gt; \\(O(nV), V=\\sum v_i\\) Instead of using exact values \\(v_1,...,v_n\\), use scaled values \\(v_i=\\)multiple of \\(\\epsilon^{-1}\\). (e.x. \\(v_1=347,238,947, v_2=434,357,833\\Rightarrow v_1=347,v_2=434\\))</p>"},{"location":"csc373/p_np.html","title":"P-NP Problem","text":""},{"location":"csc373/p_np.html#goal","title":"Goal","text":"<p>Study algorithm running time  - need precise model of computation. All models of computation are equivalent to each other to within a polynomial factor, with one exception: non-deterministic models, where there is exponential gap.</p>"},{"location":"csc373/p_np.html#pnp","title":"P/NP","text":""},{"location":"csc373/p_np.html#intuitive-definition","title":"Intuitive definition","text":"<p>\\(P=\\) set of every problem solvable by some deterministic algorithm in worst case<sup>*</sup> polynomial time \\(NP =\\) set of every problem solvable by some non-deterministic algorithm in polynomial time</p> <p>* runtime measured as a function of the exact bit size of the input Example: bit size of integers \\(\\{a_1,a_2,...,a_n\\}=\\lg a_1 + \\lg a_2 +...+\\lg a_n \\leq n\\max\\{\\lg a_1,...,\\lg a_n\\}\\)</p>"},{"location":"csc373/p_np.html#decision-problem","title":"Decision problem","text":"<p>Problem D where output is a single boolean value. </p> <p>e.x. It there a path in a graph \\(G\\) ftom \\(s\\) to \\(t\\) with weight \\(\\leq W\\)?</p> <p>We will prove hardness results about decision problems, which implies hardness of corresponding optimization problems</p> <p>\\(D:\\) the decision problem Input: object of some type \\(Q:\\) does input has property \\(P\\)?</p> <p>Language: \\(\\mathcal{L}=\\) set of string \\(x\\) that represent objects that satisfies property \\(P\\).  \\(x\\in D\\): answer for \\(x\\) is True (yes instance), \\(x\\not\\in D\\): answer for \\(x\\) is False (no instance). </p>"},{"location":"csc373/p_np.html#formal-definition","title":"Formal Definition","text":"<p>\\(P=\\) the set of all decision problems solvable by some polynomial time deterministic algorithm \\(NP =\\) the class of all decision problems solvable by some polynomial time verifier algorithm  </p>"},{"location":"csc373/p_np.html#verifier-one-type-of-non-deterministic-algorithm","title":"Verifier (one type of non-deterministic algorithm)","text":"<p>Takes input \\(x, c\\) where \\(x\\) is the input of the problem and \\(c\\) is the certificate generated by the algorithm.  - If \\(x\\in D\\), verify \\((x,c)=True\\) for some \\(c\\) - If \\(x\\not\\in D\\), verify \\((x,c)=False\\) for all \\(c\\)</p> <p>e.x. \\(COMP=\\{x\\in\\mathbb{N}\\mid x\\not\\in \\mathcal{P}\\}\\). Decision version: </p> <ul> <li>Input \\(x\\in\\mathbb{N}\\)</li> <li>\\(Q\\): Does \\(x\\) have factors other than 1? a.k.a. Does \\(x\\in COMP\\)</li> <li>Algorithm</li> </ul> <p><pre><code>for c in range(2, x): # generate\n    if c | x:     # verify\n        return True\n    return False\n</code></pre> More generally, generate-and-verify algorithms have the following structure  - for some decision problem \\(D\\), input \\(x\\) for \\(D\\)  - Generate all possible \"certificates\" \\(c\\)  - verify input \\(x\\) with the help of each \\(c\\) if verified then return True, else return False. (verifier)</p> <p>\\(D\\in NP\\) IFF \\(\\exists\\) verifier for \\(D\\) thats run in polynomial time, ignoring the time taken to generate all possible certificate. </p> <p>For \\(COMP\\): verify(x,c)<pre><code>return x mod c == 0\n</code></pre> will not take constant time for large input. Basic arithmetic operations are polynomial time. Therefore <code>verify</code> is polynomial time (as a function of \\(\\log x\\)) </p>"},{"location":"csc373/p_np.html#example-vertex-cover","title":"Example: Vertex cover","text":"<p>input \\(G=(V,E)\\) undirected and \\(k\\in\\mathbb{N}\\)  output: Does \\(G\\) contain a vertex cover \\(S. |S|=k\\) where \\(S\\subseteq V\\) every edge has \\(\\geq 1\\) on endpoints in \\(S\\)</p> <p>Claim: \\(D\\in NP\\) Describe a verifier:  verify(G,k,C)<pre><code># c is a subset of V with |c|=k\nreturn every edge has one endpoint in c\n</code></pre> runtime is \\(O(|E|k)\\in O(|V||E|)\\) - if \\((G,k)\\in D\\), \\(verify(G,k,c)=True\\) when c is a vertex cover.  - conversely, if \\(verify(G,k,c)=True\\) for some \\(c\\), then \\(c\\) is a vertex cover of size \\(k\\) Then \\((G,k)\\in D\\)</p> <p>Note that generally, continue of verify must be polynomial as a function of size(x) only (ignoring c)</p>"},{"location":"csc373/p_np.html#conp","title":"coNP","text":"<p>Example Dense Input: \\(G=(V,E)\\) undirected, \\(k\\in\\mathbb{N}\\) Output: Does every subset of \\(k\\) vertices contains at least one edge?</p> <p>When answer is yes, no easy way to verify. When answer is no, verifier  verify(G,k,c)<pre><code>if c is a subset of k vertices:\n    return not G contains none of the choose(k,2) edges between vetex of c \n</code></pre> \\((G,k)\\in Dense\\) IFF \\(\\not\\exists c, verify(G,k,c)=False\\)</p> <p>\\(coNP\\) = the set of decision problems whose no-instance can be verified in polynomial time</p> <p>every coNP question's negation is NP question.</p> <p>Believe \\(P\\neq NP\\), which there are problems that can be verified in polynomial time but cannot be answered in \\(O\\). But no proof</p>"},{"location":"csc373/p_np.html#question","title":"Question","text":"<p>Given \\(D\\in NP\\), how to show \\(D\\not\\in P\\), likely. </p> <p>Intuitively, compare the difficulty of solving problems to find hard problems in \\(NP\\). </p>"},{"location":"csc373/p_np.html#reducible","title":"Reducible","text":"<p>For any two decision problems \\(D_1,D_2\\), let \\(i_j=\\) the set of all inputs to \\(D_j\\) (yes and no instance). We say \\(D_1\\) is polynomial reducible to \\(D_2, (D_1\\rightarrow_p D_2)\\) IFF \\(\\exists f:I_1\\rightarrow I_2\\) s.t. \\(f\\) computable in polynomial time and \\(\\forall x\\in I_1, x\\in D_1\\) IFF \\(f(x)\\in D_2\\)</p>"},{"location":"csc373/p_np.html#example","title":"Example","text":"<p>independent set \\(\\rightarrow_p\\) Vector Cover independent set: given \\(G,k\\), does \\(G\\) contains an independent set of size \\(k\\) vertex cover: given \\(G,k\\), does \\(G\\) contains a vertex cover of size \\(k\\). </p> <p>Define \\(f\\): reduce(G,k)<pre><code># G,k input for indep set\nreturn (G, |V|-k) \n</code></pre></p> <p>Show \\(\\forall x\\in I_1, x\\in D_1\\) IFF \\(f(x)\\in D_2\\)</p> <p>If \\(S\\subseteq V\\) independent set, \\(|S|=k\\), then \\(V-S\\) is a vertex cover in \\(G\\). If \\(S\\subseteq V\\) vertex cover, \\(|S|-n-k\\), then \\(V-S\\) is a indep set in \\(G\\). </p>"},{"location":"csc373/p_np.html#example-questions","title":"Example Questions","text":"<ol> <li> <p>Show <code>UP</code>\\(\\in P\\) where input \\(11..1\\) is \\(n\\) 1's. Output \\(n\\) is prime. Different from normal algorithm that returns whether a number is prime. The input size is \\(n\\) instead of \\(\\log m\\) where \\(m\\) is the value of the number.</p> </li> <li> <p>Show \\(Triangle\\in P\\) where input \\(G\\) undirected output whether exists a triangle. There are \\({n\\choose 3}=n(n-1)(n-2)/6\\) combinations of vertices and check edges take constant time.  </p> </li> <li> <p>Show <code>Clique</code>\\(\\in NP\\) where input \\(G\\) undirected, \\(k\\in\\mathbb{Z}^+\\) output whether exists a \\(k\\)-clique (\\(k\\) vertices with all edges) in \\(G\\).  Given a certificate as \\(C\\subseteq V\\). return \\(|C|=k\\land \\forall u,v\\in C, (u,v)\\in E\\). Obviously can be verified in polytime \\((O(n^2))\\). If \\(G,k\\in Clique\\) then \\(\\exists C\\). If \\(\\exists C\\) then \\(G,k\\in Clique\\). </p> </li> <li> <p><code>SS</code>\\(\\in NP\\) where input \\(S\\) be a set of positive integers, \\(t\\in\\mathbb{Z}^+\\) output \\(S'\\subseteq S. \\sum S = t\\) Given a certificate as \\(S'\\) return \\(S'\\subseteq S\\land \\sum S'=t\\)</p> </li> </ol>"},{"location":"csc373/self_reducibility.html","title":"Self Reducibility","text":""},{"location":"csc373/self_reducibility.html#self-reducibility_1","title":"\"Self-reducibility\"","text":"Problem Devision Search Optimization SAT Given \\(\\phi\\), is it satisfiable Given \\(\\phi\\), find values that makes \\(\\phi\\) true - Vertex Cover Given \\(G,k\\), is there a vertex cover of size \\(k\\) Given \\(G,k\\), find a vertex cover of size \\(k\\) Given \\(G\\), find a smallest vertex cover <p>Turing Reducible For problems \\(A,B\\) (search, opt, decision) \\(A\\) is Turing reducible to B IFF   - Assuming a constant time algorithm for \\(B\\), then we can write a polynomial time algorithm for \\(A\\). </p>"},{"location":"csc373/self_reducibility.html#example-clique-search","title":"Example: Clique Search","text":"<p>Clique Given \\(G\\) undirected, \\(k\\in\\mathbb{Z}^+\\) does \\(G\\) contains a \\(k\\)-clique which \\(S\\subseteq G, |S|=k\\) with each possible edges between them. Clique Search Given \\(G\\) undirected, \\(k\\in\\mathbb{Z}^+\\), return a \\(k\\)-clique or null for non-existence. </p> <p>Claim: Clique-search Turing reducible to Clique  </p> <p>Implicit assumption problem: solving decision problems require finding certificate. (Therefore, must treat the algorithm for B as a black box, no assumption allowed, other than it solves clique)</p> <p>cs(G,k)<pre><code>if not cd(G,k):\n    return Nil\nfor v in V:\n    G = G - v if cd(G-v,k) \nreturn V\n</code></pre> runs in \\(O(n T(n) + n(n+m))\\) where \\(T(n)\\) is the runtime of <code>cd</code> is poly time compare to \\(T(n)\\). </p>"},{"location":"csc373/self_reducibility.html#correctness","title":"Correctness","text":"<p>G in each iteration contains a k-clique (by <code>cd(G,k)</code>). For each vertex that is not in the k-clique, it will be removed. </p> <p>In general, to show self reducibility</p> <ol> <li>Assume algorithm solves decision problem</li> <li>Write algorithm for the search problem, making calls to \\(D\\) as a black box. </li> <li>argue correctness and runtime.</li> </ol> <p>Note this shows that search problem is Turing reducible to decision problem.</p>"},{"location":"csc373/self_reducibility.html#example-hamiltonian-path-search","title":"Example: Hamiltonian Path search","text":"<p>given \\(G\\), find a simple path that goes through every vertex. </p> hs(G)<pre><code>if not hd(G):\n    return Nil\nfor e in E:\n    E = E - e if hd(G-e) \nreturn E\n</code></pre>"},{"location":"csc373/self_reducibility.html#example-vertex-cover","title":"Example: Vertex Cover","text":"<p>vcs(G,k)<pre><code>if not vcd(G,k):\n  return Nil\nC = []\nfor v in V and while k &gt; 0:\n  if vcd(G-v, k-1):\n      C.append(v)\n      G = G - v\n      k -= 1\nreturn c\n</code></pre> runs in \\(O(nt(n)+n(M+n))\\)</p>"},{"location":"csc373/self_reducibility.html#correctness_1","title":"Correctness","text":"<p>\\(v\\) belongs to a vertex cover of size \\(k\\) IFF <code>vcd(G-v, k-1) == True</code> If C is a vertex cover of size \\(k\\) and \\(v\\in C\\), then \\(C-\\{v\\}\\) is a vertex vover of size \\(k-1\\) in \\(G-v\\). If <code>vcd(G-v, k-1) == True</code> then there is some vertex cover \\(C\\) of size \\(k-1\\) in \\(G-v\\), then \\(C\\cup\\{v\\}\\) is a vertex cover of size \\(k\\) since when we put \\(v\\) back, all added edges are connected to \\(v\\).  </p>"},{"location":"csc373/self_reducibility.html#optimization","title":"Optimization","text":""},{"location":"csc373/self_reducibility.html#example-max-clique","title":"Example: Max clique","text":"<p>find the largest clique in \\(G\\).  </p> <p>find the largest \\(k\\) s.t. <code>cd(G,k) == True</code> (Using binary search, takes \\(O(\\log n t(n))\\)). Then return <code>cs(G,k)</code></p>"},{"location":"csc384/csp.html","title":"CSP Backtracking","text":""},{"location":"csc384/csp.html#feature-vectors","title":"Feature Vectors","text":"<p>Define a feature vector of a state as  - a set of n variables (features) - each variable has a domain of different values - A state is specified by an assignment of a value for each variable - A partial state is specified by an assignment of a value to some of the variables</p> <p>For example, for sudoku, we have 81 variables \\(a_{11},..,a_{99}\\), each variable belongs to the domain \\(\\{1,..,9\\}\\). Then, a state of the puzzle can be represented by specified value in each cell. A partial state is where some (not all) cells filled in.</p>"},{"location":"csc384/csp.html#csp-problem-setup","title":"CSP Problem Setup","text":"<p>Compare to search problem, we don't care about the sequence of actions, we only need to find a setting of variables that satisfies all of the constraints. </p> <p>Define a constraint satisfaction problem as  - a set of variables \\(V = \\{v_1,.., v_n\\}\\)  - a finite domain of possible values for each variable \\(D(v_i)\\) s.t. \\(v_i \\in D(v_i)\\)  - a set of constraints \\(C_1,...,C_m: \\mathcal P(V)\\rightarrow \\{0, 1\\}\\), which give an assignment of variables, return whether it satisfies the constraint. Note that most constraint will only check for a particular set of variables, called its scope. </p> <p>We want to find the solution as an assignment to all of the variables, s.t. \\(C_1,...,C_m\\) will all return True. A CSP is unsatisfiable if no solution exists. </p> <pre><code>class Variable:\n    def __init__(self, \n                 name: str, \n                 domain: List,\n                 value\n                 ):\n        # current assigned value\n        # variable is unassigned IFF value is None\n        self.value = value \n        # a string specifying the variable's name\n        self.name = name\n        # a set of values in variable's domain\n        self.domain = domain\n\n    def domainSize(self):\n        return len(self.domain)\n\n    def isAssigned(self):\n        return self.value is None\n\nclass Constraint:\n    def __init__(self, \n                 name: str, \n                 scope: List[Variable]):\n        # a string specifying the constraint's name\n        self.name = name\n        # s list of variables in the constraint\u2019s scope\n        self.scope = scope\n\n    def arity(self):\n        return len(self.scope)\n\n    def num_unassigned(self):\n        \"\"\" Returns number of variables in \n        constraint\u2019s scope that are not assigned\n        \"\"\"\n        return sum([v.isAssigned for v in self.scope])\n\n    def check(self):\n        \"\"\" return True if currently assigned values \n        to the variables in the scope satisfy the constraint.\n        Or num_unassigned &gt; 0\n        \"\"\"\n        return self.num_unassigned &gt; 0\n</code></pre>"},{"location":"csc384/csp.html#sudoku-example","title":"Sudoku Example","text":"<ul> <li>Variables \\(v_{11}, ..., v_{99}\\) </li> <li>domain \\(D(v_{ij}) = \\{1,...,9\\}\\) if \\(v_{ij}\\) is not filled, otherwise \\(k\\) if \\(v_{ij}\\) is filled with \\(k\\). </li> <li>constraints (27 constraints):<ul> <li>\\(\\text{all-diff}(v_{11}, ..., v_{19}), \\text{all-diff}(v_{21}, ..., v_{29}), ..., \\text{all-diff}(v_{91}, ..., v_{99})\\): row constraint</li> <li>\\(\\text{all-diff}(v_{11}, ..., v_{91}), \\text{all-diff}(v_{12}, ..., v_{92}), ..., \\text{all-diff}(v_{19}, ..., v_{99})\\): column constraint</li> <li>\\(\\text{all-diff}(v_{11}, ..., v_{33}), \\text{all-diff}(v_{41}, ..., v_{43}), ..., \\text{all-diff}(v_{77}, ..., v_{99})\\): sub square constraint</li> </ul> </li> </ul>"},{"location":"csc384/csp.html#backtracking-search","title":"Backtracking Search","text":"<p>Because CSP does not require finding a path, and traditional search does not capture additional structure of CSP problem. Although CSP can be viewed as search, we need some specifications to frame the algorithm. </p> <p>Instead of paths, we will search through the space of partial assignments. Unlike search, order does not matter. If a constraint is violated during the process of building up a solution, we can immediately reject it. When backtracking finishes, we filled in all variables, and get a solution. </p> <p>We initialize all variables with <code>var.value = None</code> and put them into <code>unAssignedVars</code>, and run <code>backtracking</code> on the unassigned variables. </p> <pre><code>def backtracking(unAssignedVars):\n    # base case: all assigned, and we output\n    if unAssignedVars.empty():\n        return [[(var.name, var.value) for var in variables]]\n    # recursively assign vars \n    solns = []\n    var = unAssignedVars.extract()\n    for val in var.domain():\n        var.value = val\n        cnstrOK = True\n        for cnstr in constrainsOf(var):\n            # if one constraint fails with all of its vars assigned\n            # then we stop\n            if cnstr.numUnassigned == 0 and not cnstr.check():\n                cnstrOK = False\n                break\n        # if no constraint is violated, we go to the next variable\n        if cnstrOK:\n            solns += backtracking(unAssignedVars)\n    # undo assignment and restore var to unAssigned\n    var.value = None\n    unAssignedVars.insert(var)\n    return solns\n\nunAssignedVars = variables\nbacktracking(unAssignedVars)\n</code></pre>"},{"location":"csc384/fc_gac.html","title":"Constraint Propagation","text":"<p>In backtracking search, we only detect constraints violations when all of its variables being assigned. Which may take much more. But if we can \"look ahead\" on yet unassigned variables, we can detect obvious failures and prune them early. </p>"},{"location":"csc384/fc_gac.html#forward-checking","title":"Forward Checking","text":"<p>When a variable is instantiated, we check all constrains that have only one uninstantiated variable (<code>cnstr.numUnassigned() == 1</code>)</p> <p>For the uninstantiated variable, we check all of its possible values, prune those that violates the constraint.</p> <p>During backtracking search, we will be making new variable assignments, and need to undo the pruning when we backtrack. Therefore, we need some more facilities </p> <pre><code>class VariableFC(Variable):\n\n    def curDomain(self):\n        \"\"\" return a list of variable's current values\n        \"\"\"\n        pass\n\n    def curDomainSize(self):\n        return len(self.curDomain())\n\n    def pruneValue(val, assignedVar, assignedVal):\n        \"\"\" remove value from variable's curDomain. remember that \n        assignedVal assigned to assignedVar is the reason for this \n        pruning\n        \"\"\" \n        pass\n\nclass ConstraintFC(Constraint):\n\n    def unassignedVars(self):\n        return [var for var in self.scope if var.value is None]\n\ndef restoreValues(var, val):\n    \"\"\" return all values pruned because of the passed var-val assignment \n    to the current domain of their respective variable.\n    \"\"\"\n    pass\n</code></pre> <pre><code>def forward_check(cnstr, assignedVar, assignedVal):\n    var = cnstr.unAssignedVars()[0]\n    # try whether the current value satisfy the constraint\n    for val in var.curDomain():\n        var.value = val\n        # if not, remove from current domain\n        if not cnstr.check():\n            var.pruneValue(var, assignedVar, assignedVal)\n    # reset var back after trying\n    var.setValue(None)\n    # if no value is possible to assign, meaning the \n    # assignedVar and assignedVal cannot be used\n    if var.curDomainSize() == 0:\n        return \"DWO\"\n    return \"OK\"\n\ndef backtrack_fc(unAssignedVars):\n    # base case: all assigned, and we output\n    if unAssignedVars.empty():\n        return [[(var.name, var.value) for var in variables]]\n    # recursively assign vars \n    solns = []\n    var = unAssignedVars.extract()\n    for val in var.domain():\n        var.value = val\n        noDWO = True\n        for cnstr in constrainsOf(var):\n            # if this assignment will make another \n            # variable have no possible value assignment\n            # then we stop\n            if cnstr.numUnassigned == 1 and not forward_check(cnstr, var, val) == \"DWO\":\n                noDWO = False\n                break\n        # if no constraint is violated, we go to the next variable\n        if noDWO:\n            solns += backtracking_fc(unAssignedVars)\n        restoreValues(var, val)\n    # undo assignment and restore var to unAssigned\n    var.value = None\n    unAssignedVars.insert(var)\n    return solns\n</code></pre>"},{"location":"csc384/fc_gac.html#minimum-remaining-value-heuristic","title":"Minimum Remaining Value Heuristic","text":"<p>With FC, we can always branch (extract from unassigned variables) on a variable with smallest current domain size. </p> <p>The idea is that if a variable has only one value left, then it is forced with the assignment, and we can go quicker in constraint propagation. </p>"},{"location":"csc384/fc_gac.html#generalized-arc-consistency","title":"Generalized Arc Consistency","text":"<p>GAC check all constraints by ensures that all constraints satisfy a certain level of consistency w.r.t. the already assigned variables. </p> <p>A constraint \\(C(V_1, ..., V_n)\\) is GAC w.r.t. \\(V_i\\) IFF for all val of \\(V_i\\), there exists values of \\(V_1,..,V_{i-1}, V_{i+1}, V_n\\) s.t. \\(C\\) is satisfied. </p> <p>\\(C\\) is GAC IFF all of its variables are in GAC. </p> <p>And the CSP is GAC IFF all of its constraints are in GAC. </p> <p>Therefore, the idea for GAC is: If we find a value \\(d\\) of variable \\(V_i\\) that is not GAC, \\(d\\) is said to be arc inconsistent and we can remove \\(d\\) from \\(dom(V_i)\\). </p> <p>propagation we prune the domain of a variable to make a constraint GAC, but this may make another constraints no longer GAC, hence we need to re-prune that constraint again until all constraints are in GAC. </p>"},{"location":"csc384/fc_gac.html#hassupport","title":"<code>hasSupport</code>","text":"<p>A var-val assignment is said to has a support in constraint \\(C\\) if exists some variable assignment s.t. the value assignments are all in variables current domains, respectively, and \\(C\\) is satisfied. A constraint is GAC if all of its scope variables \\(V_i\\), and every value in current domain of \\(V_i\\), has a support. </p> <p>We will use an additional member method for <code>Constraint</code> to check for this. </p> <pre><code>def GAC_enforce(constraints, assignedVar, assignedVal):\n    while not constraints.empty():\n        cnstr = constraints.extract()\n        for var in cnstr.scope():\n            for val in var.curDomain():\n                if not cnstr.hasSupport(var, val):\n                    var.pruneValue(val, assignedVar, assignedVal)\n                    if var.curDomainSize == 0:\n                        return \"DWO\"\n                for recheck in constrainsOf(var):\n                    if recheck is not cnstr and not recheck in constraints:\n                        constraints.insert(recheck)\n    return \"OK\"\n\ndef backtrack_gac(unAssignedVars):\n    # base case: all assigned, and we output\n    if unAssignedVars.empty():\n        return [[(var.name, var.value) for var in variables]]\n    # recursively assign vars \n    solns = []\n    var = unAssignedVars.extract()\n    for val in var.domain():\n        var.value = val\n        noDWO = True\n        if GAC_enforce(constrainsOf(var), var, val) == \"DWO\":\n            noDWO = False\n            break\n        # if no constraint is violated, we go to the next variable\n        if noDWO:\n            solns += backtracking_gac(unAssignedVars)\n        restoreValues(var, val)\n    # undo assignment and restore var to unAssigned\n    var.value = None\n    unAssignedVars.insert(var)\n    return solns\n</code></pre>"},{"location":"csc384/hsearch.html","title":"Heuristic Search","text":"<p>Define a domain specific heuristic function \\(h(p)\\) where input the path, and output the guess of the cost getting to a goal state from the final state of a path \\(p\\). Note that \\(h\\) is defined on state, instead of the whole path. Actually, we can define \\(h\\) on state only. \\(h(s) = 0\\) if <code>Goal(s) == True</code></p>"},{"location":"csc384/hsearch.html#greedy-best-first-search","title":"Greedy Best-First Search","text":"<p><code>Open</code> is implemented as a <code>priority_queue</code> with priority being the heuristic cost. </p> <p>If the heuristic perfectly predicts the actual cost from a state to the optimal goal, it will be the optimal and efficient. But in practice, it can be incomplete and very far from optimal.</p> <p>Also, note that GBFS is incomplete if the heuristic is misleading, since it will only check for locally optimal path. </p>"},{"location":"csc384/hsearch.html#a-search","title":"A* Search","text":"<p><code>Open</code> is implemented as a <code>priority_queue</code> with priority as <code>f(p) = heuristic(p) + cost(p)</code>. Therefore, instead of performing locally optimal action, \\(f\\) estimates the cost of the path from initial state to a goal state. </p>"},{"location":"csc384/hsearch.html#completeness","title":"Completeness","text":"<p>Note that A* is complete as long as the branching factor is finite, the cost for every action is finite and lower bounded by some \\(\\epsilon &gt; 0\\), and the heuristic value is finite for every path that can be extended to reach a goal state. </p> <p>The idea is that the misleading paths will keep increase as its cost is growing, and then we will go back the path that can be extended to reach a goal state. </p>"},{"location":"csc384/hsearch.html#optimality","title":"Optimality","text":"<p>Let \\(h^*(p)\\) be the cost of optimal path from \\(p\\) to a goal node. A heuristic is admissible if for all path, the heuristic underestimates the cost, \\(\\forall p. h(p)\\leq h^*(p)\\).</p> <p>If a heuristic is admissible and the cost if lower bounded by some positive number, then the search won't miss any promising paths.</p> <p>Theorem With an admissible heuristic, A* will always finds an optimal cost solution, if solution exists, the branch factor is finite, and cost is finite and positively lower bounded. </p> <p>Lemma 1 A* with an admissible heuristic never expands a path with f-value greater than the cost of an optimal solution. proof. By induction, we can show that at every iteration a predix of the optimal path is on the Open, and all other expanded paths cannot have f-val greater than the optimal partial path by the priority. We will eventually expanding the partial optimal path while keeping the maximum f at each iteration smaller than the optimal cost. </p>"},{"location":"csc384/hsearch.html#consistency","title":"Consistency","text":"<p>A monotone heuristic is a heuristic that satisfies triangle inequality, i.e. the h-val of a path is always smaller or equal to the h-val of another path plus its the cost from transforming the final state.</p> <p>Theorem monotone heuristic is admissible. </p> <p>For every path. its h-val is always smaller than the cost of transform to some optimal path and go to the goal state. </p> <p>Also, monotone implies that f-val is monotonic growing for paths expanded and the paths expanded is always minimum cost.</p>"},{"location":"csc384/hsearch.html#space-and-time","title":"Space and Time","text":"<p>If \\(h=0\\), then it's UCS, so the time and space bound is similar to UCS. But Even if we have a good heuristic, time and space will still be exponential. </p>"},{"location":"csc384/hsearch.html#iterative-deepening-a","title":"Iterative Deepening A*","text":"<p>Similar to IDS, we set f-val limit instead of depth limit. At each iteration, set new limit as the smallest f-val of any path that exceeded the previous limit. </p>"},{"location":"csc384/search.html","title":"Search","text":""},{"location":"csc384/search.html#problem-setup","title":"Problem Setup","text":"<p>State is a representation of a configuration of the problem domain.   </p> <p>State Space the set of all states. </p> <p>Initial State the starting configuration</p> <p>Goal State Some configuration one want to achieve</p> <p>Action (State Space Transitions) A function from state to another state that is defined by allowed changes to move from one state to another</p> <p>Then a solution will a sequence of actions that transform the initial state to a goal state. </p>"},{"location":"csc384/search.html#search-graph","title":"Search Graph","text":"<p>Assuming the search space is finite, we can define a Graph \\(G=(V,E)\\) where \\(V\\) is the states in the search space, with repetitions, and \\((v_i,v_j)\\in E\\) if there's an action that transforms \\(v_i\\) into \\(v_j\\). </p> <p>A search tree reflects the behavior of an algorithm as it walks through a search problem. We consider the solution depth \\(d\\) and max branching factor \\(b\\). </p> <p>Note that if we prune all circles in the graph, the search graph should be a tree rooted at the initial state, and branching out by possible actions. </p>"},{"location":"csc384/search.html#algorithm","title":"Algorithm","text":"<p>An algorithm will take inputs as   - initial state  - successor function \\(S(x)\\): return the set of states that can be reached via one action from \\(x\\).   - Goal Test \\(G(x)\\) return true iff \\(x\\) satisfies the goal condition.  - action cost function \\(C(x,A,y)\\) return the cost of the action \\(A(x)=y\\), if unreachable, then return \\(\\infty\\)</p> <p>And output sequence of actions \\((A_0, ...., A_n)\\) that will transforms initial state into the goal. The solution might be optimal in cost, or in length, but generally does not have any guarantees. </p> <p>For a algorithm we need to consider:  - completeness: will the search always find a solution if a solution exists - optimality: will the search always find the optimal cost solution - time complexity: often considered by the max number of nodes (paths) expanded - space complexity: often considered by the max number of nodes (paths) stored</p>"},{"location":"csc384/search.html#tree-search","title":"Tree Search","text":"<p>To explore the state space, iteratively apply \\(S\\) to states discovered so far, and \\(S(x)\\) will give more states. We can at the same time, annotate their action and cost. For example, \\(S(x) = \\{(y_1, a_1, [c_1]), ..., (y_k, a_k, [c_k])\\}\\). We can put states into a list called Frontier (Open) and repeatedly pulling states from Open until a goal state is found. </p> <pre><code>def tree_search(s0, Succ, Goal):\n    \"\"\"\n    s0: initial state\n    Goal: Goal test\n    Succ: successor function\n    \"\"\"\n    Open = {s0}\n    while not Open.is_empty():\n        # the selection will be defined by different algorithm\n        x = Open.pull()\n        if Goal(x):\n            return True\n        Open = Open.union(Succ(x))\n    return False\n</code></pre> <p>Note that the order of pulling from <code>Open</code> has a critical effect on the search, therefore, we need to define some order on <code>Open</code></p>"},{"location":"csc384/usearch.html","title":"Uninformed search","text":"<p>The rule remains the same for any search problem being solved, and these strategies do not take into account any domain specific information about the particular search problem.</p> <p>Note that in most of cases, we are dealing with exponential growing number of nodes and edges. And the states are often implicitly defined. We need to discover them through successor state functions and states must contain enough information to allow the successor function to perform its computation. </p>"},{"location":"csc384/usearch.html#breath-first-search-bfs","title":"Breath-First Search (BFS)","text":"<p><code>Open</code> is implemented as a <code>queue</code> (first-in-first-out). Therefore, we will always finish expanding nodes at depth \\(k\\) before moving to \\(k+1\\). </p> <ul> <li>completeness: yes, it will traverse all of the states expanded from \\(s0\\)</li> <li>optimality: shorted-path, but not necessarily least cost</li> <li>time: \\(\\sum_i^d b^i \\in O(b^{d+1})\\).</li> <li>space: \\(O(b^{d+1})\\)</li> </ul>"},{"location":"csc384/usearch.html#depth-first-search-dfs","title":"Depth-First Search (DFS)","text":"<p><code>Open</code> is implemented as a <code>stack</code> (first-in-last-out)</p> <ul> <li>completeness: No if there's circles (will fall into an infinite loop). Yes if state space is finite and without circles</li> <li>optimality: No</li> <li>time: \\(O(b^m)\\) where \\(m\\) is the length of longest path</li> <li>space: \\(O(bm)\\) Frontier only contains the current path along with the unexplored siblings of states along current path.</li> </ul>"},{"location":"csc384/usearch.html#iterative-deepening-search-ids-and-depth-limited-search-dls","title":"Iterative Deepening Search (IDS) and Depth Limited Search (DLS)","text":"<p>DLS only add states into <code>Open</code> if the path from \\(s0\\) to the state is less than or equal to the defined depth limit. <code>Open</code> will still be implemented as a <code>stack</code>. </p> <p>IDS iterative increasing the depth limit and perform DLS, until a solution is found or DLS does not cutoff any paths (no state can be expanded further more). </p> <ul> <li>completeness: yes, the same reason as finite state DFS</li> <li>optimality: shortest length (if there's a shorter path, we should have found it with the iteration of smaller depth limit) but not least cost (we can use cost limit instead of depth limit to achieve this, but it will be computationally expensive)</li> <li>time: \\(\\sum_{i}^d (d+1-i)\\cdot O(b^i)\\in O(b^d)\\)</li> <li>space: \\(O(bd)\\) since we drop the tree when one iteration of DLS ends</li> </ul> <p>Although IDS looks more complex than BFS, note that the node expansion is exponential. The time complexity of IDS can sometimes be better than \\(BFS\\) since it does not expand paths at the solution depth while BFS did. And space complexity for BFS is much worse than IDS. </p>"},{"location":"csc384/usearch.html#uniform-cost-search-ucs","title":"Uniform-Cost Search (UCS)","text":"<p><code>Open</code> is implemented as a <code>priority_queue</code> with priority being the cost. If the cost is the same, then it will perform BFS. </p> <ul> <li>completeness: yes if the cost of each action in the expansion is positive, say \\(c \\geq \\epsilon &gt; 0\\)</li> <li>optimality: optimal in cost</li> <li>time: \\(O(b^{\\lfloor c^*/\\epsilon\\rfloor+1})\\) \\(c^*\\) be the optimal cost, \\(\\epsilon\\) be the cost bound for an action</li> <li>space: \\(O(b^{\\lfloor c^*/\\epsilon\\rfloor+1})\\) </li> </ul> <p>proof of cost optimality 1. if a path \\(p2\\) is expanded after \\(p1\\), then the cost \\(c(p1)\\leq c(p2)\\) as \\(p2\\) will contains \\(p1\\) 2. the paths in the search space has strictly increasing cost</p>"},{"location":"csc384/usearch.html#path-checking-and-cycle-checking","title":"Path checking and Cycle checking","text":"<p>Note that circles (the path of state transforms may return to a known state) is a big issue. </p> <p>path checking: Each time a path is added into <code>Open</code>, check if this path is a circle. If it is, then prune the path.  To check whether it is a cycle, we only need to check whether the last added node is already in the path.  path checking is very  quick (\\(O(d)\\)), but cannot prune all cycles in the search tree. </p> <p>cycle checking: Check the last added node with all paths in the open. Only keep the path with least cost.  cycle check is effective, but time and space costly. </p>"},{"location":"csc412/dag.html","title":"Joint distributions with Graphical models","text":""},{"location":"csc412/dag.html#conditional-distributions","title":"Conditional Distributions","text":"<p>For the joint dsitribution of \\(N\\) r.v. By chain rule of probability, (prove by induction, using the definition of conditional probability)</p> \\[\\begin{align*} p(x_1, x_2, ...,x_N) &amp;= p(x_1)p(x_2|x_1)p(x_3|x_2, x_1)...p(x_N|x_{N-1}, ..., x_1) \\\\ &amp;= \\prod_{i=1}^N p(x_i\\mid x_1,..., x_{i-1}) \\end{align*}\\] <p>Note that the equation holds for any orderings. But this decomposition doesn't reduce #params. </p>"},{"location":"csc412/dag.html#conditional-independence","title":"Conditional Independence","text":"<p>Two random variables \\(A, B\\) are conditionally independent given \\(C\\), denoted as \\(X_A\\perp X_B\\mid X_C\\) IFF </p> \\[p(X_A, X_B|X_C) = p(X_A|X_C)p(X_B|X_C)\\] <p>Note that equivalently, </p> \\[P(X_A|X_B, X_C) = p(X_A|X_C), P(X_B|X_A, X_C) = p(X_B|X_C)\\]"},{"location":"csc412/dag.html#dag-models","title":"DAG Models","text":"<p>We can represent the random variables and their independence relationships using a dag \\(G = (V, E)\\) where \\(V\\) represents the random variables, and \\(E\\) represent dependence, then</p> \\[p(X_1,...,X_N) = \\prod_{i=1}^N p(x_i\\mid \\text{parents}(x_i))\\] <p>Note that a dag is a tree and has a topological order, in which we can always find node(s) with no parent. Thus, chain rule of probability still holds, and the dag describes the conditional independence along variables.</p> <p>For example, If we allow all possible conditional dependencies, that corresponds to a fully-connected DAG</p> <p>\u200b </p> <p>And for the graph given below, we have</p> \\[p(x_1, x_2, x_3, x_4, x_5, x_6)=p(x_1)p(x_2|x_1)p(x_3|x_1)p(x_4|x_2)p(x_5|x_3)p(x_6|x_2, x_5)\\] <p>\u200b </p>"},{"location":"csc412/dag.html#directed-separation","title":"Directed Separation","text":"<p>D-separation, or directed-separation is a notion of connectedness in DAGs in which two (sets of) variables may or may not be connected conditioned on a third (set of) variable(s). </p> <p>D-connection implies conditional dependence and d-separation implies conditional independence.</p> <p>Given a graph, we have that \\(X_A\\perp X_B|X_C\\) if every variable in \\(A\\) is d-separated from every variable in \\(B\\) conditioned on all the varaibles in \\(C\\). </p> <p>For a dag, we only need to consider 3 cases as the following </p> <p>\u200b </p>"},{"location":"csc412/dag.html#common-patterns","title":"Common Patterns","text":"<p>Claim 1. Chain implies that \\(X\\perp Z | Y\\). proof. The dag gives that</p> \\[p(x,y,z) = p(x)p(y|x)p(z|y)\\] \\[p(z|x,y) = \\frac{p(x,y,z)}{p(x,y)} = \\frac{ p(x)p(y|x)p(z|y)}{p(x)p(y|x)} = p(z|y)\\] <p>Claim 2. Common cause implies that \\(X\\perp Z|Y\\) proof. The dag gives that</p> \\[p(x,y,z) = p(y)p(x|y)p(z|y)\\] \\[p(x, z|y) = \\frac{p(x,y,z)}{p(y)} = \\frac{ p(y)p(x|y)p(z|y)}{p(y)} = p(x|y)p(z|y)\\] <p>Claim 3. Explaining Away implies that \\(X\\not\\perp Z|Y\\) </p> \\[p(x,y,z) = p(x)p(z)p(y|x,z)\\] \\[p(z|x, y) = \\frac{p(x)p(z)p(y|x,z)}{p(x)p(y|x)} = \\frac{ p(z)p(y|x,z)}{p(y|x)} \\neq p(z|y)\\]"},{"location":"csc412/dag.html#bayes-ball","title":"Bayes Ball","text":"<p>With the three patterns defined above, we have the algorithm to determine whether \\(X\\perp Y|Z\\) in an arbitray graph model using the rules defined below. </p> <p>\u200b </p> <p>First, we shade all \\(Z\\) variables, and then we follow the rules to see whether a path exists between \\(X,Y\\). If exists, then they are dependent. However, implementing such path finding algorithm is complex. An easier implementation arises from the defined rules, as follows. </p> dseparation(G, X, Z)<pre><code># G is the dag, \n# X is a list of vertices as source variables, \n# Z is a list of vertices as the observations\nreduce G to only include variables in X and its ancestors\nfor v in G.V:\n    if v has more than 2 parents:\n        add undirect edges to each pair of parent vertices\n    make all directed edges undirected \n    remove all Z vertices and edges attached to them\n    the connected vertices are independent of each other\n</code></pre> Source code <pre><code>import igraph as ig\nimport matplotlib.pyplot as plt\nPLOT_SETTINGS = dict(\n    vertex_color=\"white\", \n    vertex_label_size=14, edge_color=\"gray\", autocurve=True, margin=(0,0,0,0)\n)\n\nE = []\nfor i in range(1, 6):\n    for j in range(i):\n        E.append((j,i))\ng = ig.Graph(n=6, edges=E, directed=True)\nfig, ax = plt.subplots(figsize=(4, 4)); ax.set_axis_off(); ax.set_title(\"N=6, full model\")\nig.plot(g, target=ax, vertex_label=[1,2,3,4,5,6], layout=g.layout(), **PLOT_SETTINGS)\nfig.tight_layout()\nfig.savefig(\"../assets/dag_1.jpg\")\n\n\ng = ig.Graph(n=6, edges=[(0, 1), (0, 2), (2, 4), (4, 5), (1, 5), (1, 3)], directed=True)\nfig, ax = plt.subplots(figsize=(4, 4)); ax.set_axis_off();\nig.plot(g, target=ax, vertex_label=[1,2,3,4,5,6], \n        layout=g.layout(\"rt\"), **PLOT_SETTINGS)\nfig.tight_layout()\nfig.savefig(\"../assets/dag_2.jpg\")\n\n\ncases = [\"Chain\", \"Common Cause\", \"Explaning Away\"]\nedge_sets = [[(0, 1), (1, 2)], [(1, 0), (1, 2)], [(2, 1), (0, 1)]]\nfig, axs = plt.subplots(1, 3, figsize=(12, 4)); \nfor i, (case, edges) in enumerate(zip(cases, edge_sets)):\n    g = ig.Graph(n=3, edges=edges, directed=True)\n    axs[i].set_axis_off(); axs[i].set_title(case)\n    ig.plot(g, target=axs[i], vertex_label=[\"X\", \"Y\", \"Z\"], \n            layout=g.layout(\"rt\"), **PLOT_SETTINGS)\nfig.tight_layout()\nfig.savefig(\"../assets/dag_3.jpg\")  \n</code></pre>"},{"location":"csc412/inference.html","title":"Exact Inference","text":""},{"location":"csc412/inference.html#exact-inference-as-conditional-distribution","title":"Exact Inference as Conditional Distribution","text":"<p>As one of the tasks on Probabilistic Models.</p> <p>Consider a probabilistic models, where we are given </p> <ul> <li>some observed evidence \\(X_F\\), and </li> <li>some unobserved random variables \\(X_F\\) that we are interested in the distribution </li> <li>other variables \\(X_R\\) that are not observed and not intersted. </li> </ul> <p>Inference is intersted in finding the conditional distribution</p> \\[p(X_F|X_E) = \\frac{p(X_F, X_E)}{\\sum_{X_F}p(X_F, X_E)} = \\frac{\\sum_{X_R}p(X_F, X_E, X_R)}{\\sum_{X_R} \\sum_{X_F}p(X_F, X_E, X_R)}\\] <p>Thus, we need to marginalize all \\(X_R\\), and consider the conditional probability. </p>"},{"location":"csc412/inference.html#variable-elimination","title":"Variable Elimination","text":"<p>Consider the conditional distributino encountered, note that we need to do a huge number of summations. For example, consider a simple chaining of variables \\(A\\rightarrow B\\rightarrow C\\rightarrow D\\) and we are interested in </p> \\[p(D) = \\sum_{A,B,C}p(A,B,C,D) = \\sum_{A,B,C} p(A)p(B|A)p(C|B)p(D|C)\\] <p>If we do the summation naively, it will be</p> <pre><code>p = 0\nfor a in A:\n    for b in B:\n        for c in C:\n            p += p(a) * p(b|a) * p(c|b) * p(d|c)\n</code></pre> <p>Resulting \\(O(k^n)\\) time, where \\(k\\) \\(k\\) is the number of states in each variable and \\(n\\) is the number of variables. </p> <p>On the other hand, we can use dynamic programming by decomposing the triple summations, to do varaible elimination. Obverse that </p> \\[\\begin{align*} p(D) &amp;= \\sum_A\\sum_B\\sum_C p(A)p(B|A)p(C|B)p(D|C)\\\\ &amp;= \\sum_C p(D|C)\\sum_Bp(C|B)\\sum_A p(A)p(B|A)\\\\ &amp;= \\sum_C p(D|C)\\sum_Bp(C|B)p(B)\\\\ &amp;= \\sum_C p(D|C)p(C) \\end{align*}\\] <p>Thus, the runtime is reduced to \\(O(nk^2)\\)</p>"},{"location":"csc412/inference.html#intermediate-factors","title":"Intermediate Factors","text":"<p>Consider the distribution given by </p> \\[P(X,A,B,C) = p(X)p(A|X)p(B|A)p(C|B,X)\\] <p>Suppose that we'd like to marginalize over \\(X\\), so that </p> \\[P(A,B,C) = \\sum_X p(X)p(A|X)p(B|A)p(C|B,X) = p(B|A)\\sum_X p(X)p(A|X)p(C|B,X)\\] <p>However, \\(\\sum_X p(X)p(A|X)p(C|B,X)\\) is not a valid conditional or marginal distribution, since it is unnormalized. </p> <p>Note that the only purpose we write these intermediate distribution is to cache them in dynamic tables for the final computation results. Thus, we don't necessarily need them to be a distribution, until we finish the computations. </p> <p>Additionally, for each conditional distributions \\(P(A|B)\\), it is a function of variables \\(A,B\\). Thus, we introduce factor \\(\\phi\\) which are not necessarily normalized distributions, but describe the local relationship between random variables.</p> <p>In addition, for the summation that we want to temporarily store. We introduce another intermediate factor \\(\\tau\\), for example, we can let \\(\\tau(A,B,C) = \\sum_X p(X)p(A|X)p(C|B,X)\\) so that we have \\(X\\) eliminated. More formally, </p> \\[\\tau(Y) = \\sum_z \\prod_{\\phi\\in \\Phi}(z_{scope(\\phi)\\cap Z}, y_{scope(\\phi)\\cap Y}).\\forall Y\\] <p>where, for dag, \\(\\Phi\\) is given by </p> \\[\\Phi = \\{p(x_i | \\text{parents}(x_i))\\}_{i=1}^N\\]"},{"location":"csc412/inference.html#ve-implementation","title":"VE Implementation","text":"<p>Note that the above VE algorithm is an abstraction. Where we are summing up probability functions for each state. Now, consider an implementation where each variable has finite number of states, and each state \\(p(X=x)\\) is associated with a fixed number so that the probability functions are well-defined.</p> <p>Consider a set of conditional probabilities \\(\\phi\\in\\Phi\\), a set of query variables \\(X_f \\in Q\\), set of evidence variables \\(X_e \\in E\\) with observed values \\(X_e = x_e\\) and a sequence of remaining variables \\(X_r\\in Z\\). </p> VE(Phi, Q, E, R)<pre><code>for each observed variable Xe in E:\n    for each factor phi(..., Xe) that mentioned Xe:\n        replace factor with restricted factor phi(..., Xe=xe)\nfor each Xr in Z:\n    Phi_Xr = the set of factors in Phi that mentioned Xr\n    tau = sum(prod(Phi_Xr))\n    remove Phi_Xr from Phi\n    add tau to Phi\n# all variables are eliminated now\nreturn normalize(prod(Phi))\n</code></pre>"},{"location":"csc412/inference.html#factors","title":"Factors","text":"<p>Each factor \\(\\phi\\) or \\(\\tau\\) is a function that takes a specific state set of scoped variables, and return a positive real number. Thus, they are implemented as a lookup table, where each line is the specific state config, and its associated value. For each table, there are \\(\\prod_{X_i\\in scope(\\phi)} |X_i|\\) states (table rows). </p> <p>For \\(\\phi\\)'s, we directly obtain them from the conditional probability functions at initalization time. For example, we initialize \\(\\phi(A,B) = p(A|B)\\). For \\(\\tau\\), we obtain them from <code>prod</code> and <code>sum</code>. </p> <pre><code>import pandas as pd\nf = pd.DataFrame({\"A\": [0, 0, 1, 1], \"B\": [0, 1, 0, 1], \"value\": [.9, .1, .4, .6]})\nf\n</code></pre> A B value 0 0 0 0.9 1 0 1 0.1 2 1 0 0.4 3 1 1 0.6 <pre><code>g = pd.DataFrame({\"B\": [0, 0, 1, 1], \"C\": [0, 1, 0, 1], \"value\": [.7, .3, .8, .2]})\ng\n</code></pre> B C value 0 0 0 0.7 1 0 1 0.3 2 1 0 0.8 3 1 1 0.2"},{"location":"csc412/inference.html#product","title":"Product","text":"<p><code>prod(f,g)</code> takes two factors (tables) \\(f,g\\) with a scope variable in common, and returns a new factor \\(h\\). </p> <p>We take the inner join of the two factors, and multiply the values for each row. </p> <pre><code>def prod(f, g):\n    f = f.rename(columns={\"value\": \"value_x\"})\n    g = g.rename(columns={\"value\": \"value_y\"})\n    h = f.merge(g)\n    h['value'] = h['value_x'] * h['value_y']\n    h = h.drop(['value_x', 'value_y'], axis=1)\n    return h\nh_prod = prod(f, g)\nh_prod\n</code></pre> A B C value 0 0 0 0 0.63 1 0 0 1 0.27 2 1 0 0 0.28 3 1 0 1 0.12 4 0 1 0 0.08 5 0 1 1 0.02 6 1 1 0 0.48 7 1 1 1 0.12"},{"location":"csc412/inference.html#sum","title":"Sum","text":"<p><code>sum(f, X)</code> takes a factor \\(f\\) and a variable \\(X\\), and returns a new factor by summing up \\(X\\) from \\(f\\). </p> <pre><code>def sum(f, X):\n    f_group = f.groupby(list(set(f.columns) - {X, \"value\"}))[['value']].sum()\n    new_f = f_group.reset_index()\n    return new_f\n\nh_sum = sum(h_prod, \"C\")\nh_sum\n</code></pre> B A value 0 0 0 0.9 1 0 1 0.4 2 1 0 0.1 3 1 1 0.6"},{"location":"csc412/inference.html#restriction","title":"Restriction","text":"<p><code>restrict(f, X, x)</code> takes factor \\(f\\), an evidence variable \\(X\\) and the evidence value \\(x\\), and returns a new factor that only contains rows that \\(X=x\\). </p> <pre><code>def restrict(f, X, x):\n    return f.loc[f[X] == x].drop(X, axis=1)\nh_r = restrict(h_prod, \"C\", 1)\nh_r\n</code></pre> A B value 1 0 0 0.27 3 1 0 0.12 5 0 1 0.02 7 1 1 0.12"},{"location":"csc412/inference.html#implementation","title":"Implementation","text":"VE(Phi, Q, E, R)<pre><code>\"\"\"Variable Elimination\nArgs:\n    Phi: A list of factors as pd.DataFrame\n    Q: A list of str, representing the query variable\n    E: A list of (str, state), representing the evidence var and evidence\n    R: A list of str, given the elimination ordering\n\"\"\"\nfor evar, evidence in E:\n    for i, f in enumerate(Phi):\n        if evar in f.columns:\n            Phi[i] = restrict(f, evar, evidence)\n\nfor var in R:\n    tau = None\n    to_remove = []\n    for i, f in enumerate(Phi):\n        if var in f.columns:\n            tau = prod(f, tau) if tau is not None else f\n            to_remove.append(i)\n    while len(to_remove) &gt; 0:\n        del Phi[to_remove.pop()]\n    if tau is not None:\n        tau = sum(tau, var)\n        Phi.append(tau)\np = Phi[0]\nfor tau in Phi[1:]:\n    p = prod(p, tau)\np['value'] /= p['value'].sum()\nreturn p   \n</code></pre>"},{"location":"csc412/inference.html#ve-ordering-and-message-passing","title":"VE Ordering and Message Passing","text":"<p>Consider a model \\(T=(V,E)\\) be a tree. Let \\(N(i)\\) be the neighbors of vertex \\(i\\). Then, the joint distribution is</p> \\[p(x_1,...,x_n) = \\frac{1}{Z}\\prod_{i\\in V}\\phi(x_i)\\prod_{(i,j)\\in E} \\phi_{ij}(x_i, x_j)\\] <p>where the factors are initialized from given conditional probabilities and \\(Z\\) is the normalizer. </p> <p>Now, define the message passing as </p> \\[m_{j\\rightarrow i}(x_i) = \\sum_{X_j} \\phi_j(x_j) \\phi_{ij}(x_i, x_j)\\prod_{k\\in N(j) - \\{i\\}}m_{k\\rightarrow j}(x_j)\\] <p>If \\(x_j\\) is observed with value \\(\\bar x_j\\), since we will restrict \\(x_j = \\bar x_j\\), the message passing becomes</p> \\[m_{j\\rightarrow i}(x_i) = \\phi_j(\\bar x_j) \\phi_{ij}(x_i, \\bar x_j)\\prod_{k\\in N(j) - \\{i\\}}m_{k\\rightarrow j}(\\bar x_j)\\] <p>Once the message passing is complete, we can compute beliefs </p> \\[b(x_i)\\propto \\phi_i(x_i) \\prod_{j\\in N(i)} m_{j\\rightarrow i}(x_i), p(x_i) = \\frac{1}{Z}b(x_i)\\] <p>In the case of a tree, the leaf will only have its parent being the neighbor. Therefore, if we start message passing from each leaf, and then propagate till the root, we can cache the numerical values of the message passing on each edge, without recomputing any edge. </p> <p>Thus, by the tree property, we have the message passing algorithm</p> <ol> <li>choose any vertex be the root \\(r\\). </li> <li>message passing from all leafs to \\(r\\), and then message passing from \\(r\\) to leafs</li> <li>For each query variable, compute belifs and normalize it</li> </ol>"},{"location":"csc412/inference.html#message-passing-for-ve","title":"Message Passing for VE","text":"<p>Note that the time complexity of VE is  </p> \\[O(mk^{N_{\\max}})\\] <p>where \\(m\\) is the number of initial factors, \\(k\\) is the number of states for each r.v. , \\(N_{\\max}\\) is the max number of random variables inside some summation. Thus, the ordering for VE is important for the running time. </p> <p>Determining the optimal ordering on a arbitrary graph is NP-hard. However, we have optimal orderings on trees, where any elimination ordering that goes from the leaves inwards towards any root will be optimal. </p> <p>If we have a DAGM that is a tree, we can directly eliminate variables from the leaf till the query variables. In this case, we will have optimal runtime and the computation of message passing is actually the same as VE.</p>"},{"location":"csc412/mc.html","title":"Hidden Markov Models","text":"<p>A sequential data is a sequence of random states \\(x_{1:T}\\) where \\(x_{i}\\)'s aren't i.i.d., but depends on (in most cases, previous) states in the sequence. </p> <p>Examples of sequential data includes time-series (stock prices, videos) or any ordered sequence (gene sequences, texts). </p> <p>Of course, this definition gives almost no simplifications over the intractable nature of the joint factorization. Therefore, we need stronger assumptions. </p>"},{"location":"csc412/mc.html#stationary-markov-chain","title":"Stationary Markov Chain","text":"<p>\u200b </p> <p>For a  k-order Markov chain, we assume that for each state \\(x_t\\), it only depends on the previous \\(k\\) states, which means </p> \\[\\forall t \\in \\{1,\\cdots,T\\}. p(x_t|x_{1:t-1}) = p(x_t| x_{t-k:t-1})\\] <p>so that the joint distribution becomes</p> \\[p(x_{1:T}) = \\prod_{t=1}^{T} p(x_t| x_{t-k:t-1})\\] <p>for example, we are interested in first order Markov chains, where each state only depends on its previous state. </p> \\[p(x_t|x_{1:t-1}) = p(x_t| x_{t-1}), p(x_{1:T}) = \\prod_{t=1}^{T} p(x_t| x_{t-1})\\] <p>As an example, we want to forecast the weather tomorrow, if I only look at today's weather, then it's a first-order markov chain. If today and yesterday, then it's second order. </p>"},{"location":"csc412/mc.html#stationary-assumption","title":"Stationary Assumption","text":"<p>Further make the stationary assumption, if the distribution generating the data does not change through time, then it is a stationary/homogenous Markov chain. For first order MC, this means that </p> \\[\\forall t, k. p(x_t|x_{t-1}) = p(x_{t+k}|x_{t-1+k})\\]"},{"location":"csc412/mc.html#parameterizations","title":"Parameterizations","text":"<p>Assume for stationary model, and the hidden r.v. \\(z\\) is discrete, we need to parameterize 1. Initial distribution \\(\\pi(i) = p(z_1 = i)\\). The probability of the first hidden variable being in state \\(i\\).Obviously we need something to start with.  2. Transition distribution \\(T(i,j) = p(x_{t+1}=j|x_t=i)\\), the probability of moving from state \\(i\\) to state \\(j\\). </p> <p>Consider a discrete MC with \\(K\\) states. At each time \\(x_t\\), it falls onto \\(i\\in\\{1,...,K\\}=:\\mathcal S\\). Thus, to parameterize the distribution over \\(X\\), we need to parameterize \\(K-1\\) states since the last states is implicit \\(p(x=K|\\cdots)=1-\\sum_{i=1}^{K-1} p(x=i|\\cdots)\\). Then, with the stationary assumption, for a m-order MC, we need to assign the probability</p> \\[p(x_t = j| x_{t-1} =i_1, x_{t-2}=i_2,...,x_{t-m} = i_m)\\] <p>for each \\(j,i_1,...,i_m\\in \\mathcal S\\).</p> <p>Thus, we have \\((K-1)K^m\\) parameters.</p>"},{"location":"csc412/mc.html#chapman-kolmogorov-equations","title":"Chapman-Kolmogorov Equations","text":"<p>For first-order MC, consider the transition distribution \\(p(x_t = j| x_{t-1} =i)\\), which gives the probability (real number) from state \\(i\\) to state \\(j\\). Therefore, define the transition matrix </p> \\[A: K\\times K. A_{ij} = p(x_t = j| x_{t-1} =i)\\] <p>Now, suppose that we want to estimate \\(x_{t+2}\\) with observed \\(x_t\\) (\\(x_{t+1}\\) is unobserved). </p> \\[\\begin{align*} p(x_{t+2}=j|x_t = i) &amp;= \\sum_{k=1}^{K}  p(x_{t+1}=k|x_t=i) p(x_{t+2}=j|x_{t+1} = k)\\\\ &amp;= \\sum_{k=1}^{K}A_{ik}A_{kj}\\\\ &amp;= A^2_{ij} \\end{align*}\\] <p>Claim (CKE) Define \\(A_{ij}(n):= p(x_{t+n}=j|x_t=i)\\), then \\(A(n) = A(1)^n\\). proof. directly perform induction using the above discovery.</p>"},{"location":"csc412/mc.html#hidden-markov-models_1","title":"Hidden Markov Models","text":"<p>In some cases, the observed states of a random variable is not the sequence itself, but a consequence of some hidden states. HMMs hide the temporal dependence by keeping it in the unobserved state. For each observation \\(x_t\\), we associate a corresponding unobserved hidden/latent variable \\(z_t\\).</p> <p>\u200b </p> <p>The joint distribution of the model is </p> \\[p(x_1:T, z_1:T) = \\prod_{1}^T p(z_t|z_{t-1}) p(x_t|z_t)\\] <p>Unlike simple Markov chains, the observations are not limited by a Markov assumption of any order. I.e. \\(x_t\\) isn't neceessarily independent of any other observation, no matter how many other observations we make.</p>"},{"location":"csc412/mc.html#parameterization","title":"Parameterization","text":"<p>In addition to the initial distribution and transition distribution, we need an additional Emission probability </p> \\[\\epsilon_i(x) = p(x|z_t = i)\\] <p>the probability of an observed r.v. \\(x\\) given the state of hidden varaible that \"emitted\" it. </p> <p>Note that with the stationary assumption, we need \\(k\\) emission probability functions \\(\\epsilon_i(x)\\) for \\(i = 1,2,...,k\\). </p>"},{"location":"csc412/mc.html#forward-backward-algorithm","title":"Forward-backward Algorithm","text":"<p>Given observations \\(x_{1:T}\\), we are interested in estimate the latent sequence \\(z_1:T\\). That's to compute </p> \\[a_t(j):= p(z_t = j|x_{1:T})\\] <p>for each \\(t\\). Assuming that the HMM is fully parameterized with initial \\(\\pi(z_1)\\), transition matrix \\(p(z_t|z_{t-1})\\), and emission \\(\\epsilon(x_t|z_t)\\). </p> <p>Then, we divide the latent sequence into 3 parts</p> <p>Filtering \\(p(z_t|x_{1:T})\\) posterior over current hidden state. Prediction \\(p(z_{t+k}|x_{1:T})\\) posterior over future hidden state.   Prediction \\(p(z_{t-k}|x_{1:T})\\) posterior over past hidden state.   </p> <p>Note that </p> \\[\\begin{align*} p(z_t|x_{1:T}) &amp;\\propto p(z_t, x_{1:T})\\\\ &amp;= p(z_t, x_{1:t})p(x_{t+1:T}|z_t, x_{1:t})\\\\ &amp;= p(z_t, x_{1:t})p(x_{t+1:T}|z_t) &amp;{x_{t+1:T}\\perp x_{1:t}|z}\\\\ &amp;\\propto \\alpha_t\\cdot\\beta_t \\end{align*}\\]"},{"location":"csc412/mc.html#forward-recursion","title":"Forward Recursion","text":"<p>Now consider the forward recursion, we have</p> \\[\\begin{align*} a_t(j) &amp;= p(z_t=j, x_{1:t})\\\\  &amp;= \\sum_{i}p(z_{t-1}=i, z_t=j, x_{1:t})\\\\ &amp;= \\sum_{i}\\big[p(x_t|z_{t-1}=i, z_t = j, x_{1:t-1}) \\\\ &amp;\\quad\\qquad p(z_t=j|z_{t-1}=i, x_{1:t-1})\\\\ &amp;\\quad\\qquad p(z_{t-1}=i, x_{1:t-1})\\big]\\\\ &amp;= p(x_t|z_t=j) \\sum_i p(z_t=j|z_{t-1}=i)a_{t-1}(i)\\\\ &amp;=  A_{ij} \\sum_i \\epsilon_t(j)a_{t-1}(i) \\end{align*}\\] <p>where \\(A_{ij} = p(z_t=j|z_{t-1}=i)\\) is the translation matrix, \\(\\epsilon_t(j) = p(x_t|z_t=j)=\\) is the emission. Thus, we can write it into matrix form </p> \\[a_t \\propto \\epsilon_t \\otimes A^\\top a_{t-1}\\] <p>where \\(a_t: K\\times 1 = [a_t(1), a_t(2), ..., a_t(K)]^\\top, \\epsilon_t = [\\epsilon_t(1), \\epsilon_t(2), ..., \\epsilon_t(K)]^\\top, A:K\\times K, A_{ij} = p(x_t = j| x_{t-1} =i)\\) and \\(\\otimes\\) denotes element-wise multiplication. </p>"},{"location":"csc412/mc.html#backward-recursion","title":"Backward Recursion","text":"<p>Similarly, we have backward recursion</p> \\[\\begin{align*} \\beta_t(i) &amp;= p(x_{t+1:T}|z_t = i)\\\\ &amp;= \\sum_j p(z_{t+1}=j, x_{t+1:T}|z_t = i)\\\\ &amp;= \\sum_j p(x_{t+2:T}|z_{t+1}=j, z_t=i, x_{t+1})\\\\ &amp;\\qquad\\quad p(x_{t+1}|z_{t+1}=j, z_t = i)\\\\ &amp;\\qquad\\quad p(z_{t+1}=j|z_t=i)\\\\ &amp;= \\sum_j \\beta_{t+1}(j) p(x_{t+1}|z_{t+1}=j) p(z_{t+1}=j|z_t=i)\\\\ &amp;= \\sum_j \\beta_{t+1}(j) \\epsilon_{t+1}(j) A_{ij} \\end{align*}\\] <p>and similarly the matrix form </p> \\[\\beta_t = A\\cdot (\\epsilon_{t+1}\\otimes\\beta_{t+1})\\]"},{"location":"csc412/models.html","title":"Probabilistic Models","text":"<p>Given r.v. \\(X = (X_1,..., X_N)\\) that are either observed or unobserved. We want a model that captures the relationship between these variables. The approach of probabilistic generative models is to relate all variables by a learned joint probability distribution \\(p_\\theta(X_1,...,X_N)\\).</p> <p>We make the assumption that the variables are generated by some distribution \\((X_1,..., X_N) \\sim p_*(X)\\). Then, density estimation learns the joint probability distribution by choosing the parameters \\(\\theta\\) of a specified parametric joint distribution \\(p_\\theta(X)\\) that best matches \\(p_*(X)\\). </p> <p>Therefore, we are interested in (e.g. the sense of classical stats course)  - How should we specify \\(p_\\theta\\) (which class of distribution)  - How to measure the \"best match\" (goodness of fit)  - How to find the best \\(\\theta\\) (MLE, MAP, etc.)</p>"},{"location":"csc412/models.html#probabilistic-perspectives-on-ml-tasks","title":"Probabilistic Perspectives on ML Tasks","text":"<p>For a ML task, we are given input data \\(X\\subseteq \\mathcal X\\) and output data \\(Y\\subseteq \\mathcal Y\\) (for continuous) or \\(C\\in\\mathcal C\\) (for discrete classes, or labels). </p> <p>From deep learning (nerual nets) perspective, we want to recover some function \\(f_\\Theta: \\mathcal X\\rightarrow \\mathcal Y\\). And by universal function theorem, we use parameters of the NN as the parameters \\(\\Theta\\), and measure how good our \\(f_\\theta\\) matches the underlying \\(f_*\\) by defining the task specific loss. </p> <p>In the sense of probability theory, instead of dataset, we treat \\(X, Y,C\\) are random variables, and each data point is a sample. Then we have the joint probability over these random variables \\(P(X, Y)\\) or \\(P(X,C)\\), and by Bayesian theorem</p> \\[P(Y|X) = \\frac{p(X,Y)}{p(X)} = \\frac{p(X,Y)}{\\int_\\mathcal Y p(X,Y)dY}\\] \\[P(Y|C) = \\frac{p(X,C)}{p(X)} = \\frac{p(X,C)}{\\sum_\\mathcal C p(X,C)}\\]"},{"location":"csc412/models.html#observed-vs-unobserved","title":"Observed vs. Unobserved","text":"<p>In general supervised vs unsupervised learning, in this probabilistic perspective is given by whether a random variable is observed or unobserved.</p> <p>For supervised dataset \\(\\{x_i, c_i\\}^N \\sim p(X, C)\\), the only thing need to find is the conditional distribution \\(p(C|X)\\), so that for each input \\(x\\), we can pick the label with the maximum conditional probability</p> \\[c_* = \\arg\\max_{c\\in\\mathcal C}\\{p(C=c\\mid X=x)\\}\\] <p>For unsupervised dataset \\(\\{x_i\\}^N \\sim p(X, C)\\), we don't change the generative assumption, the data \\(x_i\\) is still distributed according to a class label \\(C=c_i\\) even though it is unknown in the dataset. </p> <p>Thus, the goal is the same, we want to find the conditional distribution \\(P(C|X)\\).</p> <p>Furthermore, we may have latent variables or hidden variables, that are not observed but has large contribution to the modelling. By introducing latent variables, we will be able to naturally describe and capture abstract features of our input data.</p>"},{"location":"csc412/models.html#tasks-on-probabilistic-models","title":"Tasks on Probabilistic  Models","text":"<p>The fundamental operations we will perform on a probabilistic model are - Generate data or sample new data points from the model - Estimate likelihood marginalizing or observing all variables, and then we get the probability of the all variables taking on those specific values.  - Inference expected value of some variables given others which are either observed or marginalized. - Learning Set the parameters of the joint distribution given some observed data to maximize the probability the observed data.</p>"},{"location":"csc412/models.html#challenges-with-probabilistic-models","title":"Challenges with Probabilistic Models","text":"<ul> <li>The computations are efficient of marginal and conditional distributions. Note that margianlizations often involves integration, which is often computationally intractable. </li> <li>They have compact representation so the size of parameterization scales well for joint distribution over many variables.</li> </ul> <p>Unfortunately, these two challenges are still huge for Probabilistic Models in ML tasks. </p>"},{"location":"csc412/models.html#parameterized-joint-distribution","title":"Parameterized Joint Distribution","text":"<p>Consider the joint distribution of a few (finite) discrete random variables, say 3 random variables \\(X_1 \\in \\{x_{11}, x_{12}, ..., x_{1N_1}\\}, X_2 = \\{x_{21}, x_{22}, ..., x_{2N_2}\\}, X_3 =\\{x_{31}, x_{32}, ..., x_{3N_1}\\}\\) so that \\(N_i = |X_i|\\) be the size of the set. </p> <p>Then define a valid probability distribution so that \\(P(X)\\geq 0, \\sum_x P(X=x) = 1\\), by assigning, or parameterizing each possible combination of states. Thus, for each \\(x_{1i}\\in X_1, x_{2j}\\in X_2, x_{3k}\\in X_3\\), we have that </p> \\[p(X_1 = x_{1i}, X_2 = x_{2j}, X_3 = x_{3k}) = \\theta_{ijk} \\in \\mathbb R^{\\geq 0}\\] <p>Note that there are \\(N_1\\times N_2\\times N_3\\) parameters. </p> <p>With the joint distribution, we can compute marginals such as </p> \\[p(X_1 = x_{1i}, X_2 = x_{2j}) = \\sum_{x\\in X_3} p(X_1 = x_{1i}, X_2 = x_{2j}, X_3=x)\\] <p>or conditional probabilities through Bayesian inference as </p> \\[p(X_1 = x_{1i}, X_2 = x_{2j} | X_3 = x_{3k}) = \\frac{p(X_1 = x_{1i}, X_2 = x_{2j})}{p(X_3 = x_{3k})}\\] <p>where both sides are computed from marginals. </p>"},{"location":"csc412/models.html#dimensionality","title":"Dimensionality","text":"<p>As mentioned, a fully parameterized joint distribution can have \\(\\prod_1^F N_i\\) where \\(F\\) is the number of random variables, which is huge. </p> <p>The primary way we will achieve this is to make assumptions about the independence between variables. Note that if \\(X_1, X_2\\) are independent, then we have that </p> \\[p(X_1, X_2, X_3) = p(X_1)p(X_2)P(X3|X_1, X_2)\\] <p>Thus the number of parameters is only \\((N_1 + N_2)N_3\\). </p> <p>For random variables \\(X_1,...,X_F\\), if all of them are mutually independent, then \\(p(X_1,..., X_F) = \\prod_1^F p(X_i)\\), and we only need \\(\\sum_1^F N_i\\) parameters. </p>"},{"location":"csc412/models.html#likelihood-function","title":"Likelihood Function","text":"<p>For a given set of parameters describing the distribution, we have focused on the density function \\(p(x|\\theta)\\). However, our goal is to optimize (non-fixed) \\(\\theta\\), from observations \\(\\mathcal D\\) (fixed).</p> <p>Thus, define the likelihood function \\(\\mathcal L(\\theta; x) = p(x|\\theta)\\) and log-likelihood function \\(l(\\theta; x) = \\log(\\mathcal L(\\theta; x))\\)</p> <p>Suppose that each observation \\(x^{(m)}\\) in the dataset is i.i.d. then, </p> \\[\\mathcal L(\\theta; \\mathcal D) = p( \\mathcal D|\\theta) = \\prod_m p(x^{(m)}|\\theta)\\] \\[l(\\theta; \\mathcal D) = \\log(p( \\mathcal D|\\theta)) = \\sum_m \\log(p(x^{(m)}|\\theta))\\] <p>Note that the goal for likelihood function is to optimize for \\(\\theta\\), for example Maximum likelihood estimation (MLE)</p> \\[\\hat\\theta = \\arg\\max_\\theta(\\mathcal L(\\theta; \\mathcal D)) = \\arg\\max_\\theta(\\mathcal l(\\theta; \\mathcal D))\\] <p>Thus, the log-likelihood, by applying the sum of logs, is much more computational tractable. </p>"},{"location":"csc412/models.html#sufficient-statistics","title":"Sufficient Statistics","text":"<p>A statistic is a (possibly vector valued) deterministic function of a (set of) random variable(s).</p> <p>A sufficient statistic is a statistic that conveys exactly the same information about the generative distribution as the entire data itself. </p> <p>A statitic \\(T(X)\\) is sufficient for \\(X\\) if </p> \\[T(x^{(1)}) = T(x^{(2)})\\implies \\forall \\theta\\in\\Theta. L(\\theta; x^{(1)}) = L(\\theta; x^{(2)})\\] <p>equivalently, </p> \\[p(\\theta | T(X)) = p(\\theta | X)\\]"},{"location":"csc412/models.html#neyman-factorization-theorem","title":"Neyman Factorization Theorem","text":"<p>If \\(T\\) is a sufficient statistic of \\(X\\), then exists \\(f, g\\) s.t. </p> \\[p(\\theta | T(X)) = p(\\theta | X) = h(x, T(x)) g(T(x), \\theta)\\] <p>which means we can decompose the conditional probability into a function of only \\(x\\) and \\(T\\), and a function of only \\(T\\) and \\(\\theta\\). </p> <p>For example, the normal distribution is given by </p> \\[p(x|\\mu, \\sigma) = \\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp(-\\frac{1}{2\\sigma^2}(x-\\mu)^2)\\] <p>Then, let \\(T(x) = [x, x^2]^T\\), we have that </p> \\[\\begin{align*} p(x|\\mu, \\sigma) &amp;= \\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp(-\\frac{1}{2\\sigma^2}(x-\\mu)^2)\\\\ &amp;= \\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp([\\frac{u}{\\sigma^2}, \\frac{-1}{2\\sigma^2}] \\cdot [x, x^2])\\\\ \\end{align*}\\] <p>where \\(h(x, T(x)) =  \\frac{1}{\\sqrt{2\\pi}\\sigma}, g(T(x), \\theta) = \\exp([\\frac{u}{\\sigma^2}, \\frac{-1}{2\\sigma^2}] \\cdot T(x))\\)</p>"},{"location":"csc412/sampling.html","title":"Sampling","text":"<p>A sample from a distribution \\(p(x)\\) is a single realization \\(x\\) whose probability distribution is \\(p(x)\\). Here, \\(x\\) can be high-dimensional or simply real valued.</p> <p>The main objectives of sampling methods is to </p> <ul> <li>Generate random samples \\(\\{x^{(r)}\\}^N_{r=1}\\) from a given probability distribution \\(p(x)\\). </li> <li> <p>To estimate expectations of functions \\(\\phi(x)\\), under the distribution \\(p(x)\\)</p> \\[\\Phi = E_{x\\sim p(x)} [\\phi(x)] = \\int \\phi(x)p(x)dx\\] </li> </ul> <p>For example, we are interested in the mean of some function \\(f\\), under distribution \\(p(x)\\). Then we have </p> \\[\\Phi = E_{x\\sim p(x)} [f(x)] \\]"},{"location":"csc412/sampling.html#simple-monte-carlo","title":"Simple Monte Carlo","text":"<p>For the expectation \\(\\Phi = E_{x\\sim p(x)} [\\phi(x)] = \\int \\phi(x)p(x)dx\\), we can estimate the integral by Monte Carlo integration, i.e. generate \\(R\\) samples \\(\\{x^{(r)}\\}_{r=1}^R\\) from \\(p(x)\\), and taking average</p> \\[\\Phi = E_{x\\sim p(x)} [\\phi(x)] = \\int \\phi(x)p(x)dx = R^{-1}\\sum_{r=1}^R \\phi(x^{(r)}) := \\hat\\Phi\\]"},{"location":"csc412/sampling.html#properties-of-simple-monte-carlo","title":"Properties of Simple Monte Carlo","text":"<p>Claim 1 \\(\\hat\\Phi\\) is a consistent estimator of \\(\\Phi\\). </p> <p>proof. Directly from LLN. </p> <p>Claim 2 \\(\\hat\\Phi\\) is a unbiased estimator of \\(\\Phi\\). proof. </p> \\[\\begin{align*} E(\\hat \\Phi) &amp;= R^{-1}\\sum_{r=1}^R E(\\phi(x^{(r)}))\\\\ &amp;= \\frac{R}{R} E_{x\\sim p(x)}(\\phi(x))\\\\ &amp;= \\Phi \\end{align*}\\] <p>Claim 3 The variance of \\(\\hat\\Phi\\) decreases with rate \\(1/R\\).  </p> <p>proof. By consistency and unbiaseness</p> \\[var(\\hat\\Phi) = \\frac{1}{R^2}\\sum_{r=1}^R var(\\phi(x^{(r)})) = R^{-1}var(\\phi(x))\\]"},{"location":"csc412/sampling.html#normalizing-constant","title":"Normalizing Constant","text":"<p>Given an arbitrary continuous, positive function \\(f: \\mathbb R^n \\rightarrow \\mathbb R\\) and the function is integrable over \\(\\mathbb R^n\\). Say, \\(\\int_{\\mathbb R^n} f(\\mathbf x)d\\mathbf x=Z\\). Then, we can have a density </p> \\[p(\\mathbf x) = \\frac{f(\\mathbf x)}{Z}\\] <p>However, the normalizer \\(Z\\), in many cases, requires computing a high-dim integral, which is computationally intractable (exponential to the dimension). Also, drawing samples from \\(p(\\mathbf x)\\) is a challenge, especially in high-dim spaces. </p>"},{"location":"csc412/sampling.html#importance-sampling","title":"Importance Sampling","text":"<p>Importance sampling is a method for estimating the expectation of a function \\(\\phi\\). </p> <p>Suppose that we wish to draw samples from \\(\\tilde p(x)\\) by</p> \\[p(x)=\\frac{\\tilde p(x)}{Z_p}\\] <p>And we have a simpler density \\(q(x)\\) which is easy to sample from and easy to evaluate up to normalizing constant </p> \\[q(x) = \\frac{\\tilde q(x)}{Z_q}\\] <p>In importance sampling,  we first generate \\(R\\) samples from \\(q(x)\\). </p> \\[\\{x^{(r)}\\}_1^R \\sim q(x)\\] <p>Then we have an estimate of \\(\\phi\\) over density \\(q(x)\\) as</p> \\[\\Phi = E_{x\\sim q(x)}[\\phi(x)] = R^{-1}\\sum_{r=1}^R \\phi(x^{(r)}):=\\hat\\Phi\\] <p>The only problem is that the this is an estimation over \\(q\\). However, notice that at values of \\(x\\), we can represents \\(\\tilde p\\) with a weights function \\(\\tilde p(x) = \\tilde w(x)\\tilde q(x)\\), since we know \\(\\tilde p(x), \\tilde q(x)\\) over their domain. </p> <p>Then, note that for our sampled points we have \\(\\tilde p(x^{(r)}) = \\tilde w(x^{(r)})\\tilde q(x^{(r)})\\), which </p> \\[R^{-1}\\sum_{r=1}^R \\tilde w(x^{(r)}) = E_{x\\sim q(x)}[\\frac{\\tilde p(x^{(r)}}{\\tilde q(x^{(r)})}] = \\int \\frac{\\tilde p(x^{(r)}}{\\tilde q(x^{(r)})}q(x)dx = \\frac{Z_p}{Z_q}\\] <p>and thus for our estimator under \\(p\\) from estimator under \\(q\\) is</p> \\[\\begin{align*} \\Phi &amp;= \\int\\phi(x)p(x)dx\\\\  &amp;= \\int \\phi(x)w(x)p(x)dx \\\\ &amp;\\approx R^{-1}\\sum_{r=1}^R \\phi(x^{(r)})w(x^{(r)})\\\\ &amp;= \\approx R^{-1}\\sum_{r=1}^R \\phi(x^{(r)})\\frac{\\tilde p(x^{(r)})/Z_p}{{\\tilde q(x^{(r)})/Z_q}}\\\\ &amp;= \\frac{Z_q}{Z_p}R^{-1} \\sum_{r=1}^R\\phi(x^{(r)})\\tilde w(x^{(r)})\\\\ &amp;\\approx (R^{-1}\\sum_{r=1}^R \\tilde w(x^{(r)}))^{-1}R^{-1}\\sum_{r=1}^R\\phi(x^{(r)})\\tilde w(x^{(r)})\\\\ &amp;=\\sum_{r=1}^R\\phi(x^{(r)})\\frac{\\tilde w(x^{(r)})}{\\sum_{r=1}^R \\tilde w(x^{(r)})}=:\\hat\\Phi_{iw} \\end{align*}\\]"},{"location":"csc412/sampling.html#rejection-sampling","title":"Rejection Sampling","text":"<p>Another sampling method is rejection sampling. For a given \\(\\tilde p(x)\\), we find a simpler proposal density \\(q(x)\\) and which \\(\\tilde q(x) = Z_q q(x)\\). </p> <p>Then, we further assume that we have some constant \\(c_0\\) s.t. \\(c_0\\tilde q(x) &gt;\\tilde p(x). \\forall x\\in\\mathcal S\\). The idea is that we have a simpler density \\(q\\), such that the scaled \\(\\tilde q\\) is above to cage (over-estimate) \\(p\\) for all input \\(x\\), so that we can reject part of the samples. </p> <p>First, we generate a sample \\(x ~ q(x)\\) and \\(u\\sim \\text{Uniform}[0, c\\tilde q(x)]\\). Then, if \\(u &gt; \\tilde p(x)\\), then \\(x\\) is outside of \\(\\tilde p\\) so we reject such \\(x\\). Otherwise, we accept \\(x\\) into \\(\\{x^{(r)}\\}\\). </p> <p>Claim rejection sampling samples \\(x\\sim p(x)\\). </p> <p>proof. Consider our sampling method, we have \\(x\\sim q(x), u|x \\sim \\text{Uniform}[0, c\\tilde q(x)]\\), and \\(x\\) is accepted is conditional on \\(u \\leq \\tilde p(x)\\). Thus, consider the probability over any set \\(A\\subseteq \\mathcal S\\).  First note that the probability</p> \\[P_{x\\sim p}(x\\in A) = \\int_A p(x)dx = \\int\\mathbb I(x\\in A)p(x)dx = E_{x\\sim p}[\\mathbb{I}(x\\in A)]\\] <p>Thus,</p> \\[\\begin{align*} P_{x\\sim p}(x\\in A \\mid u\\leq \\tilde p(x)) &amp;= \\frac{p_{x\\sim p}(x\\in A, u\\leq \\tilde p(x))}{E_{x\\sim q}p(u\\leq \\tilde p(x) | x)}\\\\ &amp;= \\frac{ E_{x\\sim q}[\\mathbb I(x\\in A) P(u\\leq \\tilde p(x)|x)]}{E_{x\\sim q}[\\frac{\\tilde p(x)}{c\\tilde q(x)}]}\\\\ &amp;= \\frac{E_{x\\sim q}[\\mathbb I(x\\in A)\\frac{\\tilde p(x)}{c\\tilde q(x)}]}{Z_p/cZ_q}\\\\ &amp;= P_{x\\sim p}(x\\in A)\\frac{Z_p}{cZ_q} / \\frac{Z_p}{cZ_q}\\\\ &amp;= P_{x\\sim p}(x\\in A) \\end{align*}\\]"},{"location":"csc412/sampling.html#curse-of-dimension","title":"Curse of Dimension","text":"<p>Note that in high dimensions, a caging over some function will be very hard. Therefore, \\(c\\) will be huge and the acceptance rate \\(\\frac{Z_p}{cZ_q}\\) will be exponentially reduced with increased dimensionality. </p>"},{"location":"csc412/sampling.html#metropolis-hastings-algorithm","title":"Metropolis-Hastings Algorithm","text":"<p>Importance sampling and rejection sampling work well only if the \\(q\\sim p\\). However, such \\(q\\) is very hard to find in high dimensions. Instead, we could make use of a proposal density \\(q\\) which depends on the current state \\(x^{(t)}\\). </p> <p>Given some function \\(\\tilde p(x)\\) and a proposal conditional density \\(q(x_t|x_{t-1})\\). The procedure is</p> <ol> <li>Generate a new state \\(x'\\) from the proposal density \\(q(x'|x^{(t)})\\). </li> <li>Compute acceptance probability</li> </ol> \\[a = \\min(1, \\frac{\\tilde p(x')q(x^{(t)}|x')}{\\tilde p(x^{(t)})q(x'|x^{(t)})})\\] <p>the new state \\(x'\\) is accepted with probability \\(a\\) 3. If accepted, then \\(x^{(t+1)} = x'\\), otherwise \\(x^{(t+1)} = x^{(t)}\\)</p>"},{"location":"csc412/vae.html","title":"Variational Autoencoders","text":"<p>Dimension Reduction and Autoencoder Basics</p>"},{"location":"csc412/vae.html#autoencoders","title":"Autoencoders","text":"<p>As a recap, an encoder-decoder is a fair of function (often non-linear functions implmented as neural networks). </p> <ul> <li>Encoder \\(g: D\\rightarrow F\\)</li> <li>Decoder \\(f: F\\rightarrow D\\)</li> </ul> <p>where \\(D\\) is the  input space, \\(F\\) is the feature vector, where \\(\\dim(F)\\ll \\dim(D)\\). </p> <p>The goal of an atuoencoder is that for any input </p> \\[\\forall x \\in D. \\tilde x = f(g(x)) \\approx x\\] <p>Therefore, we could compress input data \\(x\\) into a lower dimension features, for storage, comparisons, visualizations, etc. </p>"},{"location":"csc412/vae.html#proximity-issue","title":"Proximity Issue","text":"<p>Note that \\(f,g\\) are non-linear, and decoder can not an inverse function of \\(f\\neq g^{-1}\\). Thus, small change in the input space does not corresponds to small change in the feature space. In fact, \\(f\\) and \\(g\\) can acts as some hashing functions that prevents proximity. </p> <p>In addition, if the data space has discontinuities, for example, several clusters with remote means and small variance. Then, if we sample from the \"vacuum\" in the input space, then the feature can be totally random. </p>"},{"location":"csc412/vae.html#variational-autoencoders_1","title":"Variational Autoencoders","text":"<p>VAEs solve proximity issue by introducing uncertainty. By adding noise, VAEs force the encoder to output more uniformly continuous results. The encoder outputs a distribution \\(q_\\theta(z|x)\\) instead of a deterministic feature vector. Normally, we will use Gaussian distribution for \\(q\\), so that encoder outputs \\((\\mu, \\sigma) = f(x)\\), in practice \\(\\Sigma\\) is the diagonal matrix. </p> <p>The idea of VAE comes from variational inference. Let the feature vector be the laten variable, and input data be the observations, the autoencoder aims to recover the posterior \\(p(z|x)\\) from likelihood and prior. And we use \\(q_\\theta\\) to approximate such \\(p\\) due to computational limits. For optimization / training VAEs, we use ELBO as its loss. Thus, we have </p> <ul> <li>Encoder \\(q_{\\phi_i}(z|x_i) = \\mathcal N(\\mu_i, \\sigma_i^2)\\) where we store the mean \\(\\mu_i\\) and log std \\(\\sigma_i\\) for each input. </li> <li>Decoder \\(f(z_i) = \\tilde \\theta\\), typically a neural network that output parameters for a class of distribution. </li> </ul>"},{"location":"csc412/vae.html#pipeline","title":"Pipeline","text":"<p>For each given input \\(x_i\\), we have the forward path as</p> <ol> <li>The encoder NN \\(g\\) output \\(\\phi_i = g(x_i)\\).</li> <li>Sample latent vector \\(z_i \\sim q_{\\phi_i}(z|x_i)\\).</li> <li>The decoder NN \\(f\\) output \\(\\theta = f(z_i)\\).</li> <li>Sample decoded sample \\(\\hat x_i = p_{\\theta}(x|z)\\)</li> </ol> <p>Consider the backward path, we have two ELBOs for \\(p_\\theta\\) and \\(q_\\phi\\) be the losses, note that ELBO loss requires the reparameterization tricks, similar to VI.</p>"},{"location":"csc412/vae.html#amortized-vae","title":"Amortized VAE","text":"<p>Instead of doing VI from scratch for each \\(x_i\\), we learn a function that can look at the dataset all together. Therefore, instead of learning separate parameters \\(\\phi_i\\) for each input, we learn a single global distribution \\(\\phi\\) that specifies the parameters of the recognition model, and \\(\\phi\\) is the parameter sets we want to store. </p> <p>The basic for AVAE pipeline is very similar to VAE, except that we now have a high-dimensional model. Thus, we sample from </p> \\[z_i \\sim q_{\\phi}(z|x_i) = \\mathcal N(\\mu_\\phi(x_i), \\Sigma_\\phi(x_i))\\]"},{"location":"csc412/vae.html#implementation-example-mnist","title":"Implementation Example: MNIST","text":"<p>Using the classical MNIST example</p> <ul> <li>Input data: \\(28 * 28\\) pixels of value \\([0,1]\\), representing grayscale.</li> <li>Likelihood function \\(p_\\theta(x|z) = \\text{Bernoulli}(\\theta)\\)</li> <li>Approximate posterior \\(q_{\\phi_i}(z|x_i) = \\mathcal N(\\mu_i, \\sigma_i)\\)</li> <li>Loss be the ELBO loss + Reconstruction loss</li> </ul> <p><pre><code> class VAE(nn.Module):\n     def __init__(self, x_dim, h_dim1, h_dim2, z_dim):\n         super(VAE, self).__init__()\n\n         # encoder part\n         self.fc1 = nn.Linear(x_dim, h_dim1)\n         self.fc2 = nn.Linear(h_dim1, h_dim2)\n         self.fc31 = nn.Linear(h_dim2, z_dim)\n         self.fc32 = nn.Linear(h_dim2, z_dim)\n         # decoder part\n         self.fc4 = nn.Linear(z_dim, h_dim2)\n         self.fc5 = nn.Linear(h_dim2, h_dim1)\n         self.fc6 = nn.Linear(h_dim1, x_dim)\n\n     def encoder(self, x):\n         h = F.relu(self.fc1(x))\n         h = F.relu(self.fc2(h))\n         return self.fc31(h), self.fc32(h) # mu, log_var\n\n     def sampling(self, mu, log_var):\n         std = torch.exp(0.5*log_var)\n         eps = torch.randn_like(std)\n         return eps.mul(std).add_(mu) # return z sample\n\n     def decoder(self, z):\n         h = F.relu(self.fc4(z))\n         h = F.relu(self.fc5(h))\n         return torch.sigmoid(self.fc6(h)) \n\n     def forward(self, x):\n         mu, log_var = self.encoder(x.view(-1, 784))\n         z = self.sampling(mu, log_var)\n         return self.decoder(z), mu, log_var\n\n def ELBO_loss(recon_x, x, mu, log_var):\n     # ~ Bernoulli ELBO\n     BCE = F.binary_cross_entropy(recon_x, x.view(-1, 784), reduction='sum')\n     # Normal ELBO\n     KLD = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n     return BCE + KLD\n\n def train(model, train_loader, optimizer):\n     model.train()\n     train_loss = 0\n     for data, _ in train_loader:\n         data = data.cuda()\n         optimizer.zero_grad()\n         recon_batch, mu, log_var = model(data)\n         loss = ELBO_loss(recon_batch, data, mu, log_var)\n         loss.backward()\n         train_loss += loss.item()\n         optimizer.step()\n     return train_loss / len(train_loader.dataset)\n\n def test(model, test_loader):\n     model.eval()\n     test_loss= 0\n     with torch.no_grad():\n         for data, _ in test_loader:\n             data = data.cuda()\n             recon, mu, log_var = model(data)\n             # sum up batch loss\n             test_loss += ELBO_loss(recon, data, mu, log_var).item()\n     return test_loss / len(test_loader.dataset)\n</code></pre> The training process</p> <p></p> <p>The reconstruction</p> <p></p> <p>Random samples from the hidden vector code</p> <p></p> Source code <pre><code>import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchvision import datasets, transforms\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\n# --8&lt;-- [start:vae]\nclass VAE(nn.Module):\n    def __init__(self, x_dim, h_dim1, h_dim2, z_dim):\n        super(VAE, self).__init__()\n\n        # encoder part\n        self.fc1 = nn.Linear(x_dim, h_dim1)\n        self.fc2 = nn.Linear(h_dim1, h_dim2)\n        self.fc31 = nn.Linear(h_dim2, z_dim)\n        self.fc32 = nn.Linear(h_dim2, z_dim)\n        # decoder part\n        self.fc4 = nn.Linear(z_dim, h_dim2)\n        self.fc5 = nn.Linear(h_dim2, h_dim1)\n        self.fc6 = nn.Linear(h_dim1, x_dim)\n\n    def encoder(self, x):\n        h = F.relu(self.fc1(x))\n        h = F.relu(self.fc2(h))\n        return self.fc31(h), self.fc32(h) # mu, log_var\n\n    def sampling(self, mu, log_var):\n        std = torch.exp(0.5*log_var)\n        eps = torch.randn_like(std)\n        return eps.mul(std).add_(mu) # return z sample\n\n    def decoder(self, z):\n        h = F.relu(self.fc4(z))\n        h = F.relu(self.fc5(h))\n        return torch.sigmoid(self.fc6(h)) \n\n    def forward(self, x):\n        mu, log_var = self.encoder(x.view(-1, 784))\n        z = self.sampling(mu, log_var)\n        return self.decoder(z), mu, log_var\n\ndef ELBO_loss(recon_x, x, mu, log_var):\n    # ~ Bernoulli ELBO\n    BCE = F.binary_cross_entropy(recon_x, x.view(-1, 784), reduction='sum')\n    # Normal ELBO\n    KLD = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n    return BCE + KLD\n\ndef train(model, train_loader, optimizer):\n    model.train()\n    train_loss = 0\n    for data, _ in train_loader:\n        data = data.cuda()\n        optimizer.zero_grad()\n        recon_batch, mu, log_var = model(data)\n        loss = ELBO_loss(recon_batch, data, mu, log_var)\n        loss.backward()\n        train_loss += loss.item()\n        optimizer.step()\n    return train_loss / len(train_loader.dataset)\n\ndef test(model, test_loader):\n    model.eval()\n    test_loss= 0\n    with torch.no_grad():\n        for data, _ in test_loader:\n            data = data.cuda()\n            recon, mu, log_var = model(data)\n            # sum up batch loss\n            test_loss += ELBO_loss(recon, data, mu, log_var).item()\n    return test_loss / len(test_loader.dataset)\n# --8&lt;-- [end:vae]\n\n# MNIST Dataset\ntrain_dataset = datasets.MNIST(root='./mnist_data/', train=True, transform=transforms.ToTensor(), download=True)\ntest_dataset = datasets.MNIST(root='./mnist_data/', train=False, transform=transforms.ToTensor(), download=False)\n# Data Loader (Input Pipeline)\ntrain_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=100, shuffle=True)\ntest_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=1000, shuffle=False)\n\n# build model\nvae = VAE(x_dim=784, h_dim1= 512, h_dim2=256, z_dim=2).cuda()\noptimizer = torch.optim.Adam(vae.parameters())\nnepoch = 40\ntrain_loss = np.empty(nepoch)\ntest_loss = np.empty(nepoch)\nfrom tqdm import tqdm\nfor epoch in tqdm(range(nepoch)):\n    train_loss[epoch] = train(vae, train_loader, optimizer)\n    test_loss[epoch] = test(vae, test_loader)\n\nplt.figure(figsize=(6,4))\nplt.title(\"Loss\")\nplt.plot(train_loss, label=\"train loss\")\nplt.plot(test_loss, label=\"test loss\")\nplt.savefig(\"../assets/vae_loss.png\")\n\n\nfig, axs = plt.subplots(2, 4, figsize=(16, 8))\nfrom random import sample\nwith torch.no_grad():\n  for i, idx in enumerate(sample(range(len(test_dataset)), 4)):\n    input = test_dataset[idx][0]\n    axs[0, i].imshow(input.view(28, 28).numpy(), cmap=\"gray\")\n    input = input.cuda()\n    recon, mu, log_var  = vae(input)\n    axs[1, i].imshow(recon.view(28, 28).detach().cpu().numpy(), cmap=\"gray\")\n    axs[1, i].set_axis_off(); axs[0, i].set_axis_off()\nfig.savefig(\"../assets/vae_recons.png\")\n\n\nfig, axs = plt.subplots(2, 4, figsize=(16, 8.4))\nwith torch.no_grad():\n  for i in range(8):\n    z = torch.randn(2)\n    axs[i%2, i//2].set_title(f\"mu={z[0]:.6f}, sigma={z[1]:.6f}\")\n    z = z.cuda()\n    sample = vae.decoder(z).cuda()\n    axs[i%2, i//2].imshow(sample.view(28, 28).detach().cpu().numpy(), cmap=\"gray\")\n    axs[i%2, i//2].set_axis_off()\nfig.savefig(\"../assets/vae_random.png\")\n</code></pre>"},{"location":"csc412/vi.html","title":"Variational Inference","text":"<p>Consider the probabilistic model \\(p(X,Z)\\) where \\(x_{1:T}\\) are observations and \\(z_{1:N}\\) are unobserved latent variables. </p> <p>The conditional distribution we are interested in, or the posterior inference is</p> \\[\\text{posterior} := p_\\theta(z|x) = \\frac{p(x|z)p(z)}{p(x)} = \\frac{p(x|z)p(z)}{\\int p(x,z) dz} =: \\frac{\\text{likelihood} \\cdot \\text{prior}}{\\text{marginal}}\\] <p>At suggested by the integral, this computation is intractable. Thus, we need to estimate the posterior using approximate inference. Thus, we need </p> <ul> <li>some function family \\(q_\\phi(z)\\) with parameter \\(\\phi\\). </li> <li>For example, the normal distribution family, where \\(\\phi = (\\vec \\mu, \\Sigma)\\)</li> <li>some distance measurement between \\(q_\\phi, p_\\theta\\). </li> <li>optimization on the distance to get the best \\(\\phi\\). </li> </ul>"},{"location":"csc412/vi.html#kullback-leibler-divergence-kl-divergence","title":"Kullback-Leibler Divergence (KL Divergence)","text":"<p>Given the joint distribution \\(p(X) = \\frac{1}{Z}\\tilde p(X)\\), we find an approximation function \\(q_\\phi(X)\\) from a class of distribution functions, where \\(\\phi\\) is the parameter. Then, adjust \\(\\phi\\) so that \\(p\\sim q\\). That is </p> \\[\\bar E_{x\\sim p}[(f(x))] \\approx \\bar E_{x\\sim q}[(f(x))]\\] <p>Define the KL divergence be </p> \\[D_{KL}(q\\parallel p) = E_{x\\sim q}\\log(\\frac{q(x)}{p(x)}) = \\sum_{\\hat x} q(\\hat x)\\log\\frac{q(\\hat x)}{p(\\hat x)}\\]"},{"location":"csc412/vi.html#properies","title":"Properies","text":"<p>Claim 1 \\(\\forall p, q\\) be discrete density functions, \\(D_{KL}(q\\parallel p) \\geq 0\\); and \\(D_{KL}(q\\parallel p) = 0\\) IFF \\(q=p\\).</p> <p>proof. Consider \\(\\sum_{\\hat x} q(\\hat x)\\log\\frac{q(\\hat x)}{p(\\hat x)}\\) where \\(p,q\\) are density functions, since we only evaluate on the data samples, it is discrete. Consider \\(\\hat x\\) where \\(q(\\hat x) &gt; 0\\) and denote each of such \\(q(\\hat x), p(\\hat x)\\) as \\(q_i, p_i\\) for simpler notation, then</p> \\[\\begin{align*} \\sum_i q_i\\log\\frac{q_i}{p_i} &amp;= \\sum_i q_i(-\\log\\frac{p_i}{q_i})\\\\ &amp;\\geq -\\sum_i q_i (\\frac{p_i}{q_i}-1) &amp;\\forall x &gt; 0, \\log x\\leq x-1\\\\ &amp;=\\sum_i q_i - \\sum_i p_i\\\\ &amp;=1-\\sum_i p_i\\\\ &amp;\\geq 0 \\end{align*}\\] <p>For equality, we use the fact that \\(\\log 1 = 0\\) for all \\(\\hat x\\) s.t. \\(p(\\hat x) &gt; 0\\), and in the other case we have \\(p(\\hat x) = 0 \\implies q(\\hat x) = 0\\). </p> <p>Claim 2 Generally, \\(D_{KL}(q\\parallel p) \\neq D_{KL}(p\\parallel q)\\). </p> <p>proof. Quite obvious, since log function is non-linear. </p>"},{"location":"csc412/vi.html#information-projection-vs-moment-projection","title":"Information Projection vs. Moment Projection","text":"<p>Since \\(D_{KL}(q\\parallel p) \\neq D_{KL}(p\\parallel q)\\), we have two different measurement. where </p> <p>Information Projection optimizes on \\(D_{KL}(q\\parallel p)\\) Moment Projection optimizes on \\(D_{KL}(p\\parallel q)\\) </p> <p>First note that when \\(p\\approx q\\), \\(\\log(q/p) \\approx \\log(p/q) \\approx \\log 1 = 0\\) thus both projection have small values. However, consider the shape of \\(\\log(a/b)\\), when the denominator is small, it will apply a much larger penalty. Thus, the choice of projection depends on the desired properties of wanted \\(q\\). </p> KL divergence<pre><code>def KL_divergence(p, q):\n    p = p.flatten()\n    q = q.flatten()\n    idx = (p != 0) &amp; (q != 0)\n    p = p[idx]\n    q = q[idx]\n    return np.sum(p * np.log(p / q))\n</code></pre> Source code <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\n\n# --8&lt;-- [start:kl]\ndef KL_divergence(p, q):\n    p = p.flatten()\n    q = q.flatten()\n    idx = (p != 0) &amp; (q != 0)\n    p = p[idx]\n    q = q[idx]\n    return np.sum(p * np.log(p / q))\n# --8&lt;-- [end:kl]\n\n\nfrom scipy.stats import multivariate_normal\n\n\nx, y = np.meshgrid(np.linspace(-2, 4, 100), np.linspace(-2, 4, 100))\npos= np.dstack((x, y))\nNormal_01 = multivariate_normal([0, 0], np.identity(2))\nNormal_21 = multivariate_normal([2, 2], np.identity(2))\nNormal_12 = multivariate_normal([1, 1], [[1, .5], [.5, 1]])\n\npi = Normal_01.pdf(pos) + Normal_21.pdf(pos); \nqi = Normal_12.pdf(pos); \npi /= pi.sum() # p ~ N([0, 0], I) + N([2, 2], I)\nqi /= qi.sum() # q ~ N([1, 1], [[1, 0.5], [0.5, 1]])\nplt.figure(figsize=(12, 6))\nplt.subplot(121)\nplt.contour(x, y, pi, cmap=\"Reds\"); plt.contour(x, y, qi, cmap=\"Blues\");\nplt.title(f\"KL(q, p) = {KL_divergence(qi, pi):.4f}\\n\" + \n          f\"KL(p, q) = {KL_divergence(pi, qi):.4f}\")\nplt.axis(\"off\")\nplt.gca().set_aspect(\"equal\")\nplt.tight_layout()\n\npi = Normal_01.pdf(pos) + Normal_21.pdf(pos);\nqi = Normal_21.pdf(pos);\npi /= pi.sum() # p ~ N([0, 0], I) + N([2, 2], I)\nqi /= qi.sum() # q ~ N([2, 2], I)\nplt.subplot(122)\nplt.contour(x, y, pi, cmap=\"Reds\"); plt.contour(x, y, qi, cmap=\"Blues\");\nplt.title(f\"KL(q, p) = {KL_divergence(qi, pi):.4f}\\n\" + \n          f\"KL(p, q) = {KL_divergence(pi, qi):.4f}\")\nplt.axis(\"off\")\nplt.gca().set_aspect(\"equal\")\nplt.tight_layout()\nplt.savefig(\"../assets/vi_1.jpg\")\n</code></pre> <p></p>"},{"location":"csc412/vi.html#evidence-lower-bound-elbo","title":"Evidence Lower Bound (ELBO)","text":"<p>Now, consider the optimization problem</p> \\[q^* = \\argmin_{\\theta\\in\\Theta} D_{KL}(q_\\theta \\parallel p)\\] <p>Note that </p> \\[\\begin{align*} D_{KL}(q_\\theta \\parallel p) &amp;= E_{z\\sim q_\\theta} \\log(\\frac{q_\\theta(z|x)}{p(z|x)})\\\\ &amp;=  E_{z\\sim q_\\theta} \\log(\\frac{q_\\theta(z|x) p(x)}{p(z,x)})\\\\ &amp;=  E_{z\\sim q_\\theta} \\log(\\frac{q_\\theta(z|x)}{p(z,x)}) + E_{z\\sim q_\\theta} p(x) \\end{align*}\\] <p>Since \\(x\\) is observed, \\(E_{z\\sim q_\\theta} p(x)\\) is fixed and independent of \\(\\theta\\). </p> <p>Thus, define the objective function s.t. minimizing \\(D_{KL}(q_\\theta \\parallel p)\\) is the same as maximizing  </p> \\[\\mathcal L(\\phi; x) := -E_{z\\sim q_\\theta} \\log(\\frac{q_\\theta(z|x)}{p(z,x)})\\] <p>Call \\(\\mathcal L(\\phi)\\) ELBO and note that \\(-\\mathcal L(\\phi) + \\log p(x) = D_{KL} \\geq 0\\implies \\mathcal L(\\phi)\\leq \\log p(x)\\).</p>"},{"location":"csc412/vi.html#reparameterization-trick","title":"Reparameterization Trick","text":"<p>Now consider </p> \\[\\mathcal L(\\phi; x) := -E_{z\\sim q_\\theta} \\log(\\frac{q_\\theta(z|x)}{p(z,x)}) = E_{z\\sim q_\\theta} (\\log p(x,z)-\\log q_\\theta(z))\\] <p>and we are optimizing the function by </p> \\[\\nabla_\\phi \\mathcal L = \\nabla_\\phi E_{z\\sim q_\\theta} (\\log p(x,z)-\\log q_\\theta(z))\\] <p>However, this causes a problem that the expection \\(E_{z\\sim q_\\theta}\\) depends on \\(q_\\theta\\), thus we cannot put \\(\\nabla_\\theta\\) into the expectation. </p> <p>Thus, we need to reparameterize the expectation distribution, so that expectation does not depend on \\(\\phi\\). The idea is that we use another random variable \\(\\epsilon\\) from a fixed distribution \\(p(\\epsilon)\\), eg. \\(\\text{Unif}(0,1)\\) or \\(N(0,1)\\). Then, take some translation function \\(T(\\epsilon, \\phi)\\) s.t. \\(z =T(\\epsilon, \\phi) \\sim q_\\theta(z)\\).  Thus, we reparameterized the expectation as</p> \\[\\begin{align*} \\nabla_\\phi \\mathcal L &amp;= \\nabla_\\phi E_{z\\sim q_\\theta} (\\log p(x,z)-\\log q_\\theta(z))\\\\ &amp;= \\nabla_\\phi E_{\\epsilon\\sim p(\\epsilon)} (\\log p(x,T(\\epsilon, \\phi))-\\log q_\\theta(T(\\epsilon, \\phi)))\\\\ &amp;=  E_{\\epsilon\\sim p(\\epsilon)} \\nabla_\\phi(\\log p(x,T(\\epsilon, \\phi))-\\log q_\\theta(T(\\epsilon, \\phi))) \\end{align*}\\]"},{"location":"csc412/vi.html#stochasitc-variational-infernce","title":"Stochasitc Variational Infernce","text":"<p>Look at \\(\\nabla_\\phi \\mathcal L = E_{\\epsilon\\sim p(\\epsilon)} \\nabla_\\phi(\\log p(x,T(\\epsilon, \\phi))-\\log q_\\theta(T(\\epsilon, \\phi)))\\), it is very similar to the gradient descent problem in neural networks. Thus, similar to SGD, we can do SVI which, at each optimization step, takes a mini-batch to estimate the sample expectation as </p> \\[\\hat E_{\\epsilon\\sim p(\\epsilon)} \\nabla_\\phi (\\cdots) \\approx m^{-1}\\sum_{i=1}^m \\nabla_\\theta \\log(\\cdots)\\]"},{"location":"csc417/1d_mass_spring.html","title":"1D Mass Spring System","text":""},{"location":"csc417/1d_mass_spring.html#generalized-coordinates","title":"Generalized coordinates","text":"<p>Given a particle of mass \\(m\\) </p> \\[q = x(t)\\] <p>so that the generalized velocity is </p> \\[\\dot q = v(t)\\]"},{"location":"csc417/1d_mass_spring.html#kinetic-energy","title":"Kinetic Energy","text":"<p>In 1-D the kinetic energy is \\(\\frac 12 mv^2 = \\frac 12 m\\dot q\\)</p>"},{"location":"csc417/1d_mass_spring.html#potential-energy","title":"Potential Energy","text":"<p>By Hooke's Law, force is linearly proportional to stretch in spring, i.e. </p> \\[f = -kx\\] <p>for some stiffness coefficient \\(k\\), then the total mechanical work is </p> \\[W = \\int \\underset{\\text{force}}{-kx(t)}\\underset{\\text{displacement}}{v(t)} dt = \\int -kx dx = -\\frac12kx^2\\] <p>and the potential energy is the negative of work</p> \\[V = -W = \\frac12 kx^2\\]"},{"location":"csc417/1d_mass_spring.html#equation-of-motion","title":"Equation of Motion","text":"<p>Therefore, we have </p> \\[L = \\frac12 m\\dot q^2 - \\frac12 kq^2\\] <p>By Euler Lagrange Equation</p> \\[\\begin{align*} \\frac{\\partial L}{\\partial q} &amp;= \\frac{d}{dt}\\frac{\\partial L}{\\partial \\dot q}\\\\ -kq &amp;= \\frac{d}{dt}(m\\dot q)\\\\ -kq &amp;= m\\ddot q\\\\ \\end{align*}\\]"},{"location":"csc417/1d_mass_spring.html#time-integration","title":"Time Integration","text":"<p>Note that \\(q:\\mathbb R\\rightarrow \\mathbb R\\) is the mapping from time to the position of the particle at that time. Time integration </p> <ul> <li>input: A ODE \\(\\ddot q = f(q, \\dot q)\\) and the initial conditions \\(q_0 = q(t_0), \\dot q_0 = \\dot q(t_0)\\)</li> <li>output: the discrete update equation \\(q^{t+1} = f(q^t, q^{t+1},...,\\dot q^t, \\dot q^{t+1}, ...)\\)</li> </ul>"},{"location":"csc417/1d_mass_spring.html#the-coupled-first-order-systems","title":"The Coupled First Order Systems","text":"<p>Note that we have a second-order ODE, \\(m\\ddot q = -kq\\), replaces $\\dot q $ with velocity \\(v\\), we can transform the system into a coupled first order ODE</p> \\[m\\dot v = -kq\\] <p>rewrite into matrix form</p> \\[\\begin{pmatrix}m&amp;0\\\\0&amp;1\\end{pmatrix}\\frac{d}{dt}\\begin{pmatrix}v\\\\q\\end{pmatrix} = \\begin{pmatrix}0&amp;-k\\\\1&amp;0\\end{pmatrix}\\begin{pmatrix}v\\\\q\\end{pmatrix}\\] <p>Denote \\(\\mathbf y = [v, q]^T\\), the equation above is further written as </p> \\[A  \\dot{\\mathbf y} = \\mathbf f(\\mathbf y)\\]"},{"location":"csc417/1d_mass_spring.html#phase-space","title":"Phase Space","text":"<p>We define the phase space as the x-axis being the value of \\(v\\) and y-axis being \\(q\\), so that we can plot the trajectory of the position and velocity through time.</p>"},{"location":"csc417/1d_mass_spring.html#numerical-integration","title":"Numerical Integration","text":"<p>Note that the integration above is simple, while more complex equations may not be suitable for analytical solution, so we need integrate it numerically</p>"},{"location":"csc417/1d_mass_spring.html#explicit-and-implicit-integration","title":"Explicit and Implicit Integration","text":"<ul> <li>Explicit: Next time step can be computed entirely using the previous and current time step</li> <li>Implicit: Next time step is computed using future values </li> </ul>"},{"location":"csc417/1d_mass_spring.html#concerns","title":"Concerns","text":"<ul> <li>Performance: runtime / efficiency</li> <li>Stability: We don't want the spring to \"fly\" out, which means we need the solution to stay \"within\" the bounded system of the analytical solution</li> <li>Accuracy: the \"visual\" accuracy, we mostly want it looks real, even if the solution may not be mathematically correct</li> </ul>"},{"location":"csc417/1d_mass_spring.html#forward-euler-time-integration","title":"Forward Euler Time Integration","text":"<p>Replace time derivative with finite difference</p> \\[\\dot{\\mathbf y} \\approx \\frac{1}{\\Delta t} (\\mathbf y^{t+1} - \\mathbf y^t)\\] <p>so that </p> \\[\\begin{align*} A  \\frac{1}{\\Delta t} (\\mathbf y^{t+1} - \\mathbf y^t) &amp;= \\mathbf f(\\mathbf y^t)\\\\ \\mathbf y^{t+1} &amp;= \\mathbf y^t + \\Delta t A^{-1}\\mathbf f(\\mathbf y^t)\\\\ v^{t+1} &amp;= v^t - \\Delta t \\frac{k}{m}q^t\\\\ q^{t+1} &amp;= q^t + \\Delta t v^t \\end{align*}\\]"},{"location":"csc417/1d_mass_spring.html#problem","title":"Problem","text":"<p>Because we replace the derivative with the current \"slope\", the trajectory is going outwards, which is unstable. </p> <pre><code>class ForwardEuler(TimeIntegration):\n    def one_step(self):\n        q_t = self.trajectory_q[-1]\n        v_t = self.trajectory_v[-1]\n        self.trajectory_v.append(v_t - self.dt * self.k / self.m * q_t)\n        self.trajectory_q.append(q_t + self.dt * v_t)\n</code></pre> <p> </p>"},{"location":"csc417/1d_mass_spring.html#runge-kutta-time-integration","title":"Runge-Kutta Time Integration","text":"<p>To fix the issue with Forward Euler, we can average several \"slope\" to pull the trajectory back. The general idea is </p> \\[\\mathbf y^{t+1} = \\mathbf y^t + \\Delta t A^{-1}(\\alpha \\mathbf f(\\mathbf y^{t+a} + \\beta \\mathbf f(\\tilde{\\mathbf y}^{t+b})))\\] <p>where \\(\\tilde {\\mathbf y}^{t+a} = y^t + \\alpha \\Delta tA^{-1}\\mathbf f(\\mathbf y^t)\\) is the Forward Euler estimate. </p> <p>Following this template, we can have Heun's Method by taking \\(a=0, b= 1, \\alpha=\\beta=\\frac12\\)</p> \\[\\mathbf y^{t+1} = \\mathbf y^t + \\frac{\\Delta t}{2} A^{-1}(\\mathbf f(\\mathbf y^t) + \\mathbf f(\\tilde {\\mathbf y}^{t+1}))\\] <p>The most general used method is RK4, The Fourth-order Runge Kutta Method. </p> \\[\\begin{align*} \\kappa_1 &amp;= A^{-1}\\mathbf f(\\mathbf y^t)\\\\ \\kappa_2 &amp;= A^{-1}\\mathbf f(\\mathbf y^t + \\frac{\\Delta t}2 \\kappa_1)\\\\ \\kappa_3 &amp;= A^{-1}\\mathbf f(\\mathbf y^t + \\frac{\\Delta t}2 \\kappa_2)\\\\ \\kappa_4 &amp;= A^{-1}\\mathbf f(\\mathbf y^t + \\Delta t \\kappa_3)\\\\ \\mathbf y^{t+1} &amp;= \\mathbf y^t + \\frac{\\Delta t}{6}(\\kappa_1 + 2\\kappa_2 + 2\\kappa_3 + \\kappa_4) \\end{align*}\\] <pre><code>class RK4(TimeIntegration):\n    def one_step(self):\n        q_t = self.trajectory_q[-1]\n        v_t = self.trajectory_v[-1]\n        weights = np.array((1, 2, 2, 1))\n        kappa = np.empty((4, 2))\n        kappa[0, 0] = v_t\n        kappa[0, 1] = - self.k / self.m * q_t\n\n        kappa[1, 0] = v_t + self.dt * 0.5 * kappa[0, 1]\n        kappa[1, 1] = - self.k / self.m * (q_t + self.dt * 0.5 * kappa[0, 0])\n\n        kappa[2, 0] = v_t + self.dt * 0.5 * kappa[1, 1]\n        kappa[2, 1] = - self.k / self.m * (q_t + self.dt * 0.5 * kappa[1, 0])\n\n        kappa[3, 0] = v_t + self.dt * kappa[2, 1]\n        kappa[3, 1] = - self.k / self.m * (q_t + self.dt * kappa[2, 0])\n\n        self.trajectory_q.append(q_t + self.dt / 6 * np.dot(kappa[:, 0], weights))\n        self.trajectory_v.append(v_t + self.dt / 6 * np.dot(kappa[:, 1], weights))\n</code></pre> <p> </p>"},{"location":"csc417/1d_mass_spring.html#backward-euler-time-integration","title":"Backward Euler Time Integration","text":"<p>This is the implicit time integration. Compare to Forward Euler, instead of evaluating at the current step, we evaluate at the next time step, i.e.</p> \\[A  \\frac{1}{\\Delta t} (\\mathbf y^{t+1} - \\mathbf y^t) = \\mathbf f(\\mathbf y^{t+1})\\] <p>Note that the unknown \\(\\mathbf y^{t+1}\\) appears on both sides, which causes problem.  However, if we look back at \\(\\mathbf f (\\mathbf y)\\), note that </p> \\[\\mathbf f (\\mathbf y) = \\begin{pmatrix}0&amp;-k\\\\1&amp;0\\end{pmatrix}\\begin{pmatrix}v\\\\q\\end{pmatrix} = B\\mathbf y\\] <p>Since \\(\\mathbf f\\) is a linear function, we have </p> \\[\\begin{align*} A  \\frac{1}{\\Delta t} (\\mathbf y^{t+1} - \\mathbf y^t) &amp;= B\\mathbf y^{t+1}\\\\ (I - \\Delta t A^{-1}B)\\mathbf y^{t+1} &amp;= \\mathbf y^t\\\\ (1+\\Delta t^2 \\frac km) v^{t+1} &amp;= v^t - \\Delta t \\frac km q^t\\\\ q^{t+1} &amp;= q^t + \\Delta tv^{t+1} \\end{align*}\\] <p>Note that this is stable since the vector difference is the slope at \\(t+1\\), which means it \"pulls\" back the trajectory to the origin. </p> <pre><code>class BackwardEuler(TimeIntegration):\n    def one_step(self):\n        q_t = self.trajectory_q[-1]\n        v_t = self.trajectory_v[-1]\n        v = v_t - self.dt * self.k / self.m * q_t\n        v = v / (1. + self.dt * self.dt * self.k / self.m)\n        self.trajectory_v.append(v)\n        self.trajectory_q.append(q_t + self.dt * v)\n</code></pre> <p> </p>"},{"location":"csc417/1d_mass_spring.html#symplectic-euler-time-integration","title":"Symplectic Euler Time Integration","text":"<p>Note that Forward Euler causes the exploding trajectory and the Backward Euler causes damping, we can do the two integrations alternately to \"cancel out\" long term effect, i.e.</p> <p>First take an explicit velocity step</p> \\[v^{t+1} = v^t - \\Delta t \\frac km q^t\\] <p>Then take an implicit position step</p> \\[q^{t+1} = q^t + \\Delta t v^{t+1}\\] <pre><code>class Symplectic(TimeIntegration):\n    def one_step(self):\n        q_t = self.trajectory_q[-1]\n        v_t = self.trajectory_v[-1]\n        v = v_t - self.dt * self.k / self.m * q_t\n        self.trajectory_v.append(v)\n        self.trajectory_q.append(q_t + self.dt * v)\n</code></pre> <p> </p> Source code <pre><code>import numpy as np\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\n\n\nclass TimeIntegration:\n    def __init__(self, mass, stiffness, dt, q0, v0):\n        \"\"\"\n        mass: the mass of the object\n        stiffness: the stiffness coefficient of the spring\n        dt: Delta t for each step taken\n        q0: the initial position at t0\n        v0: the initial velocity at t0\n        \"\"\"\n        self.m = mass\n        self.k = stiffness\n        self.dt = dt\n        self.q0 = q0\n        self.v0 = v0\n        self.trajectory_q = [q0]\n        self.trajectory_v = [v0]\n\n    def one_step(self):\n        raise NotImplementedError\n\n    def run(self, step):\n        for _ in range(step):\n            self.one_step()\n\n    def plot(self, step, output_file):\n        self.run(step)\n        fig = make_subplots(rows=1, cols=2,\n                            subplot_titles=('Object', 'Phase Space'),\n                            column_widths=[0.7, 0.3])\n\n        fig.add_trace(go.Scatter(x=[0, self.q0], y=[\n                      0, 0], marker_size=10), 1, 1)\n\n        fig.add_trace(go.Scatter(x=[self.q0], y=[\n                      self.v0], marker_size=3), 1, 2)\n        fig.frames = [go.Frame(data=[go.Scatter(x=[0, self.trajectory_q[i]]),\n                                     go.Scatter(x=self.trajectory_q[:i], y=self.trajectory_v[:i])],\n                               traces=[0, 1]) for i in range(len(self.trajectory_q))]\n        button = dict(\n            label='Play',\n            method='animate',\n            args=[None, dict(\n                frame=dict(duration=50, redraw=False),\n                transition=dict(duration=0),\n                fromcurrent=True,\n                mode='immediate')])\n        fig.update_layout(\n            updatemenus=[\n                dict(\n                    type='buttons',\n                    showactive=False,\n                    y=0,\n                    x=1.05,\n                    xanchor='left',\n                    yanchor='bottom',\n                    buttons=[button])\n            ],\n            showlegend=False\n        )\n        fig.update_xaxes(range=[-3, 3])\n        fig.update_yaxes(range=[-20, 20])\n        fig.write_html(output_file, full_html=False, auto_open=False,\n                       include_plotlyjs=\"cdn\", auto_play=False)\n\n\nmass = 1\nstiffness = 100\ndt = 0.01\nq0 = 1\nv0 = 1\n\nSTEPS=75\n\n# --8&lt;-- [start:fe]\nclass ForwardEuler(TimeIntegration):\n    def one_step(self):\n        q_t = self.trajectory_q[-1]\n        v_t = self.trajectory_v[-1]\n        self.trajectory_v.append(v_t - self.dt * self.k / self.m * q_t)\n        self.trajectory_q.append(q_t + self.dt * v_t)\n# --8&lt;-- [end:fe]\nfe = ForwardEuler(mass, stiffness, dt, q0, v0)\nfe.plot(STEPS, \"../assets/1d_mass_spring_fe.html\")\n\n# --8&lt;-- [start:rk4]\nclass RK4(TimeIntegration):\n    def one_step(self):\n        q_t = self.trajectory_q[-1]\n        v_t = self.trajectory_v[-1]\n        weights = np.array((1, 2, 2, 1))\n        kappa = np.empty((4, 2))\n        kappa[0, 0] = v_t\n        kappa[0, 1] = - self.k / self.m * q_t\n\n        kappa[1, 0] = v_t + self.dt * 0.5 * kappa[0, 1]\n        kappa[1, 1] = - self.k / self.m * (q_t + self.dt * 0.5 * kappa[0, 0])\n\n        kappa[2, 0] = v_t + self.dt * 0.5 * kappa[1, 1]\n        kappa[2, 1] = - self.k / self.m * (q_t + self.dt * 0.5 * kappa[1, 0])\n\n        kappa[3, 0] = v_t + self.dt * kappa[2, 1]\n        kappa[3, 1] = - self.k / self.m * (q_t + self.dt * kappa[2, 0])\n\n        self.trajectory_q.append(q_t + self.dt / 6 * np.dot(kappa[:, 0], weights))\n        self.trajectory_v.append(v_t + self.dt / 6 * np.dot(kappa[:, 1], weights))\n# --8&lt;-- [end:rk4]        \nrk4 = RK4(mass, stiffness, dt, q0, v0)\nrk4.plot(STEPS, \"../assets/1d_mass_spring_rk4.html\")\n\n# --8&lt;-- [start:be]\nclass BackwardEuler(TimeIntegration):\n    def one_step(self):\n        q_t = self.trajectory_q[-1]\n        v_t = self.trajectory_v[-1]\n        v = v_t - self.dt * self.k / self.m * q_t\n        v = v / (1. + self.dt * self.dt * self.k / self.m)\n        self.trajectory_v.append(v)\n        self.trajectory_q.append(q_t + self.dt * v)\n# --8&lt;-- [end:be]\nbe = BackwardEuler(mass, stiffness, dt, q0, v0)\nbe.plot(STEPS, \"../assets/1d_mass_spring_be.html\")\n\n\n# --8&lt;-- [start:sc]\nclass Symplectic(TimeIntegration):\n    def one_step(self):\n        q_t = self.trajectory_q[-1]\n        v_t = self.trajectory_v[-1]\n        v = v_t - self.dt * self.k / self.m * q_t\n        self.trajectory_v.append(v)\n        self.trajectory_q.append(q_t + self.dt * v)\n# --8&lt;-- [end:sc]       \nsc = Symplectic(mass, stiffness, dt, q0, v0)\nsc.plot(STEPS, \"../assets/1d_mass_spring_sc.html\")\n</code></pre>"},{"location":"csc417/3d_mass_spring_system.html","title":"3D Mass Spring System","text":""},{"location":"csc417/3d_mass_spring_system.html#two-particles-and-one-string","title":"Two Particles and One String","text":"<p>Let the particles be \\(x_1(t), x_2(t)\\in\\mathbb R^3\\) and define the strings to have a rest length \\(l_0\\in\\mathbb R^+\\), then we define the generalized coordinates and generalized velocity being </p> \\[\\mathbf q = \\begin{pmatrix}\\mathbf x_0\\\\ \\mathbf x_1\\end{pmatrix}, \\dot{\\mathbf q} = \\begin{pmatrix}\\mathbf v_0\\\\ \\mathbf v_1\\end{pmatrix}\\]"},{"location":"csc417/3d_mass_spring_system.html#kinetic-energy","title":"Kinetic Energy","text":"<p>Then, the kinetic energy for each particle in the system is \\(\\frac 12 m_i\\|v_i\\|_2^2 = \\frac12 v_i^T\\underset{M_i}{(m_iI)}v_i\\) and the total kinetic energy is the sum of all the kinetic energy, i.e.</p> \\[T = \\sum_{i=0}^1 \\frac 12 \\mathbf v_i^TM_i v_i = \\frac{1}{2}\\dot{\\mathbf q}^TM\\dot{\\mathbf q}, M = \\begin{pmatrix}M_0&amp;0\\\\0&amp;M_1\\end{pmatrix}\\]"},{"location":"csc417/3d_mass_spring_system.html#potential-energy","title":"Potential Energy","text":"<p>The properties we want</p> <ul> <li>Spring should go back to original length when all external forces are removed</li> <li>Rigid motion should not change the energy'</li> <li>Energy should depend only on particle positions</li> </ul> <p>Therefore, we define Strain being \\(l - l_0\\) which is the difference of the deformed length from the rest length, </p> \\[l = \\sqrt{(x_1 - x_0)^T(x_1 - x_0)} = \\sqrt{\\big[\\begin{pmatrix}-I &amp;I\\end{pmatrix}\\begin{pmatrix}\\mathbf x_0 \\\\\\mathbf x_1\\end{pmatrix}\\big]^T\\big[\\underset{B}{\\begin{pmatrix}-I &amp;I\\end{pmatrix}}\\underset{\\mathbf q}{\\begin{pmatrix}\\mathbf x_0 \\\\\\mathbf x_1\\end{pmatrix}}\\big]} = \\sqrt{\\mathbf q^T B^TB\\mathbf q}\\] <p>then the potential energy is </p> \\[V = \\frac12 k(l-l_0)^2 = \\frac12 k(\\sqrt{\\mathbf q^TB^TB\\mathbf q} - l_0)^2\\]"},{"location":"csc417/3d_mass_spring_system.html#euler-lagrange-equation","title":"Euler-Lagrange Equation","text":"<p>Consider the Lagrangian \\(L = T - V\\), note that \\(T\\) is only related to \\(\\dot{\\mathbf q}\\), hence</p> \\[\\frac{\\partial L}{\\partial \\mathbf q} = \\frac{\\partial T}{\\partial \\mathbf q} - \\frac{\\partial V}{\\partial \\mathbf q} = -\\frac{\\partial V}{\\partial \\mathbf q}\\] <p>and the Euler-Lagrange Equation is simplified a bit as </p> \\[\\frac{d}{dt}\\frac{\\partial L}{\\partial \\dot{\\mathbf q}} = - \\frac{\\partial V}{\\partial \\mathbf q}\\] <p>Further more, we have</p> \\[\\begin{align*} \\frac{d}{dt}\\frac{\\partial L}{\\partial \\dot{\\mathbf q}} &amp;= \\frac{d}{dt}\\frac{\\partial}{\\partial \\dot{\\mathbf q}}(\\frac12 \\dot{\\mathbf q}^TM\\dot{\\mathbf q})\\\\  &amp;=\\frac{d}{dt}M\\dot{\\mathbf q}\\\\ &amp;= M\\ddot{\\mathbf q} \\end{align*}\\] <p>So that the Euler-Lagrange equation is derived to be </p> \\[M\\ddot{\\mathbf q} = - \\frac{\\partial V}{\\partial \\mathbf q}\\]"},{"location":"csc417/3d_mass_spring_system.html#n-particles","title":"N Particles","text":"<p>The generalized coordinates and generalized velocity will be </p> \\[\\mathbf q = \\begin{pmatrix}\\mathbf x_0\\\\\\vdots\\\\ \\mathbf x_n\\end{pmatrix}, \\dot{\\mathbf q} = \\begin{pmatrix}\\mathbf v_0\\\\\\vdots\\\\ \\mathbf v_n\\end{pmatrix}\\]"},{"location":"csc417/3d_mass_spring_system.html#kinetic-energy_1","title":"Kinetic Energy","text":"\\[T = \\sum_{i=0}^1 \\frac 12 \\mathbf v_i^TM_i v_i = \\frac{1}{2}\\dot{\\mathbf q}^TM\\dot{\\mathbf q}, M = \\begin{pmatrix}M_0&amp;\\cdots&amp;0\\\\\\vdots&amp;\\ddots&amp;\\vdots\\\\0&amp;\\cdots&amp;M_n\\end{pmatrix}\\in\\mathbb R^{3n\\times 3n}\\]"},{"location":"csc417/3d_mass_spring_system.html#potential-energy_1","title":"Potential Energy","text":"<p>Define the potential energy for each spring \\(j\\) being \\(V_j:\\mathbb R^{6}\\rightarrow \\mathbb R\\) so that </p> \\[V = \\sum_{j=0}^{m-1}V_j(\\mathbf x_A, \\mathbf x_B)\\] <p>To vectorize this operation, consider a selection \\(3\\times 3n\\) matrix </p> \\[S_{i} = \\begin{pmatrix}\\mathbf 0_{3\\times 3}\\cdots I_{3\\times3}\\cdots &amp;\\mathbf 0\\end{pmatrix}, S_i\\mathbf q = \\mathbf x_i\\] <p>therefore, we can have </p> \\[\\mathbf q_j = \\begin{pmatrix}x_A\\\\x_B\\end{pmatrix} = \\begin{pmatrix}S_A\\\\S_B\\end{pmatrix}\\mathbf q = E_j \\mathbf q\\] <p>So that </p> \\[V = \\sum_{j=0}^{m-1}V_j(E_j q)\\]"},{"location":"csc417/3d_mass_spring_system.html#euler-lagrange-equation_1","title":"Euler-Lagrange Equation","text":"<p>Note that the derivation in EL equation still holds, </p> \\[M\\ddot{\\mathbf q} = - \\frac{\\partial V}{\\partial \\mathbf q}\\] <p>Then, consider </p> \\[\\begin{align*} -\\frac{\\partial V}{\\partial \\mathbf q} &amp;= - \\frac{\\partial }{\\partial \\mathbf q}\\sum_{j=0}^{m-1}V_j(E_j q)\\\\ &amp;= - \\sum_{j=0}^{m-1}\\frac{\\partial }{\\partial \\mathbf q}V_j(E_j q)\\\\ &amp;= - \\sum_{j=0}^{m-1} E_j^T  \\frac{\\partial V_j}{\\partial \\mathbf q_j}(\\mathbf q_j)\\\\ &amp;= \\sum_{j=0}^{m-1} E_j^T  \\mathbf f_j(\\mathbf q_j) \\end{align*}\\]"},{"location":"csc417/3d_mass_spring_system.html#linearly-implicit-time-integration","title":"Linearly-Implicit Time Integration","text":"<p>From backward Euler, we have </p> \\[M\\dot{\\mathbf q}^{t+1} = M\\dot{\\mathbf q}^t + \\Delta t \\mathbf f(\\mathbf q^{t+1})\\] \\[\\mathbf q^{t+1} = \\mathbf q^t + \\Delta t \\dot{\\mathbf q}^{t+1}\\] <p>We substitute \\(\\mathbf f(\\mathbf q^{t+1})\\) with our position update so that </p> \\[M\\dot{\\mathbf q}^{t+1} = M\\dot{\\mathbf q}^t + \\Delta t \\mathbf f(\\mathbf q^t + \\Delta t \\dot{\\mathbf q}^{t+1})\\] <p>Then we use Taylor first order approximation</p> \\[M\\dot{\\mathbf q}^{t+1} = M\\dot{\\mathbf q}^t + \\Delta t \\mathbf f(\\mathbf q^t) + \\Delta t^2 \\frac{\\partial \\mathbf f}{\\partial \\mathbf q}\\dot{\\mathbf q}^{t+1}\\] <p>Rearrange equations, we have </p> \\[(M - \\Delta t^2 K)\\dot{\\mathbf q}^{t+1} = M\\dot{\\mathbf q}^t + \\Delta t  \\mathbf f(\\mathbf q^t)\\] <p>where \\(K = \\frac{\\partial \\mathbf f}{\\partial \\mathbf q}\\) is the stiffness matrix, since \\(\\mathbf f = -\\frac{\\partial}{\\partial \\mathbf q}\\sum^{m-1} V_j(E_j\\mathbf q)\\)</p> \\[\\begin{align*} K &amp;= \\frac{\\partial \\mathbf f}{\\partial \\mathbf q}\\\\  &amp;= -\\frac{\\partial^2}{\\partial \\mathbf q^2}\\sum^{m-1} V_j(E_j\\mathbf q)\\\\  &amp;= -\\sum^{m-1} \\frac{\\partial^2}{\\partial \\mathbf q^2} V_j(E_j\\mathbf q)\\\\  &amp;= \\sum^{m-1} -E_j^T \\frac{\\partial^2V_j}{\\partial \\mathbf q^2} E_j^T\\\\  &amp;= \\sum_{j=0}^{m-1} E_j^TK_jE_j&amp;K_j = - \\frac{\\partial^2  V_j}{\\partial\\mathbf q^2} \\end{align*}\\]"},{"location":"csc417/3d_mass_spring_system.html#fixed-boundary-conditions","title":"Fixed Boundary Conditions","text":"<p>We want some \\(x_i\\) being fixed to fixed coordinate \\(b_i\\),  let \\(\\hat{\\mathbf q} = P\\mathbf q\\) being all the non fixed points, where \\(P\\) is the selection matrix that selects non fixed points, therefore \\(P^T\\hat q + \\mathbf b\\), where \\(\\mathbf b\\) have all the fixed coordinates, then we can have </p> \\[\\mathbf q = P^T\\hat q + \\mathbf b, \\dot{\\mathbf q} = P^T \\dot{\\hat{\\mathbf q}}\\] <p>So that the updates is </p> \\[P(M - \\Delta t^2 K)P^T\\dot{\\hat{\\mathbf q}}^{t+1} = PM\\dot{\\mathbf q}^t + \\Delta t  P\\mathbf f(\\mathbf q^t)\\] \\[\\mathbf q^{t+1} = \\mathbf q^t + \\Delta t P^T \\dot{\\hat{\\mathbf {q}^{t+1}}}\\]"},{"location":"csc417/cloth.html","title":"Cloth Simulation and Collision","text":""},{"location":"csc417/cloth.html#triangular-fem","title":"Triangular FEM","text":"<p>We often simulate cloth with triangle surfaces instead of a tetrahedron. Therefore, we cannot totally rely on the volumetric approach (while we do introduce \"thickness\" to adapt the mechanics).</p>"},{"location":"csc417/cloth.html#finite-elements","title":"Finite Elements","text":"<p>Given the reference space in 3D, and the reference position as \\(\\mathbf X = (X, Y, Z)^T \\in \\mathbb R^3\\), just as tetrahedron case, we represent the finite elements at point \\(\\mathbf X\\) given by the 3 corners of the triangle \\(T=(\\mathbf X_0, \\mathbf X_1, \\mathbf X_2)\\)</p> \\[f(\\mathbf X) = \\sum_0^2 f(\\mathbf X_i) \\phi_i(\\mathbf X)\\] <p>where \\(\\phi_i\\) is the shape function, and as before, we apply the constraint \\(\\phi_0 + \\phi_1 + \\phi_2 = 1\\Rightarrow \\phi_0 = 1- \\phi_1 - \\phi_2\\). </p> <p>However, note that \\(\\mathbf X\\) may not lie exactly on the surface of the triangle. In this case, we want to find the function for the nearest point of \\(\\mathbf X\\) on triangle, i.e. \\(proj_{T}(\\mathbf X)\\). In this case, we can turn the question of solving the shape function to an optimization problem. i.e. </p> \\[\\begin{align*} \\min \\:&amp;\\|\\sum_0^2 \\mathbf X_i \\phi_i(\\mathbf X) - \\mathbf X \\|^2_2\\\\ \\text{w.r.t} \\:&amp;\\{(\\phi_0, \\phi_1, \\phi_2)\\mid \\phi_0 = 1 - \\phi_1 - \\phi_2\\} \\end{align*}\\] <p>And the solution to this optimization problem is \\begin{align*}</p> <p>T &amp; = \\begin{pmatrix}\\Delta \\mathbf X_1 &amp;\\Delta \\mathbf X_2\\end{pmatrix}\\ T^TT\\begin{pmatrix}\\phi_1 \\\\phi_2\\end{pmatrix} &amp;= T^T \\Delta \\mathbf X\\ \\phi_0 &amp;= 1 - \\phi_1 - \\phi_2 \\end{align*}</p> <p>where \\(\\Delta \\mathbf X = \\mathbf X - \\mathbf X_0\\)</p> <p>Note that this shape function is the barycentric coordinate for \\(proj_T(\\mathbf X)\\), and for points off the triangle</p> \\[\\mathbf X = \\sum_0^2 \\mathbf X_i\\phi_i(\\mathbf X) + \\alpha N = \\sum_0^2 \\mathbf X_i\\phi_i(\\mathbf X) + \\underset{\\alpha}{\\Delta \\mathbf X^TN}\\cdot N\\] <p>where \\(N\\) is the normal of \\(T\\)</p>"},{"location":"csc417/cloth.html#deformation","title":"Deformation","text":"<p>Note that we introduce \\(N_T\\), while the world space have different normal on the deformed triangle \\(\\mathbf n\\), therefore, we can simply have the deformation as </p> \\[\\mathbf x(\\mathbf X, t) = \\sum_0^x \\mathbf x_i \\phi_x(\\mathbf X) + \\Delta\\mathbf X^TN\\cdot \\mathbf n\\]"},{"location":"csc417/cloth.html#generalized-coordinates","title":"Generalized Coordinates","text":"<p>As before, let the generalized coordinates and generalized velocity for the triangle as </p> \\[\\mathbf q = \\begin{pmatrix}\\mathbf x_0\\\\\\mathbf x_1\\\\\\mathbf x_2\\end{pmatrix}, \\dot{\\mathbf q} = \\begin{pmatrix}\\dot{\\mathbf x_0}\\\\\\dot{\\mathbf x_1}\\\\\\dot{\\mathbf x_2}\\end{pmatrix}\\]"},{"location":"csc417/cloth.html#kinetic-energy","title":"Kinetic Energy","text":"<p>Let the cloth domain be \\(\\Omega\\), since the cloth is thin, we can just integrate over the triangle surface, and then multiple the thickness to get the volumetric kinetic energy, i.e. </p> \\[h\\frac12 \\int_T \\rho\\|v(\\mathbf X)\\|_2^2 dT = \\frac {1}{2} \\dot{\\mathbf q}^T\\bigg(h\\int_T\\rho M_0 dT\\bigg)\\dot{\\mathbf q}\\] <p>where </p> \\[M_0 = \\begin{pmatrix} \\phi_0\\phi_0I&amp;\\phi_0\\phi_1I&amp;\\phi_0\\phi_2I\\\\ \\phi_1\\phi_0I&amp;\\phi_1\\phi_1I&amp;\\phi_1\\phi_2I\\\\ \\phi_2\\phi_0I&amp;\\phi_2\\phi_1I&amp;\\phi_2\\phi_2I \\end{pmatrix}\\in \\mathbb R^{9\\times 9}\\] <p>and just as before, we can assemble the total kinetic energy for cloth as </p> \\[T = \\frac12 \\dot{\\mathbf q}^T \\big(\\underset{M}{\\sum_0^{m-1} E_j^TM_j E_j}\\big)\\dot{\\mathbf q}\\]"},{"location":"csc417/cloth.html#potential-energy","title":"Potential Energy","text":"<p>As before, we derive the potential energy from the deformation gradient.</p> <p>Note that the deformation can be written as </p> \\[x(\\mathbf X) = \\mathbf x_0 +  \\begin{pmatrix}\\mathbf x_0&amp;\\mathbf x_1&amp;\\mathbf x_2&amp;\\mathbf n\\end{pmatrix} \\begin{pmatrix}-\\vec 1^T(T^TT)^{-1}T^T\\\\(T^TT)^{-1}T^T\\\\\\mathbf N^T\\end{pmatrix} \\Delta \\mathbf X\\] <p>Therefore, the deformation gradient is simply </p> \\[F =\\partial_\\mathbf Xx = \\begin{pmatrix}\\mathbf x_0&amp;\\mathbf x_1&amp;\\mathbf x_2&amp;\\mathbf n\\end{pmatrix} \\begin{pmatrix}-\\vec 1^T(T^TT)^{-1}T^T\\\\(T^TT)^{-1}T^T\\\\\\mathbf N^T\\end{pmatrix}\\] <p>Note that deformation gradient in this case is constant.</p> <p>Then, we can apply the same potential energy function as in volumetric approach with constant thickness over the triangle area. </p> \\[h\\int_T \\varphi(F(\\mathbf X))dT\\]"},{"location":"csc417/cloth.html#energy-by-principal-stretches","title":"Energy by Principal Stretches","text":"<p>From volumetric approach, we have the strain represented by Right Cauchy Green Deformation </p> \\[\\Delta \\mathbf x^T\\Delta \\mathbf x - \\Delta \\mathbf X^T\\Delta \\mathbf X = \\Delta \\mathbf X^T(F^TF-I)\\Delta \\mathbf X\\] <p>Apply an eigendecomposition on the deformation, we have </p> \\[F^TF =  V\\Lambda V^T = (V\\Delta \\mathbf X)^T (\\Lambda -I) V\\Delta\\mathbf X\\] <p>where \\((V\\Delta\\mathbf X)\\) represents the rotation, and \\(\\Lambda\\) is the squared singular values of \\(F\\). Therefore, via Principal Stretches, we have </p> \\[h\\int_T \\varphi(F(\\mathbf X))dT = h\\int_T \\varphi(\\Sigma(F))dT\\] <p>where \\(\\Sigma(F) = (\\sigma_0(F), \\sigma_1(F), \\sigma_2(F))\\) is the diagonal of singular value matrix from SVD of \\(F\\).</p> <p>With this, we can filter out rotations and translations, which are unwanted as they should not change the potential energy.</p> <p>Then, we want our cloth to go back to undeformed shape when external forces are removed, therefore, we introduce the quadratic energy model Co-Rotational Linear Elasticity</p> \\[\\varphi(\\sigma_0, \\sigma_1, \\sigma_2) = \\mu \\sum_0^2 (\\sigma_i - 1)^2 + \\frac\\lambda 2(\\sum_0^2 \\sigma_i - 3)^2\\] <p>Note that $\\mathbf D\\varphi(1, 1, 1) = 0\\Rightarrow $ minimum at \\((1, 1, 1)\\), which is the identity transformation, this energy is minimized at undeformed shape. </p> <p>Since \\(F\\) is constant across the triangle, now we simply have the potential energy for the triangle be </p> \\[h\\cdot \\text{area} \\cdot \\varphi(F)\\] <p>and then assemble the total potential energy as</p> \\[V = \\sum_{0}^{m-1} h\\cdot \\text{area}_j\\cdot \\varphi(F_j(E_j\\mathbf q))\\]"},{"location":"csc417/cloth.html#the-lagrangian","title":"The Lagrangian","text":"<p>The Lagrangian and EL equation is still the same, since we only change the same of the tetrahedron. </p>"},{"location":"csc417/cloth.html#time-integration","title":"Time Integration","text":"<p>Usually we use linearly implicit integrator for cloth due to high membrane stiffness. Therefore, we still need the generalized force and generalized stiffness. </p> \\[\\begin{align*} \\mathbf f &amp;= -\\partial_{\\mathbf q}V \\\\ &amp;= -\\sum h\\cdot\\text{area}_j\\frac{\\partial}{\\partial \\mathbf q }\\varphi(F_j(E_j\\mathbf q))\\\\ &amp;= -\\sum h\\cdot\\text{area}_j E_j^T \\frac{\\partial F^T}{\\partial\\mathbf q_j}\\frac{\\partial\\varphi}{\\partial F}\\\\ \\frac{\\partial\\varphi}{\\partial F} &amp;= U\\cdot diag(\\frac{\\partial\\varphi}{\\partial\\sigma_0}, \\frac{\\partial\\varphi}{\\partial\\sigma_1}, \\frac{\\partial\\varphi}{\\partial\\sigma_2})V^T \\end{align*}\\] \\[\\begin{align*} K &amp;= -\\frac{\\partial^2V}{\\partial \\mathbf q^2}\\\\ &amp;=  -\\sum h\\cdot\\text{area}_j\\frac{\\partial^2}{\\partial \\mathbf q^2 }\\varphi(F_j(E_j\\mathbf q)) \\end{align*}\\] <p>Note that the Hessian of \\(\\varphi\\) will be extremely messy, therefore, instead of using the full Hessian to do the Newton's method, we can use an approximation of the Hessian and Quasi-Newton Method. In this case, we will have </p> \\[K = =  -\\sum h\\cdot\\text{area}_j E_j^T \\frac{\\partial F}{\\partial\\mathbf q_j}^T \\frac{\\partial^2 \\varphi}{\\partial F^2} \\frac{\\partial F}{\\partial\\mathbf q_j}E_j\\] <p>Then, the problem is left with \\(\\frac{\\partial^2 \\varphi}{\\partial F^2}\\), which involves with the SVD. </p>"},{"location":"csc417/cloth.html#derivative-of-svd","title":"Derivative of SVD","text":"<p>For a SVD \\(A = U\\Sigma V^T\\), we can parameterized by \\(A\\) as \\(U(A)\\Sigma(A)V(A)^T\\), hence the derivative w.r.t to entries of \\(A\\) can be written as </p> \\[\\frac{\\partial}{\\partial A_{ij}}U\\Sigma V^T =\\frac{\\partial U}{\\partial A_{ij}} \\Sigma V^T U\\frac{\\partial\\Sigma}{\\partial A_{ij}}V^T + U\\Sigma \\frac{\\partial V}{\\partial A_{ij}}^T\\] <p>Therefore, we can compute \\(\\frac{\\partial^2 \\varphi}{\\partial F^2}\\) for each entry as</p> \\[\\frac{\\partial}{\\partial F_{ij}}\\frac{\\partial \\varphi}{\\partial F} =\\frac{\\partial U}{\\partial F_{ij}} \\Delta\\Sigma V^T  + U\\cdot diag(\\Delta \\sigma_{ij})\\cdot V^T + U\\Delta\\Sigma \\frac{\\partial V}{\\partial F_{ij}}^T\\] <p>where </p> \\[\\Delta \\Sigma =  diag(\\frac{\\partial\\varphi}{\\partial\\sigma_0}, \\frac{\\partial\\varphi}{\\partial\\sigma_1}, \\frac{\\partial\\varphi}{\\partial\\sigma_2})\\] \\[\\Delta\\sigma_{ij} = \\begin{pmatrix} \\frac{\\partial^2\\varphi}{\\partial \\sigma_0\\sigma_0}&amp;\\frac{\\partial^2\\varphi}{\\partial \\sigma_0\\sigma_1}&amp;\\frac{\\partial^2\\varphi}{\\partial \\sigma_0\\sigma_2}\\\\ \\frac{\\partial^2\\varphi}{\\partial \\sigma_1\\sigma_0}&amp;\\frac{\\partial^2\\varphi}{\\partial \\sigma_1\\sigma_1}&amp;\\frac{\\partial^2\\varphi}{\\partial \\sigma_1\\sigma_2}\\\\ \\frac{\\partial^2\\varphi}{\\partial \\sigma_2\\sigma_0}&amp;\\frac{\\partial^2\\varphi}{\\partial \\sigma_2\\sigma_1}&amp;\\frac{\\partial^2\\varphi}{\\partial \\sigma_2\\sigma_2} \\end{pmatrix} \\begin{pmatrix} \\frac{\\partial \\sigma_0}{\\partial F_{ij}}\\\\ \\frac{\\partial \\sigma_1}{\\partial F_{ij}}\\\\ \\frac{\\partial \\sigma_2}{\\partial F_{ij}} \\end{pmatrix}\\]"},{"location":"csc417/cloth.html#vectorized-deformation-gradient","title":"Vectorized Deformation Gradient","text":"<p>As before, we need to vectorize the deformation gradient</p> \\[\\begin{align*} F &amp;= \\begin{pmatrix} \\partial_X x &amp;\\partial_Y x&amp;\\partial_Z x\\\\ \\partial_X y &amp;\\partial_Y y&amp;\\partial_Z y\\\\ \\partial_X z &amp;\\partial_Y z&amp;\\partial_Z z \\end{pmatrix}\\\\\\Rightarrow  \\vec F_{9\\times 1} &amp;= \\begin{pmatrix} \\partial_X x &amp;\\partial_Y x&amp;\\partial_Z x&amp; \\partial_X y &amp;\\partial_Y y&amp;\\partial_Z y&amp; \\partial_X z &amp;\\partial_Y z&amp;\\partial_Z z \\end{pmatrix}^T\\\\ D&amp;=\\begin{pmatrix}\\begin{pmatrix}-1&amp;-1\\end{pmatrix} \\cdot (T^TT)^{-1}T^T\\\\(T^TT)^{-1}T^T\\end{pmatrix}\\\\ \\Rightarrow B_{9\\times 9} &amp;= \\begin{pmatrix} D_{0\\cdot}&amp;\\vec 0&amp;\\vec 0&amp;D_{1\\cdot}&amp;\\vec0&amp;\\vec0&amp;D_{2\\cdot}&amp;\\vec0&amp;\\vec0\\\\ \\vec0&amp;D_{0\\cdot}&amp;\\vec 0&amp;\\vec 0&amp;D_{1\\cdot}&amp;\\vec0&amp;\\vec0&amp;D_{2\\cdot}&amp;\\vec0\\\\ \\vec0&amp;\\vec0&amp;D_{0\\cdot}&amp;\\vec 0&amp;\\vec 0&amp;D_{1\\cdot}&amp;\\vec0&amp;\\vec0&amp;D_{2\\cdot}\\\\ \\end{pmatrix}\\\\ \\text{where }D_{i\\cdot} &amp;= \\begin{pmatrix}D_{i0} &amp;D_{i1}&amp;D_{i2}\\end{pmatrix}^T,\\vec 0= \\begin{pmatrix}0&amp;0&amp;0\\end{pmatrix}^T\\\\ \\vec {\\mathbf x}_{9\\times 1} &amp;= \\begin{pmatrix}\\mathbf x_0^T&amp;\\mathbf x_1^T&amp;\\mathbf x_2^T&amp;\\mathbf x_3^T\\end{pmatrix}^T\\\\ \\vec{\\mathbf N} &amp;= \\begin{pmatrix} \\mathbf N&amp;\\vec 0&amp;\\vec 0\\\\ \\vec 0&amp;\\mathbf N&amp;\\vec 0\\\\ \\vec 0&amp;\\vec 0&amp;\\mathbf N \\end{pmatrix}\\\\ \\vec F &amp;= B \\vec{\\mathbf x} + \\vec{\\mathbf N} \\cdot \\mathbf n \\end{align*}\\] <p>Then, the gradient is </p> \\[\\partial_{\\vec {\\mathbf x}}F = B + \\vec{\\mathbf N}\\cdot \\partial_{\\vec{\\mathbf x}}{\\mathbf n}\\] <p>then, note that the normal can be written as </p> \\[\\mathbf n  = \\frac{\\Delta \\mathbf x_1\\times \\Delta \\mathbf x_2}{|\\Delta \\mathbf x_1\\times \\Delta \\mathbf x_2|}\\] <p>so that the gradient is </p> \\[ \\partial_{\\vec{\\mathbf x}}{\\mathbf n} = \\frac{1}{|\\Delta \\mathbf x_1\\times \\Delta \\mathbf x_2|} (I-\\mathbf n\\mathbf n^T)([\\Delta X_1]_\\times\\begin{pmatrix}-I&amp;0&amp;I\\end{pmatrix} - [\\Delta X_2]_\\times\\begin{pmatrix}-I&amp;I&amp;0\\end{pmatrix})\\] <p>where \\([\\cdot]_\\times\\) is the skew-symmetric matrix</p>"},{"location":"csc417/cloth.html#collision-with-sphere","title":"Collision with Sphere","text":"<p>Given the query point \\(\\mathbf x\\) and a sphere of center and radius \\((\\mathbf c, r)\\) , we need to first detect the collision and then respond. </p> <p>For the detection, simply check </p> \\[\\|\\mathbf x - \\mathbf c\\|^2 \\leq r^2\\] <p>For the response, we don't want the cloth to move into the sphere, hence we can just filter normal component of velocity. i.e.</p> \\[\\mathbf v_{filtered} = \\mathbf v + \\alpha \\mathbf n\\] <p>where the filtering component coefficient \\(\\alpha\\) satisfies</p> \\[\\mathbf n^T(v+\\alpha \\mathbf n) \\geq 0 \\Rightarrow \\alpha \\geq -\\mathbf n^T\\mathbf v\\] <p>In addition, we want the velocity change be minimal and if velocity if away from the sphere, then we do nothing. Therefore, we have </p> \\[\\alpha = -\\min(0, \\mathbf n^T\\mathbf v)\\]"},{"location":"csc417/collision.html","title":"Rigid body Collisions","text":""},{"location":"csc417/collision.html#problem-setup","title":"Problem Setup","text":"<p>For two rigid bodies \\(A, B\\). A collision happens when two rigid bodies have contact, i.e. some point on \\(A\\) has \\(0\\) distance to some point on \\(B\\). For the simulation, a collision is divided into two parts, detection and response. </p>"},{"location":"csc417/collision.html#collision-detection","title":"Collision Detection","text":"<p>We want to find whether two rigid bodies collide (whether \\(y_A, y_B\\) contact) and often want to have the contact normals \\(\\hat n\\). The detection for different types of objects are quite different. For simplicity, we only consider the collision between a moving rigid body \\(A\\) and a static plane \\(P\\). In this case, we can detect on each vertex of the rigid body and have the contact normal be defined as the outward facing normal \\(\\hat n_P\\) as contact normal. </p>"},{"location":"csc417/collision.html#collision-response","title":"Collision Response","text":"<p>Note that a rigid body is moved by the rigid transformation \\(R, p\\), so that we cannot update/response based on vertices. </p>"},{"location":"csc417/collision.html#signorini-conditions-3-rules-of-of-contact-mechanics","title":"Signorini Conditions (3 Rules of of Contact Mechanics)","text":"<p>Given the two points \\(y_A, y_B\\) be the contact point on \\(A, B\\). Consider the contact mechanics, we want our collision resolution to satisfy  1. No inter-penetration between two objects: \\(d(y_A, y_B) \\geq 0\\) where \\(d\\) is the signed distance.   2. Contact forces \"push\" objects apart: \\(f_{contact} = a\\hat n, a\\in\\mathbb R^{\\geq 0}\\)  3. Contact forces can only be applied when objects are in contact, or complementarity \\(d(y_A, y_B)\\perp a\\)</p>"},{"location":"csc417/collision.html#equations-of-motions","title":"Equations of Motions","text":"<p>Consider each contact point, record them as \\(\\mathbf y_A, \\mathbf y_B\\) (since the equations of motion computes on reference space, this can reverse the world space points back to \\(\\bar X_A, \\bar X_B\\)) and have the contact normal \\(\\mathbf n\\) towards object \\(B\\). Therefore, we have \\(\\mathbf f_B = a\\mathbf n\\) and by Newton's Third law, \\(\\mathbf f_A = -a\\mathbf n\\)</p> <p>Consider the equations of motions, </p> \\[\\begin{align*} m\\dot{\\mathbf p}^{t+1} &amp;= m\\dot{\\mathbf p}^t + \\Delta t \\mathbf f_{ext}\\\\ \\mathbf p^{t+1} &amp;= \\mathbf p^t + \\Delta t\\dot{\\mathbf p}^t\\\\ (R\\mathcal I R^T)\\omega^{t+1} &amp;= (R\\mathcal I R^T)\\omega^t + \\Delta t \\omega^t\\times ((R\\mathcal I R^T)\\omega^t)+\\Delta t\\tau^t_{ext}\\\\ R^{t+1} &amp;= \\exp([\\omega]_\\times^t\\Delta t)R^t \\end{align*}\\] <p>Note that each collision will give an external force in the world space \\(\\mathbf x\\), i.e. updates on \\(\\mathbf f_{ext}, \\tau_{ext}\\). The external torques and forces are computed (see rigid bodies notes)</p> \\[\\begin{pmatrix}\\tau_{ext}\\\\\\mathbf f_{ext}\\end{pmatrix} = J(\\bar X)^T\\mathbf f(\\bar X)\\] <p>and replace the force with \\(\\mathbf f_A(\\bar X_A) = -a\\mathbf n, \\mathbf f_B(\\bar X_A) = a\\mathbf n\\), we can have </p> \\[M\\dot{\\mathbf q}^{t+1} = M \\dot{\\mathbf q}^t + \\Delta t \\mathbf f^t + \\Delta t \\sum_{j=0}^{n_c-1} a_j J(\\mathbf y_j)^T \\hat {\\mathbf n}_j\\] <p>where  \\(M = \\begin{bmatrix}R\\mathcal I R^T &amp; \\mathbf 0\\\\\\mathbf 0 &amp;mI\\end{bmatrix}, \\dot{\\mathbf q} = \\begin{bmatrix}\\omega\\\\\\dot{\\mathbf p}\\end{bmatrix}\\), \\(\\mathbf f\\) is the external forces and torques other than the contact forces, and \\(\\hat{\\mathbf n} = \\begin{cases}-\\mathbf n &amp;\\text{Object A}\\\\\\mathbf n &amp;\\text{Object B}\\end{cases}\\). </p>"},{"location":"csc417/collision.html#constraints","title":"Constraints","text":"<p>Then, to find each \\(a_j\\), we constraint it with Signorini conditions, since rule 2 has been satisfied and we know that at time \\(t\\) the conditions must be satisfied, we can add constraints that \\(\\forall j\\)</p> \\[\\begin{align*} a_j&amp;\\geq 0\\\\ d(\\mathbf y_{jA}^t + \\Delta t \\dot{\\mathbf y}_{jA}^{t+1}, \\mathbf y_{jB}^t + \\Delta t \\dot{\\mathbf y}_{jB}^{t+1}) &amp;\\geq 0\\\\ d(\\mathbf y_{jA}^t + \\Delta t \\dot{\\mathbf y}_{jA}^{t+1}, \\mathbf y_{jB}^t + \\Delta t \\dot{\\mathbf y}_{jB}^{t+1}) &amp;\\perp a\\\\ \\end{align*}\\] <p>Then, the issue is that the signed distance is non-linear. However, note that the contact point can be viewed as infinitesimally small plane with normal \\(\\mathbf n\\), so that we can project the closest distance from \\(\\mathbf y_A\\) to the plane on \\(B\\) as \\(-\\Delta t \\mathbf n^T\\dot{\\mathbf y}_A^{t+1}\\) and similarly for \\(B\\) we have \\(\\Delta t \\mathbf n^T\\dot{\\mathbf y}_A^{t+1}\\) </p> \\[d(\\mathbf y_{jA}^t + \\Delta t \\dot{\\mathbf y}_{jA}^{t+1}, \\mathbf y_{jB}^t + \\Delta t \\dot{\\mathbf y}_{jB}^{t+1}) = \\Delta t\\mathbf n^T( \\dot{\\mathbf y}_{jB}^{t+1} -  \\dot{\\mathbf y}_{jA}^{t+1})\\] <p>and we can take away that \\(\\Delta t\\) since it won't impact the constraint. </p>"},{"location":"csc417/collision.html#multiple-collisions-on-multiple-rigid-bodies","title":"Multiple Collisions on Multiple Rigid Bodies","text":"<p>For a scene of multiple rigid bodies and multiple contacts, record the object ID \\(A, B\\), the contact point \\(\\mathbf y\\) and contact normal \\(\\mathbf n\\) (always points toward \\(B\\)), </p> <p>Note that for this collision, it will have impact on object \\(A, B\\) as </p> \\[\\begin{align*} M_A\\dot{\\mathbf q}^{t+1}_A &amp;= M_A \\dot{\\mathbf q}^t_A + \\Delta t \\mathbf f^t_A + \\Delta t \\sum_{j\\neq i}^{n_{Ac}-1} a_j J(\\mathbf y_j)^T \\hat {\\mathbf n}_j - \\Delta t a_iJ(\\mathbf y_i)^T{\\mathbf n}_i\\\\ M_B\\dot{\\mathbf q}^{t+1}_B &amp;= M_B \\dot{\\mathbf q}^t_B + \\Delta t \\mathbf f^t_B + \\Delta t \\sum_{j\\neq i}^{n_{Bc}-1} a_j J(\\mathbf y_j)^T \\hat {\\mathbf n}_j + \\Delta t a_iJ(\\mathbf y_i)^T{\\mathbf n}_i \\end{align*}\\] <p>Then, we simplify the notation by taking \\(\\mathbf f_i^A =\\sum_{j\\neq i}^{n_{Ac}-1} a_j J(\\mathbf y_j)^T \\hat {\\mathbf n}_j\\) so that the equations above becomes</p> \\[\\begin{align*} \\dot{\\mathbf q}^{t+1}_A &amp;= \\dot{\\mathbf q}^t_A + \\Delta t M_A^{-1}\\mathbf f^t_A + \\Delta t M_A^{-1}\\mathbf f_i^A - \\Delta t a_iJ(\\mathbf y_i)^T{\\mathbf n}_i\\\\ \\dot{\\mathbf q}^{t+1}_A &amp;= \\dot{\\mathbf q}_A^{t+1*} + \\Delta t M_A^{-1}\\mathbf f_i^A - \\Delta t a_iJ(\\mathbf y_i)^T{\\mathbf n}_i\\\\ \\dot{\\mathbf q}^{t+1}_B &amp;= \\dot{\\mathbf q}_B^{t+1*} + \\Delta t M_B^{-1}\\mathbf f_i^B + \\Delta t a_iJ(\\mathbf y_i)^T{\\mathbf n}_i \\end{align*}\\] <p>where \\(\\dot{\\mathbf q}_A^{t+1*}\\) is the unconstrained velocity as computed from rigid bodies mechanics, without any collisions. </p> <p>Then, since \\(\\dot{\\mathbf y}_A\\) is the world space velocity,  \\(\\dot{\\mathbf y}_{Ai} = J_A(\\mathbf y_i)\\dot{\\mathbf q}_A^{t+1}\\) and the constraints becomes </p> \\[\\begin{align*} a_j&amp;\\geq 0\\\\ \\mathbf n_i^T(J_B(\\mathbf y_i)\\dot{\\mathbf q}_B^{t+1} - J_A(\\mathbf y_i)\\dot{\\mathbf q}_A^{t+1}) &amp;\\geq 0\\\\ \\mathbf n_i^T(J_B(\\mathbf y_i)\\dot{\\mathbf q}_B^{t+1} - J_A(\\mathbf y_i)\\dot{\\mathbf q}_A^{t+1}) &amp;\\perp a_i \\end{align*}\\] <p>We can then substitute \\(\\dot{\\mathbf q}_B^{t+1}, \\dot{\\mathbf q}_A^{t+1}\\) with the equations above. </p> <p>Write \\(\\mathbf g_i^A = -J(\\mathbf y_i)^T\\mathbf n_i, \\mathbf g_i^B = J(\\mathbf y_i)^T\\mathbf n_i\\), the substitution yields </p> \\[\\begin{align*} \\delta_i &amp;= \\Delta t\\big((\\mathbf g_i^B)^T M_B^{-1} \\mathbf g_i^B + (\\mathbf g_i^A)^T M_A^{-1} \\mathbf g_i^A\\big)\\\\ \\gamma_i &amp;= (\\mathbf g_i^B)^T(\\dot{\\mathbf q}^{t+1*}_B+ \\Delta t M_B^{-1} \\mathbf f_i^B) + (\\mathbf g_i^A)^T(\\dot{\\mathbf q}^{t+1*}_A+ \\Delta t M_A^{-1} \\mathbf f_i^A)\\\\ \\delta_i a_i + \\gamma_i &amp;= \\mathbf n_i^T(J_B(\\mathbf y_i)\\dot{\\mathbf q}_B^{t+1} - J_A(\\mathbf y_i)\\dot{\\mathbf q}_A^{t+1}) \\end{align*}\\] <p>and finally the problem becomes</p> \\[\\begin{align*} \\forall i \\in \\{0,...,n_c-1\\}\\\\ a_i\\geq 0\\\\ \\delta_i a_i + \\gamma_i \\geq 0\\\\ \\delta_i a_i + \\gamma_i \\perp a_i \\end{align*}\\] <p>which yields \\(a_i = \\max(-\\gamma_i/\\delta_i, 0)\\).</p>"},{"location":"csc417/collision.html#projected-gauss-seidel","title":"Projected Gauss-Seidel","text":"<p>Note that each \\(\\gamma_i\\) involves \\(a_j\\) for \\(j\\neq i\\), hence solving all equations in parallel is hard. Instead, we use an iterative algorithm. </p> Projected Gauss-Seidel<pre><code>qdot = compute() # qdot for each rigid bodies without collisions\n\nk = 0 # iteration counter\na = [0, 0, ..., 0] # array of length n for each a_i\nconstraints = False\nwhile k &lt; MAX_ITER or not constraints:\n    k += 1\n    for i in range(n):\n        delta, gamma = equations above\n        a[i] = max(-gamma / delta, 0)\n    constraints = check_constraints(a)\n\nqdot += dt * sum([a[j] * Jacobian(y[j]) @ n_hat[j] for j in range(n)])\nq = compute() # from new qdot\n</code></pre>"},{"location":"csc417/collision.html#static-objects","title":"Static Objects","text":"<p>Sometimes we have static objects in the scene, for example the static ground which should not move. In this case, we can have its mass being infinitely large. Hence, its inverse mass is 0. Then, starting with 0 velocity, the object cannot move and contribute to any part of the collision response. </p>"},{"location":"csc417/fem.html","title":"Finite Element Method","text":""},{"location":"csc417/fem.html#motivation","title":"Motivation","text":"<p>A mass spring system is just a bunch of springs and nodes, which leaves very much \"empty\" space in the space. For some complex animation simulations, we need to find a better representation to \"fill in\" the space. We are interested in the continuum mechanics (continuum: the hypothesis that the material is uniformly the same across all points). </p>"},{"location":"csc417/fem.html#volume-by-tetrahedral","title":"Volume by Tetrahedral","text":"<p>A tetrahedral is a 3D object by connecting 4 vertices \\(x_0(t), x_1(t), x_2(t), x_3(t)\\), and we can have generalized coordinates \\(\\mathbf q=(x_0, x_1, x_2, x_3)^T\\) and \\(\\dot{\\mathbf q} = (\\dot{x_0}, \\dot{x_1}, \\dot{x_2}, \\dot{x_3})^T = (v_0,v_1,v_2,v_3)^T\\). </p>"},{"location":"csc417/fem.html#finite-elements","title":"Finite Elements","text":"<p>Let \\(\\mathbf{X_0},...,\\mathbf{X_3}\\) be the fixed position of the 4 vertices of a tetrahedral \\(T\\), \\(f\\) be some unknown function with known \\(f(\\mathbf{X_i}), i=0,...,3\\). We want to find \\(f\\) for every point in \\(T\\). We can have</p> \\[f(\\mathbf{X}) = \\sum_{i=0}^3 f(\\mathbf{X_i})\\phi_i(\\mathbf{X})\\] <p>where \\(\\phi_i\\) is the weight function, or shape function, that associate with each vertices so that \\(f(\\mathbf{X})\\) is a weighted sum of values from the 4 vertices. </p> <p>Consider some wanted properties of the equation above, let \\(f = id\\)</p> \\[\\mathbf{X} = \\sum^3_{i=0} \\mathbf{X_i}\\phi_i(\\mathbf{X})\\] <p>Note that \\(\\mathbf{X}\\in\\mathbb R^3\\), so that we currently have \\(3\\) equations and \\(4\\) unknowns \\(\\phi_0,...,\\phi_3\\). However, note that we want this act as a weighted average, so we can add one more constraint \\(\\phi_0 + ... + \\phi_3 = 1\\). This gives us \\(4\\) equations, and we can simply replace \\(\\phi_0\\) with \\(1-\\phi_1-\\phi_2-\\phi_3\\), resulting in the final system</p> \\[\\begin{pmatrix}\\Delta X\\\\\\Delta Y\\\\\\Delta Z\\end{pmatrix} =  \\begin{pmatrix}\\Delta X_1&amp;\\Delta X_2&amp;\\Delta X_3\\\\ \\Delta Y_1&amp;\\Delta Y_2&amp;\\Delta Y_3\\\\ \\Delta Z_1&amp;\\Delta Z_2&amp;\\Delta Z_3 \\end{pmatrix}  \\begin{pmatrix}\\phi_1(X)\\\\\\phi_2(X)\\\\\\phi_3(X)\\end{pmatrix}\\] <p>where \\(\\Delta X = X - X_0\\). We can think of shotting 3 rays from \\(X_0\\) and use this to construct the barycentric coordinates. Finally, we have </p> \\[\\begin{pmatrix}\\phi_1(X)\\\\\\phi_2(X)\\\\\\phi_3(X)\\end{pmatrix} = T^{-1}(\\mathbf{X}-\\mathbf{X_0}), \\phi_0 = 1-\\phi_1-\\phi_2-\\phi_3\\]"},{"location":"csc417/fem.html#deformation","title":"Deformation","text":"<p>Let \\(\\mathbf{X_0}, ..., \\mathbf{X_3}\\) be tetrahedral in the reference (undeformed) space and \\(x_0(t),...,x_3(t)\\) be the tetrahedral in the world (deformed) space. Therefore, with the \\(\\phi\\)'s found above, we have the deformed point everywhere in the tetrahedral as </p> \\[x(\\mathbf X) = \\sum_{i=0}^3 x_i(t) \\phi_i(\\mathbf X) = \\underset{N(\\mathbf X)}{\\begin{pmatrix}\\phi_0I&amp;\\phi_1I&amp;\\phi_2I&amp;\\phi_3I\\end{pmatrix}}\\underset{q(t)}{\\begin{pmatrix}x_0\\\\x_1\\\\x_2\\\\x_3\\end{pmatrix}}\\]"},{"location":"csc417/fem.html#kinetic-energy","title":"Kinetic Energy","text":""},{"location":"csc417/fem.html#per-tetrahedron","title":"Per tetrahedron","text":"<p>The kinetic energy of a tetrahedron is simply the integral of all points</p> \\[\\frac12 \\int_{T}\\rho\\|v(\\mathbf X)\\|_2^2 dT = \\frac12 \\int_{T}\\rho v^Tv dT\\] <p>where \\(\\rho\\) is the density of the material.  </p> <p>Then, since the shape function is irrelavent to \\(\\mathbf q\\), </p> \\[v(X) = \\dot x(X) = N(X)\\dot{\\mathbf q}\\] <p>Then, the integral becomes </p> \\[\\frac12 \\int_{T}\\rho  \\dot{\\mathbf q}^TN(X)^T N(X)\\dot{\\mathbf q} dT = \\frac 12\\dot{\\mathbf q}^T \\bigg(\\int_{T}\\rho  N(X)^T N(X) dT \\bigg)\\dot{\\mathbf q}\\] <p>We can all this integral inside \\(M_0\\), i.e. the mass matrix. We can expand the mass matrix as </p> \\[\\int_T\\rho \\begin{bmatrix}\\phi_0\\phi_0I &amp;\\cdots &amp;\\phi_0\\phi_3I\\\\\\vdots&amp;\\ddots&amp;\\vdots\\\\\\phi_3\\phi_0I&amp;\\cdots&amp;\\phi_3\\phi_3I\\end{bmatrix}dT\\] <p>and we can evaluate each term separately from the barycentric coordinates, for each element </p> \\[\\rho \\int_T \\phi_r(\\mathbf X)\\phi_x(\\mathbf X)dT I = 6\\rho \\cdot vol\\cdot\\int_0^1\\int_0^{1-\\phi_1}\\int_0^{1-\\phi_1-\\phi_2}(\\phi_r\\phi_s)d\\phi_3\\phi_2\\phi_1\\]"},{"location":"csc417/fem.html#the-full-object","title":"The Full Object","text":"<p>For per tetrahedron kinetic energy we have </p> \\[T_j = \\frac12 \\dot{\\mathbf q_j}^TM_j\\dot{\\mathbf q}_j\\] <p>And we have the generalized coordinates and velocity of the object as the stacking of all the vertices positions and velocities, i.e. \\(\\mathbf q = (x_0,...,x_n)^T\\), so that we can use the selection matrix to get each \\(\\mathbf{q}_j\\) from \\(\\mathbf q\\), and the total kinetic energy is</p> \\[T =  \\sum_0^{j-1}\\frac12 \\dot{\\mathbf q}^TE_j^TM_jE_j\\dot{\\mathbf q} = \\frac12 \\dot{\\mathbf q}^T\\big(\\sum_0^{j-1}E_j^TM_jE_j\\big)\\dot{\\mathbf q} = \\frac12 \\dot{\\mathbf q}^T M\\dot{\\mathbf q}\\]"},{"location":"csc417/fem.html#potential-energy","title":"Potential Energy","text":""},{"location":"csc417/fem.html#per-pair-points","title":"Per pair points","text":"<p>Note that we have the reference space positions and world space, and the corresponding positions of each point. Arbitrarily pick \\(\\mathbf X_0, \\mathbf X_1\\) from the reference space, let \\(\\Delta \\mathbf X = \\mathbf X_1 - \\mathbf X_0\\). Then, its corresponding points and vector are \\(x(\\mathbf X_0)\\) and \\(x(\\mathbf X_1)\\), so that </p> \\[\\begin{align*}\\Delta \\mathbf x &amp;= x(\\mathbf X_1) - x(\\mathbf X_0) \\\\ &amp;= x(\\mathbf X_0 + \\Delta \\mathbf X) - x(\\mathbf X_0)\\\\ &amp;\\approx x(\\mathbf X_0) + \\frac{\\partial x}{\\partial \\mathbf X}\\Delta \\mathbf X - x(\\mathbf X_0)&amp;\\text{Taylor expansion}\\\\ &amp;= \\frac{\\partial x}{\\partial \\mathbf X}\\Delta \\mathbf X\\\\ &amp;= F\\Delta \\mathbf X &amp;\\text{call deformation gradient }F \\end{align*}\\] <p>The strain between the points is represented by \\(l^2 - l^2_0\\),</p> \\[\\begin{align*} l^2 - l^2_0 &amp;= \\Delta x^T\\Delta x  - \\Delta \\mathbf X^T\\Delta \\mathbf X \\\\ &amp;= \\Delta \\mathbf X^TF^TF\\Delta \\mathbf X - \\Delta \\mathbf X^T\\Delta \\mathbf X\\\\ &amp;= \\Delta \\mathbf X^T(F^TF - I)\\Delta \\mathbf X \\end{align*}\\] <p>We call \\(F^TF\\) the Right Cauchy Green Deformation, and \\((F^TF - I)\\) Green Lagrange Strain. </p>"},{"location":"csc417/fem.html#per-tetrahedron_1","title":"Per tetrahedron","text":"<p>Like \\(\\psi\\) be a strain energy density function, which maps the strain to the potential energy. Then, we can integrate over the points as </p> \\[\\int_T \\psi(F(\\mathbf X))dT\\] <p>Then, let's consider \\(F(\\mathbf X)\\), note that </p> \\[\\begin{align*} x(\\mathbf X) &amp;= N(\\mathbf X)\\mathbf q(t)\\\\ &amp;= \\mathbf x_0 + \\begin{pmatrix}\\mathbf x_0&amp;\\mathbf x_1&amp;\\mathbf x_2&amp;\\mathbf x_3\\end{pmatrix} \\begin{pmatrix}\\begin{pmatrix}-1&amp;-1&amp;-1\\end{pmatrix} \\cdot T^{-1}\\\\T{-1}\\end{pmatrix} (\\mathbf X - \\mathbf X_0)\\\\ F = \\frac{\\partial x}{\\partial \\mathbf X} &amp;= \\begin{pmatrix}\\mathbf x_0&amp;\\mathbf x_1&amp;\\mathbf x_2&amp;\\mathbf x_3\\end{pmatrix} \\begin{pmatrix}\\begin{pmatrix}-1&amp;-1&amp;-1\\end{pmatrix} \\cdot T^{-1}\\\\T^{-1}\\end{pmatrix} \\end{align*}\\] <p>Note that \\(F\\) is constant, so that the integral simply evaluate as </p> \\[\\int_T \\psi(F(\\mathbf X))dT = vol \\cdot \\psi(F_0)\\]"},{"location":"csc417/fem.html#the-full-object_1","title":"The Full Object","text":"<p>Then, simply summing over all tetrahedrons with the usage of selection matrix</p> \\[V = \\sum_{j=0}^{m-1}vol_j \\cdot \\psi(F_j(E_j\\mathbf q))\\]"},{"location":"csc417/fem.html#the-lagrangian","title":"The Lagrangian","text":"<p>Then, plug in \\(L = T - V\\) and apply Euler-Lagrange Equation \\(\\frac{d}{dt}\\frac{\\partial L}{\\partial \\dot{\\mathbf q}} = - \\frac{\\partial V}{\\partial \\mathbf q}\\), we finally arrives </p> \\[M\\ddot{\\mathbf q} = -\\frac{\\partial V}{\\partial \\mathbf q}\\] <p>Then consider $ -\\frac{\\partial V}{\\partial \\mathbf q} = -\\sum_{j=0}^{m-1}vol_j \\cdot \\frac{\\partial}{\\partial \\mathbf q}\\psi(F_j(E_j\\mathbf q))$, this differentiation is tricky due to \\(F\\) being a matrix. We can flatten \\(F\\) to a vectorized deformation gradient. For example, a row-wise flatten will have</p> \\[\\begin{align*} F &amp;= \\begin{pmatrix} \\partial_X x &amp;\\partial_Y x&amp;\\partial_Z x\\\\ \\partial_X y &amp;\\partial_Y y&amp;\\partial_Z y\\\\ \\partial_X z &amp;\\partial_Y z&amp;\\partial_Z z \\end{pmatrix}\\\\\\Rightarrow  \\vec F_{9\\times 1} &amp;= \\begin{pmatrix} \\partial_X x &amp;\\partial_Y x&amp;\\partial_Z x&amp; \\partial_X y &amp;\\partial_Y y&amp;\\partial_Z y&amp; \\partial_X z &amp;\\partial_Y z&amp;\\partial_Z z \\end{pmatrix}^T\\\\ D&amp;=\\begin{pmatrix}\\begin{pmatrix}-1&amp;-1&amp;-1\\end{pmatrix} \\cdot T^{-1}\\\\T{-1}\\end{pmatrix}\\\\ \\Rightarrow B_{9\\times 12} &amp;= \\begin{pmatrix} D_{0\\cdot}&amp;\\vec 0&amp;\\vec 0&amp;D_{1\\cdot}&amp;\\vec0&amp;\\vec0&amp;D_{2\\cdot}&amp;\\vec0&amp;\\vec0&amp;D_{3\\cdot}&amp;\\vec0&amp;\\vec0\\\\ \\vec0&amp;D_{0\\cdot}&amp;\\vec 0&amp;\\vec 0&amp;D_{1\\cdot}&amp;\\vec0&amp;\\vec0&amp;D_{2\\cdot}&amp;\\vec0&amp;\\vec0&amp;D_{3\\cdot}&amp;\\vec0\\\\ \\vec0&amp;\\vec0&amp;D_{0\\cdot}&amp;\\vec 0&amp;\\vec 0&amp;D_{1\\cdot}&amp;\\vec0&amp;\\vec0&amp;D_{2\\cdot}&amp;\\vec0&amp;\\vec0&amp;D_{3\\cdot}\\\\ \\end{pmatrix}\\\\ \\text{where }D_{i\\cdot} &amp;= \\begin{pmatrix}D_{i0} &amp;D_{i1}&amp;D_{i2}\\end{pmatrix}^T,\\vec 0= \\begin{pmatrix}0&amp;0&amp;0\\end{pmatrix}^T\\\\ \\vec {\\mathbf x}_{12\\times 1} &amp;= \\begin{pmatrix}\\mathbf x_0^T&amp;\\mathbf x_1^T&amp;\\mathbf x_2^T&amp;\\mathbf x_3^T\\end{pmatrix}^T\\\\ \\vec F &amp;= B \\vec{\\mathbf x} \\end{align*}\\] <p>Then, we can put the new dirivatives back, we have </p> \\[\\begin{align*}  -\\frac{\\partial V}{\\partial \\mathbf q} &amp;=  -\\sum_{j=0}^{m-1}vol_j \\cdot \\frac{\\partial}{\\partial \\mathbf q}\\psi(F_j(E_j\\mathbf q))\\\\ &amp;= -\\sum_{j=0}^{m-1}vol_j \\cdot \\frac{\\partial}{\\partial \\mathbf q}\\psi(B_jE_j\\mathbf q))\\\\ &amp;= -\\sum_{j=0}^{m-1}vol_j \\cdot E_j^TB_j^T\\frac{\\partial \\psi(F_j)}{\\partial F}\\\\ \\mathbf f_j&amp;= -vol_j \\cdot B_j^T\\frac{\\partial \\psi(F_j)}{\\partial F}\\\\ \\mathbf f &amp;= \\sum_0^{m-1}E_j^T\\mathbf f_j \\end{align*}\\]"},{"location":"csc417/fem.html#time-integration-using-backward-euler","title":"Time Integration Using Backward Euler","text":"<p>Although we can still use linear implicit time integration, it's quite unstable in this case. Instead we will use backward Euler</p> \\[M\\dot{\\mathbf q}^{t+1} = M\\dot{\\mathbf q}^t + \\Delta t \\mathbf f(\\mathbf q^{t+1})\\] \\[\\mathbf q^{t+1} = \\mathbf q^t + \\Delta \\dot{\\mathbf q}^{t+1}\\] <p>Which is solving the equation </p> \\[M\\dot{\\mathbf q}^{t+1} - M\\dot{\\mathbf q}^t - \\Delta t \\mathbf f(\\mathbf q^t + \\Delta \\dot{\\mathbf q}^{t+1}) = 0\\] <p>However, this equation is non-linear and we need to do non-linear optimization. Define</p> \\[E(\\mathbf v) = \\frac 12 (\\mathbf v - \\dot{\\mathbf q}^t)^TM(\\mathbf v - \\mathbf q^t) + V(\\mathbf q^t + \\Delta t \\mathbf v)\\] <p>Note that \\(\\partial_{\\mathbf v} E = M\\dot{\\mathbf q}^{t+1} - M\\dot{\\mathbf q}^t - \\Delta t \\mathbf f(\\mathbf q^t + \\Delta \\dot{\\mathbf q}^{t+1})\\), so that when \\(E\\) is at minimum, we can have \\(\\partial E = 0\\). </p>"},{"location":"csc417/fem.html#newtons-method","title":"Newton's Method","text":"<p>(More derivations in APM462 or CSC336 notes)</p> <p>Newton's Method is an iterative algorithm as follows</p> <p><pre><code>def netwon_method(v0):\n    v = v0\n    while grad(E(v)).norm() &lt; tolerance:\n        solve Hessian(E(v)) d = - grad(E(v))\n        a = [choose a]\n        v = v + a * d\n    return v\n</code></pre> with</p> \\[grad = \\frac{\\partial E}{\\partial \\mathbf v} = M(\\mathbf v - \\dot{\\mathbf q}^t) + \\Delta t \\frac{\\partial V}{\\partial \\mathbf q}\\mid_{\\mathbf q^t + \\Delta t \\mathbf v}\\] \\[Hessian = \\frac{\\partial^2 E}{\\partial \\mathbf v^2} = M + \\Delta t^2 \\frac{\\partial^2 V}{\\partial \\mathbf q^2}\\mid_{\\mathbf q^t + \\Delta t \\mathbf v}\\]"},{"location":"csc417/fem.html#line-search","title":"Line Search","text":"<p>Note the <code>[choose a]</code> part in Newton's Method, how do we know a \\(\\alpha\\) is appropriate, which means not to small for a update and not to large that gets out of the decreasing gradient. Therefore, we can set a target value as \\(E(v) + c d^T g\\), by MVT, this will be larger than the optimum we want but smaller than \\(E(v)\\). Then, if we can find some update that's smaller than this, then we made sufficient progress. Therefore, the algorithm is</p> <pre><code>def line_search(v, d, g):\n    a = a_max #an initial guess\n    while E(v + ad) &gt; E(v) + c * d.T.dot(g) and a &gt;= tolerance:\n        a = p * a #choose some p &lt; 1, so that a is reduced\n    return a\n</code></pre>"},{"location":"csc417/fem.html#skinning","title":"Skinning","text":"<p>Note that tetrahedral mesh for the simulation is much more computation expensive. We cannot use a very fine mesh for the simulation. Instead, we will use a \"skinning\" triangle mesh. </p> <p>We start with the precondition that each vertex of the skinning mesh will be contained in the simulation mesh. Then, note that for each vertex in the skinning mesh, it is within some tetrahedron, which we can map it to the 4 vertices with barycentric coordinates. For example, for triangle \\(X_A, X_B, X_C\\) in the reference space, its position in the reformed space is </p> \\[\\begin{pmatrix}x(X_A)\\\\x(X_B)\\\\x(X_C)\\end{pmatrix} =  \\begin{pmatrix} \\phi_0(X_A)I &amp;\\phi_1(X_A)I &amp;\\phi_2(X_A)I &amp;\\phi_3(X_A)I \\\\ \\phi_0(X_B)I &amp;\\phi_1(X_B)I &amp;\\phi_2(X_B)I &amp;\\phi_3(X_B)I \\\\ \\phi_0(X_C)I &amp;\\phi_1(X_C)I &amp;\\phi_2(X_C)I &amp;\\phi_3(X_C)I  \\end{pmatrix}\\begin{pmatrix}x_0\\\\x_1\\\\x_2\\\\x_3\\end{pmatrix} \\] <p>With the generalized coordinates \\(q\\), we can then construct the skinning weights \\(W\\) or dimension \\(\\|V_{\\text{skin}} \\|\\times \\|V_{\\text{sim}}|\\) so that \\(\\mathbf x_{\\text{surface}} = W \\mathbf q\\)</p>"},{"location":"csc417/fluids.html","title":"Fluid Simulation using SPH and FLIP","text":""},{"location":"csc417/fluids.html#introduction","title":"Introduction","text":"<p>Fluid, such as gas and water, plays an important role in animations. Although its flows look simple, simulation of fluid is much more complex than that of solid. The motion of fluid is governed by the incompressible Navier-Stokes equations, and various methods have been developed to simplify and solve the system.  </p> <p>Two approaches are often taken to measure the motion of fluid. Lagragian approach treats the fluid as a set of particles, while Eulerian approach treats the fluid space as a grid field and measures quantities at each fixed point. </p> <p>In this report, we will present the particle based SPH method and particle-grid hybrid FLIP method. In SPH method, we view each particle as a blob of fluid with constant mass and varying volume. Similar to the finite element approach, for each particle position, we accumulate its density and forces by a weighted sum of particles within a radius, where the weights comes from smoothing kernels. We would then use the accumulated force to update the velocity of the particle. In FLIP method, we view each particle as a blob of fluid with constant mass and volume. We would evenly divide the space into fixed-size volumes using a staggered-grid system, and project particles' velocities onto velocity-grids of that staggered-grid system. We will then use the projected velocities of particles to calculate the pressure within each cell of the pressure-grid, and use that pressure to update each particle's own velocities. In both methods, we would use updated velocities of particles to update particles' positions.</p>"},{"location":"csc417/fluids.html#related-work","title":"Related Work","text":"<p>Due to the complexity of fluid simulation, many works aim to build an efficient, stable, and visually appealing system for fluid simulation. For example, Stam's Stable Fluids [Stam 1999]<sup>1</sup> is one of the pioneer work for real-time fluid simulation. In addition, many works focus on specific phenomenon, such as ocean waves [Hinsinger et al. 2002]<sup>2</sup> and  lava drops [Stora et al. 1999]<sup>3</sup>. </p>"},{"location":"csc417/fluids.html#problem-setup","title":"Problem Setup","text":"<p>For an arbitrary fluid, the incompressible Navier-Stokes equation describes the motion [Bridson and M\u00fcller-Fischer 2007]<sup>4</sup> as </p> \\[\\begin{align*} \\rho \\frac{\\partial \\mathbf{v}}{\\partial t} + \\rho \\nabla \\mathbf{v}\\cdot \\mathbf{v} &amp;= -\\nabla p + \\rho \\mathbf{g} + \\mu\\nabla^2\\mathbf{v}\\\\ \\text{subjected to }\\nabla \\cdot \\mathbf{v} &amp;= 0 \\end{align*}\\] <p>where \\(\\rho\\) is the density, \\(\\mathbf{v}\\) is the velocity, \\(\\mathbf{g}\\) is an external force, \\(p\\) is the pressure, and \\(\\mu\\) is the viscosity coefficient of the fluid.</p> <p>For a system of fluid, we represent it with a finite number of particles \\(\\mathcal P = \\{P_1,..., P_n\\}\\). For each particle \\(P_i\\), we store its quantities such as velocity \\(\\mathbf{v}_i\\) and  position \\(\\mathbf{p}_i\\).  </p>"},{"location":"csc417/fluids.html#sph-method","title":"SPH Method","text":"<p>Instead of the incompressible condition as equation (2), SPH method assures conservation of mass </p> \\[\\frac{\\partial{\\rho}}{\\partial{t}}  + \\nabla\\dot(\\rho t) = 0\\] <p>by assigning constant mass \\(m\\) to each particle.  Then, we can split the forces in equation (1) into pressure \\(-\\nabla p\\), external forces \\(\\rho \\mathbf{g}\\), and viscosity \\(\\mu\\nabla^2\\mathbf{v}\\). Finally, as particles are moving with the fluid, we can update the particle's velocity via self-advection. For each particle, we have</p> \\[\\frac{D \\mathbf v_i}{D t} = \\frac{1}{\\rho_i} (-\\nabla p_i + \\rho_i\\mathbf{g} + \\mu\\nabla^2\\mathbf v_i)\\] <p>To update density of each particle and forces acting on each particle, we use Smoothed-Particle Hydrodynamics, which interpolates the quantities of \\(A\\) at some location \\(\\mathbf p\\) as a weighted sum of all particles </p> \\[A(\\mathbf{ p}) = \\sum_{j=1}^n m_j\\frac{A_j}{\\rho_j}W(\\mathbf p - \\mathbf{p}_j) = m\\sum_{j=1}^n\\frac{A_j}{\\rho_j}W(\\mathbf p - \\mathbf{p}_j)\\] <p>where \\(W\\) is called the smoothing kernel, and the desired properties for \\(W\\) are being even (\\(W(\\mathbf{r}) = W(-\\mathbf r)\\)) and normalized (\\(\\int W(\\mathbf r)d\\mathbf{r} = 1\\)). An appealing property for SPH is that the quantities are functions of the position, hence taking derivative is simply taking derivative on the smoothing kernel. In addition, to reduce the computation, we may only weigh the neighborhood of the target location, which means \\(W(\\mathbf p - \\mathbf p_j) = 0\\) if \\(\\|\\mathbf p - \\mathbf p_j\\| &gt; h\\) for some support radius \\(h\\). In this paper, 3 smoothing kernels are proposed for density, pressure, and viscosity. </p>"},{"location":"csc417/fluids.html#density","title":"Density","text":"<p>Using equation (5), the density update is </p> \\[\\rho_i = m\\sum_{j}\\frac{\\rho_j}{\\rho_j}W(\\mathbf p_i - \\mathbf p_j) = m\\sum_{j}W_{\\text{poly6}}(\\mathbf p_i - \\mathbf p_j)\\] <p>with the density smoothing kernel</p> \\[W_{\\text{poly6}}(\\mathbf r) = \\frac{315}{64\\pi h^9}(h^2-\\|\\mathbf r\\|^2)^3\\]"},{"location":"csc417/fluids.html#pressure","title":"Pressure","text":"<p>Instead of directly using equation (5), the pressure update is</p> \\[-\\nabla p_i = -m\\sum_j \\frac{p_i +  p_j}{2\\rho_j}\\nabla W_{\\text{spikey}}(\\mathbf p_i-\\mathbf p_j)\\] <p>we use the mean pressure of two particles to keep it symmetric. Consider two particles interacting with each other, this equation makes sure they get the same amount of pressure. The smoothing kernel is </p> \\[W_{\\text{spikey}}(\\mathbf r) = \\frac{15}{\\pi h^6}(h-\\|\\mathbf r\\|)^3\\] <p>Because the pressure is not carried on each particle, we compute it using modified ideal gas state equation</p> \\[p_i = k(\\rho_i - \\rho_0)\\] <p>where \\(k\\) is the gas constant, \\(\\rho_0\\) is the rest density for the fluid. Since we only care about the offset of pressure from each direction, removing the constant rest density from all directions have no effect on the simulation and improves numerical stability.</p>"},{"location":"csc417/fluids.html#viscosity","title":"Viscosity","text":"<p>Since viscosity only depends on velocity differences, the update is </p> \\[\\mu\\nabla^2 \\mathbf v_i = \\mu m \\sum_j \\frac{\\mathbf v_j - \\mathbf v_i}{\\rho_i}\\nabla^2W_{\\text{visco}}(\\mathbf p_i-\\mathbf p_j)\\] <p>with smoothing kernel</p> \\[W_{\\text{visco}}(\\mathbf r) = \\frac{15}{2\\pi h^3}(-\\frac{\\|r\\|^3}{h^3} + \\frac{\\|r\\|^2}{h^2} + \\frac{h}{2\\|r\\|} - 1)\\]"},{"location":"csc417/fluids.html#surface-tension","title":"Surface Tension","text":"<p>For simulating liquid, we often model air as void. Then, surface tension, or the attractive forces pulling liquid molecules to each other, creates the imbalance on the liquid's surface. To model such force, we add a smooth color field as</p> \\[c_i = m\\sum_j \\rho_j^{-1}W(\\mathbf{p}_i - \\mathbf{p}_j)\\] <p>so that we can define its gradient field \\(\\mathbf n = \\nabla c\\) and curvature as \\(\\kappa = \\frac{\\nabla^2 c}{\\|\\mathbf n\\|}\\), and the surface tension is </p> \\[\\sigma\\kappa\\mathbf{n} = \\sigma \\frac{\\nabla^2 c \\cdot \\nabla c}{\\|\\nabla c\\|}\\]"},{"location":"csc417/fluids.html#external-forces","title":"External forces","text":"<p>For external forces such as gravity and collision forces, we consider each particle as a mass and directly apply the forces onto them. For the collision response, we simply push them away from the object and reflect its velocity around the collision normal.</p>"},{"location":"csc417/fluids.html#neighboring-search","title":"Neighboring Search","text":"<p>Note that we only need to sum up the neighboring particles within a certain radius \\(h\\), a natural optimization will be dividing the space into grid of \\(h\\), then for each particle, we only need to search for the 27 cubes. </p>"},{"location":"csc417/fluids.html#flip-method","title":"FLIP Method","text":"<p>We would first simplify the incompressible Navier-Stokes equation by removing the viscosity term, and the simplified equation looks like this:</p> \\[\\rho \\frac{\\partial \\mathbf{v}}{\\partial t} + \\rho \\nabla \\mathbf{v}\\cdot \\mathbf{v} = -\\nabla p + \\rho \\mathbf{g} \\text{ subjected to }\\nabla \\cdot \\mathbf{v} = 0\\] <p>FLIP method be divided into three parts in general to handle three different effects: advection, external forces, and pressure.</p> <p>The first two steps of the FLIP method are very simple. At each time step, we would first update each particle's position based on its current velocity, and then we would update each particle's velocity by considering the how external force, such as the gravitational force, influences particles. </p> <p>The first step takes care of the effect of advection, which states that:</p> \\[\\frac{\\partial \\mathbf{v}}{\\partial t} + \\nabla \\mathbf{v}\\cdot \\mathbf{v} = 0\\] <p>The second step takes care of the effect of external forces, which states that:</p> \\[\\rho \\frac{\\partial \\mathbf{v}}{\\partial t} = \\rho \\mathbf{g}\\] <p>The third and the most important step is to calculate pressure and use pressure to further update each particle's velocity, and this step takes care of the effect of pressure, which strates that: </p> \\[\\rho \\frac{\\partial \\mathbf{v}}{\\partial t} = -\\nabla p \\text{ subjected to }\\nabla \\cdot \\mathbf{v} = 0\\] <p>We will break this step down into multiple parts and explain each part in detail.</p>"},{"location":"csc417/fluids.html#velocity-projection","title":"Velocity projection","text":"<p>The first part is to build a three-dimensional staggered-grid system. This staggered-grid system consists of four separate three-dimensional grids. Three velocity-grids of the staggered-grid system are used to handle \\(x\\), \\(y\\), and \\(z\\) component of velocity individually, and the remaining pressure-grid is used to handle the pressure. For each of the particles and for each of the three components of the velocity, we would first find the cell of the velocity-grid that contains that particle based on that particle's position, and then project that particle\u2019s corresponding velocity component onto eight corners of that cell using trilinear interpolation.</p>"},{"location":"csc417/fluids.html#solving-pressure","title":"Solving pressure","text":"<p>We would discretize equation (17) to solve for pressure, and the resulting equation for calculating pressure looks like this: </p> \\[\\nabla\\cdot\\nabla p = \\frac{\\rho}{\\Delta t}\\nabla\\cdot \\mathbf{v}^t\\] <p>Combining with the staggered-grid system and finite difference derivative technique, we could solve for pressure \\(p\\) and obtain the pressure-grid.</p>"},{"location":"csc417/fluids.html#updating-velocity-using-pressure","title":"Updating velocity using pressure","text":"<p>After obtaining pressure-grid, we could use this equation to update velocity-grids:</p> \\[\\mathbf{v}^{t+1} = \\mathbf{v}^{t} -  \\frac{\\Delta t}{\\rho}\\nabla p\\] <p>We would project the change in velocity, which is \\(-\\frac{\\Delta t}{\\rho}\\nabla p\\) back onto particles.</p>"},{"location":"csc417/fluids.html#solid-boundary-conditions","title":"Solid boundary conditions","text":"<p>There are two more boundary conditions we need to consider. The first one is the solid boundary condition, which means that the fluid could not flow through the solid wall. It is very simple for this constraint to hold: we just need to the change the velocities on the velocity-grids that are on the solid wall to zero. We would use those changed velocity-grids to calculate the pressure-grid, and use pressure-grid to update velocities on the velocity-grids that are inside the solid wall. The velocities on the velocity-grids that are on the solid wall have already been updated to 0, and the changes in velocities on the velocity-grids that are on the solid wall would be the negative projected particle velocity. After implementing the above parts, we could simulate smoke. However, to simulate fluid like water, we still have one more constraint to implement.</p>"},{"location":"csc417/fluids.html#free-surface-boundary-conditions","title":"Free surface boundary conditions","text":"<p>One important fact is that the air would not exert pressure on the water. To simulate water, we need to make sure that the pressure is zero on the water surface. To keep track of the water surface, we would mark each cell of the pressure-grid as fluid cell or air cell. We would initialize cells containing particles to \\(-1\\) and empty cells to \\(+1\\). Note that this would result in stair-step artifacts in fluid surface, so we then applied three-dimensional Gaussian convolution to smooth that scalar filed containing \\(-1\\) and \\(+1\\). After smoothing, the cells containing positive values become air cells, and the cells containing negative values become fluid cells. We would only calculate the pressure inside the fluid cell. When the fluid cell is adjacent to an air cell, we would first find the fluid surface by applying linear interpolation to the value inside the fluid cell and value inside the adjacent air cell to find the position of zero, which represents the fluid surface, we would then modify the pressure inside the fluid cell with ghost pressure to make sure that the pressure on the fluid surface is zero. In that way, the free surface boundary conditions would be satisfied, and we could simulate water.</p>"},{"location":"csc417/fluids.html#results","title":"Results","text":"<p>We implemented these methods using Eigen [Guennebaud et al. 2010]<sup>5</sup> and libigl [Jacobson et al. 2018]<sup>6</sup> on CPU with single threading. We simulated the scene of pouring water into a box with 5625 particles. For both methods, we can animate the scene in real time. The frame rate is quite steady for FLIP method, while in SPH method the frame rate had significant drops when particles are crowded together. In both cases, the scene successfully simulates the water flows and waves. </p>"},{"location":"csc417/fluids.html#conclusion","title":"Conclusion","text":"<p>In this paper, we introduce two particle-based methods for simulating fluids. SPH method uses a purely particle based approach, and use smoothing kernels to interpolate force components from neighboring particles. FLIP method uses particles to represent fluid blobs, and a staggered-grid system to directly solve pressure. By taking different approaches, the two methods have quite different characteristics. SPH, due to its purely particle based design, is easier to implement, SPH also provides more physical coefficients, hence can model a wide class of fluids. However, SPH method is not incompressible. On the other hand, FLIP simplifies the model by assuming incompressible, inviscid conditions. FLIP has better performance on simulating water. Computational complexity is also different for each of the two methods. For SPH method, there would be more computations if the particles are compressed within a small portion of the entire space, since we need to consider more particles to compute forces for each of the particles. However, for FLIP method, there would be more computations if particles are evenly spread across the entire space since in that case, we would have more fluid cells to consider when constructing the pressure grid.</p> <ol> <li> <p>Stam, J. 1999. Stable fluids. Proceedings of the 26th annual conference on computer graphics and interactive techniques, 121\u2013128.\u00a0\u21a9</p> </li> <li> <p>Hinsinger, D., Neyret, F., and Cani, M.-P. 2002. Interactive animation of ocean waves. Proceedings of the 2002 ACM SIGGRAPH/eurographics symposium on computer animation, 161\u2013166.\u00a0\u21a9</p> </li> <li> <p>Stora, D., Agliati, P.-O., Cani, M.-P., Neyret, F., and Gascuel, J.-D. 1999. Animating lava flows.\u00a0\u21a9</p> </li> <li> <p>Bridson, R. and M\u00fcller-Fischer, M. 2007. Fluid simulation: SIGGRAPH 2007 course notes video files associated with this course are available from the citation page. In: ACM SIGGRAPH 2007 courses. 1\u201381.\u00a0\u21a9</p> </li> <li> <p>Guennebaud, G., Jacob, B., et al. 2010. Eigen v3.\u00a0\u21a9</p> </li> <li> <p>Jacobson, A., Panozzo, D., et al. 2018. libigl: A simple C++ geometry processing library.\u00a0\u21a9</p> </li> </ol>"},{"location":"csc417/introduction.html","title":"Math and Physics Background","text":""},{"location":"csc417/introduction.html#position-velocity-and-acceleration","title":"Position, Velocity, and Acceleration","text":"<p>For a particle of constant mass \\(m\\), its position at time \\(t\\) is \\(\\mathbf x(t):\\mathbb R\\rightarrow \\mathbb R^3\\) (Assuming 3D space), its velocity \\(\\mathbf v(t) = \\frac{d\\mathbf x}{dt}(t)\\) and its acceleration \\(\\mathbf a(t) = \\frac{d\\mathbf v}{dt}(t) = \\frac{d^2\\mathbf x}{dt^2}(t)\\)</p>"},{"location":"csc417/introduction.html#newtons-law","title":"Newton's Law","text":"<ol> <li>Every object will remain at rest of in uniform motion in a straight line unless compelled to change its state by the action of an external force</li> <li> <p>The force acting on ab object is equal to the time rate-of-change of the momentum </p> \\[f = \\underset{\\text{time rate of change}}{\\frac{d}{dt}}\\underset{\\text{momentum}}{m\\mathbf v}= m\\frac{d\\mathbf v}{dt} = m\\mathbf a\\] </li> <li> <p>For every action there is an equal and opposite reaction</p> </li> </ol>"},{"location":"csc417/introduction.html#variational-analytical-mechanics","title":"Variational (Analytical) Mechanics","text":"<p>Based on fundamental energies rather than two vectorial quantities</p> <ul> <li>Kinetic Energy energy due to motion</li> <li>Potential Energy energy \"held within\" an object due to position, internal stresses, charge, etc.  Potential energy has the potential to become kinetic energy</li> </ul>"},{"location":"csc417/introduction.html#problem-settings","title":"Problem Settings","text":"<p>Let the image of \\(\\mathbf f(t)\\) be the path of the motion, setting some functional </p> \\[e(\\mathbf f(t), \\dot{\\mathbf f}(t), ...) \\rightarrow \\mathbb R\\] <p>and then solve (optimize) the variational derivative to find the proper \\(\\mathbf f\\)</p>"},{"location":"csc417/introduction.html#generalized-coordinates","title":"Generalized Coordinates","text":"<p>Let the generalized coordinates be \\(\\mathbf q: \\mathbb R \\rightarrow \\mathbb R^n\\) be the actual variable parametrize the motion</p> \\[\\mathbf x(t) = \\mathbf f(\\mathbf q(t))\\] \\[\\frac{d\\mathbf x}{dt}(t) = \\underset{\\text{Jacobian}}{\\frac{d\\mathbf f}{d\\mathbf q}}\\underset{\\text{generalized velocity}}{\\dot q(t)}\\]"},{"location":"csc417/introduction.html#lagrangian","title":"Lagrangian","text":"<p>define the Lagrangian be</p> \\[L = T - V = \\text{kinetic} - \\text{potential}\\]"},{"location":"csc417/introduction.html#the-principle-of-least-action","title":"The Principle of Least Action","text":"<p>Assume the end points are known, to find the path between them by finding a stationary point of the action. </p>"},{"location":"csc417/introduction.html#action","title":"Action","text":"<p>For constant time \\(t_1, t_2\\), the action is a functional equals the integral of the Lagrangian over </p> \\[S(\\mathbf q(t), \\dot{\\mathbf q}(t)) = \\int_{t_1}^{t_2} T(\\mathbf q(t), \\dot{\\mathbf q}(t)) - V(\\mathbf q(t), \\dot{\\mathbf q}(t)) = dt = \\int_{t_1}^{t_2}L(\\mathbf q(t), \\dot{\\mathbf q}(t))dt\\]"},{"location":"csc417/introduction.html#stationary-point","title":"Stationary Point","text":"\\[S(\\mathbf q + \\delta\\mathbf q, \\dot{\\mathbf q}+\\delta \\dot{\\mathbf q}) = S(\\mathbf q(t), \\dot{\\mathbf q}(t))\\] <p>which means that perturbation \\(\\delta\\) of the trajectory \\(\\mathbf q\\) does not change the action.</p>"},{"location":"csc417/introduction.html#the-calculus-of-variations-euler-lagrange-equations","title":"The Calculus of Variations (Euler Lagrange Equations)","text":"\\[\\begin{align*} S(\\mathbf q + \\delta\\mathbf q, \\dot{\\mathbf q}+\\delta \\dot{\\mathbf q}) &amp;=\\int_{t_1}^{t_2}L(\\mathbf q + \\delta\\mathbf q, \\dot{\\mathbf q}+\\delta \\dot{\\mathbf q})dt\\\\ &amp;\\approx \\int_{t_1}^{t_2}L(\\mathbf q, \\dot{\\mathbf q})dt + \\int_{t_1}^{t_2}\\frac{\\partial L}{\\partial\\mathbf q}\\delta\\mathbf q + \\frac{\\partial L}{\\partial\\dot{\\mathbf q}}\\delta\\dot{\\mathbf q }dt\\\\ &amp;= S(\\mathbf q, \\dot{\\mathbf q}) + \\underset{\\text{first variation}}{\\delta S(\\mathbf q, \\dot{\\mathbf q})} \\end{align*}\\] <p>Therefore, principle of least action becomes </p> \\[\\delta S(\\mathbf q, \\dot{\\mathbf q}) = 0\\] <p>Then, use integration by parts (see APM462 notes for detailed steps)</p> \\[\\int_{t_1}^{t_2}\\frac{\\partial L}{\\partial\\mathbf q}\\delta\\mathbf q + \\frac{\\partial L}{\\partial\\dot{\\mathbf q}}\\delta\\dot{\\mathbf q }dt = \\int_{t_1}^{t_2}(\\frac{\\partial L}{\\partial \\mathbf q} - \\frac{d}{dt}\\frac{\\partial L}{\\partial \\dot{\\mathbf q}})\\delta \\mathbf q dt + \\underset{\\text{known end points so }=0}{\\frac{\\partial L}{\\partial \\dot{\\mathbf q}}\\delta \\mathbf q \\vert_{t_1}^{t_2}}\\] <p>Therefore, we have the Euler Lagrange equation (The lemmas are in APM462 notes)</p> \\[\\frac{\\partial L}{\\partial \\mathbf q} = \\frac{d}{dt}\\frac{\\partial L}{\\partial \\dot{\\mathbf q}}\\]"},{"location":"csc417/rigid_bodies.html","title":"Rigid Body Simulation","text":""},{"location":"csc417/rigid_bodies.html#defining-the-transformation","title":"Defining the Transformation","text":"<p>Reviewing cloth simulation and deformable objects, we simulate a deformation from reference \\(X\\) to world space \\(x\\) as </p> \\[\\Delta x\\approx \\underset{F}{\\frac{\\partial x}{\\partial X}}\\Delta X\\] <p>so that we can have the strain </p> \\[\\begin{align*} \\Delta x^T\\Delta x - \\Delta X^T\\Delta X &amp;= \\Delta X F^TF\\Delta X - \\Delta X^T\\Delta X\\\\ &amp;= \\Delta X^T(F^TF-I)\\Delta X \\end{align*}\\] <p>where \\(F^TF-I\\) is the Green Lagrange Strain. </p> <p>For a rigid object, the object cannot be deformed so that \\(F^TF-I = 0\\Rightarrow F^TF=I\\Rightarrow F\\in O(3)\\), i.e. \\(F\\) is an orthogonal matrix. For common 3D transformations, rotation matrices and reflection matrices are both orthogonal. However, reflection will have the object's topology passing through itself, which is not rigid.</p> <p>Therefore, we limit \\(F\\in SO(3)\\), i.e. rigid bodies can only rotate, and \\(SO(3)\\) is called the special orthogonal Group, </p> \\[\\forall R\\in SO(3). R^T = R^{-1}\\land \\det(R) &gt; 0\\] <p>In addition, rigid bodies can translate. Therefore, we have the rigid transformation from reference to world space as </p> \\[\\mathbf x(\\mathbf X, t) = R(t)\\mathbf X + \\mathbf p(t)\\] <p>\\(\\mathbf x, \\mathbf X \\in \\mathbb R^3\\), \\(R\\in SO(3)\\) is the rotation and \\(\\mathbf p\\in \\mathbb R^3\\) is the translation (often written in \\(\\mathbf t\\) but we don't want this to be confused with time \\(t\\)). </p>"},{"location":"csc417/rigid_bodies.html#generalized-coordinates","title":"Generalized Coordinates","text":"<p>Consider the mapping </p> \\[\\mathbf x(\\mathbf X, t) = R(t)\\mathbf X + \\mathbf p(t)\\] <p>clearly, only \\(R, \\mathbf p\\) is about time, so that our generalized coordinates is </p> \\[\\mathbf q = \\{R, \\mathbf p\\}\\] <p>Note that \\(\\mathbf q\\) is a set instead of a unified stacked vector. </p>"},{"location":"csc417/rigid_bodies.html#generalized-velocity","title":"Generalized Velocity","text":"<p>Consider the time derivative of the transformation on a single point</p> \\[\\begin{align*} \\mathbf v(\\mathbf X,t ) &amp;= \\dot{\\mathbf x}(\\mathbf X, t) \\\\ &amp;= \\frac{d}{dt}R(t)\\mathbf X + \\frac{d}{dt}\\mathbf p(t)\\\\ &amp;= \\dot{R(t)}\\mathbf X + \\dot{\\mathbf p(t)} \\end{align*}\\] <p>\\(\\dot{\\mathbf p(t)}\\) is called the linear velocity since it's just the velocity of the origin of the object moves, \\(\\dot{R(t)}\\) is the time derivative of rotation matrix, and it's more complex, we can represent it with </p> \\[\\dot{R(t)} = R{[X]_{\\times}}^TR^T\\omega\\] <p>where \\([\\cdot]_\\times\\) is the cross product matrix, </p> \\[X\\in\\mathbb R^3. [X]_{\\times} = \\begin{bmatrix}0&amp;-X_z&amp; X_y\\\\X_z&amp;0&amp;-X_z\\\\-X_y&amp;X_x&amp;0\\end{bmatrix}, a\\times b = [a]_\\times b\\] <p>\\(\\omega\\in\\mathbb R^3\\) is the angular velocity, \\(\\|\\omega\\|\\) (magnitude) encodes the speed or rotation and angle encodes the axis of rotation. </p> \\[\\begin{align*} \\mathbf v(\\mathbf X,t ) &amp;= \\dot{R(t)}\\mathbf X + \\dot{\\mathbf p(t)}\\\\ &amp;= R{[X]_{\\times}}^TR^T\\omega + \\dot{\\mathbf p(t)}\\\\ &amp;= R \\begin{bmatrix}{[X]_{\\times}}^T&amp;I\\end{bmatrix}\\begin{bmatrix}R^T&amp;0\\\\0&amp;R^T\\end{bmatrix}\\begin{bmatrix}\\omega\\\\\\dot{\\mathbf p}\\end{bmatrix}\\\\ &amp;= R \\begin{bmatrix}{[X]_{\\times}}^T&amp;I\\end{bmatrix}A\\dot{\\mathbf q} \\end{align*}\\] <p>Since \\(R, \\mathbf X\\) is constant about \\(t\\), we can define our generalized velocity \\(\\dot{\\mathbf q}\\in\\mathbb R^6 = [\\omega, \\dot{\\mathbf p}]^T\\). and we define \\(A = \\begin{bmatrix}R^T&amp;0\\\\0&amp;R^T\\end{bmatrix}\\) to simplify the expression.</p>"},{"location":"csc417/rigid_bodies.html#kinetic-energy","title":"Kinetic Energy","text":"<p>As usual, we can find kinetic energy as</p> \\[\\begin{align*} \\frac12\\int_\\Omega\\rho\\|\\mathbf v(\\mathbf X )\\|_2^2 d\\Omega &amp;= \\frac12\\int_\\Omega \\rho \\mathbf v^T\\mathbf v d\\Omega\\\\ &amp;= \\frac12\\dot{\\mathbf q}^TA^T \\big(\\rho\\int_\\Omega  \\begin{bmatrix}[\\mathbf X]_\\times {[\\mathbf X]_\\times}^T &amp;[\\mathbf X]_\\times \\\\ [\\mathbf X]_\\times&amp;I\\end{bmatrix} d\\Omega\\big)A\\dot{\\mathbf q}\\\\ &amp;= \\frac12\\dot{\\mathbf q}^TM\\dot{\\mathbf q} \\end{align*}\\] <p>where \\(M\\) is the mass matrix, \\(M = A^T \\big(M_0\\big)A\\) and \\(M_0 \\in \\mathbb R^{6\\times 6}\\) is a constant matrix integrating over the whole volume. </p>"},{"location":"csc417/rigid_bodies.html#center-of-mass","title":"Center of Mass","text":"<p>The center of mass is defined as the weighted average of points of the volume,</p> \\[\\mathbf C = \\frac1m \\int_\\Omega\\rho\\mathbf X d\\Omega, m = \\int_\\Omega \\rho d\\Omega\\] <p>With the center of Mass, we can transfer our reference space to the center-of-mass coordinates system</p> \\[\\bar{\\mathbf X} = \\mathbf X - \\mathbf C\\] <p>This is convenient for the computation of \\(M_0\\), </p> \\[M_0 =\\rho\\int_\\Omega \\begin{bmatrix}[\\mathbf X]_\\times {[\\mathbf X]_\\times}^T &amp;[\\mathbf X]_\\times \\\\ [\\mathbf X]_\\times&amp;I\\end{bmatrix} d\\Omega  = \\rho\\int_\\Omega  \\begin{bmatrix}[\\bar{\\mathbf X}]_\\times {[\\bar{\\mathbf X}]_\\times}^T &amp;[\\bar{\\mathbf X}]_\\times \\\\ [\\bar{\\mathbf X}]_\\times&amp;I\\end{bmatrix} d\\Omega\\] <p>Consider the off-diagonal entries, </p> \\[\\begin{align*} \\int_\\Omega [\\bar{\\mathbf X}]_\\times d\\Omega &amp;= \\int_\\Omega [\\mathbf X -\\mathbf C]_\\times d\\Omega\\\\ &amp;= \\int_\\Omega [\\mathbf X]_\\times d\\Omega - \\int_\\Omega [\\mathbf C]_\\times d\\Omega\\\\ &amp;= \\int_\\Omega [\\mathbf X]_\\times d\\Omega - \\int_\\Omega 1d\\Omega \\cdot [\\mathbf C]_\\times\\\\ &amp;= 0 \\end{align*}\\] <p>So that we only need to compute the diagonal</p>"},{"location":"csc417/rigid_bodies.html#surface-integral-via-divergence-theorem","title":"Surface Integral via Divergence Theorem","text":"<p>Note that \\(M_0\\) is an integral over the volume, naturally, we can use tetrahedral mesh so that we can do this integral. However, a more efficient method will use only the surface mesh. </p> <p>Divergence Theorem states that </p> \\[\\int_V \\nabla \\mathbf f (\\mathbf X) dV = \\int_A \\mathbf f(\\mathbf X) \\cdot \\mathbf ndA\\] <p>so that we can turn a volume integral to a surface integral  Therefore, consider the entries of \\(M_0\\)</p> \\[\\begin{align*} M_0 &amp;= \\rho \\int_\\Omega  \\begin{bmatrix}[\\bar{\\mathbf X}]_\\times {[\\bar{\\mathbf X}]_\\times}^T &amp;0\\\\0&amp;I\\end{bmatrix}d\\Omega \\\\  [\\bar{\\mathbf X}]_\\times {[\\bar{\\mathbf X}]_\\times}^T &amp;=  \\begin{bmatrix} \\bar{X_y}^2 + \\bar{X_z}^2 &amp; -\\bar{X_x}\\bar{X_y}&amp;-\\bar{X_x}\\bar{X_z} \\\\ -\\bar{X_x}\\bar{X_y} &amp;\\bar{X_x}^2 + \\bar{X_z}^2&amp;-\\bar{X_y}\\bar{X_z}\\\\ -\\bar{X_x}\\bar{X_z}&amp;-\\bar{X_y}\\bar{X_z}&amp;\\bar{X_x}^2 + \\bar{X_y}^2\\end{bmatrix} \\end{align*}\\] <p>\\(\\mathcal I\\) is called the inertia matrix and we can integrate \\(M_0\\) over each entry. </p> <p>Since each entry is simple enough, we can pick some integral of it and then apply divergence theorem.  For example</p> \\[\\begin{align*} \\int_\\Omega \\bar{X_y}^2 + \\bar{X_z}^2d\\Omega &amp;= \\int_T\\frac13\\begin{pmatrix}0\\\\\\bar{X_y}^3\\\\\\bar{X_z}^3\\end{pmatrix}\\cdot \\mathbf N dT \\\\ \\int_\\Omega \\bar{X_y}\\bar{X_z}d\\Omega &amp;= \\int_T\\begin{pmatrix}\\bar{X_x}\\bar{X_y}\\bar{X_z}\\\\0\\\\0\\end{pmatrix}\\cdot \\mathbf N dT \\\\ \\int_\\Omega 1d\\Omega &amp;= \\int_T\\frac13\\begin{pmatrix}\\bar{X_x}\\\\\\bar{X_y}\\\\\\bar{X_z}\\end{pmatrix}\\cdot \\mathbf N dT  \\end{align*}\\] <p>Then, because we are using triangle meshes, we can integrate over \\(T\\) through the barycentric coordinates via</p> \\[\\bar{\\mathbf X} = \\bar{\\mathbf X}_0 \\phi_0 + \\bar{\\mathbf X}_1 \\phi_1+\\bar{\\mathbf X}_2 \\phi_2\\] <p>Then, define \\(h: (\\phi_0, \\phi_1, \\phi_2)\\rightarrow \\mathbb R\\) and integrate as </p> \\[2\\text{Area}\\int_0^1\\int_0^{1-\\phi}h(1-\\phi_1-\\phi_2, \\phi_1, \\phi_2)d\\phi_2d\\phi_1\\] <p>Therefore, we can obtain </p> \\[M_0 =    \\begin{bmatrix}\\rho \\int_\\Omega[\\bar{\\mathbf X}]_\\times {[\\bar{\\mathbf X}]_\\times}^Td\\Omega &amp;0\\\\0&amp;mI\\end{bmatrix} = \\begin{bmatrix}\\mathcal I&amp;0\\\\0&amp;mI\\end{bmatrix}\\] <p>where \\(\\mathcal I\\) is the inertia matrix</p>"},{"location":"csc417/rigid_bodies.html#final-form-of-kinetic-energy","title":"Final Form of Kinetic Energy","text":"<p>Put every thing together </p> \\[\\begin{align*} T &amp;= \\frac12 \\dot{\\mathbf q}^TA^T M_0 A\\dot{\\mathbf q}\\\\ &amp;= \\frac 12\\dot{\\mathbf q}^T \\begin{bmatrix}R^T&amp;0\\\\0&amp;R^T\\end{bmatrix}^T \\begin{bmatrix}\\mathcal I&amp;0\\\\0&amp;mI\\end{bmatrix} \\begin{bmatrix}R^T&amp;0\\\\0&amp;R^T\\end{bmatrix}\\dot{\\mathbf q} \\\\ &amp;= \\frac 12\\dot{\\mathbf q}^T \\begin{bmatrix}R\\mathcal I R^T&amp;0\\\\0&amp;mI\\end{bmatrix}\\dot{\\mathbf q} \\end{align*}\\]"},{"location":"csc417/rigid_bodies.html#potential-energy","title":"Potential Energy","text":"<p>Since the object is rigid and cannot deform, </p> \\[V = 0\\]"},{"location":"csc417/rigid_bodies.html#newton-euler-equations-the-lagrangian","title":"Newton-Euler Equations (The Lagrangian)","text":"<p>Since \\(V = 0\\), we have \\(L = T\\) so that the Euler-Lagrange Equation gives Newton-Euler Equations </p> <ol> <li> <p>Conservation of Angular Momentum</p> \\[(R\\mathcal I R^T)\\dot\\omega = \\omega \\times ((R\\mathcal IR^T)\\omega) + \\tau_{ext}\\] <p>Where \\(\\dot\\omega\\) is the angular acceleration, \\(\\tau_{ext}\\) is the external torque and \\(\\omega \\times ((R\\mathcal IR^T)\\omega)\\) is the quadratic velocity vector 2. Conservation of Linear Momentum </p> \\[mI\\ddot{\\mathbf p} = \\mathbf f_{ext}\\] <p>where \\(\\ddot{\\mathbf p}\\) is the acceleration and \\(\\mathbf f_{ext}\\) is the external force.</p> </li> </ol>"},{"location":"csc417/rigid_bodies.html#external-torques-and-forces","title":"External Torques and Forces","text":"<p>For a force applied to some position \\(\\mathbf x\\) as \\(\\mathbf f(\\mathbf x)\\), this will result in a rotation as well as a translation. Note that </p> \\[\\mathbf v(\\bar{\\mathbf X}, t) = R  \\begin{bmatrix}{[X]_{\\times}}^T&amp;I\\end{bmatrix} \\begin{bmatrix}R^T&amp;0\\\\0&amp;R^T\\end{bmatrix} \\begin{bmatrix}\\omega\\\\\\dot{\\mathbf p}\\end{bmatrix}\\] <p>Let \\(J \\in\\mathbb R^{3\\times 6} = R \\begin{bmatrix}{[X]_{\\times}}^T&amp;I\\end{bmatrix}\\begin{bmatrix}R^T&amp;0\\\\0&amp;R^T\\end{bmatrix}\\) be the rigid body Jacobian matrix, which converts the generalized velocity to the velocity on some point</p> \\[\\mathbf v(\\bar{\\mathbf X}, t) = J \\dot{\\mathbf q}\\] <p>Therefore, given \\(\\mathbf x\\) in the world space, we first inverse it back to reference space w.r.t. the center-of-mass</p> \\[\\bar{\\mathbf X} = R^T(\\mathbf x - p)-\\mathbf C\\] <p>and then we can compute \\(f(\\bar{\\mathbf X})\\) and \\(J(\\bar{\\mathbf X})\\) so that </p> \\[\\begin{pmatrix}\\tau_{ext}\\\\\\mathbf f_{ext}\\end{pmatrix} = J(\\bar{\\mathbf X})^T\\mathbf f(\\bar{\\mathbf X})\\]"},{"location":"csc417/rigid_bodies.html#time-integration","title":"Time Integration","text":"<p>The NE Equations are 2 second order ODEs, and they are not inter-related. Therefore, we can derive update equations independently. </p>"},{"location":"csc417/rigid_bodies.html#update-on-translation","title":"Update on Translation","text":"<p>The update for linear momentum is simple. Note that this is just the translation of the center of mass, we can think this as the update of the point mass, the rule is simply </p> \\[\\begin{align*} m\\dot{\\mathbf p}^{t+1} &amp;= m\\dot{\\mathbf p}^t + \\Delta t \\mathbf f_{ext}\\\\ \\mathbf p^{t+1} &amp;= \\mathbf p^t + \\Delta t\\dot{\\mathbf p}^t \\end{align*}\\]"},{"location":"csc417/rigid_bodies.html#update-on-rotation","title":"Update on Rotation","text":"<p>For the angular velocity update, we can simply using the Forward Euler, i.e. approximate </p> \\[\\dot\\omega = \\Delta t^{-1}(\\omega^{t+1}-\\omega^t)\\] <p>so that the update is </p> \\[(R\\mathcal I R^T)\\omega^{t+1} = (R\\mathcal I R^T)\\omega^t + \\Delta t \\omega^t\\times ((R\\mathcal I R^T)\\omega^t)+\\Delta t\\tau^t_{ext}\\] <p>However, when we update the rotation, we cannot simply use the \\(x^{t+1} = x^t + \\Delta t v^T\\), since this will destroy the orthogonality.  Alternatively, we can view this as solving an initial value problem. </p> \\[d_t\\mathbf x = \\mathbf v^t, \\mathbf x(t_0) = \\mathbf x^t\\] <p>integrating yields that </p> \\[\\begin{align*} d_t\\mathbf x &amp;= \\mathbf v^t\\\\ \\int_{t0}^{t1}d\\mathbf x &amp;= \\int_{t0}^{t1}\\mathbf v^t dt\\\\ \\mathbf x(t) &amp;= \\mathbf v^t \\Delta t + \\mathbf x^t \\end{align*}\\] <p>With this, we can view \\(\\mathbf v^t\\) as the velocity of rotation around \\(\\mathbf p\\) so that </p> \\[\\begin{align*} \\mathbf v^t &amp;= \\omega^t \\times (R\\bar{\\mathbf X} - \\mathbf p -\\mathbf p)\\\\ \\mathbf y&amp;:= R\\bar{\\mathbf X}\\\\ \\mathbf v^t &amp;= d_t\\mathbf y = \\omega^t\\times \\mathbf y\\\\ \\mathbf y(t_0) &amp;= \\mathbf y^t \\mathbf y(t_1) = \\exp([\\omega]_\\times^t\\Delta t)\\mathbf y(t_0)\\\\ R^{t+1} &amp;= \\exp([\\omega]_\\times^t\\Delta t)R^t \\end{align*}\\]"},{"location":"csc417/rotations.html","title":"Rotation Matrix Time Derivatives","text":""},{"location":"csc417/rotations.html#problem-setup","title":"Problem Setup","text":"<p>For a point \\(x_0 \\in\\mathbb R^4\\), the rotation is a transformation defined as </p> \\[x(\\Delta t) = R(\\Delta t) x_0\\] <p>where \\(R(\\Delta t) \\in SO(3)\\). </p> <p>Then, through the transformation through time \\(t\\) will create a trajectory. We come up with the velocity of \\(x\\) as </p> \\[\\dot x (\\Delta t) = \\dot R(\\Delta t) x_0\\] <p>So that the velocity is the time derivative of the rotation matrix. </p>"},{"location":"csc417/rotations.html#angular-velocity","title":"Angular Velocity","text":"<p>Without loss of generality, assume \\(x\\) is rotated along the rotation with the axis of rotation \\(\\vec a\\in\\mathbb R^3\\). Therefore, \\(x\\) is travel in a circle.  Let \\(\\dot\\theta\\) be the change in angle. Let \\(v: \\mathbb R\\rightarrow \\mathbb R^3\\) be the velocity of \\(x\\), decompose \\(v(t) = a(t)d(t)\\) where \\(a\\) is the magnitude and \\(d\\) is the direction. </p> <p>Since we are rotating around \\(\\vec a\\), i.e. the plane that contains the circle is orthogonal to \\(\\vec a\\). Hence \\(d\\perp \\vec a\\). In addition, a rotation is orthogonal matrix so that \\(v\\perp x\\), since \\(\\vec a,v,x\\) are mutually orthogonal, we can uniquely determine \\(d\\) from </p> \\[d = \\frac{\\vec a \\times x}{\\|x\\|}\\] <p>Then, consider the magnitude \\(a\\), let \\(y = x + \\Delta tv\\). The angle formed by \\(y\\) and \\(x\\) is \\(\\dot\\theta \\Delta t\\). So that we can have </p> \\[a\\Delta t = \\|x\\|\\tan(\\dot\\theta) = \\|x\\|\\frac{\\sin(\\dot\\theta\\Delta t)}{\\cos(\\dot\\theta\\Delta t)}\\] <p>Therefore, we can have </p> \\[\\lim_{\\Delta t\\rightarrow 0} \\|x\\|\\frac{\\sin(\\dot\\theta\\Delta t)}{\\cos(\\dot\\theta\\Delta t)} = \\|x\\|\\frac{\\dot\\theta\\Delta t}{1} = \\|x\\|\\dot\\theta\\Delta t\\] <p>so that </p> \\[a = \\|x\\|\\dot\\theta\\] <p>and then</p> \\[v = ad=\\dot\\theta\\|x\\|\\frac{\\vec a\\times x}{\\|x\\|} = (\\dot\\theta \\vec a) \\times x = \\omega \\times x\\] <p>Therefore, we obtain the angular velocity \\(\\omega\\), which includes the velocity of angle and the rotation direction.</p>"},{"location":"csc417/rotations.html#rotation-matrix","title":"Rotation Matrix","text":"<p>Since cross product can be written into cross matrix form as matrix multiplication, $v=\\omega\\times x $ can then be understood as </p> \\[\\frac{dx}{dt} = [\\omega]_\\times x\\] <p>which is a linear ODE, and has analytical solution</p> \\[x(t) = \\exp([\\omega]_\\times t) x\\] <p>where \\(\\exp(M)\\) is the matrix exponential. </p>"},{"location":"csc417/rotations.html#matrix-exponential","title":"Matrix Exponential","text":"<p>For an invertible matrix \\(A\\in\\mathbb R^{n\\times n}\\). the matrix exponential \\(\\exp(A)\\) is given as </p> \\[\\exp(A) = V\\begin{bmatrix}e^{\\lambda_1}&amp;\\cdots&amp;0\\\\\\vdots&amp;\\ddots&amp;\\vdots\\\\0&amp;\\cdots&amp;e^{\\lambda_n}\\end{bmatrix}V^{-1}\\] <p>Where \\(A=V\\Lambda A^{-1}\\) is the Eigen decomposition. However, as we did the decomposition, it is not very efficient. </p> <p>For our problem, since \\([\\omega]_{\\times}\\) is a cross product matrix, hence \\(3\\times 3\\) skew-symmetric, we can use Rodrigues' Rotation Formula. First, we can break \\(\\omega t\\) into the axis of rotation and angle of rotation, as \\(\\vec a\\) and \\(\\theta\\)</p> \\[\\omega t = \\frac{\\omega}{\\|\\omega\\|}{\\|\\omega\\|t} = \\vec a \\theta\\] <p>then Rodrigues' Rotation Formula gives </p> \\[R(t) = I + \\sin(\\theta)[\\vec a]_\\times + (1-\\cos(\\theta)) {[\\vec a]_\\times}^2\\]"},{"location":"csc417/rotations.html#relationship-between-r-and-w","title":"Relationship between R and w","text":"<p>Note that we have \\(\\dot x(t) = \\dot R(t)x_0\\) and the equation above gives \\(\\dot x(t) = [\\omega]_\\times x\\), therefore, we have found the relation that </p> \\[\\dot R = [\\omega]_\\times\\]"},{"location":"csc417/rotations.html#general-equation","title":"General Equation","text":"<p>Consider the explicit equation with a fixed \\(t_0\\)</p> \\[x(t_0+\\Delta t) = \\Delta R(\\Delta t) R(t_0)x_0\\] <p>and its time derivative is </p> \\[v = \\frac{dx}{d\\Delta t} = \\Delta \\dot R(\\Delta t) R(t_0)x_0\\] <p>Note that from the derivations above, since \\(R(t_0)x_0\\) is a fixed point, \\(v\\) is just the time derivative of rotation at time \\(t_0\\) so that </p> \\[v = [\\omega]_\\times Rx_0\\] <p>Another form of this equation is</p> \\[\\begin{align*} v &amp;= \\omega \\times Rx_0\\\\ &amp;= -(Rx_0)\\times \\omega\\\\ &amp;= R{[x_0]_\\times}^TR^T \\omega \\end{align*}\\] <p>then this form gives that \\(v\\) is linear in \\(\\omega\\).</p> Source code <pre><code>import plotly.express as px\nimport plotly.graph_objects as go\nimport numpy as np\n\ndef rodrigues(k, v, theta):\n    k /= np.linalg.norm(k)\n    cross = np.cross(k, v)\n    cos = np.cos(theta)[:, None]\n    sin = np.sin(theta)[:, None]\n    dot = np.dot(k, v)\n    v_p = np.tile(v, (len(theta), 1))\n    k_p = np.tile(k, (len(theta), 1))\n    return v_p * cos + sin * cross  +  dot * k_p * (1-cos)\n\ndef plot(output_file):\n    axis = np.random.random(3)\n    axis /= np.linalg.norm(axis)\n    vector = np.random.random(3)\n    angle = np.linspace(0, 2 *np.pi, 100)\n    dt = 0.2\n    points = rodrigues(axis, vector, angle)\n    alpha = vector.dot(axis) / np.linalg.norm(axis) * axis \n    d = np.cross(axis, vector) / np.linalg.norm(vector)\n    derivative = vector + dt * d\n\n    trajectory = go.Scatter3d(x=points[:, 0], y=points[:, 1], z=points[:, 2], \n                              mode=\"lines\", name=\"x trajectory\")\n    plot_axis = go.Scatter3d(x=[0, axis[0]],y=[0, axis[1]],z=[0, axis[2]], \n                             name=\"a_vec\")\n    plot_vector = go.Scatter3d(x=[0, vector[0]],y=[0, vector[1]],z=[0, vector[2]], \n                               name=\"x\")\n    plot_normal = go.Scatter3d(x=[alpha[0], vector[0]], y=[alpha[1], vector[1]], z=[alpha[2], vector[2]], \n                               name=\"trajectory normal\", mode=\"lines\", line=dict(dash=\"dot\"))\n    plot_y = go.Scatter3d(x=[vector[0], derivative[0], 0], y=[vector[1], derivative[1], 0], z=[vector[2], derivative[2], 0], \n                          name=\"y\")\n    fig = go.Figure(data=[trajectory, plot_axis, plot_vector, plot_normal, plot_y])\n    fig.update_layout(margin=dict(l=0, r=0, b=0, t=0), \n                      scene_aspectmode='cube',\n                      legend=dict(x=0, y=0))\n    with open(output_file, \"w\") as f:\n        f.write(fig.to_json())\n\noutput_file = \"../assets/rotation.json\"\nplot(output_file)\n</code></pre>"},{"location":"csc418/bvh.html","title":"Bounding Volume Hierarchy","text":""},{"location":"csc418/bvh.html#object-partitioning-vs-space-partitioning-schemes","title":"Object Partitioning vs. Space Partitioning Schemes","text":"<p>Object partitioning schemes are to bound objects into a big collection (box, sphere, convex hull) so that we can quickly query the data structure. The space can be shared within multiple bounding.</p> <p>Space partitioning schemes are to divide the space into subspaces. </p>"},{"location":"csc418/bvh.html#types-of-bounding-boxes","title":"Types of Bounding boxes","text":""},{"location":"csc418/bvh.html#axis-aligned-bounding-box","title":"Axis Aligned Bounding Box","text":"<p>Consider a 2D example, an AABB is defined by 4 lines </p> \\[x = x_{min}, x = x_{max}, y=y_{min}, y=y_{max}\\] <p>where the point is within the bounding box if  </p> \\[(x, y) \\in [x_{min}, x_{max}]\\times[y_{min}, y_{max}]\\] <p>Easy to build, but waste a lot of areas</p>"},{"location":"csc418/bvh.html#sphere","title":"Sphere","text":"<p>For a group of objects \\(A_1,...,A_n\\), let the </p> \\[c = \\sum_{i=1}^n \\frac{A_i}{n}, r = \\max\\{\\|A_i - c\\|\\}\\] <p>However, note that intersecting with a sphere involves solving a quadratic equation hence more computations. Also, it wastes a lot of areas</p>"},{"location":"csc418/bvh.html#oriented-bounding-box","title":"Oriented Bounding box","text":"<p>For a group of objects \\(A_1,..., A_n\\), do PCA so that we can take the first 2 largest principal components, and make it our axis. </p> <p>Little area wasted, but computations for interactions and constructions</p>"},{"location":"csc418/bvh.html#convex-hull","title":"Convex Hull","text":"<p>No area wasted, but takes much computations for constructing such trees.</p>"},{"location":"csc418/bvh.html#axis-aligned-bounding-box-tree","title":"Axis Aligned Bounding Box Tree","text":""},{"location":"csc418/bvh.html#intersecting-an-aabb","title":"Intersecting an AABB","text":"<p>for a ray \\(e+td\\), it can intersect with the 4 lines as </p> \\[t_{xmin}=(x_{min} - x_e)/x_d, t_{xmax}=(x_{max} - x_e)/x_d\\] \\[t_{ymin}=(y_{min} - y_e)/x_d, t_{ymax}=(y_{max} - y_e)/x_d\\] <p>and an intersection happens if </p> \\[[t_{xmin}, t_{xmax}]\\cap [t_{ymin}, t_{ymax}]\\neq \\emptyset\\Rightarrow \\exists t &gt; 0. e+td \\in  [x_{min}, x_{max}]\\times[y_{min}, y_{max}]\\] <p>For the actual implementation, notice that if \\(d_x &lt; 0\\), then \\(t_{xmin} &lt; t_{xmax}\\) so that we need one if check.  </p> <p>Also, note that \\(d_x=0\\) will lead to divide by 0 error and \\(t_{xmin}=t_{xmax} = \\infty\\), so we use \\(a_x = d_x^{-1}\\) to handle such case so that for each coordinate we check </p> 2D segment intersection<pre><code>a = 1 / x_d\nif a &gt;= 0:\n    t_xmin = a * (x_min - x_e)\n    t_xmax = a * (x_max - x_e)\nelse:\n    t_xmax = a * (x_min - x_e)\n    t_xmin = a * (x_max - x_e)\n</code></pre> Boundbox<pre><code>import numpy as np\nimport matplotlib.pyplot as plt\n\nclass Triangle(Object):\n    def __init__(self, corners):\n        \"\"\" corners is a 3 * 3 list of list \n        each row is a corner\n        \"\"\"\n        self.corners = np.array(corners)\n\n    def intersect(self, ray_d, ray_e, min_t, max_t):\n        abc = self.corners[0] - self.corners[1]\n        def_ = self.corners[0] - self.corners[2]\n        ghi = ray_d\n        jkl = self.corners[0] - ray_e\n\n        M = abc.dot(np.cross(def_, ghi))\n        time = - def_.dot(np.cross(abc, jkl)) / M\n\n        if time &lt; min_t or time &gt; max_t:\n            return - np.inf\n        gamma = ghi.dot(np.cross(abc, jkl)) / M\n        if gamma &lt; 0 or gamma &gt; 1:\n            return - np.inf\n        beta = jkl.dot(np.cross(def_, ghi)) / M\n        if beta &lt; 0 or beta &gt; 1:\n            return - np.inf\n        if beta + gamma &gt; 1:\n            return - np.inf\n        return time\n\n\nclass BoundingBox:\n    def __init__(self):\n        self.min_corner = -np.array(np.inf, np.inf, np.inf)\n        self.max_corner = np.array(np.inf, np.inf, np.inf)\n\n    def __init__(self, min_corner, max_corner):\n        self.min_corner = np.array(min_corner)\n        self.max_corner = np.array(max_corner)\n\n    def intersect(self, ray_d, ray_e, min_t, max_t):\n        a = 1 / ray_d\n        diff_min = self.min_corner - ray_e\n        diff_max = self.max_corner - ray_e\n        t_xmin = a[0] * diff_min[0] if a[0] &gt;= 0 else a[0] * diff_max[0]\n        t_xmax = a[0] * diff_max[0] if a[0] &gt;= 0 else a[0] * diff_min[0]\n        t_ymin = a[1] * diff_min[1] if a[1] &gt;= 0 else a[1] * diff_max[1]\n        t_ymax = a[1] * diff_max[1] if a[1] &gt;= 0 else a[1] * diff_min[1]\n        t_zmin = a[2] * diff_min[2] if a[2] &gt;= 0 else a[2] * diff_max[2]\n        t_zmax = a[2] * diff_max[2] if a[2] &gt;= 0 else a[2] * diff_min[2]\n        if t_xminn &gt; t_ymax or t_ymin&gt; t_xmax \\\n            or t_xmin &gt; t_zmax or t_zmin &gt; t_xmax \\\n            or t_zmin &gt; t_ymax or t_ymin &gt; t_zmax:\n            return False\n        return t_max &gt; max(t_xmax, t_ymax, t_zmax) \\ \n            and t_min &lt; max(t_xmin, t_ymin, t_zmin)\n\n    def insert(self, obj):\n        if isinstance(obj, BoundingBox):\n            self.min_corner = np.min(np.vstack((self.min_corner, obj.min_corner)), axis=0)\n            self.max_corner = np.max(np.vstack((self.max_corner, obj.max_corner)), axis=0)\n        if isinstance(obj, Triangle):\n            self.min_corner = np.min(np.concatenate((self.min_corner[np.newaxis, :], \n                                                     obj.corners), axis=0),\n                                     axis=0)\n            self.max_corner = np.max(np.concatenate((self.max_corner[np.newaxis, :], \n                                                     obj.corners), axis=0),\n                                     axis=0)\n\n    def center(self):\n        return (self.min_corner + self.max_corner) / 2\n</code></pre>"},{"location":"csc418/bvh.html#aabb-tree","title":"AABB Tree","text":"<p>An AABB tree is constructed recursively, at each level, it is the bounding box that encloses all the item, and it further divide into subtrees, each subtree will have a smaller bounding box. </p> <p>AABB tree will have bounding box overlapping.</p> AABBTree<pre><code>class AABBTree:\n    \"\"\" An AABB Tree\n    box:   (BoundingBox) the bounding box that contains all the items\n    left:  (AABBTree) the left subtree\n    right: (AABBTree) the right subtree\n    \"\"\"\n    self.box = None\n    self.left = None\n    self.right = None\n    def __init__(self, objs):\n\n        # base case, if this is the leaf node\n        if len(objs) == 1:\n            self.box = obj\n            return\n\n        # insert all objects into the top level bounding box\n        self.box = BoundingBox()\n        for obj in objs:\n            self.box.insert(obj)\n\n        # determine the axis split\n        axis = np.argmax(self.box.max_corner - self.box.min_corner)\n        left, right = [], []\n        for obj in objs:\n            if obj.center()[axis] &lt; self.center()[axis]:\n                left.append(obj)\n            else:\n                right.append(obj)\n\n        # do a random split if all items are in one side\n        if (len(left) == 0) != (len(right) == 0):\n            left, right = [], []\n            curr = 0\n            for obj in objs:\n                if curr % 2 == 1:\n                    left.append(obj)\n                else:\n                    right.append(obj)\n                curr += 1\n        # recursively build the subtrees\n        if len(left) &gt; 0:\n            self.left = AABBTree(left)\n        if len(right) &gt; 0:\n            self.right = AABBTree(right)\n\n    def intersect(self, ray_d, ray_e, min_t):\n\n        # base case 1: if this is the leaf\n        if isinstance(self.box, Triangle):\n            return self.box.intersect(ray_d, ray_e, min_t)\n        # base case 2: if this is not the leaf\n        if isinstance(self.box, BoundingBox) and not self.box.intersect(ray_d, ray_e, min_t):\n            return -np.inf\n\n        # recursive call\n        if self.left is not None:\n            t_left = self.left.intersect(ray_d, ray_e, min_t, max_t)\n        if self.right is not None:\n            t_right = self.right.intersect(ray_d, ray_e, min_t, max_t)\n        if t_left &gt; - np.inf and t_right &gt; - np.inf:\n            return min(t_left, t_right)\n        else:\n            return max(t_left, t_right)\n</code></pre>"},{"location":"csc418/bvh.html#distancing-using-bfs","title":"Distancing Using BFS","text":"<p>BFS Notes</p> <pre><code>class Point:\n    def __init__(self, p):\n        self.p = np.array(p)\n\n    def distance(self, q):\n        return np.sum((p - q) ** 2)\n\n\nclass PriorityQueue:\n    \"\"\" A poor priority queue implementation\n    \"\"\"\n    def __init__(self):\n        self.items = []\n        self.priorities = []\n\n    def append(self, priority, item):\n        self.items.append(item)\n        self.priorities.append(priority)\n\n    def __len__(self):\n        return len(self.items)\n\n    def dequeue(self):\n        max_p = max(self.priorities)\n        for i in range(len(self.items)):\n            if self.priorities[i] == max_p:\n                return self.priorities.pop(i), self.items.pop(i)\n\ndef point_box_sqrd(q, box):\n    \"\"\" Measures the squared distance between\n    a query point p and a bounding box\n    \"\"\"\n    d = np.max(np.vstack((box.min_corner - q, q - box.max_corner)), axis=0)\n    return np.sum(d * d)\n\ndef point_AABBTree_sqrd(q, root):\n    \"\"\" Measures the squared distance between\n    a query point p and an AABBTree\n    \"\"\"\n    pq = PriorityQueue()\n\n    # bfs\n    pq.append(point_box_sqrd(q, root.box), root)\n    sqrd = max_sqrd + 1\n\n    while len(pq) &gt; 0:\n        curr_prio, curr_item = pq.dequeue()\n        if curr_prio &lt; sqrd:\n            # if is a leaf\n            if isinstance(curr_item.box, Point):\n                new_sqrd = curr_item.box.distance(q)\n                sqrd = new_sqrd if new_sqrd &lt; sqrd else sqrd\n            else:\n                if curr_item.left is not None:\n                    pq.append(point_box_sqrd(q, curr_item.left.box), curr_item.left)\n                if curr_item.right is not None:\n                    pq.append(point_box_sqrd(q, curr_item.right.box), curr_item.right)\n    return sqrd\n</code></pre>"},{"location":"csc418/kinematics.html","title":"Kinematics","text":""},{"location":"csc418/kinematics.html#bone","title":"Bone","text":"<p>A bone is a UI widget for visualizing and controlling a 3D rigid transformation. </p>"},{"location":"csc418/kinematics.html#canonical-bone","title":"Canonical Bone","text":"<p>A canonical bone of length \\(l\\) lies across the x-axis is two points: the tail \\((0, 0, 0)\\) and tip \\((l, 0, 0)\\). it can be twisted around \\(x\\) axis, bending around \\(z\\) axis, and then twist along \\(y\\) axis. Note that twist around \\(x\\) axis has no impact on the position of tail or tip, but it impacts the skinning bind to the bone. </p>"},{"location":"csc418/kinematics.html#rest-bone","title":"Rest Bone","text":"<p>To need to map the canonical bones to its position and orientation in the underformed model. For each bone, define a rigid transformation</p> \\[\\hat T_{3\\times 4} = \\big[[\\hat R]_{3\\times 3}[\\hat t]_{3\\times 1}\\big]\\] <p>where \\(\\hat R\\) is the rotation matrix and \\(\\hat t\\) is the translation matrix. Note that the rigid transformation is homogeneous, so that the rest tail's position is </p> \\[\\hat s = \\hat T \\begin{bmatrix}0\\\\0\\\\0\\\\1\\end{bmatrix} = \\hat R\\cdot 0 + \\hat t = \\hat t\\] <p>and the rest tip \\(\\hat d\\) is at </p> \\[\\hat d = \\hat T \\begin{bmatrix}l\\\\0\\\\0\\\\1\\end{bmatrix} = \\hat R \\begin{bmatrix}l\\\\0\\\\0\\end{bmatrix} + \\hat t\\] <p>Typically, a rest tail is coincident with the rest tip of its parent. </p>"},{"location":"csc418/kinematics.html#pose-bone","title":"Pose Bone","text":"<p>Each rest bone undergoes a rigid transformation \\(T\\), composed of rotation \\(R\\) and translation \\(t\\), mapping each of its rest point \\(\\hat x\\) to posed position \\(x\\), i.e. \\(x = T\\hat x\\). In particular, we would like each bone to rotate about its parent's tip, but this position is determined by the parent's pose transformation, which in turn should rotate about the grandparent's tip and so on.</p>"},{"location":"csc418/kinematics.html#forward-kinematics","title":"Forward Kinematics","text":"<p>For each bone \\(i\\), the rigid pose transformation \\(T_i\\)is determined by the aggregate relative rotations \\(\\bar{R}_{i_{3\\times3}}\\) between \\(i\\) and its parent \\(p_i\\). </p> \\[T_i = T_{p_i}\\begin{bmatrix}&amp;[\\hat T_i]&amp;&amp;\\\\0&amp;0&amp;0&amp;1\\end{bmatrix} \\begin{bmatrix}&amp;&amp;&amp;0\\\\&amp;[\\bar R_i]&amp;&amp;0\\\\&amp;&amp;&amp;0\\\\0&amp;0&amp;0&amp;1\\end{bmatrix} \\begin{bmatrix}&amp;[\\hat T_i]&amp;&amp;\\\\0&amp;0&amp;0&amp;1\\end{bmatrix}^{-1}\\] <p>From right to left, we first transforms the bone back to rest bone, then apply the aggregate rotations, transform backs to its position, and then transforms around its parent \\(T_{p_i}\\). Note that this algorithm is defined recursively, for a bone at the root, its parent transformation is \\(I\\).  Consider some tail \\(\\hat s_i\\), note that rotation won't change \\(\\hat s_i\\)'s position, so that </p> \\[T_i\\hat s_i = T_{p_i}\\hat s_i = T_{p_i}\\hat{d}_{p_i}\\] <p>which is the position of the tip of its parent bone.</p> <p>The aggregate rotation matrix \\(\\bar{R}_i\\) can be represented as \\((\\theta_{i1}, \\theta_{i2}, \\theta_{i3})\\), </p> \\[\\begin{bmatrix}&amp;&amp;&amp;0\\\\&amp;[\\bar R_i]&amp;&amp;0\\\\&amp;&amp;&amp;0\\\\0&amp;0&amp;0&amp;1\\end{bmatrix} = \\begin{bmatrix}&amp;&amp;&amp;0\\\\&amp;[\\bar R_x(\\theta_{i3})]&amp;&amp;0\\\\&amp;&amp;&amp;0\\\\0&amp;0&amp;0&amp;1\\end{bmatrix} \\begin{bmatrix}&amp;&amp;&amp;0\\\\&amp;[\\bar R_x(\\theta_{i2}]&amp;&amp;0\\\\&amp;&amp;&amp;0\\\\0&amp;0&amp;0&amp;1\\end{bmatrix} \\begin{bmatrix}&amp;&amp;&amp;0\\\\&amp;[\\bar R_x(\\theta_{i1}]&amp;&amp;0\\\\&amp;&amp;&amp;0\\\\0&amp;0&amp;0&amp;1\\end{bmatrix}\\] <p>which is the twist-bend-twist. </p>"},{"location":"csc418/kinematics.html#keyframe-animation","title":"Keyframe Animation","text":"<p>Note that is too difficult to define the Euler angles for every bone every frame manually. Instead, we define some keyframes \\(\\big(t_{j}, (\\theta_{ij1}, \\theta_{ij2}, \\theta_{ij3})\\big)\\) for some time point \\(t_1,...,t_m\\), and determine the relative bone transformations for each frame by interpolate values. A linear interpolation on the Euler angles is easiest, while it will give a choppy robotic animation. </p>"},{"location":"csc418/kinematics.html#catmull-rom-interpolation","title":"Catmull Rom Interpolation","text":"<pre><code>def catmull_rom_interpolation(kfs, t):\n    \"\"\" return the interpolated data\n\n        kfs: Tuple[List[float], List[data]]  \n                             sorted keyframes as (time, data)\n        t:   float           query times\n    \"\"\"\n    times, pos = kfs\n    if len(times) &lt; 2 or t &gt; times[-1] or t &lt; times[0]:\n        raise ValueError(t)\n    found_idx = -1\n    for kf_idx, time in enumerate(times):\n        if t &lt; time:\n            found_idx = kf_idx - 1\n            break\n    P = [pos[found_idx], pos[found_idx], \n         pos[found_idx+1], pos[found_idx+1]]\n    if found_idx &gt; 0:\n        P[0] = pos[found_idx-1]\n    if found_idx &lt; len(kfs) - 2:\n        P[3] = pos[found_idx+2]\n    t = (t - times[found_idx]) / (times[found_idx + 1] - times[found_idx])\n    interpolated =  (         2*P[1]                  )\n    interpolated += ( -P[0] +            P[2]         ) * t\n    interpolated += (2*P[0] - 5*P[1] + 4*P[2] -   P[3]) * t**2\n    interpolated += ( -P[0] + 3*P[1] - 3*P[2] +   P[3]) * t**3\n    return interpolated / 2\n</code></pre>"},{"location":"csc418/kinematics.html#inverse-kinematics","title":"Inverse Kinematics","text":"<p>Instead of manually tuning the angles, we want to minimize the distance between the tip of the bone (usually fingers, or the endpoint) to the wanted position \\((x, y, z)\\). We can consider this as an optimization problem. </p> <p>Define \\(a \\in \\mathbb R^{3m} := [\\theta_{11}, \\theta_{12}, \\theta_{13},...,\\theta_{m1}, \\theta_{m2}, \\theta_{m3}]^T\\) be the collections of Euler angles of all bones \\(1,...,m\\) \\(x_b(a) = T_b\\hat d_b\\) be the position of the tip of bone \\(b\\), note that \\(T_b\\) depends on \\(\\theta_{b1}, \\theta_{b2}, \\theta_{b3}, T_{p_i}\\) \\(\\hat x_{b}\\) be the wanted position of some bone \\(b\\).  </p> <p>Therefore, we are to optimize</p> \\[\\min_a \\sum_{i=1}^k \\bigg[E(x(a)) = \\|x_{b_i}(a) - \\hat{x}_{b_i}\\|^2\\bigg]\\] <p>In addition, we can specify the min/max values for the Euler angles, for example, </p> \\[0 \\leq \\theta_1\\leq 0, 0\\leq \\theta_2\\leq 170, 0\\leq \\theta_3\\leq 0\\] <p>will allow only the bending in one direction, then we can stack all the constraint as \\(a_{\\min}, a_{\\max}\\) and optimize under such constraint. </p> <p>For some dimensionality </p> \\[E(x) = E(x_{b_1},..., x_{b_k}):\\mathbb R^k\\rightarrow \\mathbb R\\] \\[x(a) = x(\\theta_{11}, \\theta_{12}, \\theta_{13}, ..., \\theta_{m1}, \\theta_{m2}, \\theta_{m3}):\\mathbb R^{3m} \\rightarrow \\mathbb R^k\\]"},{"location":"csc418/kinematics.html#projected-gradient-descent","title":"Projected Gradient Descent","text":"<p>As a non-linear optimization problem, we use gradient descent </p> \\[a\\leftarrow a - \\sigma\\big(\\frac{d}{da}E(x(a))\\big)^T = a - \\sigma \\big(\\frac{d}{da}x(a)\\big)^T\\big(\\frac{d}{dx}E(x)\\big)\\] <p>Define \\(J:= \\frac{d}{da}x(a)\\in \\mathbb R^{k\\times 3m}\\)  so that </p> \\[a\\leftarrow a - \\sigma J^T\\frac{d}{dx}E(x)\\] <p>and to make sure the constraint \\(a_{\\min}, a_{\\max}\\) is preserved, after each step, we clip \\(a_i\\) by \\([a_{\\min}, a_{\\max}]\\)</p> <p>Then, to calculate this</p> \\[\\begin{align*} J_{i,j} &amp;= \\frac{\\partial x_i}{\\partial a_j}\\\\ &amp;= \\lim_{h\\rightarrow0}\\frac1h\\big(x_i(a + he_j)\\ - x_i(a)\\big) &amp;\\text{limit def.}\\\\ &amp;\\approx \\frac1h\\big(x_i(a + he_j)\\ - x_i(a)\\big) &amp;\\text{finite differentiation} \\end{align*}\\] <p>For each \\(x_i(a+h e_j)\\), we need \\(3m\\) calls to forward kinematics function and we have \\(k\\) bones to consider, i.e. \\(O(mk)\\) operations.</p> \\[\\frac{\\partial}{\\partial x_i}E(x) = 2(x_{i}(a) - \\hat x_i)\\] <p>which requires \\(O(k)\\) calls. </p> <p>Also, note that this whole system is a bunch of matrix calculates, and we can use automatic differentiation. </p>"},{"location":"csc418/kinematics.html#line-search","title":"Line search","text":"<p>To determine a proper \\(\\sigma\\), we simply start with a very large \\(\\sigma\\), check whether \\(E(a - \\sigma J^T d_xE) &lt; E(a)\\), if not, then reduce \\(\\sigma\\) by a factor. </p>"},{"location":"csc418/kinematics.html#linear-blend-skinning","title":"Linear Blend Skinning","text":"<p>For each vertex of the object model, let \\(\\hat v_i\\in\\mathbb R^3\\) be its rest position we bind it with several bones, then the pose position </p> \\[v_i = \\sum_{j=1}^m w_{i,j} T_j \\begin{bmatrix}\\hat v_i\\\\1\\end{bmatrix}\\] <p>where \\(w_{i,j}\\) is the weight of vertex \\(i\\) on bone \\(j\\). </p> Source code <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\n\n\n# --8&lt;-- [start:catmull]\ndef catmull_rom_interpolation(kfs, t):\n    \"\"\" return the interpolated data\n\n        kfs: Tuple[List[float], List[data]]  \n                             sorted keyframes as (time, data)\n        t:   float           query times\n    \"\"\"\n    times, pos = kfs\n    if len(times) &lt; 2 or t &gt; times[-1] or t &lt; times[0]:\n        raise ValueError(t)\n    found_idx = -1\n    for kf_idx, time in enumerate(times):\n        if t &lt; time:\n            found_idx = kf_idx - 1\n            break\n    P = [pos[found_idx], pos[found_idx], \n         pos[found_idx+1], pos[found_idx+1]]\n    if found_idx &gt; 0:\n        P[0] = pos[found_idx-1]\n    if found_idx &lt; len(kfs) - 2:\n        P[3] = pos[found_idx+2]\n    t = (t - times[found_idx]) / (times[found_idx + 1] - times[found_idx])\n    interpolated =  (         2*P[1]                  )\n    interpolated += ( -P[0] +            P[2]         ) * t\n    interpolated += (2*P[0] - 5*P[1] + 4*P[2] -   P[3]) * t**2\n    interpolated += ( -P[0] + 3*P[1] - 3*P[2] +   P[3]) * t**3\n    return interpolated / 2\n# --8&lt;-- [end:catmull]\n\n\n\nN = 10\ntimes = np.linspace(0, 10, N)\npos = np.random.uniform(0, 10, N)\nkfs = (times, pos)\nts = np.arange(times.min(), times.max(), .1)\ninterpolated = [catmull_rom_interpolation(kfs, t) for t in ts]\nplt.figure(figsize=(6, 4))\nplt.scatter(times, pos)\nplt.plot(ts, interpolated)\nplt.tight_layout()\nplt.savefig(\"../assets/catmull_rom_interpolation.png\")\n</code></pre>"},{"location":"csc418/mass_spring_system.html","title":"Mass-Spring System","text":""},{"location":"csc418/mass_spring_system.html#mass-and-spring","title":"Mass and Spring","text":"<p>We model the physical behavior of the system as a network of point masses \\(P = \\{(p_1, m_1),...,(p_n, m_n)\\}\\), where \\(p_1\\in\\mathbb R^3\\) is the position and \\(m_i\\) is the mass, and springs \\(E = \\{(p_i, p_j)\\mid 1\\leq i\\neq j \\leq n\\}\\), so that the shape is a graph \\(G = (P, E)\\).  </p>"},{"location":"csc418/mass_spring_system.html#goal","title":"Goal","text":"<p>Following Newton's second law, \\(f = ma\\) where \\(f,a\\in\\mathbb R^3\\). For each point mass, we need to have </p> \\[f_{i} = T_{i} + f^{\\text{ext}}\\] <p>where \\(T_{i} = \\sum_{j, (p_i, p_j)\\in E}f^{\\text{elastic}}(p_i, p_j)\\) is the sum of forces coming from any incident spring  \\(f^{\\text{ext}}\\) is the external forces applied on the point mass</p>"},{"location":"csc418/mass_spring_system.html#spring-potential-energy-vpi-pj","title":"Spring Potential Energy V(p<sub>i</sub>, p<sub>j</sub>)","text":"<p>For each spring \\((p_i, p_j)\\), define its stiffness \\(k &gt; 0\\) (assuming all springs have the same stiffness for now) and rest length \\(r_{i,j}\\in\\mathbb R\\). The potential energy \\(V(p_i, p_j)\\) is defined as </p> \\[V(p_i, p_j):\\mathbb R^{3\\times 2}\\rightarrow \\mathbb R:= \\frac k2(\\|p_i-p_j\\|-r_{ij})^2\\] <p>Note that \\(\\frac12\\) is because the force is attached with two vertices, hence we only calculate half on each vertex.</p> <p>The force exerted by the spring on each mass is the partial derivative of the potential energy \\(V\\) with respect to the corresponding mass position. For example, for \\(p_i\\) we have </p> \\[f_{ij} = - \\frac{\\partial V}{\\partial p_i}\\in\\mathbb R^3\\]"},{"location":"csc418/mass_spring_system.html#acceleration-of-the-point-mass-ait","title":"Acceleration of the point mass a<sub>i</sub><sup>t</sup>","text":"<p>Define \\(p_i^t = p_i(t):\\mathbb R^{\\geq 0}\\rightarrow \\mathbb R^3\\) be the position for mass \\(i\\) at time \\(t\\), and velocities \\({p'}_i^t = \\frac{\\partial p_i}{\\partial t}(t)\\in\\mathbb R^3\\). Given \\(p_i^0\\) and \\({p'}_i^0\\), which is the initial conditions of the simulation system. Also, note that \\(p_i^t\\) is determined only on the last know value and won't trace back, i.e. given any \\(t\\geq 0\\), we can still treat these values as the initial conditions for the remaining time.   </p> <p>For the purpose of the simulation, we only need to know the position of each mass at discrete time, so we can use discrete approximation. Therefore, \\({p'}_i^t\\) can be approximated by finite difference </p> \\[{p'}_i^t = \\frac{p_i^t - p_i^{t-\\Delta t}}{\\Delta t}\\] <p>Then, we use central finite difference to approximate the acceleration at time \\(t\\)</p> \\[\\begin{align*} a_i^t = {p''}_i^t &amp;= \\frac{ {p'}_i^{t+\\Delta t} - {p'}_i^{t}}{\\Delta t}\\\\ &amp;= \\frac{1}{\\Delta t}\\big(\\frac{p_i^{t+\\Delta t} - p_i^t}{\\Delta t} - \\frac{p_i^{t} - p_i^{t-\\Delta t}}{\\Delta t}\\big)\\\\ &amp;= \\frac{p^{t+\\Delta t} - 2p_i^{t} + p^{t-\\Delta t}}{(\\Delta t)^2} \\end{align*}\\]"},{"location":"csc418/mass_spring_system.html#time-integration","title":"Time integration","text":"<p>Coming back to the goal, since each point mass should follow Newton's second law, we can sum them together and integrate only \\(p\\), i.e. </p> \\[\\begin{align*} V(P) &amp;= \\frac12\\sum_{i,j}k(\\|p_i - p_j\\|-r_{ij})^2\\\\ T(P) &amp;= (\\Delta t)^2 \\bigg\\{\\sum_i m_i \\big(\\frac{p_i - 2p_i^t + p_i^{t-\\Delta t}}{(\\Delta t)^2}\\big)^2\\bigg\\}\\\\ F^{\\text{ext}}(P) &amp;= \\sum_i p_i^Tf_i^{\\text{ext}}\\\\ P &amp;= (p_1,..., p_n)\\in\\mathbb R^{3\\times n} \\end{align*}\\] <p>We want </p> \\[V - F^{\\text{ext}} = T\\] <p>Therefore, we can view the problem as a optimization problem, i.e. </p> \\[p^{t+\\Delta t} = \\arg\\min_{P} V(P)-T(P)-F^{\\text{ext}}(P)\\]"},{"location":"csc418/mass_spring_system.html#fast-simulation-of-mass-spring-system-paper","title":"Fast Simulation of Mass-Spring System (Paper)","text":"<p>Observe that the non linear energy  can be written as a small optimization problem</p> \\[(\\|p_i-p_j\\|-r_{ij})^2 = \\min_{d_{ij}\\in\\mathbb R^3, \\|d_{ij}\\|=r_{ij}}\\|(p_i - p_j) - d_{ij}\\|^2\\] <p>Therefore, suppose we know the vector \\(d_{ij}\\) corresponds to the unknown optimal solution \\(p^{t+\\Delta t}\\), then treating \\(d_{ij}\\) as constant, we cound find the optimal solution by </p> \\[\\begin{align*} p^{t+\\Delta t} &amp;= \\arg\\min_p \\hat E(p)\\\\ \\hat E(P) &amp;=\\big(\\frac12\\sum_{ij}k\\|(p_i-p_j)-d_{ij}\\|^2\\big) - \\Delta t^2\\\\ &amp;\\quad - (\\Delta t)^2 \\bigg\\{\\sum_i m_i \\big(\\frac{p_i - 2p_i^t + p_i^{t-\\Delta t}}{(\\Delta t)^2}\\big)^2\\bigg\\}\\\\ &amp;\\quad -\\sum_i p_i^Tf_i^{\\text{ext}} \\end{align*}\\] <p>Which is quadratic w.r.t. \\(P\\) and we can have the solution at </p> \\[d_P \\hat E = 0\\] <p>Therefore, we can define a local-global iterative algorithm.   </p> <p>while condition not satisfied:</p> <ul> <li>Given current \\(P\\), determine \\(d_{ij}\\) from the small optimization problem</li> <li>Find \\(P\\) that minimizes \\(\\hat E\\)</li> </ul>"},{"location":"csc418/mass_spring_system.html#matrices","title":"Matrices","text":"<p>To allow parallel computation, we need to write the equations into matrix form. </p> <p>Define \\(P = [p_1,...,p_n]\\in \\mathbb R^{n\\times 3}\\), and similarly \\(P^t, P^{t-\\Delta t} \\in \\mathbb R^{n\\times 3}\\), let \\(M = diag(m_1,...,m_n)\\in \\mathbb R^{n\\times n}\\), we have </p> \\[\\begin{align*} T(P) &amp;= (\\Delta t)^2 \\bigg\\{\\sum_i m_i \\big(\\frac{p_i - 2p_i^t + p_i^{t-\\Delta t}}{(\\Delta t)^2}\\big)^2\\bigg\\}\\\\ &amp;= \\frac1{\\Delta t^2}\\bigg\\{(p_i - 2p_i^t + p_i^{t-\\Delta t})^Tm_i(p_i - 2p_i^t + p_i^{t-\\Delta t})\\bigg\\}\\\\ &amp;= \\frac1{\\Delta t^2}tr(P-2P^t + P^{t-\\Delta t})^TM(P-2P^t + P^{t-\\Delta t}) \\end{align*}\\] <p>Define \\(A\\in\\mathbb \\{-1, 0, 1\\}^{|E|\\times |V|}\\) be the matrix that represents the edges, where each row \\(e\\) represents one edge \\(e = (i, j)\\) as</p> \\[A_{ek} = \\begin{cases}1&amp;k=i\\\\-1&amp;k=j\\\\0&amp;\\text{otherwise}\\end{cases}\\] <p>Let \\(d = \\begin{bmatrix}d_{e_1}\\\\...\\\\d_{e_m}\\end{bmatrix}\\in\\mathbb R^{\\|E\\|\\times 3}\\) being \\(d_{ij}\\)'s stacked vertically, so that </p> \\[\\hat V(P) = \\sum_{ij}\\frac{k}2 \\|(p_i - p_j) -d_{ij}\\|^2 = \\frac{k}2\\big[(AP-d)^T(AP-d)\\big]\\] <p>Finally, let \\(f^{\\text{ext}} =\\begin{bmatrix}f_1^{\\text{ext}}\\\\...\\\\f_n^{\\text{ext}}\\end{bmatrix}\\) so that </p> \\[F(P) = tr(P^Tf^{\\text{ext}})\\] <p>and we can combine the equations together and write it into quadratic form, with some computation, we can have </p> \\[\\begin{align*} Q &amp;= kA^TA + \\frac1{\\Delta t^2}M\\\\ b&amp;= kA^Td + \\frac{1}{\\Delta t^2}M(2P^t - P^{t-\\Delta t})\\\\ P^{t+\\Delta t} &amp;= \\arg\\min_p \\frac12 tr(P^TQP) - tr(P^Tb) \\end{align*}\\] <p>Note this is the quadratic formula and </p> \\[\\nabla \\hat E(p) = QP - b = 0\\] <p>solve to have \\(P = Q^{-1}b\\) Note that \\(Q\\) involves only \\(A, M, k, \\Delta t\\), which are all constantly defined, hence it can be precomputed and factorized into \\(Q = LL^T\\), which \\(L\\) is triangular, hence solve \\(QP = b\\) (\\(O(n^3)\\) ops) becomes solve \\(LL^TP = b\\) (\\(O(n^2)\\) ops)</p>"},{"location":"csc418/mass_spring_system.html#sparse-matrices","title":"Sparse Matrices","text":"<p>Note that in most cases, \\(A, M\\) are quite sparse, where \\(\\frac{2}{n}\\) of \\(A\\) and \\(\\frac1n\\) of \\(M\\) are non-zero. Instead of storing the matrix as \\(O(n^2)\\) entries, using a sparse matrix, where takes only <code>n * (int, int, float)</code> space. Also, operations on sparse matrix are fewer \\(O(n^{\\approx 1.5})\\) for precompute \\(Q\\) and \\(O(n)\\) for substitution.</p>"},{"location":"csc418/mass_spring_system.html#pinned-vertices","title":"Pinned Vertices","text":"<p>Note that if we simulate gravity \\(g\\) within \\(f^{\\text{ext}}\\), all the mass point will be pulled down quickly, therefore, we need to fix some mass point as pinned vertices, i.e. \\(p_k = p_k^{rest}, \\forall k\\) is pinned. To implement this equality constraint, we use penalty method, i.e. add an extremely large penalty onto the pinned vertices, </p> \\[\\frac w2 \\sum_{k} \\|p_k - p_k^{rest}\\|^2\\] <p>when \\(p_k\\) moves away from \\(p^{rest}\\), the potential energy is much larger than the energy in the system, hence \\(p_k\\) will be forced to be \\(p_k^{rest}\\).  </p> <p>Therefore, we can have \\(C\\in \\mathbb R^{|pinned|\\times n}, C_{ki} = \\mathbb I(p_i\\text{ is the kth pinned vertex})\\) and the penalty term becomes </p> \\[\\frac w2 tr((CP-CP^{rest})^T(CP-CP^{rest}))\\] <p>so that we have </p> \\[Q_{C} = wC^TC, b_C = wC^TCP^{rest}\\] <p>And we need to solve \\((Q+Q_C)P = (b+b_C)\\) Note that \\(Q+Q_C\\) is still constant, hence prefactorization still works.</p>"},{"location":"csc418/meshes.html","title":"Meshes","text":""},{"location":"csc418/meshes.html#triangle-meshes","title":"Triangle Meshes","text":"<p>In order to save bandwidth and disk spaces, the goal is to represent a surface as a network of triangles that connect to one another through shared vertices and edges. </p>"},{"location":"csc418/meshes.html#mesh-topology-manifold","title":"Mesh Topology (Manifold)","text":"<p>A 2D manifold is a surface in which a small neighborhood around any point could be smoothed out into a bit of flat surface. </p> <p>To verify a 2D mesh, we check that   1. every edge is shared by exactly two triangles  2. Every vertex has a single, complete loop of triangles around it </p> <p>Note that a mesh may not be closed, so that on the boundaries of a mesh with relaxed requirements, we check that   1. Every edge is used by either one or two triangles.  2. Every vertex connects to a single edge-connected set of triangles</p>"},{"location":"csc418/meshes.html#mesh-orientation","title":"Mesh orientation","text":"<p>The orientation defines the \"front/outside\" and \"back/inside\" of a mesh. The front will have counterclockwise order. A connected mesh is consistently oriented if its triangles all agree on which side is front, or equivalently every pair of adjacent triangles is consistently oriented. </p>"},{"location":"csc418/meshes.html#indexed-meshes","title":"Indexed Meshes","text":"<p>The simplest way to store a geometry is to store \\(n\\) vertices as</p> \\[V = \\{v_{0},..., v_{n-1}\\}, v_i \\in \\mathbb R^3\\] <p>and \\(m\\) faces/meshes as </p> \\[F = \\{F_{0}, ..., F_{m-1}\\}, F_{m_1} = (i, j, k) \\in \\{0, ..., n-1\\}^3\\] <p>Note that \\(i,j,k\\) can also define the orientation of the mesh triangles</p>"},{"location":"csc418/meshes.html#texture-mapping","title":"Texture Mapping","text":"<p>Texture mapping is a process of mapping image information onto a surface, i.e. </p> \\[T:\\mathbb R^2 \\rightarrow \\mathbb R^3\\] <p>Note that a flat surface is very easy to map (as a bilinear interpolation).</p>"},{"location":"csc418/meshes.html#obj-files","title":".OBJ Files","text":"<p>Geometric vertex A vertex can be specified in a line starting with the letter v. That is followed by \\((x,y,z[,w])\\) coordinates. \\(W\\) is optional and defaults to 1.0.  <pre><code># List of geometric vertices, with (x, y, z [,w]) coordinates, \n# w is optional and defaults to 1.0.\nv 0.123 0.234 0.345 1.0\nv ...\n...\n</code></pre></p> <p>Parameter space vertices A free-form geometry statement can be specified in a line starting with the string vp. Define points in parameter space of a curve or surface. u only is required for curve points, u and v for surface points and control points of non-rational trimming curves, and u, v and w (weight) for control points of rational trimming curves.</p> <pre><code># Parameter space vertices in ( u [,v] [,w] ) form; \n# free form geometry statement ( see below )\nvp 0.310000 3.210000 2.100000\nvp ...\n...\n</code></pre> <p>Face elements Faces are defined using lists of vertex, texture and normal indices in the format vertex_index/texture_index/normal_index for which each index starts at 1 and increases corresponding to the order in which the referenced element was defined. Polygons such as quadrilaterals can be defined by using more than three indices.</p> <p>OBJ files also support free-form geometry which use curves and surfaces to define objects, such as NURBS surfaces.</p> <p>Vertex indices A valid vertex index matches the corresponding vertex elements of a previously defined vertex list. If an index is positive then it refers to the offset in that vertex list, starting at 1. If an index is negative then it relatively refers to the end of the vertex list, -1 referring to the last element.</p> <p>Each face can contain three or more vertices.</p> <p>Vertex texture coordinate indices Optionally, texture coordinate indices can be used to specify texture coordinates when defining a face. To add a texture coordinate index to a vertex index when defining a face, one must put a slash immediately after the vertex index and then put the texture coordinate index. No spaces are permitted before or after the slash. A valid texture coordinate index starts from 1 and matches the corresponding element in the previously defined list of texture coordinates. Each face can contain three or more elements.</p> <p>Vertex normal indices Optionally, normal indices can be used to specify normal vectors for vertices when defining a face. To add a normal index to a vertex index when defining a face, one must put a second slash after the texture coordinate index and then put the normal index. A valid normal index starts from 1 and matches the corresponding element in the previously defined list of normals. Each face can contain three or more elements.</p> <pre><code># List of texture coordinates, in (u, [,v ,w]) coordinates,\n# these will vary between 0 and 1. v, w are optional and default to 0.\nvt 0.500 1 [0]\nvt ...\n...\n# List of vertex normals in (x,y,z) form; \n# normals might not be unit vectors.\nvn 0.707 0.000 0.707\nvn ...\n...\n\n# Polygonal face element\nf 1 2 3\nf 3/1 4/2 5/3\nf 6/4/1 3/5/3 7/6/5\nf 7//1 8//2 9//3\nf ...\n...\n</code></pre> <p>Line element Records starting with the letter \"l\" specify the order of the vertices which build a polyline. <pre><code>l 5 8 1 2 4 9\n</code></pre></p> Quad face cube<pre><code>def cube():\n    # The 8 vertices\n    V = np.array([\n        [1, -1, 1], [-1, -1, 1], [-1, 1, 1], [1, 1, 1], \n        [1, 1, -1], [-1, 1, -1], [-1, -1, -1], [1, -1, -1]])\n    # The 6 faces\n    F = np.array([\n        [0, 7, 4, 3], [0, 3, 2, 1], [1, 2, 5, 6],\n        [7, 6, 5, 4], [3, 4, 5, 2], [0, 1, 6, 7]\n    ])\n\n    # 14 points on the cube\n    UV = np.array([\n        [0.  , 0.25], [0.25, 0.25], [0.25, 0.  ], [0.5 , 0.  ],\n        [0.5 , 0.25], [0.75, 0.25], [1.  , 0.25], [1.  , 0.5 ],\n        [0.75, 0.5 ], [0.5 , 0.5 ], [0.5 , 0.75], [0.25, 0.75],\n        [0.25, 0.5 ], [0.  , 0.5 ]\n    ])\n\n    # Map the 14 UV points onto the endpoints of the face\n    UF = np.array([\n        [ 1,  4,  9, 12], [12,  9, 10, 11], [ 5,  6,  7,  8],\n        [ 1,  2,  3,  4], [ 4,  5,  8,  9], [ 0,  1, 12, 13]\n    ])\n\n    # The normal direction of each face\n    NV = np.array([\n        [ 1,  0,  0], [ 0,  0,  1], [-1,  0,  0], \n        [ 0,  0, -1], [ 0,  1,  0], [ 0, -1,  0]\n    ])\n\n    return V, F, UV, UF, NV\n</code></pre> <p></p>"},{"location":"csc418/meshes.html#sphere-mapping","title":"Sphere Mapping","text":"<p>Note that a sphere centered at \\(c = (c_x, c_y, c_z)\\) and radius \\(r\\) can be represented as the image of </p> \\[S:\\mathbb  [0, 2\\pi) \\times [0, \\pi]\\rightarrow \\mathbb R^3:= (\\theta, \\phi) = c + r(\\sin\\phi\\cos\\theta, \\sin\\phi\\sin\\theta, \\cos\\phi)\\] <p>Therefore, we can sample evenly take \\(u\\times v\\) values from the domain</p> Quad face sphere<pre><code>def sphere(num_faces_u=16, num_faces_v=16):\n    thetas = np.tile(np.linspace(0, 2 * np.pi, num_faces_u+1), num_faces_v)[:, None]\n    phis = np.repeat(np.linspace(0, np.pi, num_faces_v), num_faces_u+1)[:, None]\n\n    V = np.hstack([\n        np.sin(phis) * np.cos(thetas),\n        np.sin(thetas) * np.sin(phis),\n        np.cos(phis)\n    ])\n\n    f_base = np.tile(np.arange(1, num_faces_u+1), num_faces_v)\n    f_base = f_base + (num_faces_v)* np.repeat(np.arange(num_faces_v), num_faces_u)\n\n    F = np.hstack([\n        f_base[:, None], \n        f_base[:, None] + 1, \n        f_base[:, None] + num_faces_u + 1, \n        f_base[:, None] + num_faces_u\n    ])\n\n    UV = np.hstack([\n        np.tile(np.linspace(0, 1, num_faces_u+1), num_faces_v)[:, None],\n        np.repeat(np.linspace(0, 1, num_faces_u+1), num_faces_v)[:, None]\n    ])\n\n    UF = F\n\n    # We approx the normal by the top left vertex\n    NV = -V\n    return V, F, UV, UF, NV\n</code></pre> <p></p>"},{"location":"csc418/raster_image.html","title":"Raster Image","text":""},{"location":"csc418/raster_image.html#image","title":"Image","text":"<p>Consider image as function</p> \\[I(x, y): R\\subset \\mathbb R^2 \\rightarrow V\\] <p>where \\(R\\) is the area (in most cases rectangle area) and \\(V\\) is the set of possible pixel values.  For example, for a grayscale image, \\(V= \\mathbb R^+\\), a.k.a. the brightness. For a idealized color image, with RGB values at each pixel, then \\(V=(\\mathbb R^3)^+\\)</p> <p>A raster iamge is then the sample of the continuous image. Each pixel is a sample and the rectangular domain of a \\(W\\times H\\) image is </p> \\[R = [-0.5, W-0.5]\\times [-0.5, H-0.5]\\]"},{"location":"csc418/raster_image.html#pixels-and-subpixels","title":"Pixels and subpixels","text":"<p>The raw color measurements made by modern digital cameras are typically stored with a single color channel per pixel. This information is stored as a seemingly 1-channel image, but with an understood convention for interpreting each pixel as the red, green or blue intensity value given some pattern. The most common is the Bayer pattern. In this assignment, we'll assume the top left pixel is green, its right neighbor is blue and neighbor below is red, and its kitty-corner neighbor is also green.</p> <p>To demosaic an image, we would like to create a full rgb image without downsampling the image resolution. So for each pixel, we'll use the exact color sample when it's available and average available neighbors (in all 8 directions) to fill in missing colors. This simple linear interpolation-based method has some blurring artifacts and can be improved with more complex methods.</p> Simulate Bayer Filter<pre><code>def simulate_bayer_filter(image):\n    output = np.empty(image.shape[:2])\n    output[::2, ::2] = image[::2, ::2, 1] \n    output[1::2, 1::2] = image[1::2, 1::2, 1]\n    output[::2, 1::2] = image[::2, 1::2, 0]\n    output[1::2, ::2] = image[1::2, ::2, 2]\n    return output\n</code></pre> Demosaic<pre><code>def demosaic(image):\n    corner_kernel = np.array([[.25, 0, .25], \n                              [0,   0, 0], \n                              [.25, 0, .25]])\n    cross_kernel = np.array([[0.,  .25, 0. ], \n                             [.25, 0.,  .25], \n                             [0.,  .25, 0. ]])\n    h_kernel = np.array([[.5, 0., .5]])\n    v_kernel = h_kernel.T\n\n    output = np.empty(list(image.shape) + [3])\n\n    # fill in the R value\n    # if bayer pixel is red, take self\n    output[::2, 1::2, 0] = image[::2, 1::2]\n    # if bayer pixel is green, \n    output[::2, ::2, 0] = convolve2d(image, h_kernel, mode=\"same\")[::2, ::2]\n    output[1::2, 1::2, 0] = convolve2d(image, v_kernel, mode=\"same\")[1::2, 1::2]\n    # if bayer pixel is blue, take corners\n    output[1::2, ::2, 0] = convolve2d(image, corner_kernel, mode=\"same\")[1::2, ::2]\n\n    # fill in the G value\n    # if bayer pixel is red or blue, take the cross\n    output[::2, 1::2, 1] = convolve2d(image, cross_kernel, mode=\"same\")[::2, 1::2]\n    output[1::2, ::2, 1] = convolve2d(image, cross_kernel, mode=\"same\")[1::2, ::2]\n    # if bayer pixel is green, take self\n    output[::2, ::2, 1] = image[::2, ::2]\n    output[1::2, 1::2, 1] = image[1::2, 1::2]\n\n    # fill in the blue value\n    # if bayer pixel is red, take corners\n    output[::2, 1::2, 2] = convolve2d(image, corner_kernel, mode=\"same\")[::2, 1::2]\n    # if bayer pixel is green. take top and botton neightbors\n    output[::2, ::2, 2] = convolve2d(image, v_kernel, mode=\"same\")[::2, ::2]\n    output[1::2, 1::2, 2] = convolve2d(image, h_kernel, mode=\"same\")[1::2, 1::2]\n    # if bayer pixel is blue, take self\n    output[1::2, ::2, 2] = image[1::2, ::2]\n    return output.astype(np.uint8)\n</code></pre> <p></p>"},{"location":"csc418/raster_image.html#conversion-between-rgb-and-hsv","title":"Conversion between RGB and HSV","text":"<p>HSV stands for hue, saturation, and value. Is another way of representing an image. Which is to have a more closely align with the way human vision perceives color-making attributes. \\(H\\in [0, 360]\\) is a periodic measurement and \\(S\\in [0, 1], V\\in[0, 1]\\)</p>"},{"location":"csc418/raster_image.html#rgb-to-hsv","title":"RGB to HSV","text":"<p>Given RGB values in \\([0, 1]^3\\), First calculate the chroma range, </p> \\[C_{\\min} = \\min(R, G,B), C_{\\max} = \\max(R, G, B), \\Delta = C_{\\max} - C_{\\min}\\] <p>Value</p> \\[V = C_{\\max}\\] <p>\\(V\\) is the max of lightness among the RGB channels</p> <p>Hue</p> \\[H = \\begin{cases} 0 &amp;\\Delta = 0\\\\ 60 (\\frac{G-B}\\Delta) &amp; C_{\\max} = R\\\\ 60(\\frac{B-R}{\\Delta} + 2) &amp;C_{\\max} = G\\\\ 60(\\frac{R-G}{\\Delta} + 4) &amp;C_{\\max} = B \\end{cases}\\] <p>Note that when \\(C_{\\min} = C_{\\max}\\Leftrightarrow \\Delta = 0\\), this means the color is black/white.  </p> <p>Saturation </p> \\[S = \\begin{cases} 0 &amp;\\Delta = 0\\\\ \\Delta / C_{\\max} &amp;C_{\\max} \\neq 0 \\end{cases}\\]"},{"location":"csc418/raster_image.html#hsv-to-rgb","title":"HSV to RGB","text":"<p>Given \\(H\\in [0, 360), S, V \\in [0,1]\\) (Note that since \\(H\\) is periodic, if \\(H\\in \\mathbb R\\), just take mod of 360)  </p> \\[\\begin{align*} C &amp;= V\\cdot S\\\\ X &amp;= C(1-|(\\frac{H}{60}\\mod 2) - 1|)\\\\ m &amp;= V - C\\\\ RGB_{raw} &amp;= \\begin{cases} (C, X, 0) &amp;H\\in [0, 60)\\\\ (X, C, 0) &amp;H\\in [60, 120)\\\\ (0, C, X) &amp;H\\in [120, 180)\\\\ (0, X, C) &amp;H\\in [180, 240)\\\\ (X, 0, C) &amp;H\\in [240, 300)\\\\ (C, 0, X) &amp;H\\in [300, 360)\\\\ \\end{cases}\\\\ RGB &amp;= 255(RGB_{raw} + m) \\end{align*}\\] <pre><code>def rgb2hsv(image):\n    output = np.zeros(image.shape)\n    if image.dtype == np.uint8:\n        image = image.astype(float) / 255\n    r, g, b = image[:, :, 0], image[:, :, 1], image[:, :, 2]\n    c_max = np.max(image, axis=2)\n    c_min = np.min(image, axis=2)\n    delta = c_max - c_min\n    r_indice = np.logical_and(c_max == r, delta &gt; 0)\n    output[r_indice, 0] = 60 * ((g[r_indice] - b[r_indice]) / delta[r_indice])\n\n    g_indice = np.logical_and(c_max == g, delta &gt; 0)\n    output[g_indice, 0] = 60 * ((b[g_indice] - r[g_indice]) / delta[g_indice] + 2)\n\n    b_indice = np.logical_and(c_max == b, delta &gt; 0)\n    output[b_indice, 0] = 60 * ((r[b_indice] - g[b_indice]) / delta[b_indice] + 4)\n\n    output[:, :, 0] = np.mod(output[:, :, 0], 360)\n\n    output[c_max &gt; 0, 1] = delta[c_max &gt; 0] / c_max[c_max &gt; 0]\n    output[:, :, 2] = c_max\n    return output\n\ndef hsv2rgb(image):\n    h, s, v = image[:, :, 0], image[:, :, 1], image[:, :, 2]\n    c = v * s\n    x = c * (1 - np.abs(np.mod(h/60, 2) - 1))\n    m = v - c\n\n    rgb = np.zeros(image.shape)\n\n    cond = np.logical_and(h &gt;= 0, h &lt; 60)\n    rgb[cond, 0] = c[cond]\n    rgb[cond, 1] = x[cond]\n\n    cond = np.logical_and(h &gt;= 60, h &lt; 120)\n    rgb[cond, 0] = x[cond]\n    rgb[cond, 1] = c[cond]\n\n    cond = np.logical_and(h &gt;= 120, h &lt; 180)\n    rgb[cond, 1] = c[cond]\n    rgb[cond, 2] = x[cond]\n\n    cond = np.logical_and(h &gt;= 180, h &lt; 240)\n    rgb[cond, 1] = x[cond]\n    rgb[cond, 2] = c[cond]\n\n    cond = np.logical_and(h &gt;= 240, h &lt; 300)\n    rgb[cond, 0] = x[cond]\n    rgb[cond, 2] = c[cond]\n\n    cond = np.logical_and(h &gt;= 300, h &lt;= 360)\n    rgb[cond, 0] = c[cond]\n    rgb[cond, 2] = x[cond]\n\n    return np.round(255 * (rgb + m[:, :, np.newaxis])).astype(np.uint8)\n</code></pre> <p></p> <p>With HSV images, we can easily tune the lightness, hue, and saturation</p> <pre><code>def hue_shift(image, shift):\n    output = image.copy()\n    output[:, :, 0] = np.mod(output[:, :, 0] + shift, 360)\n    return output\n\ndef saturate_change(image, factor):\n    output = image.copy()\n    output[:, :, 1] = np.clip(output[:, :, 1] * (1 + factor), 0, 1)\n    return output\n\ndef lightness_change(image, factor):\n    output = image.copy()\n    output[:, :, 2] = np.clip(output[:, :, 2] * (1 + factor), 0, 1)\n    return output\n</code></pre> <p></p>"},{"location":"csc418/raster_image.html#alpha-compositing","title":"Alpha Compositing","text":"<p>If we want to composite a foreground color \\(c_f\\) over background color \\(c_b\\) , and the fraction of the pixel covered by the foreground is \\(\\alpha\\), then we can use the formula </p> \\[c = ac_f + (1-a) c_b\\]"},{"location":"csc418/shader_pipeline.html","title":"Shader Pipeline","text":""},{"location":"csc418/shader_pipeline.html#viewing-transformations","title":"Viewing Transformations","text":"<p>The viewing transformations inputs a canonical coordinate \\((x, y, z)\\) to some \\((x, y)\\) on the 2D image space. One common system is a sequence of 3 transformations  - camera transformation \\((x, y, z)\\rightarrow (x_c, y_c, z_c)\\) given \\(eye\\) and orientation \\(u, v, w\\)  - projection transformation \\((x_c, y_c, z_c) \\rightarrow (x_v, y_v)\\), \\(x_v, y_v\\in [-1, 1]\\) all the points that  are visible in the camera space given the type of projection desired  - viewport transformation \\((x_v, y_v)\\rightarrow (I_x, I_y)\\) maps the unit image rectangle to desired rectangle in pixel coordinates</p>"},{"location":"csc418/shader_pipeline.html#perspective-projections","title":"Perspective Projections","text":""},{"location":"csc418/shader_pipeline.html#shader-pipeline-opengl","title":"Shader Pipeline (OpenGL)","text":"<p>Vertex specification Set up Vertex Array Object (VAO), which contains one or more Vertex BUffer Objects (VBO), each VBO stores some information about each vertex. For example, if we load a .obj file, then VAO (the object) may end up having several VBOs, for example, one VBO stores vertex positions, one VBO stores vertex colors, and another one stores vertex normals.</p> <p>Vertex shader Performs operation on every vertex, doing all the homogeneous transformations, i.e.</p> <ul> <li>\\(M\\): modeling transformation: to move object into world space, doing all the translations, rotations, scaling, etc. </li> <li>\\(V\\) viewing transformation/camera transformation: transforms from world coordinates to camera coordinates. </li> <li>\\(P\\) perspective projection matrix, so that we only consider vertex in the window space (visible within the camera), and normalize \\((x, y, z)\\) by \\(w\\).</li> </ul> <p>Tessellation patches of vertex data are subdivided into smaller Primitives. Tessellation control shader (TCS) determines how much tessellation to do and  tessellation evaluation shader (TES) takes the tessellated patch and computes vertex values for each generated vertex. For example Catmull\u2013Clark subdivision can be a TCS/TES algorithm.</p> <p>Rasterization Given the tessellated primitives, filled in the primitive with pixels. </p> <p>Fragment shader Given a sample-sized segment of a rasterized Primitive, fragment shader computes a set of colors and a single depth value. In our cases, it will be pixel-wise coloring.</p> <p>Note that the shader pipeline is only for <code>OpenGL</code>. In other frameworks (<code>WebGL</code>, <code>DirectX</code>, etc.) the abstractions are different. For example, <code>WebGL</code> does not have support for tessellation and you have to do it in <code>js</code> (likely on CPU). </p> vertex shader example<pre><code>varying vec3 vViewPos;\nvoid main() {\n    vPos = position;\n    vViewPos = (modelViewMatrix * vec4(position, 1.0)).xyz;\n    vNormal = normalMatrix * normalize(normal);\n    gl_Position = projectionMatrix * modelViewMatrix * vec4(position, 1.0);;   \n}\n`\n</code></pre> fragment shader example<pre><code>// fragment shader using Blinn Phong shading\nvec3 blinn_phong(vec3 ka, vec3 kd, vec3 ks, float p, vec3 n, vec3 v, vec3 l) {\n    float Ia = 0.4;\n    vec3 h = normalize(v+l);\n    return ka*Ia + kd*max(dot(l,n),0.0) + ks*pow(max(dot(h,n),0.0),p);\n\nvarying vec3 vPos;\nvarying vec3 vNormal;\nvarying vec3 vViewPos;\nvoid main() {\n    vec3 l = mat3(viewMatrix) * normalize(vec3(1, -1, -1));\n    vec3 n = normalize(vNormal);\n    vec3 v = normalize(-vViewPos);\n    vec3 color = mix(vec3(0.9,0.2,0.3), vec3(0.5,0.45,0.5), is_moon);\n    vec3 fragColor = blinn_phong(\n        color, color, vec3(1.0, 1.0, 1.0), 300.0,\n        n, v, l\n    );\n    gl_FragColor = vec4(fragColor, 1.0);\n}\n</code></pre>"},{"location":"csc418/shader_pipeline.html#value-noise-and-procedural-patterns","title":"Value Noise and Procedural Patterns","text":"<p>Other than a texture mapping, we can also generate patterns, i.e. procedural patterns. For example, if we want to make a ocean texture, we can generate some waves mesh and color it by some algorithm, instead of map a 2D image onto it. </p>"},{"location":"csc418/shader_pipeline.html#noise","title":"Noise","text":"<p>Note that in reality, lots of patterns need some sort of \"randomness\", s.t. the volume of a cloud on the sky, the waves of the water, etc. </p>"},{"location":"csc418/shader_pipeline.html#properties-of-ideal-noise","title":"Properties of Ideal Noise","text":"<ul> <li>pseudo random Given the same input, it should always return the same value. </li> <li>dimension The noise function is some \\(N:\\mathbb R^d\\rightarrow \\mathbb R\\), which is a \\(d\\)-dim noise function. </li> <li>band limited One of the frequencies dominates all others. </li> <li>continuity / differentiability We want the change in local is small, but change in global is large</li> </ul>"},{"location":"csc418/shader_pipeline.html#perlin-noise","title":"Perlin Noise","text":"<p>Perlin noise is a example of value noise, it's pseudo random, and continuous, and good in producing marble like surfaces. </p>"},{"location":"csc418/shader_pipeline.html#algorithm","title":"Algorithm","text":"<p>Grid Definition Define an n-dim grid where each point has a random n-dim unit-length gradient vector. </p> <p>Dot product Assume \\(3D\\) case and each box grid has side length 1. For query position \\((x, y, z)\\), it is located in some \\(d\\)-dim grid formed by \\(2^3\\) grid points, \\((\\lfloor x\\rfloor, \\lceil x\\rceil)\\times (\\lfloor y\\rfloor, \\lceil y\\rceil)\\times (\\lfloor z\\rfloor, \\lceil z\\rceil)\\). Generate \\(2^d\\) dotGridGradient by dot product the offset to each grid point and gradient at that grid point. </p> <p>Interpolation Note that we have \\(2^3\\) scale values, and we will have a trilinear interpolation so that we can get the value at that point. Note that we take a smooth step \\(s:\\mathbb R^d\\rightarrow \\mathbb R^d\\) as the coefficient of interpolation. \\(s\\) must have the property \\(s(0) = s(1) = s'(0) = s'(1) = 0\\), one good smooth step function is </p> \\[s(t) = 3t^2 - 2t^3\\]"},{"location":"csc418/shader_pipeline.html#improved-perlin-noise","title":"Improved Perlin Noise","text":"<p>Note that \\(s''(t) = 6 - 12t\\) is not continuous,  if the derivative of the function used to compute the normal of the displaced mesh is not continuous then it will introduce a discontinuity in these normals wherever \\(x=0, 1\\), so that we use improved smooth step</p> \\[6t^5 - 15t^4 + 10t^3\\] <p>Also, note that when random directions (gradient direction) is close to standard direction \\(e_i\\), the noise function have very high values \\(\\sim1\\) causing a \"splotchy appearance\", so instead of using random directions, we use directions randomly chosen from 12 directions \\((\\pm 1,\\pm 1,0), (\\pm 1, 0, \\pm 1), (0, \\pm 1, \\pm 1)\\)</p> Random sampling from sphere<pre><code>vec3 random_direction(vec3 seed) {\n    vec2 r2 = random2(seed);\n    float z = 2.0 * r2.x - 1.0;\n    float a = r2.y * 2.0 * 3.1415926;\n    float r = sqrt(1.0 - z*z);\n    return vec3(z, r * cos(a), r * sin(a));\n}\n</code></pre> Smooth step and improved smooth step<pre><code>vec3 smooth_step(vec3 f) { \n    return f * f * (3.0 - 2.0 * f); \n}\n// The improved version\nvec3 smooth_step( vec3 f) { \n    return 6.0*f*f*f*f*f - 15.0*f*f*f*f + 10.0*f*f*f; \n}\n</code></pre> Perlin noise from 3D Seed<pre><code>float perlin_noise(vec3 st) {\n    vec3 i = floor(st);\n    vec3 f = fract(st);\n    vec3 u = smooth_step(f);\n\n    float dotgrad000 = dot(random_direction(i + vec3(0.0, 0.0, 0.0)), f - vec3(0.0, 0.0, 0.0));\n    float dotgrad001 = dot(random_direction(i + vec3(0.0, 0.0, 1.0)), f - vec3(0.0, 0.0, 1.0));\n    float dotgrad010 = dot(random_direction(i + vec3(0.0, 1.0, 0.0)), f - vec3(0.0, 1.0, 0.0));\n    float dotgrad011 = dot(random_direction(i + vec3(0.0, 1.0, 1.0)), f - vec3(0.0, 1.0, 1.0));\n    float dotgrad100 = dot(random_direction(i + vec3(1.0, 0.0, 0.0)), f - vec3(1.0, 0.0, 0.0));\n    float dotgrad101 = dot(random_direction(i + vec3(1.0, 0.0, 1.0)), f - vec3(1.0, 0.0, 1.0));\n    float dotgrad110 = dot(random_direction(i + vec3(1.0, 1.0, 0.0)), f - vec3(1.0, 1.0, 0.0));\n    float dotgrad111 = dot(random_direction(i + vec3(1.0, 1.0, 1.0)), f - vec3(1.0, 1.0, 1.0));\n\n    float x00 = mix(dotgrad000, dotgrad100, u.x);\n    float x01 = mix(dotgrad001, dotgrad101, u.x);\n    float x10 = mix(dotgrad010, dotgrad110, u.x);\n    float x11 = mix(dotgrad011, dotgrad111, u.x);\n\n    float xy0 = mix(x00, x10, u.y);\n    float xy1 = mix(x01, x11, u.y);\n\n    return mix(xy0, xy1, u.z);\n}\n</code></pre>"},{"location":"csc418/shader_pipeline.html#bump-mapping-and-normal-mapping","title":"Bump Mapping and Normal Mapping","text":"<p>The real surface is often rough and bumpy, we use bump mapping algorithm to achieve the same effect. </p> \\[\\tilde p(p):\\mathbb R^3\\rightarrow\\mathbb R^3 := p + h(p)\\hat n (p)\\] <p>where \\(p\\) is the original position, \\(\\hat n\\) is the normal and \\(h\\) is the bump height function. </p> <p>Then, note that we have to calculate a new normal for the bumped point as </p> \\[\\tilde n(p) = \\partial_Tp\\times \\partial_Bp \\approx \\frac{\\tilde p(p +\\epsilon T)-\\tilde p(p)}{\\epsilon} \\times \\frac{\\tilde p(p +\\epsilon B)-\\tilde p(p)}{\\epsilon}\\] <p>where \\(T, B\\) are the tangent and bitangent vector where </p> \\[T = \\min\\{\\hat n \\times (0, 1, 0),  \\hat n \\times (0, 0, 1)\\}, B = T\\times N\\] <p>Note that bump mapping does not actually change the vertex position, it is used to obtain the normal mapping so that we can apply the coloring and make the surface looks \"bumpy\"</p> Using random bump to remap normal<pre><code>    vec3 bp = bump_position(is_moon, sphere_fs_in);\n    float eps = 0.0001;\n    vec3 T;\n    vec3 B;\n    tangent(n,T,B);\n    vec3 pT = sphere_fs_in+eps*T;\n    vec3 pB = sphere_fs_in+eps*B;\n    n = normalize(cross(\n        bump_position(is_moon, pT)-bp,\n        bump_position(is_moon, pB)-bp));\n    n = normalMatrix * normalize(n);\n</code></pre>"},{"location":"csc418/shader_pipeline.html#shader-demo","title":"Shader Demo","text":"<p>A demo of shaders, noise, and bump mapping</p> Shader code <pre><code>// Shader programs are merely strings and are only \n// loaded onto GPU\nconst blinn_phong = `\nvec3 blinn_phong(vec3 ka, vec3 kd, vec3 ks, float p, vec3 n, vec3 v, vec3 l) {\n    float Ia = 0.4;\n    vec3 h = normalize(v+l);\n    return ka*Ia + kd*max(dot(l,n),0.0) + ks*pow(max(dot(h,n),0.0),p);\n}\n`\n\nconst random_direction = `\n// pseudo-random 2D vector from 2D or 3D seed\nvec2 random2(vec2 st) {\n    st = vec2( \n        dot(st,vec2(127.1,311.7)),\n        dot(st,vec2(269.5,183.3))\n    );\n    return fract(sin(st)*43758.5453123);\n}\n\nvec2 random2(vec3 st){\n    vec2 S = vec2( \n        dot(st,vec3(127.1,311.7,783.089)),\n        dot(st,vec3(269.5,183.3,173.542))\n    );\n    return fract(sin(S)*43758.5453123);\n}\n// A uniform (pseudo-)random from unit sphere from a seed\nvec3 random_direction(vec3 seed) {\n    vec2 r2 = random2(seed);\n    float z = 2.0 * r2.x - 1.0;\n    float a = r2.y * 2.0 * 3.1415926;\n    float r = sqrt(1.0 - z*z);\n    return vec3(z, r * cos(a), r * sin(a));\n}\n`\n\nconst smooth_step = `\nvec3 smooth_step(vec3 f) { \n    return f * f * (3.0 - 2.0 * f); \n}\n`\n\nconst improved_smooth_step = `\nvec3 smooth_step( vec3 f) { \n    return 6.0*f*f*f*f*f - 15.0*f*f*f*f + 10.0*f*f*f; \n}\n`\nconst perlin_noise = `\nfloat perlin_noise(vec3 st) {\n    vec3 i = floor(st);\n    vec3 f = fract(st);\n    vec3 u = smooth_step(f);\n\n    float dotgrad000 = dot(random_direction(i + vec3(0.0, 0.0, 0.0)), f - vec3(0.0, 0.0, 0.0));\n    float dotgrad001 = dot(random_direction(i + vec3(0.0, 0.0, 1.0)), f - vec3(0.0, 0.0, 1.0));\n    float dotgrad010 = dot(random_direction(i + vec3(0.0, 1.0, 0.0)), f - vec3(0.0, 1.0, 0.0));\n    float dotgrad011 = dot(random_direction(i + vec3(0.0, 1.0, 1.0)), f - vec3(0.0, 1.0, 1.0));\n    float dotgrad100 = dot(random_direction(i + vec3(1.0, 0.0, 0.0)), f - vec3(1.0, 0.0, 0.0));\n    float dotgrad101 = dot(random_direction(i + vec3(1.0, 0.0, 1.0)), f - vec3(1.0, 0.0, 1.0));\n    float dotgrad110 = dot(random_direction(i + vec3(1.0, 1.0, 0.0)), f - vec3(1.0, 1.0, 0.0));\n    float dotgrad111 = dot(random_direction(i + vec3(1.0, 1.0, 1.0)), f - vec3(1.0, 1.0, 1.0));\n\n    float x00 = mix(dotgrad000, dotgrad100, u.x);\n    float x01 = mix(dotgrad001, dotgrad101, u.x);\n    float x10 = mix(dotgrad010, dotgrad110, u.x);\n    float x11 = mix(dotgrad011, dotgrad111, u.x);\n\n    float xy0 = mix(x00, x10, u.y);\n    float xy1 = mix(x01, x11, u.y);\n\n    return mix(xy0, xy1, u.z);\n}\n`\n\nconst bump = random_direction + improved_smooth_step + perlin_noise + `\nfloat smooth_heaviside( float x, float t) {\n    return (1./(1.+exp(-2.*t*(x)))-1./2.)/(1./(1.+exp(-2.*t*1.))-1./2.);\n  }\nfloat bump_height(float is_moon, vec3 s) {\nfloat b = \n    0.05 *(0.5+0.5 * smooth_heaviside(perlin_noise(1.0 * s), 10.0) * 2.0 - 1.0)\n    +(0.5 + 0.44*(1.0 - is_moon)) * (0.5 + 0.5 *smooth_heaviside((\n    +(0.6+0.14*is_moon)*perlin_noise(2.0*s)\n    +(0.2-0.04*is_moon)*perlin_noise(4.0*s)\n    ),8.0-is_moon*-s.x*7.0)*2.0-1.0)\n    +0.01*(0.5+0.5*smooth_heaviside((\n    +0.1*perlin_noise( 16.0*s)\n    ),4.0)*2.0-1.0)\n    -.5;\nreturn 0.06*b+0.07;\n}\nvec3 bump_position(float is_moon , vec3 s) {\nfloat bump = bump_height(is_moon, s);\nvec3 p = s;\nvec3 n = s;\nreturn p + 1.0*bump*n;\n}\nvoid tangent(in vec3 N, out vec3 T, out vec3 B) {\nvec3 x = vec3(1,0,0);\nT = normalize(x-dot(N,x)*N);\nB = cross(N,T);\n}\n`\nconst vertexShaders = {};\n\nvertexShaders.simpleVertexShader = `\nvarying vec3 vNormal;\nvarying vec3 vPos;\nvarying vec3 vViewPos;\nvoid main() {\n    vPos = position;\n    vViewPos = (modelViewMatrix * vec4(position, 1.0)).xyz;\n    vNormal = normalMatrix * normalize(normal);\n    gl_Position = projectionMatrix * modelViewMatrix * vec4(position, 1.0);;   \n}\n`\n\nconst fragmentShaders = {};\nfragmentShaders.normalFragmentShader = `\nvarying vec3 vNormal;\nvoid main() {\n    gl_FragColor = vec4(0.5 + 0.5 * vNormal, 1.0);\n}\n`\nfragmentShaders.posFragmentShader = `\nvarying vec3 vViewPos;\nvoid main() {\n    vec3 color = vec3(0.5,0.5,0)+vec3(0.5,0.5,0)*vViewPos.xyz;\n    gl_FragColor = vec4(color, 1.0);\n}\n`\nfragmentShaders.depthFragmentShader = `\nvarying vec3 vViewPos;\nvoid main() {\n    vec3 color = (1.0+(vViewPos.z - -3.0)/5.0)*vec3(1.0,1.0,1.0);\n    gl_FragColor = vec4(color, 1.0);\n}\n`\nfragmentShaders.phongFragmentShader = blinn_phong + `\nuniform float is_moon;\nuniform float time;\nvarying vec3 vPos;\nvarying vec3 vNormal;\nvarying vec3 vViewPos;\nvoid main() {\n    vec3 l = mat3(viewMatrix) * normalize(vec3(1, -1, -1));\n    vec3 n = normalize(vNormal);\n    vec3 v = normalize(-vViewPos);\n    vec3 color = mix(vec3(0.9,0.2,0.3), vec3(0.5,0.45,0.5), is_moon);\n    vec3 fragColor = blinn_phong(\n        color, color, vec3(1.0, 1.0, 1.0), 300.0,\n        n, v, l\n    );\n    gl_FragColor = vec4(fragColor, 1.0);\n}\n`\nfragmentShaders.perlinFragmentShader = blinn_phong + random_direction + smooth_step + perlin_noise + `\nuniform float is_moon;\nuniform float time;\nvarying vec3 vPos;\nvarying vec3 vNormal;\nvarying vec3 vViewPos;\nvoid main() {\n    vec3 l = mat3(viewMatrix) * normalize(vec3(1, -1, -1));\n    vec3 n = normalize(vNormal);\n    vec3 v = normalize(-vViewPos);\n    vec3 color = mix(vec3(0.9,0.2,0.3), vec3(0.5,0.45,0.5), is_moon);\n    vec3 sphere_fs_in = normalize(vPos);\n    float s = sin(5.0*(sphere_fs_in.y + 0.5 *perlin_noise( sphere_fs_in ))) * (0.991+0.009*perlin_noise( 2.0  * sphere_fs_in));\n    float s2 = 0.25*perlin_noise( 1.0 * sphere_fs_in ) \n    + 0.25*perlin_noise( 4.0 * sphere_fs_in ) \n    + 0.25*perlin_noise( 8.0 * sphere_fs_in ) \n    + 0.25*perlin_noise(16.0 * sphere_fs_in );\n    float s3 = max(s+0.4,0.0) * pow(min((0.5+0.5*(\n    (0.2*sin(10.0*(sphere_fs_in.x + perlin_noise( 8.0*sphere_fs_in )))\n    + 0.2*sin(15.0*(sphere_fs_in.z + perlin_noise( 8.0*sphere_fs_in )))\n    + 0.2*perlin_noise(16.0*sphere_fs_in))\n    + 0.6*perlin_noise(32.0*sphere_fs_in)\n    ))\n    ,1.0),2.0);\n    float b = 1.0-clamp(0.1*pow(s*0.5+0.5,20.0) + 0.7*(0.5*s2+0.5) + 0.2*s3, 0.0, 1.0);\n    color = blinn_phong(\n        b * color, b * color, vec3(1.0, 1.0, 1.0), 300.0,\n        n, v, l\n    );\n    gl_FragColor = vec4(color, 1.0);\n}\n`\n\nfragmentShaders.bumpFragmentShader = blinn_phong + bump + `\nuniform float is_moon;\nuniform float time;\nuniform mat3 normalMatrix;\nvarying vec3 vPos;\nvarying vec3 vNormal;\nvarying vec3 vViewPos;\nvoid main() {\n    vec3 sphere_fs_in = vPos;\n    vec3 l = mat3(viewMatrix) * normalize(vec3(1, -1, -1));\n    vec3 n = sphere_fs_in;\n    vec3 v = normalize(-vViewPos);\n    vec3 color = mix(vec3(0.9,0.2,0.3), vec3(0.5,0.45,0.5), is_moon);\n    float s = sin(5.0*(sphere_fs_in.y + 0.5 *perlin_noise( sphere_fs_in ))) \n    * (0.991+0.009*perlin_noise( 2.0  * sphere_fs_in));\n    float s2 = 0.25*perlin_noise( 1.0 * sphere_fs_in ) \n    + 0.25*perlin_noise( 4.0 * sphere_fs_in ) \n    + 0.25*perlin_noise( 8.0 * sphere_fs_in ) \n    + 0.25*perlin_noise(16.0 * sphere_fs_in );\n    float s3 = max(s+0.4,0.0) * pow(min((0.5+0.5*(\n    (0.2*sin(10.0*(sphere_fs_in.x + perlin_noise( 8.0*sphere_fs_in )))\n    + 0.2*sin(15.0*(sphere_fs_in.z + perlin_noise( 8.0*sphere_fs_in )))\n    + 0.2*perlin_noise(16.0*sphere_fs_in))\n    + 0.6*perlin_noise(32.0*sphere_fs_in)\n    ))\n    ,1.0),2.0);\n    float b = 1.0-clamp(0.1*pow(s*0.5+0.5,20.0) + 0.7*(0.5*s2+0.5) + 0.2*s3, 0.0, 1.0);\n\n    vec3 bp = bump_position(is_moon, sphere_fs_in);\n    float eps = 0.0001;\n    vec3 T;\n    vec3 B;\n    tangent(n,T,B);\n    vec3 pT = sphere_fs_in+eps*T;\n    vec3 pB = sphere_fs_in+eps*B;\n    n = normalize(cross(\n        bump_position(is_moon, pT)-bp,\n        bump_position(is_moon, pB)-bp));\n    n = normalMatrix * normalize(n);\n\n    color = blinn_phong(\n        b * color, b * color, vec3(1.0, 1.0, 1.0), 300.0,\n        n, v, l\n    );    \n    gl_FragColor = vec4(color, 1.0);\n}\n`\n\nexport { vertexShaders, fragmentShaders };\n</code></pre>"},{"location":"csc419/curvature.html","title":"Curvature","text":""},{"location":"csc419/curvature.html#define-curvature","title":"Define Curvature","text":"<p>(more interesting notes can be found in MAT363)</p> <p>For a planar curve \\(\\gamma:[0, 1]\\rightarrow \\mathbb R^2\\), consider the following definitions</p>"},{"location":"csc419/curvature.html#osculating-circle","title":"Osculating circle","text":"<p>Define the tangent direction at a point \\(p=\\gamma(s)\\) as the secant line formed by approaching \\(p\\) from \\(q\\) on the curve, i,e,</p> \\[\\mathbf t (s) = \\lim_{\\mathbf q\\rightarrow \\mathbf p}\\frac{\\mathbf q-\\mathbf p}{\\|\\mathbf q-\\mathbf p\\|} = \\lim_{t\\rightarrow s}\\frac{\\gamma(t)-\\gamma(s)}{\\|\\gamma(t)-\\gamma(s)\\|} = \\frac{\\gamma'(s)}{\\|\\gamma'(s)\\|}\\] <p>Assume without loss of generality that \\(s\\) is an arc length parameterization, so that \\(\\|\\gamma'\\| = 1\\), then the unit tangent is simply \\(\\gamma'(s)\\). </p> <p>We can also consider the limit of the circum-circle \\(C(\\mathbf q_1, \\mathbf p, \\mathbf q_2)\\) (Note that a circum-circle is the circle passing through all the points, and 3 points can uniquely defines a circle), where \\(\\mathbf q_1 = \\gamma(t_1), \\mathbf q_2=\\gamma(t_2), t_1 &lt; s &lt; t_2\\), so that we can define the limit circum-circle as </p> \\[C(\\mathbf p) = \\lim_{\\mathbf q_1\\rightarrow \\mathbf p-, \\mathbf q_2\\rightarrow \\mathbf p+} C(\\mathbf q_1, \\mathbf p, \\mathbf q_2)\\] <p>We call \\(C(\\mathbf p)\\) the ocsulating circle at the point \\(\\mathbf p\\) on \\(\\gamma\\), the tangent of the curve and the circle match as \\(\\gamma'(s)\\) and the radius \\(R(\\mathbf p)\\) of \\(C(\\mathbf p)\\) is proportional to how straight the curve is locally. \\(R\\rightarrow \\infty\\) when the curve is getting to be a line. Therefore, the radius is inversely proportional the the curvyness. Hence, define the curvature as </p> \\[\\kappa(\\mathbf p) = R(\\mathbf p)^{-1}\\] <p>Thinking about how we construct the circum-circle from the three points, we can have </p> \\[R(\\mathbf p) = \\lim_{q_1,q_2\\rightarrow p} \\frac{\\|\\mathbf q_1-\\mathbf p\\|\\|\\mathbf p-\\mathbf q_2\\|\\|\\mathbf q_2-\\mathbf q_1\\|}{2|(\\mathbf q_1-\\mathbf p)\\times (\\mathbf p-\\mathbf q_2)|}\\]"},{"location":"csc419/curvature.html#signed-curvature","title":"Signed Curvature","text":"<p>Another way of thinking about curvature is by plugging in the arc-length parameterization, where the curvature is equal to the magnitude of change in the tangent, </p> \\[\\kappa(s) = \\lim_{t\\rightarrow s}\\|\\frac{\\gamma'(t) - \\gamma'(s)}{t-s}\\| = \\|\\gamma''(s)\\|\\] <p>Since we are having arc-length parameterization, the only change to the tangent \\(\\gamma'\\) is the change in direction. This means that the change is orthogonal to the tangent, i.e.</p> \\[\\gamma''\\cdot \\gamma' = 0\\Rightarrow y''\\cdot\\hat{\\mathbf n} = \\pm \\kappa \\hat{\\mathbf n}\\] <p>If we define an orientation of our curve (we have starting point and ending point as from \\(\\gamma(0)\\) to \\(\\gamma(1)\\)), then we can endow the curvature with a sign based on whether \\(C(\\mathbf p)\\) lies on the left of right side of the curve. And since the orthogonal relationship, the vector from \\(\\mathbf p\\) to the circle's center \\(c(\\mathbf p)\\) must be perpendicular to the tangent, i.e. the positive or negative normal directions. </p> <p>If the orientation agrees with increasing the arc-length parameter \\(s\\), then the sign can be determined by comparing the second derivative \\(\\gamma''\\) to the unit normal \\(\\hat{\\mathbf n} = \\gamma'^{\\perp}\\). The signed curvature at \\(\\mathbf p\\) is </p> \\[k(\\mathbf p) = \\text{sign} (\\gamma''(\\mathbf p)\\cdot\\hat{\\mathbf n})\\cdot \\kappa(\\mathbf p) = \\gamma''(\\mathbf p)\\cdot \\hat{\\mathbf n}\\]"},{"location":"csc419/curvature.html#moving-point-analogy","title":"Moving point analogy","text":"<p>Thinking about the curve as the trajectory formed by a moving particle, parameterized by time \\(t\\), hence \\(\\gamma'(t)\\) is the velocity and \\(\\|\\gamma'(t)\\|\\) the speed. Reparameterize the trajectory with a perfectly uniform speed \\(\\|\\gamma'(s(t))\\| = 1\\). </p> <p>Then, the curvature is the amount of \"turning\". Imagine this is driven by a steering wheel, then straight line is no turning, the circle is turning the wheel to a particular fixed angle (left = positive, right = negative) and changing the steering wheel is equal to changing the curvature. </p>"},{"location":"csc419/curvature.html#turning-number","title":"Turning number","text":"<p>The integrated signed curvature around a closed curve must be an integer multiple of \\(2\\pi\\), i.e. </p> \\[\\oint_S k(s)ds = 2\\pi\\tau\\] <p>\\(\\tau\\) is called the turning number of the curve. </p> <p>Note that we can accumulating the \"turning\", since we are on a closed curve, thinking about the moving point analogy, the point will eventually turning back for any starting point. Therefore, the accumulated direction can turn an integer times of a circle. </p>"},{"location":"csc419/curvature.html#discrete-curvature","title":"Discrete Curvature","text":"<p>To discretize a curve, we need to fit it to a piecewise linear chain of segments. And for each point on the chain, define the discrete curvature as the change in tangent direction</p> <p>\u200b </p> <p>The turning number still holds in this case, since we will \"turning back\" eventually, i.e. </p> \\[\\sum k_i = 2\\pi\\tau\\]"},{"location":"csc419/curvature.html#surface-curvature","title":"Surface Curvature","text":"<p>For a surface \\(\\mathcal S\\), note that along different directions, we have different curves (for example, thinking about a potato chip). Therefore, it's more difficult to understand the curvature of a surface. </p> <p>More specifically, we want to find some path/direction/trajectory on the surface, so that we can have a curve, and extend the definition curvature on that curve. </p>"},{"location":"csc419/curvature.html#normal-curvature","title":"Normal curvature","text":"<p>The simplest way to extend curvature on the point \\(\\mathbf p\\in \\mathcal S\\) is to \"cut\" through the surface with a plane \\(P\\) along the normal \\(\\mathbf n(\\mathbf p)\\). </p> <p>\u200b </p> <p>so that we get some curve from the intersection of \\(P\\) and \\(\\mathcal S\\). Note that there are infinitely many planes passing \\(\\mathbf p\\) and lie parallel to \\(\\mathbf n(\\mathbf q)\\), as we rotating \\(P\\) around \\(\\mathbf n\\) with any angle \\(\\varphi\\). Therefore, we can parameterize and define normal curvature for each choice of \\(\\varphi\\) as</p> \\[k_{\\mathbf n}(\\varphi, \\mathbf p) = \\gamma_\\varphi''(\\mathbf p)\\]"},{"location":"csc419/curvature.html#mean-curvature","title":"Mean curvature","text":"<p>Normal curvature defines infinitely many curvature values, which is undesirable. One way to reduce the space of normal curvature is the take the mean of all normal curvatures.</p> \\[H(\\mathbf p) = \\frac{1}{2\\pi} \\int_0^{2\\pi} k_{\\mathbf n}(\\varphi,\\mathbf p)d\\varphi\\]"},{"location":"csc419/curvature.html#maxmin-curvature","title":"Max/Min curvature","text":"<p>Another obvious way of reduction is </p> \\[k_1(\\mathbf p) = \\max_{\\varphi} k_{\\mathbf n}(\\varphi, \\mathbf p), \\quad k_2(\\mathbf p) = \\min_{\\varphi} k_{\\mathbf n}(\\varphi, \\mathbf p)\\] <p>\\(k_1,k_2\\) are referred as the principal curvatures. with corresponding angles</p> \\[\\varphi_1(\\mathbf p) = \\arg\\max_\\varphi k_{\\mathbf n}(\\varphi, \\mathbf p), \\quad \\varphi_2(\\mathbf p) = \\arg\\min_\\varphi k_{\\mathbf n}(\\varphi, \\mathbf p)\\] <p>By Euler's theorem</p> \\[k_{\\mathbf n}(\\varphi, \\mathbf p) = k_1\\cos^2\\varphi + k_2\\sin^2\\varphi\\] <p>And the corollary</p> <ol> <li>\\(\\varphi_1\\perp \\varphi_2\\)</li> <li>\\(H = \\frac12(k_1 + k_2)\\)</li> </ol>"},{"location":"csc419/curvature.html#gaussian-curvature","title":"Gaussian curvature","text":"<p>Max/min/mean curvature cannot alone distinguish between points lying on a round ball, a flat sheet of paper, and the potato chip. So we introduce Gaussian curvature, </p> \\[K = k_1k_2\\] <p>\\(K\\) maintains the disagreement in sign that categories the saddle-like behavior (positive in one direction and negative in another)</p>"},{"location":"csc419/curvature.html#relationship-to-surface-area","title":"Relationship to Surface Area","text":""},{"location":"csc419/curvature.html#mean-curvature-as-area-gradient","title":"Mean curvature as area gradient","text":"<p>Consider the problem of finding a flow of a given surface \\(\\mathcal S\\) in the direction that shrinks the surface area. We write the surface area as the integral of unit density </p> \\[A(\\mathcal S) = \\int_S 1d\\mathbf x = \\int_S \\|\\nabla \\mathbf x\\|^2 d\\mathbf x, \\|\\nabla \\mathbf x\\| = 1\\] <p>there are many expressions for \\(1\\) in the integral, but we pick the differential norm (the change in position over a small change in position), and to make sure linearity, we use the squared norm, hence the area is a quadratic function os positions and familiarly as the Dirichlet energy. </p> <p>Note that \\(A(S, \\mathbf x)\\) is a functional with varying \\(\\mathbf x\\). So that the functional derivative, for any \\(\\mathbf y\\)</p> \\[\\begin{align*} \\frac{dA}{d\\mathbf x} &amp;= \\lim_{\\epsilon\\rightarrow 0}\\frac{A(\\mathbf x + \\epsilon \\mathbf y) - A(\\mathbf x)}{\\epsilon}\\\\ &amp;= \\left [\\frac{d}{d\\epsilon}A(\\mathbf x + \\epsilon\\mathbf y)\\right]_{\\epsilon=0}\\\\ &amp;= \\left [\\frac{d}{d\\epsilon} \\int_S\\|\\nabla\\mathbf x + \\epsilon\\nabla \\mathbf y\\|^2 d\\mathbf x \\right]_{\\epsilon=0}\\\\ &amp;= \\int_S 2\\nabla \\mathbf y\\cdot \\nabla \\mathbf x d\\mathbf x\\\\ &amp;= -\\int_S \\mathbf y \\Delta \\mathbf x d\\mathbf x &amp;\\text{assume closed surface, by Green's identity} \\end{align*}\\] <p>Then consider the derivative evaluated at some point \\(\\mathbf u = (u, v)\\). Since we can freely pick \\(\\mathbf y\\), let \\(y(a) = \\mathbb I(a=\\mathbf u)\\), i.e. a \"bump\" at \\(\\mathbf u\\) and \\(0\\) elsewhere so that </p> \\[\\frac{dA}{d\\mathbf x}(\\mathbf u) = -2\\Delta \\mathbf x(\\mathbf u)\\] <p>where \\(\\Delta\\) is the Laplacian operator</p> \\[\\Delta f = \\nabla\\cdot\\nabla f = \\frac{\\partial^2f}{\\partial u^2} + \\frac{\\partial^2f}{\\partial v^2}\\] <p>In this case, we can pick the directions \\(\\varphi_1, \\varphi_2\\) as our \\(u,v\\), since the principal directions are orthogonal, so that </p> \\[\\Delta\\mathbf x = \\frac{\\partial^2 \\mathbf x}{\\partial\\varphi_1^2}+\\frac{\\partial^2 \\mathbf x}{\\partial\\varphi_2^2} = k_1\\mathbf n + k_2\\mathbf n = 2H\\mathbf n\\]"},{"location":"csc419/curvature.html#gaussian-curvature-as-area-distortion","title":"Gaussian curvature as area distortion","text":"<p>Consider Gaussian curvature \\(K=k_1k_2\\), which is the product of principal curvatures. Note that \\(K=0\\) IFF at least one of the principal curvature is \\(0\\). In this case, the surface is called developable surfaces because it can be flattened (developed) onto a flat plane (Since the principal direction is 0, it's like a rolled paper, we can then unroll through the non-zero principal direction) without stretch of shearing. Locally, Gaussian curvature measures how far from developable the surface is: how much would the local area need to stretch to become flat. </p> <p>Then, introducing the Gauss map \\(N:\\mathcal S\\rightarrow \\mathcal S^2\\) where </p> \\[N(\\mathbf p) = \\mathbf n(\\mathbf p)\\] <p>Then, consider a small patch on a curved surface. Gaussian curvature \\(K\\) can equivalently be defined as the limit of hte ratio between the area swept out by the unit normal on the Gauss map \\(A_G\\) and the area of the surface patch \\(A\\), </p> \\[K = \\lim_{A\\rightarrow 0}\\frac{A_G}{A}\\] <p>Some examples of different types of regions, the left is \\(A\\) and the right is \\(A_G\\)</p> type $A_G$ demo flat 0 cylindrical 0 spherical &gt;0 saddle &lt;0 <p>Similar to the turning number theorem for curves, the theorem for surfaces states that </p> \\[\\int_S K dA = 2\\pi\\chi(\\mathcal S)\\] <p>where \\(\\chi(\\mathcal S)\\) is the Euler characteristic  of the surface \\(\\mathcal S\\).</p> <p>Theorem for surfaces means that we cannot add Gaussian curvature to a surface without removing an equal amount some place else or changing the topology of the surface. </p>"},{"location":"csc419/curvature.html#shape-operator","title":"Shape operator","text":"<p>Let \\(\\mathbf n\\) be the unit normal for a point \\(\\mathbf p\\) on \\(\\mathcal S\\), let \\(\\mathbf v\\) s.t. \\(\\mathbf v\\cdot\\mathbf n = 0\\) (the unit tangent). Then we can ask how does the normal change as we move in the direction in \\(\\mathbf v\\) along the surface. </p> \\[S_\\mathbf p(\\mathbf v) = \\nabla \\mathbf n\\cdot\\mathbf v\\] <p>\\(S_\\mathbf p\\) is the shape operator at point \\(\\mathbf p\\). </p> <p>Note that any tangent vector can be spanned by 2 orthogonal basis tangent vectors, denote them as \\(\\mathbf e_1,\\mathbf e_2\\in\\mathbb R^2\\). The shape operator as a differential operator is linear, for \\(\\mathbf v = x_1\\mathbf e_1 + x_2\\mathbf e_2\\), we have</p> \\[S_\\mathbf p(\\mathbf v) =  \\begin{bmatrix} S_{\\mathbf p}(\\mathbf e_1)\\cdot \\mathbf e_1&amp;S_{\\mathbf p}(\\mathbf e_1)\\cdot \\mathbf e_2\\\\ S_{\\mathbf p}(\\mathbf e_2)\\cdot \\mathbf e_1&amp;S_{\\mathbf p}(\\mathbf e_2)\\cdot \\mathbf e_2 \\end{bmatrix}v\\] <p>Given \\(\\varphi_1, \\varphi_2\\) as the principal directions, we can rotate our coordinate frame to align \\(\\mathbf e_1, \\mathbf e_2\\) with the principal directions, so that </p> \\[S_\\mathbf p = \\begin{bmatrix}\\varphi_1&amp;\\varphi_2\\end{bmatrix} \\begin{bmatrix}k_1&amp;0\\\\0&amp;k_2\\end{bmatrix} \\begin{bmatrix}\\varphi_1&amp;\\varphi_2\\end{bmatrix}^T\\] <p>Note that this is the Eigen decomposition of the shape operator. Therefore, Eigen decomposition will give principal directions as eigenvectors and principal curvatures as eigenvalues. </p>"},{"location":"csc419/curvature.html#discretization","title":"Discretization","text":"<p>First, review that the discrete Laplacian for triangle meshes is </p> \\[\\Delta f \\approx M^{-1}L\\mathbf f\\] <p>where \\(M\\) is the mass matrix and \\(L\\) is the cotangent matrix and \\(\\mathbf f\\) is the discrete approximation of \\(f\\) . </p> <p>If we apply this operator to vertex positions, we will have a pointwise approximation of the mean curvature normal</p> \\[H\\mathbf n \\approx \\mathbf H = M^{-1}LV\\in\\mathbb R^{n\\times 3}\\]"},{"location":"csc419/curvature.html#discrete-gaussian-curvature-via-angle-defect","title":"Discrete Gaussian curvature via angle defect","text":"<p>On a discrete surface, curvature cannot live on flat faces and edges (we can always develop/unroll the surface along the edge). Therefore, the curvature must live at vertices. </p> <p>Using the definition in terms of the area on the Gauss map. Vertices correspond to spherical polygons connecting face normal points. The area \\(\\Omega\\) subtended on the Gauss map is called the solid angle. Conveniently, this area is simply the angle defect of internal angles \\(\\theta_f\\) incident on the \\(i\\)th vertex contributed by each \\(f\\)th incident faces. </p> \\[\\Omega_i = 2\\pi - \\sum_{f\\in faces(i)}\\theta_{if}\\] <p>Thus, our discrete analog of locally integrated Gaussian curvature is given as the angle defect at the \\(i\\)th vertex. The local integral average discrete Gaussian curvature is the angle defect divided by the local area associated with the \\(i\\)th vertex. </p> \\[K_i = \\frac{\\Omega_i}{A_i}\\] <p>By ways of closing up the Gauss map, closed polyhedral surfaces will have </p> \\[\\sum^n K_i = 2\\pi \\chi(S) = |V|-|E|+|F|\\] <p>which connects with the Euler's formula. </p>"},{"location":"csc419/curvature.html#approximation-and-eigen-decomposition-of-the-shape-operator","title":"Approximation and Eigen decomposition of the shape operator","text":"<p>Alternatively, we nned to approximate all curvatures of a surface by locally fitting an analytic surface so that we have its curvatures. The simplest is the quadratic surface. </p> <p>The algorithm proceeds as follows</p> <p>for \\(\\mathbf v\\) in \\(V\\)</p> <ol> <li> <p>gather sampling points from neighborhood. For simplicity, find vertices in the two rings of \\(\\mathbf v\\), i.e. </p> \\[\\text{Ring}_1 = \\{\\mathbf u\\mid \\mathbf u\\mathbf v \\in E\\}, \\text{Ring}_2 = \\{\\mathbf w\\mid \\exists \\mathbf u. \\mathbf u \\mathbf v\\in E\\land \\mathbf w \\mathbf u\\in E\\}, \\text{Neighbor}=\\text{Ring}_1\\cup \\text{Ring}_2\\] <p>Then, let \\(\\text{Neighbor} \\in\\mathbb R^{k\\times 3}\\) be stacked vertices, construct \\(P = \\text{Neighbor} - \\mathbf v\\) so that \\(P\\) be the position of neighboring vertices relative to \\(\\mathbf v\\). </p> </li> <li> <p>Compute PCA of \\(P\\) (eigen decomposition of \\(P^TP\\)). Let \\(S\\in \\mathbb R^{k\\times 2}\\) be the principal directions (\\(u,v\\) directions) and \\(B\\in\\mathbb R^k\\) be the height of each point in the least principal direction \\(w\\)-direction.</p> </li> <li> <p>An quadratic function as a height field surface passing through the origin is given by </p> \\[w = a_1u+a_2v + a_3u^2 + a_4uv +a_5v^2\\] <p>We have \\(k\\) sets of equations, and we can fits for the 5 parameters \\(a_i\\). </p> </li> <li> <p>The shape operator can be constructed as the product of the second and first fundamental form matrices</p> \\[S = -\\begin{bmatrix}e&amp;f\\\\f&amp;g\\end{bmatrix}\\begin{bmatrix}E&amp;F\\\\F&amp;G\\end{bmatrix}^{-1}\\] <p>The entries categorize the strech and bending in each direction</p> \\[E = 1 + a_1^2;\\quad F = a_1a_2;\\quad G=1 + a_2^2\\] \\[e =\\frac{2a_3}{\\sqrt{a_1^2 + 1 + a_2^2}};\\quad f = \\frac{a_4}{\\sqrt{a_1^2 + 1 + a_2^2}};\\quad g = \\frac{2a_5}{\\sqrt{a_1^2 + 1 + a_2^2}}\\] </li> <li> <p>Eigen decomposition of \\(S\\) gives \\(k_1, k_2\\) and the principal directions (in PCA basis)</p> </li> <li> <p>Lieft the principal tangent directions back to world \\(\\mathbb R^3\\). </p> </li> </ol>"},{"location":"csc419/deformation.html","title":"Handle-based Deformation","text":""},{"location":"csc419/deformation.html#problem-setup","title":"Problem Setup","text":"<p>The user repositions a sparse set of points  and the goal is to propagate the transformations at these \"handles\" to the rest of the shape. </p> <p>Given the point rest position \\(\\tilde x \\in \\mathbb R^3\\), we will write its unknown deformed position as \\(x\\) and the point's displacement vector \\(d := x - \\tilde x\\). </p> <p>The problem can be thought of two ways   - as a scattered data interpolation, where the handles are the sparse samples and we want to interpolate over the unknown displacement field  - as a shape optimization problem, where we try to define a new shape that retains the details of the old shape but fulfill the handle constraints. </p>"},{"location":"csc419/deformation.html#properties-to-consider-in-deformation","title":"Properties to Consider In Deformation","text":""},{"location":"csc419/deformation.html#continuity","title":"Continuity","text":"<p>The deformation should be continuous, i.e. the shape will not tear, crack or change topological features. i.e. the face matrix \\(F = \\tilde F\\) will not change, only \\(V\\) should be determined from \\(\\tilde V\\). </p>"},{"location":"csc419/deformation.html#generic-distortion-minimization","title":"Generic Distortion Minimization","text":"<p>A rest surface \\(\\tilde S\\) immersed in \\(\\mathbb R^3\\) can be described as a mapping \\(\\tilde x\\) from some 2D parametric domain \\(\\Omega\\). i.e. \\(\\tilde x: \\Omega \\subseteq \\mathbb R^2 \\rightarrow \\mathbb R^3\\), similarly define the deformed surface as \\(x:\\Omega \\rightarrow \\mathbb R^3\\) and displacement vector field as \\(d(u, v) = x(u, v) - \\tilde x(u, v)\\). </p> <p>Consider distortion minimization and the handle's position as energy minimization with constraints. Suppose we have some energy functional \\(E(x)\\) that measures the amount of distortion. A natural choice of distortion measurement is by intergrating local distortion over all points, i.e. we have the problem \\begin{align} \\text{minimize} \\quad &amp;E(x) = \\int_\\Omega |e(x)|^2 dA\\ \\text{subject to} \\quad &amp;{x:\\Omega \\rightarrow \\mathbb R^3\\mid\\forall i = {1,...,k}.  x(u_i, v_i) = g_i} \\end{align} where \\(e\\) is the measurement of local distortion, the function is upon our choice. </p>"},{"location":"csc419/deformation.html#gradient-based-energy","title":"Gradient based Energy","text":"<p>If we assume that the deformation is small then we can measure the distortion as the magnitude of gradient of the displacement field, i.e. if the displacement field has large variations or sudden changes then it induces a lot of distortion. </p> \\[\\int_\\Omega \\|e(x)\\|^2 dA = \\int_\\Omega \\|\\mathbf Dd\\|_F^2 dA \\] <p>where \\(\\mathbf Dd\\) is the Jacobian of \\(d\\).  </p> <p>The discretized version over our triangle mesh gives </p> \\[tr(D^TLD)\\text{ w.r.t } D_{handles} = g_{handles} - \\tilde V_{handles}\\] <p>However this methods is not smooth at constraints and the influence of handles dies off too quickly. By minimizing the Dirichlet energy, each coordinate of \"displacement field\" is a harmonic function. Intuitively (however abstractly) we can think of each function as diffusing the user's constraints as if they were heat values. As such, each function diffuses quickly to an average, slowly varying value over most of the domain. As a displacement field a constant value for each coordinate function would mean a translation: most of the shape is simply translated.</p> <p>Note that Jacobian is a linear operator, i.e. </p> \\[\\min_d\\int \\|\\mathbf D d\\|_F^2 dA = \\min_x \\int \\| \\mathbf D x - \\mathbf D\\tilde x\\|_F^2 dA\\] <p>If we think of the gradient of the position function  (with respect to the underlying parameterization) as a local geometric feature descriptor then this energy can be re-understood as measuring the difference in this feature before and after the deformation. This is very sensible as we are trying to measure distortion. We would expect that a low-distortion deformation would correspond with a small change to local features.</p>"},{"location":"csc419/deformation.html#laplacian-based-energy","title":"Laplacian based Energy","text":"<p>If we model distortion as the change in a local feature descriptor, then a natural local and relative descriptor would be one that compared the position of some point on the shape to the average of its local neighborhood. Consider the Laplace operator as taking exactly the difference of a function value at a point and the average over an infinitesimal region </p> \\[\\Delta f (x) = \\big[\\lim_{|B_\\delta(x)|\\rightarrow 0} B_\\delta(x)^{-1}\\int_B f(z)dz\\big] - f(x)\\] <p>When applied to the embedding function \\(x\\) the Laplace operator computes the differences in position between a point and its local neighborhood. The vector points in the normal direction and its magnitude corresponds inversely with how flat the surface is locally. </p> <p>We can use calculus of variations apply Green's identity twice to show that minimizers of the squared Laplacian energy are bi-harmonic functions (\\(\\Delta^2 d = 0\\)). This will allow us to ensure continuity of first derivatives across constrained values at handles. </p>"},{"location":"csc419/deformation.html#discretized-approximation","title":"Discretized Approximation","text":"<p>(For more details of the discretization and symbols, go to smoothing)</p> <p>We can make sure our discrete Laplacian operator \\(L\\). This matrix computes the locally integrated Laplacian of a given function specified by per=vertex values \\(\\mathbf f\\). We can approximate the point-wise Laplacian by the local integral average of the Laplaicna \\(M^{-1}L\\mathbf f\\). Integrating this over the mesh we have the complete approximation</p> \\[\\int_\\Omega \\|\\Delta d\\|^2 dA \\approx tr(D^TLM^TMM^{-1}LD) = tr(D^T\\underset{Q}{L^TM^{-1}L}D)\\] <p>where \\(M\\) is the mass-matrix for the given mesh and \\(Q = L^TM^{-1}L\\) can be thought of as the bi-Laplacian matrix.</p>"},{"location":"csc419/deformation.html#precomputation","title":"Precomputation","text":"<p>Without loss of generality, assume that the rows of the unknown displacement \\(D\\) have been sorted so that displacements corresponding to handle vertices are in the bottom part, i.e. \\(D = \\begin{pmatrix}D_u\\\\D_{handle}\\end{pmatrix}\\)</p> <p>Since the displacement at handles are, we can separate and eliminate it from optimization</p> \\[\\begin{align*} \\min_{D_u}tr(D^T Q D) &amp;= \\min_{D_u} tr\\big(\\begin{bmatrix}D^T_u&amp;D_h^T\\end{bmatrix} \\begin{bmatrix}Q_{u,u} &amp; Q_{u,h}\\\\ Q_{h,u} &amp;Q_{h,h}\\end{bmatrix} \\begin{bmatrix}D_u\\\\D_{h}\\end{bmatrix}\\big)\\\\ &amp;= \\min_{D_u} tr\\big(D^TQ_{u,u} D_u + 2D^T_u Q_{u,h}D_h\\big)\\\\ &amp;= \\min_{D_u} tr(E(D_u)) \\end{align*}\\] <p>Then, set \\(\\partial_{D_u}E = 0\\) for the solution of the quadratic optimization</p> \\[\\begin{align*} \\partial_{D_u} E &amp;= 2Q_{u,u} D_u + 2Q_{u,h} D_h\\\\ \\Rightarrow D_u &amp;= Q^{-1}_{u,u} Q_{u,h} D_h \\end{align*}\\] <p>Since \\(Q_{u,u}\\) does not change, we can prefactorize it so that applying its inverse is fast. </p>"},{"location":"csc419/deformation.html#problem","title":"Problem","text":"<p>Biharmonic displacements works well for small deformations and deformations that do not imply a large rotation. The Laplacian of the position function \\(\\Delta x\\) as a feature descriptor is not rotation invariant. This problem is also true for all linear differential features. This means that if the user transforms all the handle locations by a rigid transformation \\(T\\) these energies will measure zero. We would like global rotation invariant, but we should also like this property to apply locally so that parts of the shape can easily rotate. </p>"},{"location":"csc419/deformation.html#as-rigid-as-possible","title":"As Rigid As Possible","text":"<p>Sure each handle \\(i\\) are perfectly transformed by a rigid transformation \\(x_i = R \\tilde x_i + t\\). Then we could repair the gradient based energy above by </p> \\[\\int_\\Omega \\|\\mathbf D x - \\mathbf D(R\\tilde x + t)\\|^2 dA = \\int_\\Omega \\|\\mathbf D x - R\\mathbf D(\\tilde x)\\|^2dA\\] <p>Then, we can unify and fit the best rotation \\(R\\), i.e. </p> \\[\\arg\\min_{x, R} \\int_\\Omega \\|\\mathbf D x - R\\mathbf D(\\tilde x)\\|^2dA\\] <p>Optimizing this energy will ensure global rotation invariance. The ensure local rotation invariance, we can replace \\(R\\in SO(3)\\) with a spatially varying function \\(R:\\Omega \\rightarrow SO(3)\\) that outputs the a rotation for any point on the shape. </p> <p>For embedded solid shapes, we can take the rest shape given by \\(\\tilde x\\) as the parameterization \\(\\Omega\\), so that \\(\\mathbf D \\tilde x = I\\), thus we can writethe arap energy as the squared difference between deformation gradient and closest rotation. </p> \\[\\begin{align*} \\mathbf D x - R \\mathbf D \\tilde x &amp;= \\mathbf (D x + I - I) - RI\\\\ &amp;= (I + \\mathbf D x - \\mathbf D\\tilde x) - R\\\\ &amp;= (I + \\mathbf D d) - R\\\\ &amp;= F - R\\\\ \\int_\\Omega \\|\\mathbf D x - R\\mathbf D(\\tilde x)\\|^2dA &amp;= \\int_\\Omega \\|F-R\\|^2dA \\end{align*}\\]"},{"location":"csc419/deformation.html#discrete-energy","title":"Discrete Energy","text":"<p>For a triangle mesh with displacing vertices, the gradient of the embedding function is constant inside each triangle. In this way we can write the gradient energy as a double sum of half-edges \\(ij\\) of all faces \\(f\\) in the mesh</p> \\[\\int_\\Omega \\|\\mathbf D x - \\mathbf D\\tilde x \\|^2 dA = \\frac12 \\sum_{f\\in F}\\sum_{ij\\in f}c_{ij}\\|(v_i-v_j)- (\\tilde v_i-\\tilde v_j)\\|^2\\] <p>where \\(c_{ij} = \\cot(\\angle_{ij})\\) is cotangent of the angle opposite half-edge \\(ij\\). </p> <p>To inject localized best fit rotations, assign a unknown rotation matrix \\(R_k\\) fora each vertex \\(k\\) for the mesh and accounts for a third of the energy integrated over incident triangles. </p> \\[\\int_\\Omega \\|\\mathbf D x - \\mathbf D(R\\tilde x + t)\\|^2 dA = \\frac16\\sum_{k=1}^n\\sum_{ij\\in F(k)} c_{ij} \\| (v_i - v_j) - R_k(\\tilde v_i - \\tilde v_j)\\|^2\\]"},{"location":"csc419/deformation.html#optimization","title":"Optimization","text":"<p>Since we have \\(R_k\\) and \\(V\\) to be optimized, the simplest method is do alternating optimization, i.e. </p> <ul> <li>local step: find \\(R_k\\)'s first with fixed \\(V\\), </li> <li>Global step: then use updated \\(R_k\\)'s to optimize \\(V\\). </li> </ul> <p>The \"local\" step since each \\(R_k\\) only affects the local energy and won't interact with the other rotations. The \"global\" step since each vertex positions in \\(V\\) depends on each other, and requires a global solve. </p> <p>Note that the energy above can be separated into two parts</p> \\[\\begin{align*} &amp;\\frac 16 \\sum_{k=1}^n \\sum_{ij\\in F(k)}c_{ij} (v_i - v_j)^T (v_i - v_j)&amp;\\text{quadratic in }V\\\\ +&amp;\\frac 16 \\sum_{k=1}^n \\sum_{ij\\in F(k)}c_{ij} (v_i - v_j)^T R_k(\\tilde v_i - \\tilde v_j)&amp;\\text{linear in }V\\\\ \\end{align*}\\] <p>We can stack the \\(R_k\\) into \\(R_{3n\\times 3}\\) so that the energy is in matrix form </p> \\[tr(V^TLV) + tr(V^TKR)\\] <p>\\(L_{\\|V\\|\\times \\|V\\|}\\) is hte cotangent discrete Laplacian matrix \\(K_{\\|V\\|\\times 3\\|V\\|}\\) is the sparse matrix containing cotangents multiplied against differences across edges in the rest mesh, i.e. </p> <pre><code># K matrix is |V| * 3|V|, indexing starts with 0\nfor f in faces:\n    for (i, j) in permutations([0, 1, 2], 2):\n        for k in [0, 1, 2]:\n            for b in [0, 1, 2]:\n                # cot[i, j] is the cotangent of angle opposite to edge i,j \n                K[f[i], 3 * f[k] + b] += cot[i, j] * (v_tilde[i] - v_tilde[j])[b]\n                K[f[j], 3 * f[k] + b] -= cot[i, j] * (v_tilde[i] - v_tilde[j])[b]\n</code></pre>"},{"location":"csc419/deformation.html#local-step","title":"Local Step","text":"<p>Minimizing \\(R\\) corresponds to minimize </p> \\[tr\\big(\\underset{C^T}{(V^TK)}R\\big)\\] <p>Let \\(C = (V^TK)^T \\in \\mathbb R^{3n\\times 3}\\), \\(C_k \\in \\mathbb R^{3\\times 3}\\). Then, \\(C\\) is the stacked weighted covariance matrix and each \\(C_k\\)  is the region covered by corresponding rotation \\(R_k\\). To minimize, we simply want \\(R_k\\) to be the closest rotation matrix solved via SVD, as seen in registration. </p>"},{"location":"csc419/deformation.html#global-step","title":"Global Step","text":"<p>Minimizing \\(V\\) corresponds to minimize</p> \\[tr(V^TLV) + tr\\big(V^T\\underset{B}{(KR)}\\big)\\] <p>Adding the handle constraints to the corresponding rows of \\(V\\), this is minimized by setting all partial derivatives w.r.t. the unknowns in \\(V\\) equal to 0. </p>"},{"location":"csc419/deformation.html#pre-factorize","title":"Pre-factorize","text":"<p>Note that the global step is the same regardless of the current rotations or current handle position, we can prefactorize it. The matrix \\(K\\) also does not depend on the rotations, current positions or handle positions; so that we can prebuild \\(K\\)</p>"},{"location":"csc419/introduction.html","title":"Introduction","text":""},{"location":"csc419/introduction.html#curves","title":"Curves","text":""},{"location":"csc419/introduction.html#parametric-equation","title":"Parametric Equation","text":"<p>Represent the curve as the image of some function \\(\\vec p:\\mathbb R\\rightarrow \\mathbb R^2\\). For example \\(\\vec p(t) = (\\cos(t), \\sin(t))\\) is the curve of a circle of radius 1</p>"},{"location":"csc419/introduction.html#implicit-equation","title":"Implicit Equation","text":"<p>Represent the curve as the level set of some function, i.e. for some \\(C\\in\\mathbb R. g:\\mathbb R^2\\rightarrow\\mathbb R^1\\), the curve is \\(\\{(x, y) \\mid g(x, y) = C\\}\\). For example \\(\\{(x, y)\\mid x^2 + y^2 - 1= 0\\}\\) be the curve of a circle  of radius 1.</p>"},{"location":"csc419/introduction.html#surfaces","title":"Surfaces","text":""},{"location":"csc419/introduction.html#functional-surface","title":"Functional Surface","text":"<p>Consider a function \\(f:\\mathbb R^2\\rightarrow \\mathbb R\\), the surface is the image of \\(f\\). of a height field. However, in this case, we cannot represent a perpendicular surfaces parallel to the z-axis. </p>"},{"location":"csc419/introduction.html#parametric-equation_1","title":"Parametric Equation","text":"<p>Similar to a parametric curve, we can represent a surface as the image of \\(S:\\mathbb R^2 \\rightarrow \\mathbb R^3\\). If we can draw a map from parametric domain to 3D on the small neighborhood of each point (other than the boundary), we call it a manifold (not very topologically definition of the manifold).</p>"},{"location":"csc419/introduction.html#implicit-equation_1","title":"Implicit Equation","text":"<p>Define \\(g:\\mathbb R^3\\rightarrow \\mathbb R\\), then the curve is the level set of \\(g\\), i.e. \\(\\{(x, y, z) \\mid g(x, y, z) = C\\}\\)</p>"},{"location":"csc419/introduction.html#tangent-and-normal","title":"Tangent and normal","text":""},{"location":"csc419/introduction.html#for-curves","title":"For Curves","text":"<p>For a parametric curve, the tangent \\(\\frac{\\partial \\vec p}{\\partial t} = \\vec t\\) and unit tangent \\(\\hat t = \\frac{\\vec t}{ \\|\\vec t\\|}\\) represents the direction to which stay as on a curve.  </p> <p>For an implicit curve, the normal \\(\\nabla g\\) and the unit normal \\(\\hat n = \\frac{\\nabla g}{\\|\\nabla g\\|}\\) represents the direction to get off the curve. Note that \\(\\hat n \\cdot \\hat t = 0\\), i.e. the tangent and normal are perpendicular.</p>"},{"location":"csc419/introduction.html#for-surfaces","title":"For Surfaces","text":"<p>The normal is  still \\(\\nabla g\\) and the unit normal \\(\\hat n = \\frac{\\nabla g}{\\|\\nabla g\\|}\\).  </p> <p>However, the tangent is now a space or a plane in 2D, \\(\\{\\hat t \\mid \\hat t\\cdot \\hat n = 0\\}\\), For a parametric curve, the two easy tangent line is \\(\\hat t_1 = \\frac{\\partial S}{\\partial u}\\) and \\(\\hat t_2 = \\frac{\\partial S}{\\partial v}\\), and the tangent plane is then spanned by \\(\\hat t_1, \\hat t_2\\), if we have \\(\\hat t_1\\cdot \\hat t_2 = \\pm 1\\), i.e. \\(\\partial_uS = c\\:\\partial_vS\\), then it is a degenerate point, i.e. at that point, it is a curve instead of a surface. </p>"},{"location":"csc419/introduction.html#geometry-topology","title":"Geometry + Topology","text":"<p>A surface is then defined by geometry and topology. </p> <p>Some properties that defines geometry and topology</p> Geometry Topology position closed-ness tangent / normals orientable dimension number of boundaries  (ex. calendar has 2 boundaries) \"curvature\" number of (topological) holes  (ex. a donut has one hole) self-intersection"},{"location":"csc419/introduction.html#discrete-topology","title":"Discrete Topology","text":"<p>Note that in computer science world, there is no actual \"continuous\" space, so we need some discrete methods. </p>"},{"location":"csc419/parameterization.html","title":"Parameterization","text":""},{"location":"csc419/parameterization.html#problem-setup","title":"Problem Setup","text":"<p>Given a surface embedded or immersed in \\(\\mathbb R^3\\), we want to find its flatten map in \\(\\mathbb R^2\\). With this process, the 2D flattened mesh can be interpreted as a parameterization of the 3D surface. </p> <p>More specifically, given a list of vertices \\(V\\in\\mathbb R^{n\\times 3}\\), we can to find some \\(U\\in \\mathbb R^{n\\times 2}\\), i.t. assign the \\(uv\\) coordinates from \\(xyz\\). </p> <p>Note that in general, because we reduce the dimension. Some parts of the surface will have to be stretched and some will be squished. Surfaces with topological handles or without a boundary must be cut. </p>"},{"location":"csc419/parameterization.html#mass-spring-methods","title":"Mass Spring Methods","text":"<p>Consider the connection between parameterization and graph drawing in infoviz. If we treat our mesh a vertices and edges, i.e. a graph, then the problem can be viewed as graph drawing. Then, we can optimize over node locations with the energy</p> \\[\\arg\\min_U\\sum_{(i,j) \\in E} \\|u_i - u_j\\|^2\\] <p>where \\((i, j)\\) is an edge. Note that this is equivalent tot the potential energy of a mass-spring system. </p> <p>Of course, the trivial solution will have every vertex map to \\((0, 0)\\), so we need to add some fixed point constraints, However, if we just choose fixed points arbitrarily, then in general we will have fold over, i.t. edges cross each other, overlapping of mesh triangles, and inverted normal direction. </p>"},{"location":"csc419/parameterization.html#weighted-potential-energy","title":"Weighted Potential Energy","text":"<p>By Tutte's Embedding, we do have a way to avoid fold over. If the boundary of a disk-topology mesh is fixed to a convex polygon, then minimizing the energy above will give injective flattening. However, this method tends to have all edges with equal length. Combined with the boundary constraints, the flattened mesh will have a grid-like shape (smoothly varying edge lengths, near equilateral triangles) regardless the triangle shapes and sizes on the surface mesh. To improve, we introduce a varying spring stiffness \\(w_{ij}\\) for each edge, i.e. </p> \\[\\arg\\min_U\\sum_{(i,j) \\in E} w_{ij}\\|u_i - u_j\\|^2\\] <p>For example, let \\(w_{ij} = \\|v_i - v_j\\|^{-1}\\) so that shorter edges on the 3D mesh gets more energy. This will help the length distortion problem, while we still have the area distortion and angle distortion problem. </p> <p>Let's further consider the energy, first, vectorize the energy as </p> \\[\\arg\\min_U \\frac 12 tr(U^TLU), L = \\begin{cases}w_{ij} &amp;ij\\in E\\\\-\\sum_{l\\neq i} L_{il} &amp;i=j\\\\0&amp;\\text{otherwise}\\end{cases}\\] <p>Note that \\(L\\) is the discrete Laplacians, if \\(w_{ij}=1\\), then \\(L\\) is the uniform Laplacian, if \\(w_{ij}\\) is based on edge lengths, then \\(L\\) corresponds to a physical static equilibrium problem for a linear spring system. </p>"},{"location":"csc419/parameterization.html#dirichlet-energy","title":"Dirichlet Energy","text":"<p>If we consider the problem as a discrete representation over a 2D surface and Wobbliness distortions in the parameterization correspond to high variation in \\(u,v\\) function over the surface, then consider the energy minimization of the variation, </p> \\[\\min_{u,v} \\int_S \\|\\nabla u\\|^2 + \\|\\nabla v\\|^2 dA\\] <p>We may discretize this problem immediately using piecewise linear functions spanned by  and  values at vertices. This corresponds to using the cotangent Laplacian as \\(L\\) in the discrete minimization problem above.</p>"},{"location":"csc419/parameterization.html#least-squares-conformal-mappings","title":"Least Squares Conformal Mappings","text":""},{"location":"csc419/parameterization.html#area-distortion","title":"Area Distortion","text":"<p>We'd like that regions on \\(S\\) have a proportionally similarly sized region under the \\(u,v\\) mapping. On an infinitesimal scale, a small change in \\(x,y\\) on \\(S\\) should incur an equally small change in \\(u,v\\), i.e. </p> \\[\\det\\begin{bmatrix}\\partial_xu &amp;\\partial_yu\\\\\\partial_xv &amp;\\partial_yv\\end{bmatrix} = 1\\]"},{"location":"csc419/parameterization.html#angle-distortion","title":"Angle Distortion","text":"<p>We'd like that local regions on \\(S\\) are parameterized without shearing. This ensures that two orthogonal directions on the surface \\(S\\) correspond to two orthogonal directions on the parametric plane \\(\\mathbb R^2\\). We can capture this by requiring that a small change in \\(x,y\\) on \\(S\\) corresponds to equal magnitude, small changes in \\(u,v\\) in perpendicular directions</p> \\[\\nabla u = \\nabla v^{\\perp}\\Rightarrow \\begin{bmatrix}\\partial_xu \\\\\\partial_yu\\end{bmatrix} = \\begin{bmatrix}\\partial_yv \\\\-\\partial_xv\\end{bmatrix}\\] <p>This equality is linear in \\(u,v\\) so that we can immediately build a quadratic energy that minimizes deviation from satisfying the equation over \\(S\\), i.e. </p> \\[\\arg\\min_{u,v}\\frac12\\int_S\\|\\nabla u - \\nabla v^{\\perp}\\|^2 dA\\] <p>expand it, we have </p> \\[\\int_S \\frac 12 \\|\\nabla u\\|^2 + \\frac12\\|\\nabla v\\|^2 - \\nabla u\\cdot \\nabla v^{\\perp} dA\\] <p>the fist part is the Dirichlet energies, and then consider the second part</p> \\[\\begin{align*} \\int_S\\nabla u\\cdot \\nabla v^{\\perp} dA &amp;= \\int_S \\begin{pmatrix}\\partial_xu&amp;\\partial_yu\\\\\\partial_xv&amp;\\partial_yv\\end{pmatrix}dA \\\\ &amp;=\\int_{\\begin{pmatrix}u(S)\\\\v(S)\\end{pmatrix}}dA\\\\ &amp;= \\frac 12 \\oint_{\\partial \\mathbf u(S)}\\mathbf u(s)\\cdot \\mathbf n(s)ds &amp;\\text{Stoke's theorem} \\end{align*}\\] <p>\\(\\mathbf n\\) is the unit normal pointing in the outward direction along the boundary of the image of the mapping. So that we are integrating over the boundary of the image of mapping from \\(S\\). </p>"},{"location":"csc419/parameterization.html#discretization","title":"Discretization","text":"<p>Discretize \\(u,v\\) using piecewise-linear functions, then</p> \\[\\begin{align*} \\frac 12 \\oint_{\\partial \\mathbf u(S)}\\mathbf u(s)\\cdot \\mathbf n(s)ds &amp;= \\frac12 \\sum_{(i,j)\\in\\partial S} \\int_0^1 (\\mathbf u_i + t(\\mathbf u_j -\\mathbf u_i))\\cdot \\frac{(\\mathbf u_j -\\mathbf u_i)^{\\perp}}{\\|\\mathbf u_j - \\mathbf u_i\\|}\\frac{ds}{dt}dt\\\\ &amp;= \\frac12\\sum_{(i,j)\\in\\partial S}\\int_0^1 \\mathbf u_i(\\mathbf u_j - \\mathbf u_i)^{\\perp}dt\\\\ &amp;= \\frac12\\sum_{(i,j)\\in\\partial S}\\det(\\begin{bmatrix} \\mathbf u_i &amp; \\mathbf u_j\\end{bmatrix}) \\end{align*}\\] <p>i.e. summing over all boundary edges the determinant of matrix with vertex positions as columns. This quadratic form can be written as \\(U^T\\tilde AU\\) where \\(U\\in\\mathbb R^{2n}\\) is the vectorized \\(u, v\\) coordinates and \\(\\tilde A\\in\\mathbb R^{2n\\times 2n}\\) is the selection matrix involving only values for vertices on the boundary of \\(S\\).</p> <p>Note that \\(\\tilde A\\) is not symmetric and may cause numerical issues, replace it with \\(A = \\frac12 (\\tilde A + \\tilde A^T)\\) so that \\(A\\) is symmetric, and \\(x^T\\tilde A x = x^TAx\\) since we only need the quadratic energy.</p> <p>Putting all things together, we  have the discrete least square conformal mapping minimization problem as </p> \\[\\arg\\min_{U\\in\\mathbb R^{2n}} U^T\\bigg(\\begin{pmatrix}L&amp;0\\\\0&amp;L\\end{pmatrix} - A\\bigg)U\\] <p>Let \\(Q = \\bigg(\\begin{pmatrix}L&amp;0\\\\0&amp;L\\end{pmatrix} - A\\bigg)\\), since \\(L, A\\) are fixed from \\(V\\), we can precompute \\(Q\\)</p>"},{"location":"csc419/parameterization.html#free-boundary","title":"Free Boundary","text":"<p>Note that this problem still have the trivial \\(0\\) solution, we can fix two vertices, while this will introduce bias and sometimes we might choose two vertices that the energy would rather like to place near each other and so placing them at arbitrary positions will introduce unnecessary distortion. </p> <p>Instead we'd like natural boundary conditions, i.e. minimize the given energy in the absence of explicit boundary conditions. </p> <p>Since we don't want zero norm in our solution \\(U\\), we can enforce this by adding constraint </p> \\[\\int_S\\|u\\|^2 dA = 1 \\Rightarrow U^T\\begin{pmatrix}M&amp;0\\\\0&amp;M\\end{pmatrix}U = 1\\] <p>where \\(M\\) is the mass matrix for a piecewise linear triangle mesh so that \\(B = \\begin{pmatrix}M&amp;0\\\\0&amp;M\\end{pmatrix}\\) is the square constraint matrix. </p>"},{"location":"csc419/parameterization.html#canonical-rotation","title":"Canonical Rotation","text":"<p>The least squares conformal mapping energy is invariant to translation and rotation. The eigen decomposition process described above will naturally take care of \"picking\" a canonical translation by pulling the solution  toward the origin. The rotation it returns, however, will be arbitrary.</p>"},{"location":"csc419/registration.html","title":"Registration","text":""},{"location":"csc419/registration.html#problem","title":"Problem","text":"<p>When we scan a rigid object, we should have multiple scans from different positions and angles. However, if we don't match/align these scans, they are just multiple partial scans. Therefore, we need some way to register these points, i.e. find a spatial transformation that maps one scan to the alignment of the other. Another possible case is that we can have a coarse scan of a person and a finer scan of the face. We need to register the face.  </p> <p>We start with a complete scan of the surface \\(Y\\) and a new partial scan of the surface \\(X\\), both of them are which triangle meshes, they may not have the same number of vertices or even the same topology. </p>"},{"location":"csc419/registration.html#hausdorff-distance","title":"Hausdorff Distance","text":"<p>To measure the performance of the matching, we need a single scalar number, i.e. distance.</p>"},{"location":"csc419/registration.html#point-point-distance","title":"Point-Point Distance","text":"<p>We can start with point to point Euclidean distance, for points \\(x, y\\), let </p> \\[d(x,y) = \\|x-y\\|_2\\]"},{"location":"csc419/registration.html#point-projection-distance","title":"Point-Projection Distance","text":"<p>Then, consider the distance between a point \\(x\\) and some large object \\(Y\\), let </p> \\[d(x,Y) = \\inf_{y\\in Y} d(x, y)\\]"},{"location":"csc419/registration.html#directed-hausdorff-distance","title":"Directed Hausdorff Distance","text":"<p>Let the directed Hausdorff distance be </p> \\[D_{\\vec H}(X, Y) = \\sup_{x\\in X}\\|x-P_Y(x)\\|\\] <p>where \\(P_Y(x)\\) is the closest-point projection from \\(x\\) to \\(Y\\). Note that we use supremum, which means the worst projection is at most this distance. Also, note that this distance is \"directed\", i.e. \\(D_{\\vec H}(X,Y) \\neq D_{\\vec H}(Y, X)\\).</p>"},{"location":"csc419/registration.html#dhd-between-triangle-meshes","title":"DHD Between Triangle Meshes","text":"<p>Since we have the two triangle Meshes, we can approximate a lower bound on the Hausdorff distance by densely sampling surfaces \\(X\\) as \\(\\mathcal P_X\\). Obviously,</p> \\[D_{\\vec H}(X, Y) \\geq D_{\\vec H}(\\mathcal P_x, Y)\\] <p>And as we have more samples, it's more likely to approach the true distance. </p> <p>However, note that the \\(\\max\\) function is not continuous, hence hard to optimize. </p>"},{"location":"csc419/registration.html#random-sampling-of-a-mesh","title":"Random Sampling of a Mesh","text":"<p>We want our random variable \\(x\\in X\\) to have a uniform density \\(f=A_X^{-1}\\), where \\(A_X\\) is the surface area of \\(X\\). Therefore, we break the steps into randomly sample individual triangle, and then sample point from that triangle. Let \\(h(T) = \\frac{A_T}{A_x}\\) be the uniform distribution over triangle index \\(T\\in\\{1,...,m\\}\\), \\(g_T(x) = A_T^{-1}\\) be the uniform distribution over the triangle \\(T\\), then </p> \\[h(T)g_T(x) = \\frac{A_T}{A_X}\\frac1{A_T} = A_X^{-1} = f(x)\\]"},{"location":"csc419/registration.html#uniform-random-sampling-of-a-single-triangle","title":"Uniform random sampling of a single triangle.","text":"<p>Let \\(v_1,v_2, v_3\\) be the corners of the triangle, first pick a point uniformly in the parallelogram form by reflecting \\(v_1\\) across the line \\(\\overline{v_2v_3}\\). Then, randomly sample \\(a_1, a_2 \\in [0, 1]\\)</p> \\[x = \\begin{cases}v_1 + a_1(v_2-v_1) + a_2(v_3-v_1)&amp;a_1+a_2 \\leq 1\\\\v_1 + (1-a_1)(v_2-v_1) + (1-a_2)(v_3-v_1)&amp;a_1+a_2 &lt; 1\\end{cases}\\] <p>In case 2, we sampled a point in the reflection part of the triangle, hence we reflect it back by \\(1-a\\)</p>"},{"location":"csc419/registration.html#area-weighted-random-sampling-of-triangles","title":"Area-weighted Random Sampling of Triangles","text":"<p>Let \\(\\mathcal C_i = \\sum^i_{j=1}\\frac{A_j}{A_X}\\) be the cumsum of the relative areas. Then we randomly sample \\(a\\in [0, 1]\\) and locate the index of \\(a\\) in that cumsum interval.</p>"},{"location":"csc419/registration.html#integrated-closest-point-distance","title":"Integrated Closest-Point Distance","text":"<p>To enable optimization, we replace the sup norm with the euclidean norm, i.e. </p> \\[D_{\\vec C}(X, Y) = \\sqrt{\\int_{X}\\|x-P_Y(x)\\|^2 dA}\\] <p>So that we can define the directed matching energy from \\(Z\\) to \\(Y\\) as </p> \\[E_{\\vec C}(Z, Y) = [D_{\\vec{C}}(Z,Y)]^2 = \\int_{Z}\\|z-P_Y(z)\\|^2 dA = \\int_Z \\|f_Y(z)\\|^2 dA\\] <p>Consider the function \\(f_Y(z) = z - P_Y(z)\\). if \\(P_Y(z)\\) is a point, that \\(f(z) = z-y\\) is linear, and if \\(Y = \\{y\\mid (y-p)\\cdot n = 0\\}\\) is a infinite plane, then \\(f_Y(z) = ((z-p)\\cdot n)n\\) is also linear. However, due to \"smallest\" distance, the linearity does not hold and \\(f_Y\\) may not even be continuous. </p> <p>However, if we fix some \\(z_0\\) s.t. \\(f(z) = z-P_Y(z_0)\\), since \\(z_0\\) is fixed, and \\(P_Y(z_0)\\) is a point. the function can be optimized, takes one step further, we can do \\(f_Y(z) = ((z-P_Y(z_0))\\cdot n)n\\), which also can be optimized. Therefore, we can have an iterative algorithm</p> <pre><code>z_0 = initial guess\nwhile not converge:\n    f = linearize f(z) around z_0\n    z_0 = minimize f(z)^2\n</code></pre>"},{"location":"csc419/registration.html#icp-iterative-closest-point-algorithm","title":"ICP: Iterative Closest Point Algorithm","text":"<p>Go back to the original problem, our task is to find a rigid transformation \\(T\\)</p> \\[T(x) = Rx + t\\] <p>where \\(R\\in SO(3)\\) is the rotation matrix and \\(t\\in\\mathbb R^3\\) is the translation vector, and our problem is to </p> \\[\\arg\\min_{t, R}\\int_X \\|Rx + t - P_Y(Rx + t)\\|^2 dA\\] <p>which, as we sampled points on \\(X\\), is approximated as </p> \\[\\arg\\min_{t, R} \\sum_{\\mathcal P_x} \\|Rx + t - P_Y(Rx + t)\\|^2 \\]"},{"location":"csc419/registration.html#procedure","title":"Procedure","text":"<p>The procedure can be described as below</p> ICP(V_X F_X, V_Y, F_Y)<pre><code># init R, t, possibly as identity and zero vector\nR = identity(3)\nt = [0, 0, 0]\nwhile not converge:\n    X_points = sample from V_X F_X\n    proj = project all X_points onto V_Y F_Y\n    R, t = update rigid transformation from best matched X_points and proj\n    V_X = R(V_X) + t\n</code></pre> <p>Then, we have to consider the methods fro the rigid transformation update. As from previous discussions, we can do point-to-point or point-to-plane.</p>"},{"location":"csc419/registration.html#point-point-rigid-matching","title":"Point-Point Rigid Matching","text":"<p>We are trying to solve </p> \\[\\arg\\min_{R, t} \\sum^k \\|Rx_i + t - p_i\\|^2\\] <p>The energy is quadratic in \\(t\\), so that the optimal \\(t^*\\) can be obtained by (with unknown \\(R\\))</p> \\[\\begin{align*} \\partial_t\\|RX^T + t\\vec 1^T - P^T\\|_F^2 &amp;= 0\\\\ \\vec 1^T1t + RX^T\\vec 1 - P^T\\vec 1 &amp;= 0\\\\ t&amp;=\\frac{P^T\\vec 1 -RX^T\\vec 1}{\\vec 1^T\\vec 1}\\\\ t &amp;= \\bar p - R\\bar x \\end{align*}\\] <p>where \\(X_{k\\times 3}, P_{k\\times 3}\\) are \\(x_i, p_i\\) stacked vertically, and \\(\\vec 1_{k\\times 1}\\) is the vector filled with 1's. \\(\\|\\cdot\\|_F\\) is the Frebenius norm, which is the sum of squared elements.  \\(\\bar p = k^{-1}\\sum^k p_i, \\bar x = k^{-1}\\sum^k x_i\\) is the mean of all points. </p> <p>Then, replace \\(t = \\bar p - R\\bar x\\), the problem becomes </p> \\[\\begin{align*} R^* &amp;= \\arg\\min_{R} \\sum^k \\|Rx_i + (\\bar p - R\\bar x) - p_i\\|^2\\\\ &amp;= \\arg\\min_{R} \\sum^k \\|R(x_i-\\bar x) - (p_i - \\bar p)\\|^2\\\\ &amp;= \\arg\\min_{R} \\| R\\bar X^T - \\bar P^T\\|^2_F \\end{align*}\\] <p>where \\(\\bar X_{k\\times 3}, \\bar P_{k\\times 3}\\) are \\(x_i-\\bar x, p_i - \\bar p\\) stacked vertically. </p> <p>Then, </p> \\[\\begin{align*} R^* &amp;= \\arg\\min_{R} \\| R\\bar X^T - \\bar P^T\\|^2_F\\\\ &amp;= \\arg\\min_{R} \\|R\\bar X^T\\|_F^2 - 2\\langle R\\bar X^T, \\bar P^T\\rangle_F + \\|\\bar P^T\\|^2_F\\\\ &amp;= \\arg\\min_{R} \\|\\bar X^T\\|_F^2 - 2\\langle R\\bar X^T, \\bar P^T\\rangle_F + \\|\\bar P^T\\|^2_F &amp;R\\in SO(3)\\Rightarrow R^TR = I\\\\ &amp;= \\arg\\max_{R} \\langle R\\bar X^T, \\bar P^T\\rangle_F\\\\ &amp;= \\arg\\max_{R} \\langle R, \\bar P^T\\bar X\\rangle_F &amp;\\text{permutation property of F-norm} \\end{align*}\\]"},{"location":"csc419/registration.html#closest-rotation-matrix","title":"Closest Rotation Matrix","text":"<p>Using SVD, we can have \\(\\bar P^T\\bar X = U\\Sigma V^T\\), so that with permutation property of F-norm again</p> \\[R^* = \\arg\\max_R\\langle R, U\\Sigma V^T\\rangle =  \\arg\\max_R\\langle U^TRV, \\Sigma\\rangle\\] <p>Let \\(\\Omega = U^TRV\\) with \\(\\det \\Omega = \\det UV^T\\), since \\(U,V\\) are orthonormal, \\(\\Omega^T = \\Omega^{-1}\\). This implies that \\(R^* = U\\Omega^*V^T\\), where</p> \\[\\Omega^* = \\underset{ {\\Omega\\in O(3), \\det\\Omega = \\det UV^T} }{\\arg\\max} \\langle \\Omega, \\Sigma\\rangle_F\\] <p>so that \\(\\det R^* = 1\\). </p> <p>Because \\(\\Omega\\) is orthonormal, each col and row must have unit norm. Placing a non-zero on the off-diagonal will get \"killed\" when multiplied by the corresponding zero in \\(\\Sigma\\). So the optimal choice is to set all values 0, except the diagonal. The best choice is </p> \\[\\Omega^* = \\begin{bmatrix}1&amp;0&amp;0\\\\0&amp;1&amp;0\\\\0&amp;0&amp;\\det UV^T\\end{bmatrix}\\] <p>so that \\(\\det UV^T\\) is multiplied with the smallest \\(\\sigma\\). finally the closest rotation matrix is obtained as </p> \\[R^* = U\\Omega^*V^T\\]"},{"location":"csc419/registration.html#point-to-plane-rigid-matching","title":"Point to Plane Rigid Matching","text":"<p>We are trying to solve </p> \\[\\arg\\min_{R, t} \\sum^k \\|(Rx_i + t - p_i)\\cdot \\hat n_i \\hat n_i\\|^2\\] <p>where \\(\\hat n_i\\) is the unit normal at the located closest point \\(p_i\\). The problem can be simplified a bit as </p> \\[\\arg\\min_{R, t} \\sum^k ((Rx_i + t - p_i)\\cdot \\hat n_i)^2\\] <p>To optimize \\(R\\), we linearize the constraint that \\(R\\) stays a rotation matrix and work with a reduced set of variables.  Any rotation matrix can be written as scalar rotation angle \\(\\theta\\) around a rotation axis defined by a unit vector \\(\\hat w\\), generally, given the rotation axis, the axis-angle to matrix formula gives </p> \\[R_{\\hat w}(\\theta) = I + \\sin\\theta W + (1-\\cos\\theta)W^2\\] <p>where \\(W\\) is the skew symmetric cross product matrix </p> \\[Wx = \\hat w\\times x, W = \\begin{bmatrix}0&amp;-w_3&amp;w_2\\\\w_3&amp;0&amp;-w_1\\\\-w_2&amp;w_1&amp;0\\end{bmatrix}\\] <p>In this form, we can linearize by considering a small change in \\(\\theta\\) and \\(\\hat w\\), as</p> \\[R\\approx I + \\theta W, Rx \\approx x + \\theta \\hat w \\times x\\] <p>introducing \\(a = \\theta \\hat w\\), the problem becomes </p> \\[\\arg\\min_{a,t} \\sum^k ((x_i + a\\times x_i + t - p_i)\\cdot \\hat n_i)^2\\] <p>and let \\(u = \\begin{bmatrix}a\\\\t\\end{bmatrix}\\), </p> \\[\\arg\\min_u \\sum^k \\big(\\begin{bmatrix}(x_i\\times \\hat n_i)^T &amp;\\hat n_i^T\\end{bmatrix}u - \\hat n_i^T (p_i - x_i)\\big)^2\\] <p>Then, let \\(c_i = \\begin{bmatrix}(x_i\\times \\hat n_i)^T &amp;\\hat n_i^T\\end{bmatrix}, d_i = \\hat n_i^T (p_i - x_i)\\),  the equation above is expanded as </p> \\[\\arg\\min_u \\sum^k u^Tc_i^Tc_iu-2u^Tc_i^Td_i + d_i^Td_i = \\arg\\min_u \\sum^k u^TAu-2u^Tb\\] <p>where \\(A = \\sum^k c_i^Tc_i, b = \\sum^kc_i^Td_i\\)</p> <p>Therefore, \\([a^*, t^*] = u^* = A^{-1}b\\), as we considering \\(A\\) as a sum of \\(c_i\\), which is convex and separable.  Then, we have \\(a^* = \\underset{\\theta}{\\|a^*\\|}\\underset{\\hat w}{\\frac{a^*}{\\|a^*\\|}}\\), and we can find </p> \\[R^* = I + \\sin\\theta W^* + (1-\\cos\\theta) [W^*]^2\\]"},{"location":"csc419/registration.html#some-additional-pieces","title":"Some Additional Pieces","text":""},{"location":"csc419/registration.html#uniform-random-sampling-of-a-triangle-mesh","title":"Uniform random sampling of a triangle mesh","text":"<p>To uniform sampling a mesh \\(X\\), the pdf will be </p> \\[f_X(x) = A_X^{-1}\\] <p>we can then have two independent uniform random sampling, first, sample a triangle from \\(T=\\{1,2,...,m\\}\\) of the mesh, which  \\(h_i = \\frac{A_i}{A_X}\\) and then for each triangle, sample from \\(g_i(x) = A^{-1}_i\\), so that </p> \\[f_X(x) = h(T)g_T(x) = \\frac{A_T}{A_X}\\frac{1}{A_T} = A_X^{-1}\\] <p>Consider \\(g_T\\), a triangle can be written as parametric function \\(T(s, t) = a + s(b-a) + t(c-a)\\) where \\(0&lt;s, t&lt;1, s+t\\leq1\\). Also, note that if we release the constraint \\(s+t\\leq1\\), we get a parallelgram formed by reflecting \\(a\\) across \\(\\bar{bc}\\). Therefore, we can randomly sample \\(s, t\\) and if \\(s+t &gt; 1\\), we reflect it back by \\(1-s, 1-t\\)</p>"},{"location":"csc419/registration.html#area-weighted-random-sampling-of-triangles_1","title":"Area Weighted random Sampling of Triangles","text":"<p>Note that the cmf of \\(h\\) is </p> \\[C(i) = \\sum_{j=1}^i \\frac{A_j}{A_X} \\in (0, 1]\\] <p>Therefore, we can randomly sample \\(x\\) and find the first value of \\(C_i\\) s.t. \\(C_i &gt; x\\)</p>"},{"location":"csc419/registration.html#point-triangle-distance","title":"Point Triangle Distance","text":"<p>Given a query point \\(x\\in\\mathbb R\\) and triangle \\(T\\in \\mathbb R^{3\\times 3}\\) with corners \\(a, b,c\\). To find \\(p\\) s.t. </p> \\[p=\\arg\\min_{p\\in T}\\|x-p\\|\\] <p>Let \\(P\\) be the plane that \\(T\\) belongs to, we can simply define \\(P\\) from \\(T\\), but remove the constraints.  </p> \\[P(s,t):\\mathbb R^2\\rightarrow\\mathbb R^3:= a + s(b-a) + t(c-a)\\] <p>Let \\(q \\in P\\) be the point projected from \\(x\\) to \\(P\\). Then, </p> \\[\\|x-p\\| = \\|(x-p) + (p-q)\\|\\] <p>Note that \\(x, q\\) is fixed, so that we want to minimize \\(\\|p-q\\|\\), note that \\(p,q\\) both line on \\(P\\), thus this problem becomes a 2D problem on the plane \\(P\\).</p> <p>\u200b </p> <p>Then, to solve the 2D problem. Note that the regions are divided by \\(s=0, t=0, s+t=1\\), we can do it case by case.   - 0 =&gt; within the triangle, done  - 1, 3, 5 -&gt; point-line projection, if not on the segment range, then map it to the endpoint  - 2, 4, 6 -&gt; directly go to the endpoint</p> <p>An efficient implementation will try to reduce the number of divisions, so that the relative error is small  (See Reference here)</p>"},{"location":"csc419/smoothing.html","title":"Surface Smoothing","text":""},{"location":"csc419/smoothing.html#motivation","title":"Motivation","text":"<p>Given a scanned of a ball, due to the noise in scanning, the formed mesh may not be smooth, i.e. the transition in normal/tangent is not continuous. Therefore, we want to smooth it out. </p> <p>Mathematically, we have a data signal over a curved surface, the data signal may be   - Data denoising a scalar field defined on a static surface, we want the function of the data signal on the surface to be smooth  - Surface fairing the geometric position of the surface itself, we want the underlying geometry of the domain to be smoothed.</p>"},{"location":"csc419/smoothing.html#formation-of-the-system","title":"Formation of the System","text":""},{"location":"csc419/smoothing.html#flow-based-formation","title":"Flow-based Formation","text":"<p>We can think of the signal as undergoing a flow toward a smooth solution over some phony notion of \"time\", then we should have the PDE being set as the change in signal value \\(u\\) over time proportional to the Laplacian of the signal </p> \\[\\frac{\\partial u}{\\partial t} = \\lambda \\Delta u\\] <p>where \\(u\\) is the function of the signal, \\(\\Delta\\) is the Laplacian operator and \\(\\lambda\\) is the rate of smoothing. </p> <p>Given a noisy signal \\(f\\), intuitively we can smooth by average values with its neighborings. If we average of the small ball of nearby points, then we can have smoothing function</p> \\[u(x) = |B_\\delta(x)|^{-1}\\int_{B_\\delta(x)}f(z)dz\\] <p>If \\(\\delta\\) is small, then we will have to repeat the smoothing multiple times to see a global smoothing. Hence, we can write the current value \\(u^t\\) flows toward smooth solution by small step \\(\\Delta t\\) in time</p> \\[u^{t+\\Delta t}(x) = |B_\\delta(x)|^{-1}\\int_{B_\\delta(x)}u^t(z)dz\\] <p>Subtracting current \\(u^t(x)\\) from both sides and introducing a flow-speed parameter \\(\\lambda\\), we will have the flow equation describing the change in values as an integral of relative values</p> \\[\\frac{\\partial u}{\\partial t} = \\lambda |B_\\delta(x)|^{-1}\\int_{B_\\delta(x)}(u(z)-u(x))dz\\] <p>For harmonic functions, \\(\\Delta u =0\\), this integral becomes zero in the limit as the \\(\\delta\\rightarrow 0\\) via satisfaction of MVT. If follows for a non-harmonic \\(\\Delta u\\neq 0\\), this integral is equal to the Laplacian of \\(u\\), i.e.</p> \\[\\frac{\\partial u}{\\partial t} = \\lim_{|B_\\delta(x)|\\rightarrow 0}\\lambda |B_\\delta(x)|^{-1}\\int_{B_\\delta(x)}(u(z)-u(x))dz = \\lambda \\Delta u\\]"},{"location":"csc419/smoothing.html#energy-based-formation","title":"Energy-based Formation","text":"<p>Alternatively, think of a single smoothing operation as the solution to an minimization problem. If \\(f\\) is the noisy signal, then we want to find a signal \\(u\\) that it simultaneously minimizes its difference with \\(f\\) and minimizes its variation over the surface</p> \\[\\arg\\min_u \\frac12\\int_S(f-u)^2 + \\lambda \\|\\nabla u\\|^2 dA\\] <p>where \\(\\lambda\\) is the rate of smoothing. </p> <p>Note that \\(f\\) is unknown and we are optimizing a function w.r.t. the integral involving itself and its second order derivative. Then, this is the calculus of variations problem. Then, applying Green's first identity, we can arrive the same conclusion that </p> \\[u^*(x) - f(x)= \\lambda \\Delta u^*(x), \\forall x\\]"},{"location":"csc419/smoothing.html#implicit-smoothing-iteration","title":"Implicit Smoothing Iteration","text":"<p>Let \\(u^0 = f\\), we can compute a new smoothed function \\(u^{t+1}\\) from the current solution \\(u^t\\) by solving</p> \\[u^t(x) = (id - \\lambda \\Delta) u^{t+1}(x), \\forall x\\in S\\] <p>where \\(id\\) is the identity operator.</p>"},{"location":"csc419/smoothing.html#discrete-laplacian","title":"Discrete Laplacian","text":"<p>Note that we are doing graphics! So we will only need discrete approximations of id and Laplacian operators. Therefore, we need to produce some sparse Laplacian matrix \\(L\\in\\mathbb R^{n\\times n}\\) for a mesh of \\(n\\) vertices. </p>"},{"location":"csc419/smoothing.html#finite-element-derivation-of-the-discrete-laplacian","title":"Finite Element Derivation of the Discrete Laplacian","text":"<p>We want to approximate the Laplacian of a function \\(\\Delta u\\). First, consider \\(u\\) as a piecewise-linear function represented by scalar values at each vertex, collected in \\(\\mathbf u\\in \\mathbb R^n\\). </p> <p>Any piecewise-linear function can be expressed as a sum of values at mesh vertices times corresponding piecewise-linear basis functions (hat functions, \\(\\varphi_i\\))</p> \\[\\begin{align*} u(x) &amp;= \\sum^n u_i\\varphi_i(x)\\\\ \\varphi(x) &amp;= \\begin{cases}1&amp;x=v_i\\\\\\frac{A(x, v_j, v_k)}{A(v_i, v_j, v_k)}&amp;x\\in\\text{triangle}(i,j,k)\\\\0&amp;\\text{otherwise}\\end{cases} \\end{align*}\\] <p>By plugging the definition into smoothness energy above, we have</p> \\[\\begin{align*} \\|\\Delta u(x)\\|^2 &amp;= \\|\\nabla \\big(\\sum^n u_i\\varphi_i(x)\\big)\\|^2\\\\ &amp;= \\big(\\sum^n u_i\\nabla \\varphi_i(x)\\big)\\big(\\sum^n u_i\\nabla\\varphi_i(x)\\big)\\\\ &amp;= \\sum_i^n \\sum_j^n \\nabla\\varphi_i\\cdot\\nabla\\varphi_ju_iu_j \\end{align*}\\] <p>Therefore, </p> \\[\\int_S\\|\\nabla u(x)\\|^2dA = \\int_S\\sum_i^n \\sum_j^n \\nabla\\varphi_i\\cdot\\nabla\\varphi_ju_iu_j = u^TLu\\] <p>where </p> \\[L_{ij} = \\int_S \\nabla \\varphi_i\\nabla\\varphi_j dA\\] <p>By defining \\(\\varphi_i\\) as piecewise-linear hat functions, the values in the system matrix \\(L_{ij}\\) is uniquely determined by the geometry of the underlying mesh, known as cotangent weights. </p> <p>First note that \\(\\nabla\\varphi_i\\) are constant on each triangle, and only nonzero on triangles incident on node \\(i\\), for such trianlge \\(T_\\alpha\\), \\(\\nabla {\\varphi_i}\\) points perpendicularly from the opposite edge \\(e_i\\) with inverse magnitude equal to the height \\(h\\) of the triangle treating that opposite edge as base, </p> \\[\\|\\nabla \\varphi_i\\| = h^{-1} = \\frac{\\|e_i\\|}{2A}\\] <p>Now for neighboring nodes \\(i,j\\) connected by edge \\(e_{ij}\\). Then \\(\\nabla \\varphi_i\\) points toward node \\(i\\) perpendicular to \\(e_i\\) the same for \\(\\nabla \\varphi_j\\). Hence, call the angle formed between these two vectors \\(\\theta\\), then </p> \\[\\nabla\\varphi_i \\cdot\\nabla\\varphi_j = \\|\\nabla\\varphi_i\\|\\|\\nabla\\varphi_j\\|\\cos\\theta = \\frac{\\|e_j\\|}{2A}\\frac{\\|e_i\\|}{2A}\\cos\\theta\\] <p>call \\(a_ij = \\pi-\\theta\\) the angle between \\(e_i\\) and \\(e_j\\), then </p> \\[\\cos\\theta = -\\cos(\\pi-\\theta) = -\\cos a_{ij}\\] <p>Thereforem \\(\\frac{\\|e_j\\|}{2A}\\frac{\\|e_i\\|}{2A}\\cos\\theta = -\\frac{\\|e_j\\|}{2A}\\frac{\\|e_i\\|}{2A}\\cos a_{ij}\\), so that</p> \\[\\sin a_{ij} = \\frac{h_j}{\\|e_i\\|} = \\frac{h_i}{\\|e_j\\|}\\] <p>Then, we can replace \\(\\|e_i\\| = \\frac{h_j}{\\sin a_{ij}}\\), </p> \\[\\begin{align*} \\nabla\\varphi_i \\cdot\\nabla\\varphi_j &amp;= -\\frac{\\|e_j\\|}{2A}\\frac{\\|e_i\\|}{2A}\\cos a_{ij}\\\\ &amp;= -\\frac{\\|e_j\\|}{2A}\\frac{h_j}{2A}\\frac{\\cos a_{ij}}{\\sin a_{ij}}\\\\ &amp;= -\\frac{\\cot a_{ij}}{2A} \\end{align*}\\] <p>Similarly, inside the other triangle \\(T_\\beta\\) incident on nodes \\(i,j\\) with angle \\(\\beta_{ij}\\), we have </p> \\[\\nabla\\varphi_i \\cdot\\nabla\\varphi_j = -\\frac{\\cot\\beta_{ij}}{2B}\\] <p>Then, go back to the integral over area, </p> \\[\\int_S \\nabla \\varphi_i\\cdot \\nabla\\varphi_jdA = [A\\nabla \\varphi_i\\cdot \\nabla\\varphi_i]\\mid_{T_a} + [B\\nabla \\varphi_i\\cdot \\nabla\\varphi_i]\\mid_{T_\\beta} = -\\frac12(\\cot a_{ij} + \\cot b_{ij})\\] <p>\u200b </p>"},{"location":"csc419/smoothing.html#mass-matrix","title":"Mass Matrix","text":"<p>Treated as an operator, the Laplacian matrix \\(L\\) compute the local integral of the Laplacian of \\(u\\). In the energy based formulation this is not an issue. If we used a similar FEM derivation for the data term we should get another sparse matrix \\(M\\in\\mathbb R^{n\\times n}\\)</p> \\[\\int_S(u-f)^2dA = \\int_S \\sum_i^n\\sum_j^n \\varphi_i\\cdot\\varphi_j (u_i-f_i)(u_j-f_j)dA = (u-f)^TM(u-f)\\] <p>This matrix \\(M\\) is often diagonalized or lumped into a diagonal matrix, even in the context of FEM. </p> \\[M_{ij} = \\begin{cases}\\frac{1}{3}\\sum_t^m Area(t)\\mathbb I(v_i\\in T_t) &amp;i=j\\\\0&amp;\\text{otherwise}\\end{cases}\\] <p>Replacing every thing with discrete case, we have </p> \\[u^t = (I - \\lambda M^{-1}L)u^{t+1}\\] <p>However, this equation is had to solve due to the singularity, multiply both sides by \\(M\\) gives a better</p> \\[Mu^t = (M-\\lambda L)u^{t+1}\\] <p>Now the system matrix \\(A=M+\\lambda L\\) is symmetric and using Cholesky factorization we can solve with it. </p>"},{"location":"csc419/smoothing.html#laplace-operator-is-intrinsic","title":"Laplace Operator is Intrinsic","text":"<p>Note that \\(\\cot\\) of a triangle only depends on the edge length, we do not need to know where vertices are actually positioned in space or even which dimension the mesh is living in. Also, applying transformations to a shape that does not change lengths, will have no effect on the Laplacian. </p>"},{"location":"csc419/smoothing.html#cotangent-as-a-function-of-only-edge-length","title":"Cotangent as a function of only edge length","text":"<p>Given a triangle \\(abc\\) as shown below,  </p> <p>\u200b </p> <p>Note that \\(A = \\frac12 ah_a\\), where \\(h_a\\) is the height perpendicular to \\(a\\), hence \\(h_a = b\\sin C\\), so that </p> \\[\\sin C = \\frac{2A}{ab}\\] <p>By Heron's formula </p> \\[A = \\sqrt{s(s-a)(s-b)(s-c)}, s=\\frac12(a+b+c)\\] <p>By law of cosines</p> \\[\\cos C = \\frac{a^2+b^2-c^2}{2ab}\\] <p>Therefore, </p> \\[\\begin{align*} \\cot C &amp;= \\frac{\\cos C}{\\sin C}\\\\ &amp;= \\frac{\\frac{a^2+b^2-c^2}{2ab}}{\\frac{2\\sqrt{s(s-a)(s-b)(s-c)}}{ab}}\\\\ &amp;= \\frac{a^2+b^2-c^2}{4\\sqrt{s(s-a)(s-b)(s-c)}} \\end{align*}\\]"},{"location":"csc419/smoothing.html#solutions","title":"Solutions","text":""},{"location":"csc419/smoothing.html#data-denoising","title":"Data Denoising","text":"<p>Our geometry of the domain is not changing, we only change the scalar function living upon it. Therefore, build our discrete Laplacian \\(L\\) and mass function \\(M\\) and apply above formula with a chosen \\(\\lambda\\) parameter. </p>"},{"location":"csc419/smoothing.html#geometric-smoothing","title":"Geometric Smoothing","text":"<p>The geometry will change the Laplacian, hence also \\(L,M\\) in the discrete setting. Therefore, if signal \\(u\\) is replaced with the positions of points on the surface (say vertices \\(V\\) in discrete case), then the smoothing iteration update rule is non-linear if we write as </p> \\[M^{t+1}V^T = (M^{t+1}-\\lambda L^{t+1})V^{t+1}\\] <p>However, assuming small changes in \\(V\\) have a negligible effect on \\(L, M\\) then we can discretize explicitly by having \\(L,M\\) before the update</p> \\[M^tV^T = (M^t-\\lambda V^t)V^{t+1}\\]"},{"location":"csc419/sparse_icp.html","title":"Sparse Iterative Closest Point","text":"<p>Sparse Iterative Closest Point C++ Implementation</p>"},{"location":"csc419/sparse_icp.html#registration-and-icp","title":"Registration and ICP","text":"<p>Check the Notes on  CSC419 - Registration</p>"},{"location":"csc419/sparse_icp.html#problems-with-icp","title":"Problems with ICP","text":"<ul> <li>Incorrect closest-point correspondences.</li> <li>Not robust to utliers, or the correspondence is not ideal on the target shape.</li> <li>The solution for defining outliers require lots of coefficients tuning</li> </ul>"},{"location":"csc419/sparse_icp.html#model-outliers-using-sparsity","title":"Model Outliers using Sparsity","text":"<p>Let the distance between each correspondence be \\(z\\). Therefore, the ideal model should have inliners having \\(\\lVert z\\rVert_2\\approx 0\\) and the outliers having \\(\\lVert z\\rVert_2 \\gg 0\\). However, this is not enforced in ICP. For other ICP related methods, the outlier is achieved by other heuristics. In this paper, the author tries to enforce this by replacing the euclidean distance with other \\(l_p\\) norm, where \\(p&lt;1\\). </p>"},{"location":"csc419/sparse_icp.html#sparsity-enforcement-by-lp-norm","title":"Sparsity Enforcement By l<sub>p</sub> Norm","text":"<p>\\(l_p\\) norm is defined as </p> \\[L_p(\\textbf z) = (\\sum_i |z_i|^p)^{1/p}\\] <p>Then, \\(l_0\\) norm is \\(L_0(\\mathbf z) = \\sum \\mathbb I(z_i = 0)\\), i.e. the number of non-zero elements. \\(l_1\\) norm is \\(L_1(\\mathbf z) = \\sum |z_i|\\), i.e. the absolute sum of elements.  \\(l_2\\) norm is Euclidean norm.</p> <p>From the \\(l_p\\) norm's definition, obviously the best option for enforcing sparsity is to use \\(l_0\\) norm, but \\(l_0\\) norm is non-convex, it is unable to optimize. </p> <p>\\(l_1\\) regularization is a very simple technique used in many fields.such as machine learning (lasso regression). Where \\(l_2\\) regularizations (ridge regression) aims to regularizes the weights, \\(l_1\\) regularizations will more likely to reduce some weights (variables) to 0. </p> <p>Instead of using \\(p=1\\), the paper chooses to use \\(p&lt;1\\) as a relaxation of \\(l_0\\) norm. In this case, the problem is non-convex, and the efficient optimization is a bit different. The author proposes Alternating Direction Method of Multipliers (ADMM) to tackle this problem. </p> <pre><code>import matplotlib.pyplot as plt\nimport numpy as np\nx = np.arange(-1.5, 1.5, 0.01)\nfor p in (0.1, 0.5, 1, 2):\n    plt.plot(x, np.abs(x)**p, label=f\"p={p}\")\nplt.legend();\n</code></pre> <p>\u200b </p>"},{"location":"csc419/sparse_icp.html#robust-distance-function","title":"Robust Distance Function","text":"<p>The original ICP works as the correspondence and rigid matching steps, i.e. </p> \\[\\arg\\min_Y \\sum^n d(Rx_i + t, y_i)\\] \\[\\arg\\min_{R, t} d(Rx_i + t, y_i), R \\in SO(k), t\\in \\mathbb R^k\\] <p>Typically, \\(d(\\cdot) = \\|\\cdot\\|^2_2\\) In our case, we use \\(d(\\cdot) = \\|\\cdot\\|_2^p\\), note that the norm is still Euclidean norm so that we penalize only on the corresponding points, instead of penalizing individual xyz-coordinates. </p>"},{"location":"csc419/sparse_icp.html#numerical-optimization","title":"Numerical Optimization","text":"<p>Now, we have the optimization question, but this is non-convex and non-smooth. </p> \\[\\arg\\min_Y \\sum^n \\|Rx_i + t, y_i\\|^p_2\\] \\[\\arg\\min_{R, t} \\|Rx_i + t, y_i\\|^p_2, R \\in SO(k), t\\in \\mathbb R^k\\]"},{"location":"csc419/sparse_icp.html#correspondence-step","title":"Correspondence Step","text":"<p>Note that this optimization is the same as original ICP, since we are still to find closest point. </p>"},{"location":"csc419/sparse_icp.html#rigid-matching-step","title":"Rigid Matching Step","text":"<p>Consider the optimization question</p> \\[\\arg\\min_{R, t} \\|Rx_i + t, y_i\\|^p_2, R \\in SO(k), t\\in \\mathbb R^k\\] <p>Introduce \\(Z = \\{z_1,...,z_n\\}, z_i = Rx_i + t - y_i\\), so that the question becomes constrained</p> \\[\\begin{align*} &amp;&amp;\\arg\\min_{R, t, Z} \\|z_i\\|^p_2\\\\ &amp;\\text{subject to} &amp;R \\in SO(k), t\\in \\mathbb R^k\\\\ &amp;&amp;Rx_i + t - y_i - z_i = 0 \\end{align*}\\] <p>Denote \\(\\delta_i = Rx_i + t - y_i - z_i\\). We can solve the problem using augmented Lagrangian methods</p> \\[\\mathcal L(R, t, Z, \\Lambda) = \\sum^n \\|z_i\\|^p_2 + \\lambda_i^T\\delta_i+\\frac\\mu 2\\|\\delta_i\\|_2^2\\] <p>where \\(\\Lambda = \\{\\lambda_i\\in\\mathbb R^k:i=1,...,n\\}\\) is a set of Lagrange multipliers and \\(\\mu&gt;0\\) is the penalty weight. </p> <p>Optimize the equation by ADMM</p> \\[\\begin{align*} &amp;(1)&amp;\\arg\\min_Z\\sum^n \\|z_i\\|_2^p + \\frac\\mu 2\\|z_i - h_i\\|_2^2\\\\ &amp;(2)&amp;\\arg\\min_{R,t}\\sum^n \\|Rx_i + t - c_i\\|_2^2\\\\ &amp;(3)&amp;\\lambda_i = \\lambda_i + \\mu\\delta_i \\end{align*}\\] <p>where \\(c_i = y_i + z_i - \\frac{\\lambda_i}\\mu, h_i = Rx_i + t - y_i + \\frac{\\lambda_i}\\mu\\).</p> <p>Then, \\((1)\\) is separable for each \\(z_i\\) and then can be solved by shrink operator and \\((2)\\) is the typical ICP step.</p>"},{"location":"csc419/surface_reconstruction.html","title":"Surface Reconstruction","text":"<p>Link to paper</p> <p>Note that the notes is a simplified version of the paper's implementation. We use fixed unit grid node instead of adaptive grid node. Also, we use the trilinear interpolation instead of more complex weighted methods. </p>"},{"location":"csc419/surface_reconstruction.html#problem-setup","title":"Problem Setup","text":"<p>Given a set of scanned point samples \\(P\\) on the surface of an object and the estimated point normals \\(N\\), we want to find an explicit continuous surface representation, i.e. a triangle mesh. </p>"},{"location":"csc419/surface_reconstruction.html#voxel-based-implicit-surface","title":"Voxel-based Implicit Surface","text":"<p>However, if we directly convert the point cloud to a mesh by connecting points together, it's difficult to ensure the certain topological postconditions, i.e. a closed manifold with a small number of holes. Instead we will first convert the point cloud sampling representation into an implicit surface representation. Let the unknown surface be the level set of some function \\(g:\\mathbb R^3\\rightarrow\\mathbb R\\), i.e. </p> \\[\\partial S = \\{x\\in\\mathbb R^3\\mid g(x) = c\\}\\] <p>Then, we discretize an implicit function by defining a regular 3D grid of voxels containing at least the bounding box of \\(S\\). At each node in the grid \\(x_{i,j,k}\\) we store the value of the implicit function $g(x_{i,j,k}) and then we can get the value everywhere by trilinear interpolation over the grid cell. </p>"},{"location":"csc419/surface_reconstruction.html#matching-cubes-algorithm","title":"Matching Cubes Algorithm","text":"<p>We want the implicit surface representation so that we can contoured into a triangle mesh via Marching Cubes Algorithm. </p>"},{"location":"csc419/surface_reconstruction.html#characteristic-function-of-solids","title":"Characteristic Function of Solids","text":"<p>Assume that our set of point \\(P\\) lies on the surface \\(\\partial S\\). This solid object \\(S\\) must have some non-trivial volume that we can calculate as the integral of unit density over the solid </p> \\[\\int_S dA = \\int_{\\mathbb R^3}\\mathcal x_S(x)dA = \\int_{\\mathbb R^3}\\mathbb I(x\\in S)dA\\] <p>From the Poisson Surface Reconstruction [Kazhdan et al. 2006] , the gradient of a infinitesimally mollified characteristic function </p> <ul> <li>points in the direction of the normal near the surface \\(\\partial S\\)</li> <li>is zero everywhere else</li> </ul> <p>Therefore, using points \\(P\\) and normals \\(N\\), we can optimize an implicit function \\(g\\) over a regular grid, so that \\(\\nabla g\\) meets the two properties above, and \\(g\\) will be an approximation of the mollified characteristic function. </p>"},{"location":"csc419/surface_reconstruction.html#poisson-surface-reconstruction","title":"Poisson Surface Reconstruction","text":"<p>Starting from some unrealistic assumptions (note that these assumptions are not to be made but to make the description more intuitive)</p> <ul> <li>We know each of \\(\\nabla g(x_{i,j,k})\\)</li> <li>All of input points lies perfectly at grid nodes: \\(\\forall p\\in P. \\exists x_{i,j,k} = p\\)</li> </ul> <p>With these assumptions, we simply have </p> \\[\\nabla g(x_{i,j,k}):\\mathbb R^3\\rightarrow\\mathbb R^3:= v_{i,j,k} = \\mathbb I(x_{i,j,k}\\mid \\exists p_l = x_{i,j,k}) \\:n_l\\] <p>So that \\(g\\) is defined via a simple set of linear equations</p> <p>Since our system is over determined, this can be turned into an optimization problem. </p> \\[\\min_g \\sum_i\\sum_j\\sum_k \\frac12\\|g(x_{i,j,k}) - v_{i,j,k}\\|^2\\] <p>where \\(g\\) is a vector of unknown grid nodes values, \\(g_{i,j,k} = g(x_{i,j,k})\\)</p> <p>With the assumptions, we can compute approximations of the x,y,z components of \\(\\nabla g\\) via a matrix multiplication of a \"gradient matrix\" \\(G\\) and vector unknown grid value \\(g\\), so that the optimization problem becomes</p> \\[\\min_g \\frac12\\|Gg - v\\|^2 = \\min_g\\frac12 g^TG^TGg - g^TG^Tv + v^Tv\\] <p>This is a quadratic function, so that we can set the gradient being 0</p> \\[\\frac{\\partial}{\\partial g} \\frac12 g^TG^TGg - g^TG^Tv + v^Tv = G^TGg - G^Tv = 0\\]"},{"location":"csc419/surface_reconstruction.html#gradient-estimated-from-regular-grid","title":"Gradient estimated from regular grid","text":"<p>Since we have a grid, it's straightforward to estimate the gradient as a finite difference. For example,  \\(\\frac{\\partial g}{\\partial x}(x_{i-1/2, j, k}) = \\frac1h(g_{i,j,k} - g_{i-1,j,k})\\). With this observation, we can construct a partial difference matrix \\(D^x \\in \\mathbb R^{(n_x -1)n_yn_z\\times n_xn_yn_z}\\), where each row \\(D^x_{i-1/2,j,k}\\) computes the partial derivative at \\(x_{i-1/2,j,k}\\), i.e. </p> \\[D^x_{i-1/2,j,k}(l) = \\begin{cases}-1 &amp;l=i-1\\\\1 &amp;l=1\\\\0&amp;\\text{otherwise}\\end{cases}\\] <p>We can do similar things on \\(D^y, D^z\\) and obtain the desired </p> \\[G = \\begin{pmatrix}D^x\\\\D^y\\\\D^z\\end{pmatrix} \\in \\mathbb R^{(n_x-1)n_yn_z + n_x(n_y-1)n_z + n_xn_y(n_z-1)\\times n_xn_yn_z}\\] <p>Of course, when we implement this, we cannot have indexing as \\(0.5\\), so we can shift the indexing down by \\(0.5\\), and do the same for the vector \\(v\\).</p>"},{"location":"csc419/surface_reconstruction.html#estimate-v","title":"Estimate v","text":"<p>Note that our normals \\(N\\) does not lie on the grid node. In order to obtain \\(v\\) on the ideal grid, we can distribute each \\(\\vec n\\) to its 8 neighboring grid nodes via trilinear interpolation weights, for example</p> \\[\\begin{align*} n_x =\\:&amp; w_{ \\frac12 ,0,0}( \\mathbf{x}_{1,\\frac14 ,\\frac12 } ) \\  v^x_{ \\frac12 ,0,0} +  \\\\   &amp;w_{\\frac32 ,0,0}( \\mathbf{x}_{1,\\frac14 ,\\frac12 } ) \\  v^x_{1\\frac12 ,0,0} +  \\\\   &amp;\\vdots \\\\   &amp;w_{\\frac32 ,1,1}( \\mathbf{x}_{1,\\frac14 ,\\frac12 } )\\ v^x_{1\\frac12 ,1,1}. \\end{align*}\\] <p>we can do this for each input normal and assmeble a parse matrix \\(W^x\\in n\\times (n_x-1)n_yn_z\\) Then, we can have \\(v^x = (W^x)^TN^x\\). Doing similar for \\(v^y, v^z\\) and stack them together, we can obtain \\(v\\)</p>"},{"location":"csc419/surface_reconstruction.html#poisson-equation","title":"Poisson Equation","text":"<p>Consider some discrete energy minimization problem </p> \\[E(g) = \\int_{\\Omega}\\|\\nabla g - V\\|^2 dA\\] <p>the Poisson's equation states that the minimizers on \\(\\Omega\\) will satisfy</p> \\[\\nabla\\cdot \\nabla g = \\nabla\\cdot v\\]"},{"location":"csc420/blob_detection.html","title":"Image Features - Blob Detection","text":""},{"location":"csc420/blob_detection.html#goal","title":"Goal","text":"<p>Independently scale interest points in each image, such that the detections are repeatable across different scales. </p>"},{"location":"csc420/blob_detection.html#general-idea","title":"General Idea","text":"<p>Extract features as a variety of scales, e.g., by multiple resolutions in a image pyramid, and then matching features at the \"corresponding\" level</p> <p>With the Harris corner detector we can find a maxima in a spatial search window, then find scale that gives local maxima of a function \\(f\\) in both position and scale. </p>"},{"location":"csc420/blob_detection.html#blob-detection-laplacian-of-gaussian","title":"\"Blob\" Detection - Laplacian of Gaussian","text":"\\[\\nabla^2g(x,y,\\sigma)=-(\\pi\\sigma^4)^{-1}(1-\\frac{x^2+y^2}{2\\sigma^2})\\exp(-\\frac{x^2+y^2}{2\\sigma^2})\\] <p>Define the characteristic scale as the scale that produced peak of the Laplacian response, so that such interest points are local maxima in both position and scale</p>"},{"location":"csc420/blob_detection.html#difference-of-gaussian","title":"Difference of Gaussian","text":"<p>Problem with LoG: LoG is not separable, and larger the \\(\\sigma\\), larger the filter is. </p> <p>Consider the approximation by finite differencing of Gaussian</p> \\[DoG := G(x,y,k\\sigma) - G(x,y,\\sigma)\\] <p>where \\(G\\) is the Gaussian function DoG is separable since Gaussian is separable, hence its difference. </p> Source code <pre><code>import cv2\nimport numpy as np\nfrom plotly.subplots import make_subplots\nimport plotly.graph_objects as go\nimport scipy.ndimage as scim\n\nfig = make_subplots(3, 2, shared_xaxes=True)\n\nN = 50\nx = np.zeros(N)\nx[N//3:N//3*2] = 1\nsigma = 0\nfor i in range(3):\n    for j in range(2):\n        fig.add_trace(\n            go.Scatter(\n                x=np.arange(N), \n                y=scim.gaussian_laplace(x, sigma),\n                name=f\"sigma={sigma}\"\n            ),\n            row=i+1, col=j+1\n        )\n        sigma += 1.5\nfig.update_layout(margin={'t': 0, 'l': 0, 'b': 0, 'r': 0})\nwith open(\"../assets/blob_detection_log.json\", \"w\") as f:\n    f.write(fig.to_json())\n\n\nx = np.linspace(-10, 10, N)\nsigmas, ks = [1, 2, 3], [0.5, 0.7, 0.95]\n\nfig = make_subplots(3, 3, shared_xaxes=True, shared_yaxes=True)\n\nfor i in range(9):\n    sigma = sigmas[i % 3]\n    k = ks[i // 3]\n    g = 1 / sigma * np.exp(-(x ** 2) / (2 * (sigma ** 2)))\n    g2 = 1 / (sigma * k) * np.exp(-(x ** 2) / (2 * ((sigma*k) ** 2)))\n    fig.add_trace(\n        go.Scatter(\n            x=x,y=g-g2,name=f\"k={k}, sigma={sigma}\"\n        ),\n        row=i%3+1, col=i//3+1\n    )\nfig.update_layout(margin={'t': 0, 'l': 0, 'b': 0, 'r': 0})\nwith open(\"../assets/blob_detection_dog.json\", \"w\") as f:\n    f.write(fig.to_json())\n</code></pre>"},{"location":"csc420/camera_model.html","title":"Homography - Basics and Camera Model","text":""},{"location":"csc420/camera_model.html#affine-transformations","title":"Affine Transformations","text":"\\[\\begin{bmatrix}x'\\\\y'\\end{bmatrix} =  \\begin{bmatrix}a&amp;b&amp;e\\\\c&amp;d&amp;f\\end{bmatrix} \\begin{bmatrix}x\\\\y\\\\1\\end{bmatrix} \\] <p>is the combination of a linear transformation \\(\\begin{bmatrix}a&amp;b\\\\c&amp;d\\end{bmatrix}\\) plus a translation $\\begin{bmatrix}e\\f\\end{bmatrix} $</p>"},{"location":"csc420/camera_model.html#properties","title":"Properties","text":"<ul> <li>Origin \\(\\Rightarrow (e, f)\\)</li> <li>Lines map to lines </li> <li>Parallel lines remain parallel</li> <li>Preserves ratios</li> <li>Closed under composition</li> <li>Rectangles go to parallelograms</li> </ul> <p>\u200b </p>"},{"location":"csc420/camera_model.html#projective-transformations","title":"Projective Transformations","text":"\\[w\\begin{bmatrix}x'\\\\y'\\\\1\\end{bmatrix} =  \\begin{bmatrix}a&amp;b&amp;e\\\\c&amp;d&amp;f\\\\g&amp;h&amp;i\\end{bmatrix} \\begin{bmatrix}x\\\\y\\\\1\\end{bmatrix}\\] <p>Note that this is an expansion of affine transformation, when \\(g=h=0\\) and \\(i=1\\)</p>"},{"location":"csc420/camera_model.html#properties_1","title":"Properties","text":"<ul> <li>origin does not necessarily map to origin</li> <li>Lines map to lines</li> <li>parallel are not necessarily remain parallel</li> <li>Do not preserve ratios</li> <li>closed under composition</li> <li>rectangles goes to quadrilateral</li> </ul>"},{"location":"csc420/camera_model.html#solving-homographies","title":"Solving Homographies","text":"<p>Let \\((x_i,y_i)\\) be a point on the reference (model) image, and \\((x_i',y_i')\\) be its matching in the test image, \\(H_{3\\times 3}\\) be the projective matrix.  So that </p> \\[a\\begin{bmatrix}x_i'\\\\y_i'\\\\1\\end{bmatrix} =  \\begin{bmatrix}h_{00}&amp;h_{01}&amp;h_{02}\\\\ h_{10}&amp;h_{11}&amp;h_{12}\\\\h_{20}&amp;h_{21}&amp;h_{22}\\end{bmatrix} \\begin{bmatrix}x_i\\\\y_i\\\\1\\end{bmatrix}\\] <p>We need to solve the 9 unknowns \\(h_{[0:2, 0:2]}\\)  First, note that</p> \\[a = h_{20}x_i + h_{21}y_i + h_{22}\\] \\[\\implies ax_i' = h_{00}x_i + h_{01}y_1 + h_{02}\\Rightarrow x_i' = \\frac{h_{00}x_i + h_{01}y_1 + h_{02}}{h_{20}x_i + h_{21}y_i + h_{22}}\\] \\[y_i' = \\frac{h_{10}x_i + h_{11}y_1 + h_{12}}{h_{20}x_i + h_{21}y_i + h_{22}}\\] <p>rewrite them as linear equations</p> \\[\\begin{align*} -(h_{20}x_i + h_{21}y_i + h_{22})x_i' + h_{00}x_i + h_{01}y_1 + h_{02} = 0\\\\ -(h_{20}x_i + h_{21}y_i + h_{22})y_i' + h_{10}x_i + h_{11}y_1 + h_{12} = 0 \\end{align*}\\] \\[\\begin{bmatrix} x_i &amp;y_i&amp;1&amp;0&amp;0&amp;0&amp;-x_i'x_i&amp;-x_i'y_i &amp;-x_i'\\\\ 0 &amp;0&amp;0&amp;x_i&amp;y_i&amp;1&amp;-y_i'x_i&amp;-y_i'y_i &amp;-y_i' \\end{bmatrix} h_F = \\vec 0\\] <p>where \\(h_F=[h_{00}, h_{01},...,h_{21},h_{22}]^T\\) is the flatten vector of \\(H\\)</p> <p>Since we have \\(n\\) sets of matching points, we can have </p> \\[A_{2n\\times 9} h_{F_9\\times 1} = \\vec 0_{2n}\\] <p>and we want to find </p> \\[h = arg\\min \\|Ah\\|_2^2\\] <p>Since \\(h\\) is only defined up to scale, we can first solve for unit vector and then multiply back scale. so that we can add constraint  \\(\\|h\\|_2 = 1\\) and </p> \\[h = arg\\min_{\\|h\\|_2=1}\\|Ah\\|_2^2\\] <p>For constrained optimization, we use method of Lagrange Multipliers where $\\hat h = $eigenvector of \\(A^TA\\) with smallest eigenvalue. </p> <p>Notice that although we seem to have \\(9\\) unknowns, we actually only have \\(8\\), since the last one is calculated from the constraint. So that we only need \\(4\\) matching to form a deterministic or over-deterministic system. </p>"},{"location":"csc420/camera_model.html#camera-modeling-projection","title":"Camera Modeling Projection","text":""},{"location":"csc420/camera_model.html#set-up","title":"Set up","text":"<p>To simplify the model, assume we have a pinhole camera, and we are working with the virtual image plane, since the real image plane is upside down. </p> <p>For the 3-D coordinate system, using the right handed coordinate system call the optical center or cemera center \\(C:= (0,0)\\) \\(Z\\) axis be the optical or principal axis, which will be orthogonal to the image plane. \\(X,Y\\) axes are parallel to the image. So that \\(Z\\)-axis points to the object, \\(X\\)-axis points away and \\(Y\\)-axis points top. </p> <p>principal point \\(p\\): the optical axis intersects the image plane. focal length \\(f:=d(C,p)\\) is the distance from the camera center to the principal point. image coordinate system the \\((x,y)\\) coordinate used to represent the 2D image plane. Let the origin be the bottom left corner, so that \\(p=(p_x,p_y)\\)</p> <p>Taking some point \\(Q\\) living in the 3D, connected to the origin, call the connection line projection line. Call its intersection with the image plane \\(q\\). However, we can notice that every point on the projection line will be mapped to \\(q\\) in the 2D image. </p>"},{"location":"csc420/camera_model.html#equations","title":"Equations","text":"<p>Consider the point \\(Q = (X,Y,Z)^T\\), then its corresponding projection point \\(q=(X\\frac{f}{Z}, Y\\frac{f}{Z},f)\\) using similar triangles. In the 2D projection space, \\(q = p + (X\\frac{f}{Z}, Y\\frac{f}{Z}) = (X\\frac{f}{Z} + p_x, Y\\frac{f}{Z}+p_y)\\)</p>"},{"location":"csc420/camera_model.html#homogeneous-coordinates","title":"Homogeneous Coordinates","text":"<p>Add a \\(1\\) for the vector, e.g. \\((x,y)\\Rightarrow [x,y,1]^T\\). In homogeneous coordinates, scaling doesn't affect anything. i.e. \\([x,y,1]^T \\sim [wx,wy,w]^T\\). Therefore, we can write \\(q\\) as </p> \\[\\hat q = \\begin{bmatrix}X\\frac{f}{Z}+p_x\\\\Y\\frac{f}{Z}+p_y\\\\1 \\end{bmatrix}\\sim \\begin{bmatrix}fX+Zp_x\\\\fY+Zp_y\\\\Z \\end{bmatrix}\\] <p>Then, the mapping from \\(Q\\) to \\(q\\) is </p> \\[ \\begin{bmatrix}fX+Zp_x\\\\fY+Zp_y\\\\Z \\end{bmatrix}  = \\begin{bmatrix}f&amp;0&amp;p_x \\\\ 0&amp;f&amp;p_y \\\\ 0&amp;0&amp;1\\end{bmatrix} \\begin{bmatrix}X\\\\Y\\\\Z \\end{bmatrix} \\] \\[\\hat q = KQ, q = \\hat q / Z\\] <p>\\(K\\) is the camera calibration matrix or intrinsic parameter matrix. The parameters in \\(K\\) are called internal camera parameters. </p>"},{"location":"csc420/camera_model.html#some-properties","title":"Some properties","text":"<ul> <li>Many to one: any points along same ray map to same point</li> <li>Points, lines, and planes are preserved </li> <li>Any line through principal point projects to a point</li> <li>Any plane through principal point projects to line</li> </ul>"},{"location":"csc420/camera_model.html#vanishing-points","title":"Vanishing points","text":"<p>All lines in the same direction in 3D will intersect at the same vanishing point in 2D. </p> <p>Consider a line that pass through \\(V\\) with direction \\(D\\), i.e. the line \\(X = V+tD\\)</p> \\[KX = \\begin{bmatrix}f&amp;0&amp;p_x \\\\ 0&amp;f&amp;p_y \\\\ 0&amp;0&amp;1\\end{bmatrix} \\begin{bmatrix}V_x+tD_x \\\\ V_y+tD_y \\\\ V_z+tD_z\\end{bmatrix} =  \\begin{bmatrix}fV_x+ftD_x + p_xV_z + tp_xD_z\\\\ fV_y+ftD_y + p_yV_z + tp_yD_z \\\\ V_z+tD_z\\end{bmatrix}\\] <p>so that the project point \\((x, y)\\) will be  \\((x,y) = \\lim_{t\\rightarrow\\infty} (\\frac{fV_x+ftD_x + p_xV_z + tp_xD_z}{V_z + tD_z}, \\frac{fV_y+ftD_y + p_yV_z + tp_yD_z}{V_z + tD_z}) = (\\frac{fD_x + p_xD_z}{D_z}, \\frac{fD_y+p_yD_z}{D_z})\\) is only about \\(D\\) and independent of \\(V\\). </p> <p>However, lines parallel to image plane are still parallel, we say that they intersect at infinity.  Consider \\(X_1,X_2\\) be two line parallel to the image plane, i.e. \\(V_z = 0, D_{1z}=D_{2z}\\) so that the distance (only demonstrate \\(x\\)-distance)</p> \\[\\begin{align*}d_x &amp;= \\frac{fV_{1x}+ftD_x + p_xV_{1z} + tp_xD_z}{V_{1z} + tD_z} - \\frac{fV_{2x}+ftD_x + p_xV_{2z} + tp_xD_z}{V_{2z} + tD_z}\\\\ &amp;= \\frac{fV_{1x}+ftD_x + p_xV_{1z}}{V_{1z}} - \\frac{fV_{2x}+ftD_x + p_xV_{2z}}{V_{2z}}&amp;D_z = 0\\\\ &amp;=C + \\frac{tfD_x(V_{2z} - V_{1z})}{V_{1z}V_{2z}}\\\\ &amp;= C + 0&amp;V_{1z}=V_{2z}\\end{align*}\\]"},{"location":"csc420/camera_model.html#example","title":"Example","text":"<p>Suppose we have a photo taken from a car with a camera on top, and we know the camera intrinsic matrix \\(K\\), WTF the incline of the hill \\(\\theta\\). </p> <p>Since the vertical lines now is not parallel to the image plane (the image plane is perpendicular to the inclined ground), we can find the vanishing points in these 2D lines, which is \\([w\\cdot vp_x, w\\cdot vp_y, w]^T\\). Therefore, </p> \\[\\begin{bmatrix}w\\cdot vp_x\\\\w\\cdot vp_y\\\\w\\end{bmatrix}=KD\\Rightarrow D = wK^{-1}\\begin{bmatrix} vp_x\\\\vp_y\\\\1\\end{bmatrix}\\] <p>Then, we can find \\(D\\) and normalizes it. </p>"},{"location":"csc420/camera_model.html#vanishing-line","title":"Vanishing Line","text":"<p>Parallel lines in 3D projects to intersected lines in 2D. However, the converse is not necessarily true.</p> <p>For lines on the same 3D plane, the vanishing points lie on a line, i.e. vanishing line. The ground plane is the horizon line. </p> <p>Parallel plnes in 3D the the same horizon line in the image. </p>"},{"location":"csc420/camera_model.html#camera-coordinate-system","title":"Camera Coordinate System","text":"<p>Since we are measuring everything from 2D images taken by the camera, we need to project the 3D points onto our camera coordinate system, so that we can describe them and get them back. </p> <p>To do such projection, we need   - Camera position \\(c\\) (in world coordinates)  - Camera orientation \\((\\vec u,\\vec v,\\vec w)\\) (in world coordinates)  - \\(K\\)</p> <p>So that for point \\(Q\\) in the world coordinates, \\(Q-c\\) is the projection in the camera coordinates, \\(R\\begin{bmatrix}u&amp;v&amp;w\\end{bmatrix}=I\\) looking for rotation to canonical orientation. Then, because orientation is orthogonal, \\(RR^T = I\\Rightarrow R = \\begin{bmatrix}u^T&amp;v^T&amp;w^T\\end{bmatrix}\\)</p> <p>Therefore, the camera coordinate for \\((x,y,z)\\) in world coordinate is</p> \\[\\begin{bmatrix}X'\\\\Y'\\\\Z'\\end{bmatrix} = R(\\begin{bmatrix}X\\\\Y\\\\Z\\end{bmatrix} - c) = \\begin{bmatrix}R&amp;-Rc\\end{bmatrix}\\begin{bmatrix}X\\\\Y\\\\Z\\\\1\\end{bmatrix}\\]"},{"location":"csc420/camera_model.html#projection-matrix","title":"Projection Matrix","text":"<p>We can describe all the cumulative effect of all intrinsic and extrinsic parameters using the projection matrix \\(P\\). so that the 2D projection \\((x,y)\\) from \\((x,y,z)\\) can be described through homogeneous coordinates</p> \\[q = \\begin{bmatrix}ax\\\\ay\\\\a\\end{bmatrix} = P\\begin{bmatrix}x\\\\y\\\\z\\\\1\\end{bmatrix}\\] <p>Where to get a point \\(q\\) in the image plane, we need to compute \\(P[x,y,z,1]^T\\), and divide all coordinates with the third coordinate, and drop the last coordinate. </p> \\[P_{3\\times 4} =  \\underset{K}{\\begin{bmatrix}f&amp;0&amp;p_x\\\\0&amp;f&amp;p_y\\\\0&amp;0&amp;1\\end{bmatrix}}\\: \\underset{\\text{projection}}{\\begin{bmatrix}1&amp;0&amp;0&amp;0\\\\0&amp;1&amp;0&amp;0\\\\0&amp;0&amp;1&amp;0\\end{bmatrix}}\\: \\underset{\\text{rotation}}{\\begin{bmatrix}&amp;&amp;&amp;0\\\\&amp;R&amp;&amp;0\\\\&amp;&amp;&amp;0\\\\0&amp;0&amp;0&amp;1\\end{bmatrix}}\\: \\underset{translation}{\\begin{bmatrix}&amp;&amp;&amp;\\\\&amp;I&amp;&amp;t\\\\&amp;&amp;&amp;\\\\0&amp;0&amp;0&amp;1\\end{bmatrix}}\\: \\] <p>Sometimes we use more compact form, </p> \\[P = K[R\\quad t] = K[R\\mid t]\\]"},{"location":"csc420/edge_detection.html","title":"Edge Detection","text":""},{"location":"csc420/edge_detection.html#how-to-find-edges","title":"How to find edges","text":"<p>An edge is a place of rapid change in image intensity function \\(\\Rightarrow\\) extrema of derivative</p> <p>Since the image is discrete in pixels, take first-order forward discrete derivative (finite difference)</p> \\[\\frac{\\partial f(x,y)}{\\partial x} = \\lim_{h \\rightarrow 0}\\frac{f(x+h, y) - f(x,y)}{g}\\approx \\frac{f(x+1, y) - f(x,y)}{1}\\] <p>\u200b </p>"},{"location":"csc420/edge_detection.html#finite-difference-filters","title":"Finite Difference Filters","text":"<p>Similarly, there are several common filters</p> <p>Prewitt</p> \\[M_x = \\begin{bmatrix} -1 &amp; 0 &amp; 1\\\\ -1 &amp; 0 &amp; 1\\\\ -1 &amp; 0 &amp; 1 \\end{bmatrix}, M_y = \\begin{bmatrix} 1 &amp; 1 &amp; 1\\\\ 0 &amp; 0 &amp; 0\\\\ -1 &amp; -1 &amp;1 \\end{bmatrix}\\] <p>Sobel</p> \\[M_x = \\begin{bmatrix} -1 &amp; 0 &amp; 1\\\\ -2 &amp; 0 &amp; 2\\\\ -1 &amp; 0 &amp; 1 \\end{bmatrix}, M_y = \\begin{bmatrix} 1 &amp; 2 &amp; 1\\\\ 0 &amp; 0 &amp; 0\\\\ -1 &amp; -2 &amp; 1 \\end{bmatrix}\\] <p>Roberts</p> \\[M_x = \\begin{bmatrix} 0 &amp; 1\\\\ -1 &amp; 0 \\end{bmatrix}, M_y = \\begin{bmatrix} 1 &amp; 0 \\\\ 0 &amp; -1 \\end{bmatrix}\\] <p>\u200b </p>"},{"location":"csc420/edge_detection.html#image-gradient","title":"Image Gradient","text":"<p>The gradient of an image is \\(\\Delta f = [\\partial_x f, \\partial_y f]\\) The gradient points in the direction of most rapid change in intensity The gradient direction (orientation of edge normal) is \\(\\theta = \\arctan(\\partial_y f / \\partial_x f)\\) The edge strength is given by the magnitude \\(\\|\\Delta f\\| = \\sqrt{\\partial_xf^2 + \\partial_yf^2}\\)</p>"},{"location":"csc420/edge_detection.html#effect-of-noise","title":"Effect of Noise","text":"<p>Consider a noisy image \\(f\\), where there are high frequency changes locally</p> <p>\u200b </p> <p>Therefore, we need to first smooth the image with some filter \\(h\\) and then looks for the peak in \\(\\partial_x (h*f)\\)</p> <p>Differentiation property of convolution </p> \\[\\partial_x (h*f) = \\partial_x h * f = h * \\partial_x f\\] <p>(proof from Fourier Series property of convolution, omitted for complexity) Useful because we can save the operation only on our filter, which is often much smaller than the image  </p>"},{"location":"csc420/edge_detection.html#canny-edge-detection","title":"Canny Edge Detection","text":"<p>One of the best known classical edge detection algorithm</p> <ul> <li> <p>Filter image with derivative of Gaussian for horizontal and vertical direction.    </p> </li> <li> <p>Find magnitude and orientation of gradient</p> </li> <li> <p>Non-maximum suppression</p> <ul> <li>purpose: to find \"the largest\" edge, so that the blurred, or small edges are removed. Overall, the edges will be thinner. </li> <li>Implementation: For each pixel \\((x,y)\\), along the direction of the gradient \\(\\theta\\), check whether the pixel has the largest magnitude among the neighboring pixels on the positive and negative directions. If yes, then keep it, otherwise, suppress it. </li> </ul> </li> <li> <p>Hysteresis thresholding</p> <ul> <li>Purpose: since high threshold will wipe out too many edges while small threshold preserves too many unwanted small edges. Hysteresis thresholding will be able to connect stronger edges with thinner edges so that the edges are connected and show the true feature. </li> <li>Implementation: used two thresholds, high threshold and low threshold, to get two sets of resulted edges. Then, the final result will be the strong edges, plus all thin edges (from low threshold) that are connected between two strong edges (from high threshold).</li> </ul> </li> </ul> <p>\u200b </p>"},{"location":"csc420/edge_detection.html#laplacian-of-gaussians","title":"Laplacian of Gaussians","text":"<p>Using \\(\\partial^2_x h * f\\) as the filter, detecting edge by zero-crossing of bottom graph</p> <p>\u200b </p> Source code <pre><code>import cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nimage_sq = cv2.imread('../assets/blue-square.png', cv2.IMREAD_GRAYSCALE)\nimage_yu = cv2.imread('../assets/yurina.jpg', cv2.IMREAD_GRAYSCALE)\nH, W = image_yu.shape\nimage_sq = cv2.resize(image_sq, (W, H))\nF_dx = np.array([[-1, 1]])\nF_dy = F_dx.T\n\nimage = np.vstack((image_sq, image_yu))\noutput = np.hstack([\n    image, \n    cv2.filter2D(image, -1, F_dx, delta=122),\n    cv2.filter2D(image, -1, F_dy, delta=122)\n])\noutput = cv2.putText(output, \"original\", (0, 20),cv2.FONT_HERSHEY_SIMPLEX, .5, (0,0,0), 1)\noutput = cv2.putText(output, \"[-1, 1]\", (W, 20), cv2.FONT_HERSHEY_SIMPLEX, .5, (0,0,0), 1)\noutput = cv2.putText(output, \"[-1, 1].T\", (W*2, 20), cv2.FONT_HERSHEY_SIMPLEX, .5, (0,0,0), 1)\ncv2.imwrite('../assets/edge_detection_1.jpg', output)\n\nsobel_x = np.array([\n    [-1, 0, 1],\n    [-2, 0, 2],\n    [-1, 0, 1]\n])\nsobel_y = sobel_x.T\noutput = np.hstack([\n    image_yu, \n    cv2.filter2D(image_yu, -1, sobel_x, delta=122),\n    cv2.filter2D(image_yu, -1, sobel_y, delta=122)\n])\noutput = cv2.putText(output, \"original\", (0, 20),cv2.FONT_HERSHEY_SIMPLEX, .5, (255,255,255), 1)\noutput = cv2.putText(output, \"Sobel x, size=3\", (W, 20), cv2.FONT_HERSHEY_SIMPLEX, .5, (255,255,255), 1)\noutput = cv2.putText(output, \"Sobel y, size=3\", (W*2, 20), cv2.FONT_HERSHEY_SIMPLEX, .5, (255,255,255), 1)\ncv2.imwrite('../assets/edge_detection_sobel.jpg', output)\n\n\nrand = 60 + np.random.rand(500) * 5\nrand = np.append(rand, np.arange(65, 35, -3))\nrand = np.append(rand, 30 + np.random.rand(500) * 5)\nksize, sigma = 11, 5\nkernel = cv2.getGaussianKernel(ksize, sigma)\nsmoothed = np.array([np.dot(kernel.T, rand[i:i+ksize]) for i in range(len(rand) - ksize)])\nfig, axs = plt.subplots(2, 2, figsize=(6, 5))\naxs[0][0].plot(rand)\naxs[0][0].set_title(\"f(x)\")\naxs[0][1].plot(rand[1:] - rand[:-1])\naxs[0][1].set_title(\"d/dx f(x)\")\naxs[1][0].plot(smoothed)\naxs[1][0].set_title(\"smoothed with Gaussian h\")\naxs[1][1].plot(smoothed[1:] - smoothed[:-1])\naxs[1][1].set_title(\"d/dx h * f\");\nfig.tight_layout()\nfig.savefig(\"../assets/edge_detection_noise.jpg\")\n\ncannys = []\nfig, axs = plt.subplots(3, 3, figsize=(12, 12))\nfor i in range(3):\n    for j in range(3):\n        low = 10 + i * 30\n        high = 100 + j * 50\n        img = np.ones((H + 22, W)) * 255\n        img[22:, :] = cv2.Canny(image_yu, low, high)\n        cv2.putText(img, f\"low={low}, high={high}\", \n                    (0, 20),cv2.FONT_HERSHEY_SIMPLEX, .5, (0,0,0), 1)\n        cannys.append(img)\ncannys = np.vstack([\n    np.hstack(cannys[:3]),\n    np.hstack(cannys[3:6]),\n    np.hstack(cannys[6:])\n])\ncv2.imwrite(\"../assets/edge_detection_canny.jpg\", cannys)\n\n\nrand = 60 + np.random.rand(500)\nrand = np.append(rand, np.arange(60, 0, -5))\nrand = np.append(rand, np.random.rand(500))\nksize, sigma = 51, 5\nkernel = cv2.getGaussianKernel(ksize, sigma)\n\nsmoothed = np.array([np.dot(kernel.T, rand[i:i+ksize]) for i in range(len(rand) - ksize)])\nd_smoothed = smoothed[1:] - smoothed[:-1]\nfig, axs = plt.subplots(1, 2, figsize=(7, 3))\naxs[0].plot(rand)\naxs[0].set_title(r\"$f(x)$\")\naxs[1].plot(d_smoothed[1:] - d_smoothed[:-1])\naxs[1].set_title(r\"$\\frac{d^2}{dx^2} h \\cdot f$\")\nfig.tight_layout()\nfig.savefig(\"../assets/edge_detection_log.jpg\")\n</code></pre>"},{"location":"csc420/harris_corner_detector.html","title":"Image Features - Harris Corner Detector","text":"<pre><code>import cv2\nimport numpy as np\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nfrom scipy.signal import convolve2d\n\ndef imshow(img, ax=None, title=\"\", bgr=True):\n    # since plt and cv2 have different RGB sorting\n    if bgr:\n        img = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n    if ax == None:\n        plt.imshow(img.astype(np.uint8))\n        plt.axis(\"off\")\n        plt.title(title)\n    else:\n        ax.imshow(img.astype(np.uint8))\n        ax.set_axis_off()\n        ax.set_title(title)\n\nplt.rcParams[\"figure.figsize\"] = (12,6)\n</code></pre>"},{"location":"csc420/harris_corner_detector.html#image-stitching","title":"Image Stitching","text":"<p>When matching to images, and even to merge into one. Detection identify the interest points Description extract feature vector around each interest point Matching determine correspondence between descriptors in two views</p> <p>The goal is to detect the same points in both images, the number of points must be generated in each image independently, and must have a sufficient number for comparison, while at the same time not too many, which makes the algorithm slow. </p>"},{"location":"csc420/harris_corner_detector.html#what-are-good-characteristics-for-a-interest-point","title":"What are good characteristics for a interest point","text":"<p>Must have somewhat unique texture in the local sense Large contrast changes, i.e. gradients Edge is less useful since line segments can be localized on any place with the same orientation (aperture problem) Therefore, we need at least two different orientations, which can be reliably matched</p>"},{"location":"csc420/harris_corner_detector.html#corner-detection","title":"Corner Detection","text":"<p>\"Corner like\" patch can be reliably matched</p> <p>We should easily recognize the corner \"point\" by looking through a small window, where there should have a large change in intensity in all directions. </p>"},{"location":"csc420/harris_corner_detector.html#weighed-summed-square-difference","title":"Weighed summed square difference","text":"\\[E_{WSSD}(u,v) = \\sum_x \\sum_y w(x,y)[I(x+u, u+v) - I(x,y)]^2\\] <p>where \\(w: \\mathbb N^2 \\rightarrow \\mathbb R\\) is the weighted function, for example indicator \\(\\mathbb I((x,y) \\text{ in window})\\), or 2-D Gaussian. </p> <p>Them, fit \\(I(x+u, y+v)\\) by a first-order Taylor series expansion about \\(x,y\\): </p> \\[I(x+u, y+v)\\approx I(x,y) + u\\cdot \\frac{\\partial I}{\\partial x}(x,y) + v\\cdot \\frac{\\partial l}{\\partial y}(x,y)\\] <p>Then, we can approximate \\(I\\) by a series of polynomials, so that we can plugging in for \\(E_{WSSD}\\)</p> \\[\\begin{align*} E_{WSSD}(u,v) &amp;= \\sum_x \\sum_y w(x,y)[I(x+u, u+v) - I(x,y)]^2\\\\ &amp;\\approx \\sum_x\\sum_y w(x,y)(I(x,y) + u\\cdot I_x + v\\cdot I_y - I(x,y))^2\\\\ &amp;= \\sum_x\\sum_y w(x,y)(u^2I_x^2 + 2u\\cdot v\\cdot I_x\\cdot I_y + v^2I_y^2)\\\\ &amp;= \\sum_x \\sum_y w(x,y) \\cdot \\begin{bmatrix}u&amp;v\\end{bmatrix} \\begin{bmatrix}I_x^2&amp;I_x\\cdot I_y\\\\I_x\\cdot I_y &amp;I_y^2\\end{bmatrix} \\begin{bmatrix}u\\\\v\\end{bmatrix}\\\\ \\text{Let }M &amp;:= \\sum_x\\sum_y w(x,y)\\begin{bmatrix}I_x^2&amp;I_x\\cdot I_y\\\\I_x\\cdot I_y &amp;I_y^2\\end{bmatrix}\\\\ &amp;= \\begin{bmatrix}u&amp;v\\end{bmatrix} M \\begin{bmatrix}u\\\\v\\end{bmatrix} \\end{align*}\\] <p>\u200b </p> <p>The images are \\(I, I_x, I_y, I_{x}I_y, I_{x}I_x, I_{y}I_y\\)</p> <p>Note that \\(M\\) is independent of \\(u,v\\).</p> <p>Consider using an indicator weight function, then \\(M = \\begin{bmatrix}\\sum_{x,y} I_x^2 &amp;\\sum_{x,y} I_x\\cdot I_y\\\\\\sum_{x,y} I_x\\cdot I_y &amp;\\sum_{x,y} I_y^2\\end{bmatrix}\\) defines the shape. Generally, \\(E_{WSSD}(u,v)\\) will be a ellipse</p> <p>Note that \\(M\\) is symmetric \\(2\\times 2\\) matrix and is diagonalizable, i.e. \\(M = V diag(\\lambda_1, \\lambda_2), V^{-1}\\), and \\(\\lambda_{\\min}^{-1/2}, \\lambda_{max}^{-1/2} \\propto\\) length of the radii of the ellipse and indicate the direction of of slowest change and fastest change, respectively. </p> <p>Then, a perpendicular corner will have large lambdas and \\(\\lambda_1 \\approx \\lambda_2\\), for edges, \\(\\lambda_a &gt;&gt; \\lambda_b\\), for \"flat\" region, lambdas are small.</p>"},{"location":"csc420/harris_corner_detector.html#harris-corner-detector","title":"Harris Corner Detector","text":"<p>Given such characteristics of lambda, we can propose a rotationally invariant criteria </p> \\[R := \\lambda_1\\lambda_2 - \\alpha(\\lambda_1+\\lambda_2)^2 = det(M) - \\alpha \\cdot trace(M)^2\\] <p>Where \\(\\alpha \\in (0.04, 0.06)\\) is a constant coefficient</p> <p>\\(R&lt;0\\) then edge \\(R &gt; 0\\) then corner \\(|R| &lt; \\epsilon\\) for some small threshold then flat region</p> <p>\u200b </p>"},{"location":"csc420/harris_corner_detector.html#general-procedure","title":"General Procedure","text":"<ul> <li>compute vertical and horizontal gradients \\(I_x,I_y\\)</li> <li>compute \\(I_x^2, I_y^2, I_x\\cdot I_y\\) </li> <li>compute Gaussian weighted \\(M\\) for each pixel</li> <li>compute \\(R = det(M) - \\alpha trace(M)^2\\) for each image window, as corner score</li> <li>Find points above threshold \\(R &gt; S\\) </li> <li>Non-maximum suppression to keep only the local maxima</li> </ul>"},{"location":"csc420/harris_corner_detector.html#properties","title":"Properties","text":"<ul> <li>Rotation and Shift invariant (for Harris corner detector)</li> <li>NOT scale invariant (if an image is enlarged, then we cannot find the corner with the same window size)</li> </ul> Source code <pre><code>import cv2\nimport numpy as np\nimport plotly.express as px\n\nimg = cv2.imread(\"../assets/Corners.jpg\", cv2.IMREAD_GRAYSCALE).astype(float)\nimg_blurred = cv2.GaussianBlur(img, (3, 3), 1)\n\ndef to_image(data):\n    return (255 * (data - data.min()) / (data.max() - data.min())).astype(np.uint8)\n\nF_dx = np.array([[-1, 1]])\nF_dy = F_dx.T\n\nimg_dx = cv2.filter2D(img_blurred, -1, F_dx, delta=122)\nimg_dy = cv2.filter2D(img_blurred, -1, F_dy, delta=122)\n\n\noutput = np.vstack([\n    np.hstack([to_image(img_blurred), to_image(img_dx), to_image(img_dy)]),\n    np.hstack([to_image(img_dx * img_dy), to_image(img_dx * img_dx), to_image(img_dy * img_dy)]),\n])\ncv2.imwrite(\"../assets/harris_corner_1.jpg\", output)\n\npatch = img[300: 350, 50: 100]\nI_x = cv2.filter2D(patch, -1, np.array([[-1, 1]]))\nI_y = cv2.filter2D(patch, -1, np.array([[-1], [1]]))\n\nM = np.array([[np.sum(I_x ** 2), np.sum(I_x * I_y)], \n              [np.sum(I_x * I_y), np.sum(I_y ** 2)]])\nus, vs = np.meshgrid(np.linspace(-2, 2, 20), np.linspace(-2, 2, 20))\nvec = np.empty((20, 20, 1, 2))\nvec[:, :, 0, 0] = us\nvec[:, :, 0, 1] = vs\nvec_t = np.transpose(vec, (0,1,3,2))\n\nfig = px.imshow((vec @ M @ vec_t)[:, :, 0, 0])\nfig.update_xaxes(showticklabels=False)\nfig.update_yaxes(showticklabels=False)\nwith open(\"../assets/harris_corner_wssd.json\", \"w\") as f:\n    f.write(fig.to_json())\n\npatches_starts = [(300, 50), (30, 10), (130, 330)]\npatch_size = 40\noutput = img.copy()\nfor y, x in patches_starts:\n    patch = img[y:y+patch_size, x:x+patch_size]\n    output = cv2.rectangle(\n        output, \n        (x, y), (x+patch_size, y+patch_size), \n        (127, 127, 127), 2\n    )\n    I_x = cv2.filter2D(patch, -1, np.array([[-1, 1]]))\n    I_y = cv2.filter2D(patch, -1, np.array([[-1], [1]]))\n\n    M = np.array([[np.sum(I_x ** 2), np.sum(I_x * I_y)], \n                [np.sum(I_x * I_y), np.sum(I_y ** 2)]])\n\n    eig = np.round(np.linalg.eigvals(M), 2)\n    R = eig[0] * eig[1] - 0.04*(eig[0] + eig[1])**2\n    output = cv2.putText(output, f\"lmbd=({eig[0]},{eig[1]})\", \n                (x, y-2),cv2.FONT_HERSHEY_SIMPLEX, .5, (0,0,0), 1)\n    output = cv2.putText(output, f\"R={R}\", \n                (x, y-12),cv2.FONT_HERSHEY_SIMPLEX, .5, (0,0,0), 1)\ncv2.imwrite(\"../assets/harris_corner.jpg\", output)\n</code></pre>"},{"location":"csc420/image_pyramids.html","title":"Image Pyramids","text":""},{"location":"csc420/image_pyramids.html#image-downsampling","title":"Image Downsampling","text":""},{"location":"csc420/image_pyramids.html#initial-thought","title":"Initial thought","text":"<p>To resize of image to \\(\\frac{1}{n}\\), resample the image by picking pixel per \\(n\\) pixels</p> <p>\u200b </p>"},{"location":"csc420/image_pyramids.html#problem-aliasing","title":"Problem: Aliasing","text":"<p>Occurs when sampling rate is not high enough to capture the amount of detail in the image. </p> <p>Below is an extremum, which we lost half of the important information</p> <p>\u200b </p>"},{"location":"csc420/image_pyramids.html#nyquist-rate","title":"Nyquist rate","text":"<p>To avoid such aliasing issue, one should sample at least \\(2\\times \\max{f}\\), i.e. at least twice the highest frequency. </p> <p>With twice the frequency, we can make sure the information is caught, while in other cases, it might not be true</p>"},{"location":"csc420/image_pyramids.html#gaussian-pre-filtering","title":"Gaussian pre-filtering","text":"<p>When downsampling, the original image has too high frequencies, results in information loss. The high frequencies are caused by shape edges, hence we can smooth the image to filter out high frequencies, with a low-pass filter</p> <p>\u200b </p>"},{"location":"csc420/image_pyramids.html#gaussian-pyramids","title":"Gaussian Pyramids","text":"<p>A sequence of images created with Gaussian blurring and downsampling is called a Gaussian pyramid or mip map</p>"},{"location":"csc420/image_pyramids.html#image-up-sampling","title":"Image Up-Sampling","text":""},{"location":"csc420/image_pyramids.html#interpolation","title":"Interpolation","text":"<p>Since an image is a discrete point-sampling of a continuous function, i.e. \\(F(x,y) = quantize\\{f(x/d, y/d)\\}\\). If we could somehow reconstruct the original continuous function, we can generate the image with any resolution and scale. </p> <p>However, we are unable to obtain such \\(f\\), but we can guess an approximation</p>"},{"location":"csc420/image_pyramids.html#nearest-neighbor","title":"Nearest neighbor","text":"\\[f(x) = F(\\arg\\min\\{d(x - x_i)\\})\\]"},{"location":"csc420/image_pyramids.html#linear-interpolation","title":"Linear interpolation","text":"<p>Given \\(x \\in [x_i, x_{i+1}], G(x) = \\frac{x_{i+1}-x}{x_{i+1}-x_i}F(x_i) + \\frac{x-x_{i+1}}{x_{i+1}-x_i}F(x_{i+1})\\)</p>"},{"location":"csc420/image_pyramids.html#via-convolution-1d","title":"Via Convolution 1D","text":"<p>To upsampling a line \\(F= p_1, p_2, ..., p_n\\), insert \\(n*0\\)s between \\(p_i, p_{i+1}\\), make into \\(G = p_i. 0, 0, ..., 0, p_{i+1}\\) Take convolution filter \\(h = [0, 1/d, 2/d, ..., d/d, (d-1)/d, ... 0]\\) \\(h*G\\) is desired </p> <p>Similar idea goes to 2D</p>"},{"location":"csc420/image_pyramids.html#bilinear-interpolation","title":"Bilinear interpolation","text":"<p>Given \\(Q_{00}=(x_0,y_0), Q_{01}=(x_0, y_1), Q_{10}=(x_1,y_0), Q_{11}=(x_1,y_1)\\). Interpolate along \\(x\\)-axis</p> \\[F(x,y_0)\\approx \\frac{x_1 - x}{x_1 - x_0}F(Q_{00}) +\\frac{x - x_0}{x_1 - x_0}F(Q_{10})\\] \\[F(x,y_1)\\approx \\frac{x_1 - x}{x_1 - x_0}F(Q_{01}) +\\frac{x - x_0}{x_1 - x_0}F(Q_{11})\\] <p>Then, interpolate along \\(y\\)-axis</p> \\[F(x,y) \\approx \\frac{y_1 - x}{y_1 - y_0}F(x,y_0) +\\frac{y - y_0}{y_1 - y_0}F(x,y_1)\\] \\[\\begin{align*}  F(x,y)= &amp;\\frac{(x_1-x)(y_1-y)}{(x_1-x_0)(y_1-y_0)}F(Q_{00})\\\\ + &amp;\\frac{(x-x_0)(y_1-y)}{(x_1-x_0)(y_1-y_0)}F(Q_{10})\\\\ + &amp;\\frac{(x_1-x)(y-y_0)}{(x_1-x_0)(y_1-y_0)}F(Q_{01})\\\\ +&amp;\\frac{(x-x_0)(y-y_0)}{(x_1-x_0)(y_1-y_0)}F(Q_{11}) \\end{align*}\\] <p>Therefore, suppose \\(\\|x_1-x_0\\| = \\|y_1-y_0\\| = 1\\), i.e. the 4 points form a unit square, and let \\(x-x_0 = d_x, y-y_0 = d_y\\). </p> \\[F(x,y)=(1-d_x)(1-d_y)F(Q_{00})+d_x(1-d_y)F(Q_{10}) + (1-d_x)d_yF(Q_{01}) + d_xd_yF(Q_{11})\\] <p>Which can be represented by the dot product of \\(\\vec v^T \\cdot \\vec v\\) where \\(v\\) is the vector filter in 1-D case. </p> Source code <pre><code>import cv2\nimport numpy as np\nimport plotly.graph_objects as go\n\nimg = cv2.imread(\"../assets/yurina.jpg\")\n\nH, W, C = img.shape\noutput = np.zeros((H, W * 3, C), dtype=np.uint8)\noutput[:, :W, :] = img\noutput[:, W:2*W, :] = cv2.resize(img[::2,::2,:], (W, H), interpolation=cv2.INTER_NEAREST)\noutput[:, 2*W:, :] = cv2.resize(img[::4,::4,:], (W, H), interpolation=cv2.INTER_NEAREST)\ncv2.imwrite(\"../assets/image_downsample.jpg\", output)\n\n\nimg2 = cv2.imread(\"../assets/window.jpg\")\nimg2 = cv2.resize(img2, (W, H))\noutput = np.empty((H, W*2, 3), dtype=np.uint8)\nfor i in range(img.shape[1]):\n    output[:, 2*i] = img2[:, i]\n    output[:, 2*i+1] = img[:, i]\ncv2.imwrite(\"../assets/image_downsample_2.jpg\", output)\n\n\n\nx = np.arange(0, 40, 0.2)\nx2 = np.arange(0, 40, 3)\nfig = go.Figure(data=[\n    go.Scatter(x=x, y=np.cos(x), name=\"sample rate=0.2\"),\n    go.Scatter(x=x2,y=np.cos(x2), name=\"sample rate=3\")\n])\nwith open(\"../assets/nyquist_cos.json\", \"w\") as f:\n    f.write(fig.to_json())\n\nimg3 = cv2.GaussianBlur(output, (3, 3), 3)[:, ::2]\ncv2.imwrite(\"../assets/image_downsample_3.jpg\", img3)\n</code></pre>"},{"location":"csc420/indexing.html","title":"Features Fast Retrival","text":"<p>For a image matching task, note that in the previous courses we extract local features (i.e. SIFT) on some interest points and performs a least-distance matching. However, consider a database of \\(N\\) images, performing such matching takes at least \\(O(N)\\) times. </p> <p><pre><code>def make_dataset(N=100, load_dir=None, save_dir=None):\n    \"\"\" N: the number of images for the dataset\n        return a database of N images and their keypoints and descriptors \n    \"\"\"\n    images = []\n    keypoints = []\n    descriptors = []\n\n    if load_dir:\n        for path in os.listdir(load_dir):\n            image = cv2.imread(os.path.join(load_dir, path))\n            kps, des = sift.detectAndCompute(image, None)\n            images.append(image)\n            keypoints.append(kps)\n            descriptors.append(des)\n        return images, keypoints, descriptors\n\n    # https://picsum.photos/ will return random images \n    url = \"https://picsum.photos/200/\"\n    for i in tqdm(range(N)):\n        kps = None\n        while not kps:\n            image = np.asarray(bytearray(requests.get(url).content), dtype=\"uint8\")\n            image = cv2.imdecode(image, cv2.IMREAD_COLOR)\n            kps, des = sift.detectAndCompute(image, None)\n        if save_dir:\n            cv2.imwrite(os.path.join(save_dir, f\"{i}.png\"), image)\n        images.append(image)\n        keypoints.append(kps)\n        descriptors.append(des)\n    return images, keypoints, descriptors\n</code></pre> The first 20 examples of the dataset includes</p> <p>\u200b </p> <p>Consider a text documents, an efficient way of find all pages is to use an index, we can build a similar index on the images. </p> <p>For a database of images \\(\\mathcal D = \\{I_1,..., I_N\\}\\), each image \\(I_i\\) has a set of \\(m_i\\) interest points and their corresponding \\(D\\)-dim descriptors \\((\\mathcal P_i \\subseteq \\mathbb R^{2\\times m_i}, \\mathcal F_i \\subseteq \\mathbb R^{D\\times m})\\). In SIFT case, \\(D = 128\\). </p>"},{"location":"csc420/indexing.html#visual-words","title":"Visual Words","text":"<p>One natural way is to use the descriptors as our visual word, while we have \\(N\\times m\\) (assuming each image has \\(m\\) descriptors) different vectors. In order to construct a dictionary of visual words, we perform some kinds of clustering algorithm on all the descriptors. One easy algorithm will be k-means (see CSC311 notes for more details). The \\(k\\) cluster centers will then be the dictionary and we assign each descriptor \\(f\\) to one of the cluster centers \\(\\{W_1, ..., W_k\\}\\) by \\(\\arg\\min_i\\|f- W_i\\|\\).</p> <pre><code>def construct_visual_dictionary(descriptors, k):\n    \"\"\" descriptors: a list of m_i * D descriptors\n        k:           int, size of the dictionary\n        returns a a list where ith element is the words of the ith image\n        and the kmeans object\n    \"\"\"\n    descriptors_stack = np.vstack(descriptors)\n    kmeans  = KMeans(n_clusters=k).fit(descriptors_stack)\n    labels = kmeans.labels_\n    image_labels = []\n    row = 0\n    for i, image_descriptors in enumerate(descriptors):\n        image_labels.append(labels[row: row + image_descriptors.shape[0]])\n        row += image_descriptors.shape[0]\n    return image_labels, kmeans\n</code></pre>"},{"location":"csc420/indexing.html#inverted-file-index","title":"Inverted File Index","text":"<p>Now, each image is assigned with a number of words. As a book index do, for each word \\(W_i\\), we construct an inverted index \\(\\{I\\in\\mathcal D\\mid W_i\\in I\\}\\)</p> <pre><code>def inverted_file_index(image_labels, k):\n    \"\"\" \n        returns a list of set where ith element is the\n        set of documents that has the word i\n    \"\"\"\n    dictionary = []\n    for _ in range(k):\n        dictionary.append(set())\n    for image, labels in enumerate(image_labels):\n        for word in labels:\n            dictionary[word].add(image)\n    return dictionary\ndictionary = inverted_file_index(img_words, k)\n</code></pre>"},{"location":"csc420/indexing.html#bag-of-words","title":"Bag of Words","text":"<p>Now we are retrieve the query image by checking all images in the database that have at least one visual word in common. However, this can still be too large. We need some measurement of similarity between query image and retrieved images. Then just match query to top \\(K\\) most similar images. </p> <p>Then, to define \"similarity\", we consider the distribution of word occurrences within the images. However, if one word appears in many images and has a big count in each image, the similarity will be dominated by it. Therefore, for each image, we make an image description vector with term frequency-inverse document frequency</p> \\[\\mathbf t = [t_1, t_2,..., t_k], t_i = \\frac{n_{id}}{n_d}\\log(\\frac{N}{n_i})\\] <p>where \\(n_{id}\\) is the number of occurrences of word \\(i\\) in image \\(d\\), \\(n_d\\) is the number of words in image \\(d\\) and \\(n_i\\) is the number of occurrence of word \\(i\\) in database, \\(N\\) is the total number of images in the whole database</p> <pre><code>def compute_tfidf(img_words, k):\n    \"\"\" returns a 2D image description vertor where \n        ith row is a tfidf vector t_i\n    \"\"\"\n    N = len(img_words)\n    t = np.zeros((N, k))\n    for i, words in enumerate(img_words):\n        for d in words:\n            t[i, d] += 1\n    nd = np.sum(t, axis=1)[:, None]\n    ni = np.sum(t, axis=0)[None, :]\n    return t / nd * np.log(N / ni)\n</code></pre> <p>Then, the similarity between two images \\(t_i, t_j\\) is measure as </p> \\[sim(t_i, t_j) = \\frac{t_i\\cdot t_j}{\\|t_i\\|\\|t_j\\|}\\] <pre><code>def sim(t, i, K):\n    \"\"\" given the tfidf matrix t, find the similarity of \n        t_i against all other's and return the top K similar \n        ones\n    \"\"\"\n    t_i = t[i][:, None]\n    norms = np.linalg.norm(t, axis=1)\n    sim = (t @ t_i) / ((norms * norms[i])[:, None])\n    return sim.argsort(axis=0)[::-1][1:K+1].reshape(K)\n</code></pre> <p>Then, we can choose top \\(K\\) best ranked images and do spatial verification (RANSAC match transformation matrix). </p> <pre><code>def find_candidates(kp_q, des_q, kmeans, dictionary, K):\n    \"\"\" make a query given the query image, the \n        inverted file index dictionary, \n        the kmeans object, and \n        returns the top K similar image's index\n    \"\"\"\n    k = len(kmeans.cluster_centers_)\n    query_words = kmeans.predict(des_q)\n    word_set = set(query_words)\n    candidates = set()\n    for word in word_set:\n        candidates |= dictionary[word]\n    candidates = list(candidates)\n    cand_img_words = [query_words]\n    for cand_idx in candidates:\n        cand_img_words.append(img_words[cand_idx])\n    tfitf = compute_tfidf(cand_img_words, k)\n    similarity = sim(tfitf, 0, K)\n    top_K = []\n    for i in similarity:\n        top_K.append(candidates[i-1])\n    return top_K\n</code></pre> <p>To verify the result we make some transformation on an arbitray image in the database and see if we can find it back</p> <p>\u200b </p>"},{"location":"csc420/indexing.html#vocabulary-tree","title":"Vocabulary Tree","text":"<p>To further improve the speed, we can use a hierarchical clustering for the words. For example, we can do a kMeans clustering on all the vectors first, then for each cluster, we run kMeans on the vectors that belongs to the cluster, and we can recursively do so. When querying, we do the same thing, i.e. predict the clustering, then go to the next level kmeans.</p> Source code <pre><code>import cv2\nimport numpy as np\nfrom sklearn.cluster import KMeans\nimport requests\nfrom tqdm import tqdm\n\nimport os\n\nsift = cv2.SIFT_create()\n\n# --8&lt;-- [start:dataset]\ndef make_dataset(N=100, load_dir=None, save_dir=None):\n    \"\"\" N: the number of images for the dataset\n        return a database of N images and their keypoints and descriptors \n    \"\"\"\n    images = []\n    keypoints = []\n    descriptors = []\n\n    if load_dir:\n        for path in os.listdir(load_dir):\n            image = cv2.imread(os.path.join(load_dir, path))\n            kps, des = sift.detectAndCompute(image, None)\n            images.append(image)\n            keypoints.append(kps)\n            descriptors.append(des)\n        return images, keypoints, descriptors\n\n    # https://picsum.photos/ will return random images \n    url = \"https://picsum.photos/200/\"\n    for i in tqdm(range(N)):\n        kps = None\n        while not kps:\n            image = np.asarray(bytearray(requests.get(url).content), dtype=\"uint8\")\n            image = cv2.imdecode(image, cv2.IMREAD_COLOR)\n            kps, des = sift.detectAndCompute(image, None)\n        if save_dir:\n            cv2.imwrite(os.path.join(save_dir, f\"{i}.png\"), image)\n        images.append(image)\n        keypoints.append(kps)\n        descriptors.append(des)\n    return images, keypoints, descriptors\n# --8&lt;-- [end:dataset]\nimages, kps, descriptors = make_dataset(50);\n\n\nfirst_20_images = np.vstack([\n    np.hstack(images[0:5]), \n    np.hstack(images[5:10]), \n    np.hstack(images[10:15]),\n    np.hstack(images[15:20])\n])\nfirst_20_images = cv2.resize(first_20_images, (500, 400))\ncv2.imwrite(\"../assets/indexing_example.jpg\", first_20_images)\n\n\nk = 100\n\n# --8&lt;-- [start:dict]\ndef construct_visual_dictionary(descriptors, k):\n    \"\"\" descriptors: a list of m_i * D descriptors\n        k:           int, size of the dictionary\n        returns a a list where ith element is the words of the ith image\n        and the kmeans object\n    \"\"\"\n    descriptors_stack = np.vstack(descriptors)\n    kmeans  = KMeans(n_clusters=k).fit(descriptors_stack)\n    labels = kmeans.labels_\n    image_labels = []\n    row = 0\n    for i, image_descriptors in enumerate(descriptors):\n        image_labels.append(labels[row: row + image_descriptors.shape[0]])\n        row += image_descriptors.shape[0]\n    return image_labels, kmeans\n# --8&lt;-- [end:dict]\nimg_words, kmeans = construct_visual_dictionary(descriptors, k)\n\n# --8&lt;-- [start:invert]\ndef inverted_file_index(image_labels, k):\n    \"\"\" \n        returns a list of set where ith element is the\n        set of documents that has the word i\n    \"\"\"\n    dictionary = []\n    for _ in range(k):\n        dictionary.append(set())\n    for image, labels in enumerate(image_labels):\n        for word in labels:\n            dictionary[word].add(image)\n    return dictionary\ndictionary = inverted_file_index(img_words, k)\n# --8&lt;-- [end:invert]\n\n# --8&lt;-- [start:tfidf]\ndef compute_tfidf(img_words, k):\n    \"\"\" returns a 2D image description vertor where \n        ith row is a tfidf vector t_i\n    \"\"\"\n    N = len(img_words)\n    t = np.zeros((N, k))\n    for i, words in enumerate(img_words):\n        for d in words:\n            t[i, d] += 1\n    nd = np.sum(t, axis=1)[:, None]\n    ni = np.sum(t, axis=0)[None, :]\n    return t / nd * np.log(N / ni)\n# --8&lt;-- [end:tfidf]\nt = compute_tfidf(img_words, k)\n\n# --8&lt;-- [start:sim]\ndef sim(t, i, K):\n    \"\"\" given the tfidf matrix t, find the similarity of \n        t_i against all other's and return the top K similar \n        ones\n    \"\"\"\n    t_i = t[i][:, None]\n    norms = np.linalg.norm(t, axis=1)\n    sim = (t @ t_i) / ((norms * norms[i])[:, None])\n    return sim.argsort(axis=0)[::-1][1:K+1].reshape(K)\n# --8&lt;-- [end:sim]\n\n# --8&lt;-- [start:main]\ndef find_candidates(kp_q, des_q, kmeans, dictionary, K):\n    \"\"\" make a query given the query image, the \n        inverted file index dictionary, \n        the kmeans object, and \n        returns the top K similar image's index\n    \"\"\"\n    k = len(kmeans.cluster_centers_)\n    query_words = kmeans.predict(des_q)\n    word_set = set(query_words)\n    candidates = set()\n    for word in word_set:\n        candidates |= dictionary[word]\n    candidates = list(candidates)\n    cand_img_words = [query_words]\n    for cand_idx in candidates:\n        cand_img_words.append(img_words[cand_idx])\n    tfitf = compute_tfidf(cand_img_words, k)\n    similarity = sim(tfitf, 0, K)\n    top_K = []\n    for i in similarity:\n        top_K.append(candidates[i-1])\n    return top_K\n# --8&lt;-- [end:main]\n\ndef test_match(img1, img2, kp1, des1, kp2, des2):\n    \"\"\" Feature Matching + Homography according to \n        https://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/\n        py_feature2d/py_feature_homography/py_feature_homography.html\n        Input the keypoints and descriptors of the query image and the \n        test image, return the matching demo image\n    \"\"\"\n    FLANN_INDEX_KDTREE = 0\n    index_params = dict(algorithm=FLANN_INDEX_KDTREE, trees=5)\n    search_params = dict(checks = 50)\n    flann = cv2.FlannBasedMatcher(index_params, search_params)\n    matches = flann.knnMatch(des1,des2,k=2)\n\n    # store all the good matches as per Lowe's ratio test.\n    good = []\n    for m,n in matches:\n        if m.distance &lt; 0.7*n.distance:\n            good.append(m)\n\n    return cv2.drawMatches(img1,kp1,img2,kp2,good,None)\n\n\n# we make some transformation on an arbitray image in the database\n# and see if we can find it back\nrand_idx = np.random.randint(0, len(images))\nquery_image = images[rand_idx]\npoints2 = np.float32([[10,20], [199,10], [10,179], [180,150]])\npoints1 = np.float32([[0,0], [199,0], [10,199], [199,199]])\nM = cv2.getPerspectiveTransform(points1, points2)\nquery_image = cv2.warpPerspective(query_image,M,(200, 200))\n\n\n\nkp_q, des_q = sift.detectAndCompute(query_image, None)\ndb_candidates = find_candidates(kp_q, des_q, kmeans, dictionary, 6)\ndemo = None\nfor i in range(0, len(db_candidates), 2):\n    cand = db_candidates[i]\n    matching_col_1 = test_match(\n        query_image, images[cand],\n        kp_q, des_q, \n        kps[cand], descriptors[cand]\n    )\n    try:\n        cand = db_candidates[i + 1]\n        matching_col_2 = test_match(query_image, images[cand], \n                                    kp_q, des_q, \n                                    kps[cand], descriptors[cand])\n    except IndexError:\n        mathching_col_2 = np.zeros(query_image.shape)\n    if demo is None:\n        demo = np.hstack((matching_col_1, matching_col_2))\n    else:\n        demo = np.vstack((demo, np.hstack((matching_col_1, matching_col_2))))\ncv2.imwrite(\"../assets/indexing_demo.jpg\", demo)\n</code></pre>"},{"location":"csc420/linear_filters.html","title":"Linear Filters","text":""},{"location":"csc420/linear_filters.html#images","title":"Images","text":"<p>Denote a image (matrix of integer values) as \\(I\\) For gray-scale images, \\(I_{m\\times n}\\); where \\(I(i,j)\\) is called intensity For color images, \\(I_{m\\times n\\times \\{1,2,3\\}}\\), 3 for RGB values</p> <p>Alternatively, think of a grayscale image as a mapping \\(I:\\mathbb N^2 \\rightarrow \\{0,1,...,255\\}\\), i.e. position \\((i,j)\\rightarrow\\) gray-scale, where 0 is black, 255 is white</p> <p>Example For image \\(I(i,j)\\); \\(I(i,j)+50\\) lighten the image, \\(I(i,-j)\\) rotate the image horizontally</p>"},{"location":"csc420/linear_filters.html#image-filters","title":"Image filters","text":"<p>Modify the pixels in an image based on some function of a local neighborhood of each pixel  </p> <p>Can be used to enhance (denoise), detect patterns (matching), extract information (texture, edges)</p>"},{"location":"csc420/linear_filters.html#boundary-effects","title":"Boundary Effects","text":"<p>Consider the boundary of the image, there are three modes: <code>full, same, valid</code> </p>"},{"location":"csc420/linear_filters.html#correlation","title":"Correlation","text":"<p>Given a image \\(I\\), the filtered image is </p> \\[G(i,j) = \\sum_{u=-k}^k \\sum_{v=-k}^k F(u,v)\\cdot I(i+u, j + v)\\] <p>The entries of the weight kernel of mask \\(F(u,v)\\) are often called the filter coefficients Denote this correlation operator \\(F\\otimes I\\)</p> <p>OpenCV <code>cv2.filter2d</code></p> <p>We can also write correlation in a more compact form using vectors, i.e.</p> <p>Let \\(\\mathbf f = F\\) be the kernel matrix, \\(\\mathbf t_{ij} = T_{i,j}(I_{\\{i-k, i+k\\} \\times \\{j-k, j+k\\}})\\) be the matrix of the image centered at \\((i,j)\\) and of size \\(2k + 1\\), then \\(G(i,j)=\\mathbf f\\cdot \\mathbf t_{ij}\\)</p> <p>And the full image </p> \\[\\mathbf G_{m\\times n} =  \\begin{bmatrix} G(0,0) &amp;G(0, 1)&amp;\\cdots &amp;G(0, n) \\\\ G(1,0) &amp;G(1, 1)&amp;\\cdots &amp;G(1, n) \\\\ \\vdots &amp;\\vdots&amp;\\ddots &amp;\\vdots \\\\ G(m,0) &amp;G(m, 1)&amp;\\cdots &amp;G(m, n) \\\\ \\end{bmatrix}\\]"},{"location":"csc420/linear_filters.html#example-of-filter-kernels","title":"Example of Filter kernels","text":""},{"location":"csc420/linear_filters.html#unchanged","title":"Unchanged","text":"<p>\\(\\begin{bmatrix} 0 &amp;0 &amp;0 \\\\ 0 &amp;1 &amp;0 \\\\ 0 &amp;0 &amp;0 \\end{bmatrix}\\)</p>"},{"location":"csc420/linear_filters.html#shifting-to-the-left-by-1px","title":"shifting to the left by 1px","text":"<p>\\(\\begin{bmatrix} 0 &amp;0 &amp;0 \\\\ 0 &amp;0 &amp;1 \\\\ 0 &amp;0 &amp;0 \\end{bmatrix}\\)</p> <pre><code># shift down by 25px\nk_shifting = np.zeros((50, 50))\nk_shifting[0, 24] = 1\n</code></pre> <p>\u200b </p>"},{"location":"csc420/linear_filters.html#sharpening","title":"Sharpening","text":"\\[M1 = \\begin{bmatrix} 0 &amp;0 &amp;0 \\\\ 0 &amp;2 &amp;0 \\\\ 0 &amp;0 &amp;0 \\end{bmatrix} -  \\frac{1}{9} \\begin{bmatrix} 1 &amp;1 &amp;1 \\\\ 1 &amp;1 &amp;1 \\\\ 1 &amp;1 &amp;1 \\end{bmatrix}\\] <p>To enhance the effect, using a larger center value</p> \\[M2 = \\begin{bmatrix} 0 &amp;0 &amp;0 \\\\ 0 &amp;5 &amp;0 \\\\ 0 &amp;0 &amp;0 \\end{bmatrix} -  \\frac{4}{9} \\begin{bmatrix} 1 &amp;1 &amp;1 \\\\ 1 &amp;1 &amp;1 \\\\ 1 &amp;1 &amp;1 \\end{bmatrix}\\] <p>If we enlarge the matrix to 5</p> \\[M3 = \\vec 0_{5\\times 5} + [5]_{center} -  \\frac{4}{25} J_{5\\times 5}\\] <pre><code>k_sharp_3_2 = - np.ones((3,3), np.float32) / 9\nk_sharp_3_2[1,1] = 2\nk_sharp_3_2 = k_sharp_3_2 \n\nk_sharp_3_5 = - np.ones((3,3), np.float32) *4 / 9\nk_sharp_3_5[1,1] = 5\n\nk_sharp_5_5 = - np.ones((5,5), np.float32) *4 / 25\nk_sharp_5_5[2,2] = 5\n</code></pre> <p>\u200b </p>"},{"location":"csc420/linear_filters.html#smoothing","title":"Smoothing","text":""},{"location":"csc420/linear_filters.html#smoothing-1-moving-averaging-filter","title":"Smoothing 1: Moving Averaging filter","text":"<p>Simplest thing is to replace each pixel by the average of its neighbors, i.e.  \\(F\\) will be a \\((2k+1)^{-2} J_{(2k+1)\\times (2k+1)}\\) matrix </p> <p>Assumption neighboring pixels are similar, and noise is independent of pixels. </p> <p>For example, the following uses \\(\\frac{1}{9}J_{3\\times 3}\\) and \\(\\frac{1}{25}J_{5\\times 5}\\), note that the matrix should add up to one, hence the image is normalized. </p> <pre><code>k_3 = np.ones((3,3),np.float32) / 9\nk_5 = np.ones((5,5),np.float32) / 25\nk_11 = np.ones((11,11),np.float32) / 121\n</code></pre> <p>\u200b </p>"},{"location":"csc420/linear_filters.html#smoothing-2-gaussian-filter","title":"Smoothing 2: Gaussian Filter","text":"<p>When we want nearest neighboring pixels to have the most influence on the output, which can removes high-frequency components from the image (low-pass filter).</p> <p>For example, </p> \\[F = \\frac{1}{16} \\begin{bmatrix} 1 &amp;2 &amp;1 \\\\ 2 &amp;4 &amp;2 \\\\ 1 &amp;2 &amp;1 \\end{bmatrix}\\] <p>The Gaussian kernel is an approximation of a 2d Gaussian function </p> \\[h_{\\sigma}(u,v)=\\frac{1}{2\\pi \\sigma^2}\\exp(-\\frac{u^2 + v^2}{\\sigma^2})\\] <pre><code># simple version of a square Gaussian function\n# this filter is equivalent to cv2.GaussianBlur(img, (size, size), sigma)\ndef Gaussian(size, sigma):\n    coef = 1 / (2 * np.pi * sigma **2)\n    gaus = np.fromfunction(lambda x, y: coef * np.e \\\n                           ** ((-1*((x-(size-1)/2)**2+(y-(size-1)/2)**2))/(2*sigma**2)), (size, size))\n    return gaus / np.sum(gaus)\n</code></pre> <p>\u200b </p> <p>A more general form of the Gaussian kernel is obtained by the multi-variante Gaussian distribution \\(\\mathcal N(\\mu, \\Sigma)\\), where </p> \\[P(\\vec x) = \\frac{1}{(2\\pi)^{d/2} |\\Sigma|^{1/2}} \\exp(-\\frac{(\\vec x - \\mu)^T \\Sigma^{-1}(\\vec x - \\mu)}{2})\\]"},{"location":"csc420/linear_filters.html#properties-of-smoothing","title":"Properties of Smoothing","text":"<ul> <li>All values are positive</li> <li>The kernel sums up to 1 to prevent re-scaling of the image</li> <li>Remove high-frequency components (edges); low-pass filter</li> </ul>"},{"location":"csc420/linear_filters.html#filtering-image-to-find-image-crop-normalized-cross-correlation","title":"Filtering image to find image crop (Normalized cross-correlation)","text":"<p>Let \\(\\mathbf f\\) be a image crop, \\(\\mathbf t\\) be the original image, then </p> \\[G(i,j) = \\frac{\\mathbf f^T \\mathbf t_{ij}}{\\|\\mathbf f^T \\|\\|\\mathbf t_{ij}\\|}\\] <p>is the \"normalized score\" \\((0\\sim 1)\\), where \\(1\\) indicates the matching</p> <p>Consider the following image crop</p> <pre><code>def findMatch(crop, target):\n    target = cv2.cvtColor(target, cv2.COLOR_BGR2GRAY).astype(float)\n    crop = cv2.cvtColor(crop, cv2.COLOR_BGR2GRAY).astype(float)\n    H_crop, W_crop = crop.shape[:2]\n    ave_kernel = np.ones((H_crop, W_crop)) / (H_crop * W_crop)\n    kNorm = (np.sum(crop * crop)) ** 0.5\n    targetNorm = np.sqrt(cv2.filter2D(target * target, -1, ave_kernel) * (H_crop * W_crop))\n    conv = cv2.filter2D(target, -1, crop / kNorm)\n    return conv / targetNorm\n</code></pre> <p>\u200b </p>"},{"location":"csc420/linear_filters.html#convolution","title":"Convolution","text":"<p>A convolution \\(F*I\\) is equivalent to flip \\(F\\) along the diagonal and apply correlation. i.e. </p> \\[G(i,j) = \\sum_{u=-k}^k\\sum_{v={-k}}^k F(u,v)\\cdot I(i-u, j-v)\\] <p>Obviously, for a symmetric filter matrix, convolution and correlation will do the same</p> <p>Convolution is the natural linear feature, and it is  </p> <ul> <li>commutative \\(f*g = g*f\\), </li> <li>associative \\(f*(g*h) = (f*g)*h\\), </li> <li>distributive \\(f*(g+h)=f*g + f*h\\)</li> <li>associative with scalar \\(\\lambda (f*g)=(\\lambda f)*g\\)</li> <li>The Fourier transform of two convolved images is the product of their individual Fourier transforms \\(\\mathcal F(f*g) =\\mathcal F(f)\\cdot \\mathcal F(g)\\)</li> </ul>"},{"location":"csc420/linear_filters.html#separable-filters","title":"Separable Filters","text":"<p>A convolution filter is separable if it can be written as the outer product of two 1D filters. i.e. \\(F = vh^T\\), then \\(F*I = v*(h*I)\\) by associative property. </p> <p>Example </p> \\[ k^{-2} J = k^{-1}[1 \\:1 \\:... \\:1]^T k^{-1}[1 \\:1 \\:... \\:1]\\] \\[ \\frac{1}{16} \\begin{bmatrix} 1 &amp;2 &amp;1 \\\\ 2 &amp;4 &amp;2 \\\\ 1 &amp;2 &amp;1 \\end{bmatrix}  = \\frac{1}{4} \\begin{bmatrix} 1\\\\ 2\\\\ 1 \\end{bmatrix} \\frac{1}{4} \\begin{bmatrix} 1&amp;2&amp;1 \\end{bmatrix}\\] \\[ \\begin{bmatrix} -1 &amp;0 &amp;1 \\\\ -2 &amp;0 &amp;2 \\\\ -1 &amp;0 &amp;1 \\end{bmatrix}  =  \\begin{bmatrix} 1\\\\ 2\\\\ 1 \\end{bmatrix} \\begin{bmatrix} -1&amp;0&amp;1 \\end{bmatrix}\\] <pre><code>kernel = np.array([[-1, 0, 1], \n                   [-2, 0, 2], \n                   [-1, 0, 1]])\nkernel_h = np.array([[-1, 0, 1]])\nkernel_v = np.array([[1, 2, 1]]).T\n# note that images are in uint8\n# and the 1D filters are not normalized\nimg_gray = img_gray.astype(float)\nimg_k = cv2.filter2D(img_gray, -1, kernel)\nimg_k1 = cv2.filter2D(img_gray, -1, kernel_h)\nimg_k2 = cv2.filter2D(img_k1, -1, kernel_v)\n\nprint(np.max(np.abs(img_k - img_k2)))\n#&gt;&gt; 0.0\n</code></pre> <p>\u200b </p>"},{"location":"csc420/linear_filters.html#how-to-tell-is-separable","title":"How to tell is separable","text":"<p>Quickcheck it has rank 1 (otherwise it cannot be written as 2 1D array)</p> <p>Singular value decomposition (SVD) decompose by \\(F = \\mathbf U \\Sigma \\mathbf V^T = \\sum_i \\sigma_i u_i v_i^T\\) with \\(\\Sigma = diag(\\sigma_i)\\) if only one singular value \\(\\sigma_i\\) is non-zero, then it is separable and \\(\\sqrt{\\sigma_1}\\mathbf u, \\sqrt{\\sigma_1} \\mathbf v_1^T\\) are the 1D filters</p> Source code <pre><code>import cv2\nimport numpy as np\nfrom scipy.signal import convolve2d\n\n# load as gray scale\nimg_gray = cv2.imread(\"../assets/yurina.jpg\", cv2.IMREAD_GRAYSCALE)\n# load as colored\nimg = cv2.imread(\"../assets/yurina.jpg\")\n\n# --8&lt;-- [start:shift]\n# shift down by 25px\nk_shifting = np.zeros((50, 50))\nk_shifting[0, 24] = 1\n# --8&lt;-- [end:shift]\nimg_shifting = cv2.filter2D(img, -1, k_shifting, borderType=cv2.BORDER_CONSTANT)\ncv2.imwrite(\"../assets/filter_shift.jpg\", np.hstack((img, img_shifting)))\n\n# --8&lt;-- [start:sharpen]\nk_sharp_3_2 = - np.ones((3,3), np.float32) / 9\nk_sharp_3_2[1,1] = 2\nk_sharp_3_2 = k_sharp_3_2 \n\nk_sharp_3_5 = - np.ones((3,3), np.float32) *4 / 9\nk_sharp_3_5[1,1] = 5\n\nk_sharp_5_5 = - np.ones((5,5), np.float32) *4 / 25\nk_sharp_5_5[2,2] = 5\n# --8&lt;-- [end:sharpen]\n\nimg_sharp_3_2 = cv2.filter2D(img, -1, k_sharp_3_2)\nimg_sharp_3_5 = cv2.filter2D(img, -1, k_sharp_3_5)\nimg_sharp_5_5 = cv2.filter2D(img, -1, k_sharp_5_5)\n\ncv2.imwrite(\"../assets/filter_sharpen.jpg\", np.hstack((img_sharp_3_2, img_sharp_3_5, img_sharp_5_5)))\n\n# --8&lt;-- [start:average]\nk_3 = np.ones((3,3),np.float32) / 9\nk_5 = np.ones((5,5),np.float32) / 25\nk_11 = np.ones((11,11),np.float32) / 121\n# --8&lt;-- [end:average]\n\nimg_3 = cv2.filter2D(img,-1,k_3)\nimg_5 = cv2.filter2D(img,-1,k_5)\nimg_11 = cv2.filter2D(img,-1,k_11)\n\ncv2.imwrite(\"../assets/filter_ave.jpg\", np.hstack((img_3, img_5, img_11)))\n\n# --8&lt;-- [start:gauss]\n# simple version of a square Gaussian function\n# this filter is equivalent to cv2.GaussianBlur(img, (size, size), sigma)\ndef Gaussian(size, sigma):\n    coef = 1 / (2 * np.pi * sigma **2)\n    gaus = np.fromfunction(lambda x, y: coef * np.e \\\n                           ** ((-1*((x-(size-1)/2)**2+(y-(size-1)/2)**2))/(2*sigma**2)), (size, size))\n    return gaus / np.sum(gaus)\n# --8&lt;-- [end:gauss]\n\nsmoothed = []\nfor size in [5, 10, 15]:\n    row = []\n    for sigma in [1, 3, 5]:\n        title = f\"fsize={size}, sigma={sigma}\"\n        temp = cv2.filter2D(img, -1, Gaussian(size, sigma))\n        temp = cv2.putText(temp, title, (10, 10),cv2.FONT_HERSHEY_SIMPLEX, .5, (255,255,255), 1)\n        row.append(temp)\n    smoothed.append(np.hstack(row))\ncv2.imwrite(\"../assets/filter_gaussian.jpg\", np.vstack(smoothed))\n# --8&lt;-- [start:match]\ndef findMatch(crop, target):\n    target = cv2.cvtColor(target, cv2.COLOR_BGR2GRAY).astype(float)\n    crop = cv2.cvtColor(crop, cv2.COLOR_BGR2GRAY).astype(float)\n    H_crop, W_crop = crop.shape[:2]\n    ave_kernel = np.ones((H_crop, W_crop)) / (H_crop * W_crop)\n    kNorm = (np.sum(crop * crop)) ** 0.5\n    targetNorm = np.sqrt(cv2.filter2D(target * target, -1, ave_kernel) * (H_crop * W_crop))\n    conv = cv2.filter2D(target, -1, crop / kNorm)\n    return conv / targetNorm\n# --8&lt;-- [end:match]    \nimg_crop = img.copy()\ncv2.rectangle(img_crop, (15, 35), (65, 85), 255)\n\ncrop = img[35: 85, 15: 65]\nmatched = findMatch(crop, img) * 255\ncv2.rectangle(matched, (15, 35), (65, 85), 255)\ncv2.imwrite(\n    \"../assets/filter_match.jpg\", \n    np.hstack((img_crop, np.tile(matched[:, :, None], 3)))\n)\n\n# --8&lt;-- [start:sep]   \nkernel = np.array([[-1, 0, 1], \n                   [-2, 0, 2], \n                   [-1, 0, 1]])\nkernel_h = np.array([[-1, 0, 1]])\nkernel_v = np.array([[1, 2, 1]]).T\n# note that images are in uint8\n# and the 1D filters are not normalized\nimg_gray = img_gray.astype(float)\nimg_k = cv2.filter2D(img_gray, -1, kernel)\nimg_k1 = cv2.filter2D(img_gray, -1, kernel_h)\nimg_k2 = cv2.filter2D(img_k1, -1, kernel_v)\n\nprint(np.max(np.abs(img_k - img_k2)))\n#&gt;&gt; 0.0\n# --8&lt;-- [end:sep]   \ncv2.putText(img_k, f\"Sobel_x 3*3\", \n                    (20, 200),cv2.FONT_HERSHEY_SIMPLEX, .5, (255,255,255), 1)\ncv2.putText(img_k1, f\"h\", \n                    (20, 200),cv2.FONT_HERSHEY_SIMPLEX, .5, (255,255,255), 1)\ncv2.putText(img_k2, f\"v * h\", \n                    (20, 200),cv2.FONT_HERSHEY_SIMPLEX, .5, (255,255,255), 1)\n\n\ncv2.imwrite(\"../assets/filter_sep.jpg\", np.hstack((img_k, img_k1, img_k2)))\n</code></pre>"},{"location":"csc420/sift.html","title":"Scale Invariant Feature Transform","text":"<pre><code>import cv2\nimport numpy as np\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n\ndef imshow(img, ax=None, title=\"\", bgr=True):\n    # since plt and cv2 have different RGB sorting\n    if bgr:\n        img = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n    if ax == None:\n        plt.imshow(img.astype(np.uint8))\n        plt.axis(\"off\")\n        plt.title(title)\n    else:\n        ax.imshow(img.astype(np.uint8), cmap=\"gray\")\n        ax.set_axis_off()\n        ax.set_title(title)\n\nplt.rcParams[\"figure.figsize\"] = (12,6)\n</code></pre> <p>Distinctive Image Features from Scale-Invariant Keypoints [David G. Lowe, 2004]</p>"},{"location":"csc420/sift.html#log-and-dog","title":"LoG and DoG","text":""},{"location":"csc420/sift.html#laplacian-of-gaussian","title":"Laplacian of Gaussian","text":"<p>Let \\(G\\) be the Gaussian function, \\(G = (2\\pi\\sigma^2)^{-1} \\exp(-\\frac{x^2+y^2}{2\\sigma^2})\\)</p> \\[\\begin{align*} \\frac{\\partial G}{\\partial x} &amp; (2\\pi\\sigma^2)^{-1}  \\exp(-\\frac{x^2+y^2}{2\\sigma^2}) \\frac{-x}{\\sigma^2}\\\\ \\frac{\\partial^2G}{\\partial x^2} &amp;= (2\\pi\\sigma^2)^{-1} \\big[\\exp(-\\frac{x^2+y^2}{2\\sigma^2})\\frac{-x^2}{\\sigma^4} + \\exp(-\\frac{x^2+y^2}{2\\sigma^2})\\frac{-1}{\\sigma^2}\\big] \\\\ &amp;= (2\\pi\\sigma^2)^{-1} \\frac{x^2 - \\sigma^2}{\\sigma^4} \\exp(-\\frac{x^2+y^2}{2\\sigma^2})\\\\ \\nabla^2 G &amp;= \\frac{\\partial^2G}{\\partial x^2} + \\frac{\\partial^2G}{\\partial y^2} \\\\ &amp;= (2\\pi\\sigma^2)^{-1} \\frac{x^2 - 2\\sigma^2 + y^2}{\\sigma^4} \\exp(-\\frac{x^2+y^2}{2\\sigma^2}) \\end{align*}\\]"},{"location":"csc420/sift.html#difference-of-gaussian","title":"Difference of Gaussian","text":"\\[\\begin{align*} \\frac{\\partial G}{\\partial \\sigma} &amp;= \\frac{-1}{\\pi\\sigma^3}\\exp(-\\frac{x^2+y^2}{2\\sigma^2}) + \\frac{1}{2\\pi\\sigma^2}\\exp(-\\frac{x^2+y^2}{2\\sigma^2})\\frac{x^2+y^2}{\\sigma^3} \\\\ &amp;= (2\\pi\\sigma^2)^{-1}\\exp(-\\frac{x^2+y^2}{2\\sigma^2})(\\frac{x^2+y^2}{\\sigma^3}-\\frac{2\\sigma^2}{\\sigma^3})\\\\ &amp;= (2\\pi\\sigma^2)^{-1}\\exp(-\\frac{x^2+y^2}{2\\sigma^2})(\\frac{x^2+y^2}{\\sigma^4}-\\frac{2\\sigma^2}{\\sigma^4})\\sigma\\\\ &amp;= \\sigma\\nabla^2G \\end{align*}\\] <p>By the limit definition of derivative,</p> \\[\\frac{\\partial G}{\\partial \\sigma} = \\lim_{h\\rightarrow 0} \\frac{G(x,y, \\sigma+h) - G(x, y,\\sigma)}{h}\\approx lim_{k\\rightarrow 1} \\frac{G(x,y, k\\sigma) - G(x, y,\\sigma)}{(k-1)\\sigma}\\] <p>So that </p> \\[DoG := G(x,y, k\\sigma) - G(x, y,\\sigma) \\approx (k-1)\\sigma\\sigma \\nabla^2G = (k-1)\\sigma^2 \\nabla^2G\\]"},{"location":"csc420/sift.html#scale-space-extrema-octave","title":"Scale-space extrema (Octave)","text":"<p>To identify locations and scales that can be repeatably assigned under differing views of the same object. </p>"},{"location":"csc420/sift.html#constructing-octave","title":"Constructing octave","text":"<p>Generate \\(m\\) octaves (or image pyramids), each with \\(n\\) different blurring scale (\\(k^1\\sigma, ..., k^n\\sigma\\)). Generally, we choose \\(k = 2^{1/s}, s\\in\\mathbb Z\\), and each octave contains \\(s+3\\) scales, i.e. \\(2^{1/s}, 2^{2/s},...2^{1+3/s}\\). Then, for the next level of the octave, down-sample from the one with \\(2\\sigma\\), then keep do the next. </p> <p>Example If \\(s = 2\\), i.e. \\(k = \\sqrt{2}\\)</p> scale1 scale2 scale3 scale4 scale5 octave1 \\(\\sqrt{2}\\) \\(2\\) \\(2\\sqrt{2}\\) \\(4\\) \\(4\\sqrt{2}\\) octave2 \\(2\\sqrt{2}\\) \\(4\\) \\(4\\sqrt{2}\\) \\(8\\) \\(8\\sqrt{2}\\) octave3 \\(4\\sqrt{2}\\) \\(8\\) \\(8\\sqrt{2}\\) \\(16\\) \\(16\\sqrt{2}\\) octave4 \\(8\\sqrt{2}\\) \\(16\\) \\(16\\sqrt{2}\\) \\(32\\) \\(32\\sqrt{2}\\) <p>Note that \\(G(0, \\sigma_1)* G(0, \\sigma_2) = G(0, \\sqrt{\\sigma_{1}^2 + \\sigma_{2}^2})\\) so that within one octave, we continously convolve the image with scale \\(k\\). Also, downsampling by \\(s\\) will divide the current scale by \\(s\\). </p> <pre><code>def construct_octaves(image, n_octaves: int, s: int, sigma: float):\n    \"\"\"\n    image: the original image\n    n_octaves: number of octaves\n    s: down sampling rate\n    sigma: scale\n    \"\"\"\n    octaves = []\n    for _ in range(n_octaves):\n        octave = [image.copy()]\n        for _ in range(s - 1):\n            image = cv2.GaussianBlur(image, (5, 5), sigma)\n            octave.append(image.copy())\n        image = image[::2,::2]\n        octaves.append(octave)\n\n    return octaves\n</code></pre> <p>\u200b </p>"},{"location":"csc420/sift.html#taking-dog","title":"Taking DoG","text":"<p>Then, for each octave, take DoG by taking the difference between adjacent scales. Note that \\(k\\) is unchanged for each DoG. Then, decide the matching frequency and record the matching. </p> <pre><code>DoGs = [\n    [octave[i] - octave[i-1] for i in range(1, len(octave))]\n    for octave in octaves\n]\n</code></pre> <p>\u200b </p>"},{"location":"csc420/sift.html#find-local-extrema","title":"Find Local Extrema","text":"<p>Find the local extrema by searching each scale in each octave, except the first and last scale, i.e. \\(1,...,s+1\\). \"Localization\" means the 8 pixels around it, 9 pixels above it, and 9 pixels below it. </p> <pre><code>def find_local_extrema(above, self, below):\n    \"\"\" Find the local extrema contained in self\"\"\"\n    key_points = []\n    rows, cols = self.shape\n    for r in range(1, rows-1):\n        for c in range(1, cols-1):\n            window = np.empty((3, 3, 3))\n            window[:, :, 0] = above[r-1: r+2, c-1: c+2]\n            window[:, :, 1] = self[r-1: r+2, c-1: c+2]\n            window[:, :, 2] = below[r-1: r+2, c-1: c+2]\n            if self[r, c] == np.max(window) or self[r, c] == np.min(window):\n                key_points.append((r, c))\n    return key_points\n</code></pre>"},{"location":"csc420/sift.html#key-points-localization-orientation-invariant","title":"Key Points Localization (orientation invariant)","text":"<p>For each chosen local extrema point, apply Harris corner detector with appropriate window size to remove all the edges and flats. </p>"},{"location":"csc420/stereo.html","title":"Homography - Stereo","text":""},{"location":"csc420/stereo.html#homography","title":"Homography","text":"<p>Given the same object and two images taken from different worlds. WTF the transformation from one to another.</p>"},{"location":"csc420/stereo.html#3d-relationship","title":"3D relationship","text":"<p>Let the object be a plane (say a book). Then we can characterize the plane by one point \\(d\\) and two independent vectors \\(a,b\\) on the plane, i.e. we create a coordinate system with an origin and two basis. Therefore, we can describe any point \\(X\\) on the plane by  \\(X = d + \\alpha a + \\beta b\\). Let \\(X_1 = d_1 + \\alpha a_1 + \\beta b_1, X_2 = d_2 + \\alpha a_2 + \\beta b_2\\) where \\(X_1, X_2\\) represents the two planes in 3D of the two world, and represents the same location relative to the same object plane. </p> <p>Using homogeneous coordinates</p> \\[X_1 =  \\underset{A_1}{\\begin{bmatrix} a_{11}&amp;b_{11}&amp;d_{11}\\\\ a_{12}&amp;b_{12}&amp;d_{12}\\\\ a_{13}&amp;b_{13}&amp;d_{13} \\end{bmatrix}} \\begin{bmatrix} \\alpha\\\\\\beta\\\\1  \\end{bmatrix}, X_2 =  \\underset{A_2}{\\begin{bmatrix} a_{21}&amp;b_{21}&amp;d_{21}\\\\ a_{22}&amp;b_{22}&amp;d_{22}\\\\ a_{23}&amp;b_{23}&amp;d_{23} \\end{bmatrix}} \\begin{bmatrix} \\alpha\\\\\\beta\\\\1  \\end{bmatrix}\\] <p>And the transformation \\(T\\) between them gives \\(X_2 = T X_1\\), i.e. \\(A_2\\begin{bmatrix} \\alpha\\\\\\beta\\\\1  \\end{bmatrix} = TA_1\\begin{bmatrix} \\alpha\\\\\\beta\\\\1  \\end{bmatrix}, \\forall \\alpha, \\beta\\). Therefore, \\(T = A_2A_1^{-1}\\)</p>"},{"location":"csc420/stereo.html#2d-relationship","title":"2D relationship","text":"<p>Consider the projective (image) plane. Let \\(K_1,K_2\\) denotes the different intrinsic parameters. Then \\(w_1\\begin{bmatrix}x_1\\\\y_1\\\\1 \\end{bmatrix} = K_1X_1, w_2\\begin{bmatrix}x_2\\\\y_2\\\\1 \\end{bmatrix} = K_2X_2\\), known that \\(X_2 = TX_1\\), so that </p> \\[w_2\\begin{bmatrix}x_2\\\\y_2\\\\1 \\end{bmatrix} = K_2TX_1 = K_2T(K^{-1}_x K_1)X_1= K_2TK_1^{-1}w_1\\begin{bmatrix}x_1\\\\y_1\\\\1 \\end{bmatrix}\\] <p>Note that \\(K_2TK_1^{-1}\\) is a \\(3\\times 3\\) matrix. Therefore, without knowing all the 3D location and even the camera parameters, we can still do the transformation by finding \\(K_2TK_1^{-1}\\) as only one matrix</p>"},{"location":"csc420/stereo.html#rotating-the-camera","title":"Rotating the camera","text":"<p>Consider the panorama mode of the camera. Assuming of the rotation of the camera happens. </p> <p>Let \\(R\\) be the rotation of the camera, then \\(R^T\\) is the rotation of the 3D points. Let \\(X_1,X_2\\) be the same point of the object in the two camera coordinates. Then, let \\(T:= R^T\\), i.e. the transformation is a rotation, then </p> \\[w_2\\begin{bmatrix}x_2\\\\y_2\\\\1 \\end{bmatrix} =w_1 K_2TK_1^{-1}\\begin{bmatrix}x_1\\\\y_1\\\\1 \\end{bmatrix}\\]"},{"location":"csc420/stereo.html#moving-the-camera","title":"Moving the camera","text":"<p>Let the movement be \\(t\\), then \\(X_2 = X_1 -t\\), so that in the homogeneous coordinate of the 2D image</p> \\[w_2\\begin{bmatrix}x_2\\\\y_2\\\\1\\end{bmatrix} = KX_2 = K(X_1-t) = w_1\\begin{bmatrix}x_1\\\\y_1\\\\1\\end{bmatrix} - Kt\\] <p>However, changing \\(w_1\\) will give different points in the second image. </p> <p>On the opposite, if we have \\((x_1,y_1), (x_2,y_2), K, t\\), we can calculate \\(w_1, w_2\\), hence compute the point in 3D. </p> <p>This fact is called stereo. </p>"},{"location":"csc420/stereo.html#stereo","title":"Stereo","text":"<p>A 2D image cannot represent the depth in 3D world, i.e. \\(Z\\)-axis is dropped. However, using stereo cameras we can reconstruct depth. </p>"},{"location":"csc420/stereo.html#parallel-stereo-cameras","title":"Parallel stereo Cameras","text":"<p>Let \\(O_t,O_r\\) be two camera centers, \\(O_r = O_t + [T, 0, 0]^T\\), i.e. We apply a movement \\(t\\) to the left camera along the \\(x\\)-axis to get our right camera. </p> <p>Consider \\(p_l = (x_l, y_l), p_r = (x_r,y_r)\\) be the projected points of \\(Q\\) on left, right image plane, respectively. </p> <p>Since we only move the camera along the x-axis, \\((p_l\\sim p_r) \\parallel (O_l\\sim O_r)\\), hence \\(y:= y_l = y_r\\). </p> <p>Also, noticing that we are only interested in \\(Z\\), i.e. the depth. Consider the connection between \\(p_l, p_r, Z\\), ignoring \\(y\\) since it is unchanged. Therefore, we can project the image from the y-axis, and see the relationship in 2D \\((x,z)\\) coordinate. </p> <p>\u200b </p> <p>Noticing that \\(\\Delta PX_lX_r \\sim \\Delta PP_lO_r\\). so that </p> \\[\\frac{T}{x_r-x_l} = \\frac{Z}{f}\\Rightarrow Z = \\frac{fT}{x_r - x_l}\\] <p>Then, to get \\(x_r\\), we need to match the point in two images. Since we know \\(y\\) of the point is constant, we can do a sliding window scan along the same \\(y\\), then patch around \\(x_r\\) to get the highest similarity. </p>"},{"location":"csc420/stereo.html#general-stereo-cameras","title":"General Stereo Cameras","text":""},{"location":"csc420/stereo.html#basic-setting","title":"Basic Setting","text":"<p>\u200b </p> <p>Epipole \\(e_l, e_r\\) where line \\(O_l, O_r\\) intersects the image planes Projective line \\(O_lp_l, O_rp_r\\) will project to the right, left image plane. Note that the line will not (necessarily) be horizontal. Epipolar plane the triangular 2D plane in 3D space that it formed by \\(O_lO_rP\\), where \\(P\\) is an arbitrary point in the camera's view point. Epipolar line the intersection between the epipolar plane and the image plane. However, all epipolar line will intersection at \\(e_l, e_r\\)</p> <p>Given \\(p_l\\) in the left image plane, we are able to compute the epipolar plane passing through \\(p_l\\) and hence the correspondent epilolar line in right image, hence search \\(p_r\\) on a line. </p>"},{"location":"csc420/stereo.html#cross-product","title":"Cross Product","text":"<p>For two \\(3\\times 1\\) vectors \\(u,v\\), the cross product is defined as </p> \\[u\\times v = [u]_x v, [u]_x = \\begin{bmatrix}0&amp;-u_3&amp;u_2\\\\u_3&amp;0&amp;-u_1\\\\-u_2&amp;u_1&amp;0\\end{bmatrix}, u\\times v=\\begin{bmatrix}u_2v_3-u_3v_2\\\\u_3v_1-u_1v_3\\\\u_1v_2-u_2v_1\\end{bmatrix}\\] <p>Note that \\(u\\times u =0\\) and \\(u\\parallel v \\Leftrightarrow u\\times v = 0\\)</p> <p>Geometrically, the cross product defines the line perpendicular to both \\(u,v\\), i.e. \\(u\\cdot(u\\times v) = v\\cdot(u\\times v) = 0\\). </p>"},{"location":"csc420/stereo.html#essential-matrix-e","title":"Essential Matrix E","text":"<p>maps 3D points from the frame of one camera to the frame of another camera. </p> <p>Consider \\(X_l, X_r\\) be the same 3D point in left, right camera frames. Because we are changing the perspective on the same point, we are get the transformation by a translation and a rotation</p> \\[\\begin{align*} X_l &amp;= RX_r + T\\\\ T\\times X_l &amp;= T\\times RX_r + T\\times T\\\\ X_l\\cdot (T\\times X_l) &amp;= X_l\\cdot (T\\times RX_r) + 0\\\\ 0 &amp;= X_l\\cdot T\\times RX_r&amp;(T\\times X_l)\\perp X_l\\\\ X_l\\cdot([T]_xRX_r) &amp;= 0\\\\ X_l^TEX_r &amp;= 0&amp;E:=[T]_xR \\end{align*}\\]"},{"location":"csc420/stereo.html#fundamental-matrix-f","title":"Fundamental Matrix F","text":"<p>Let \\(I_r\\) be the epipolar line corresponding to \\(p_l\\), the fundamental matrix \\(I_r= Fp_l\\). Note that for any \\(p_l\\), \\(F\\) is unchanged. </p> <p>Note that for any point \\(P\\), its projection on the 2D image frame is \\(p = K[R\\mid t]P\\) (projection matrix)</p> <p>Since both 3D points on the image frame are the identical 3D point in the world. Let \\(X_l = [R_l\\mid t_l]P, X_r = [R_r\\mid t_r]P\\), assuming using the same camera, i.e. the same \\(K\\). Therefore, \\(p_l = KX_l, p_r = KX_r\\Rightarrow X_l = K^{-1}p_l, X_r = K^{-1}p_r\\)</p> <p>Therefore, \\((K^{-1}p_l)^T E K^{-1}p_r = 0\\), then \\(p_l^T(K^{-1})^TEK^{-1}p_r = 0\\Rightarrow p_l^TFp_r = 0\\), where \\(F:= (K^{-1})^TEK^{-1}\\)</p> <p>Noting that \\(p_l^TFp_r = 0\\Leftrightarrow p_l^T100Fp_r = 0\\), i.e. \\(F\\) is scale independent, also note that \\(E\\) is rank 2, hence \\(F\\) is rank 2. \\(\\Rightarrow \\det F = 0\\). Therefore, degree of freedom is \\(9-1-1=7\\)</p>"},{"location":"csc420/stereo.html#computing-f-8-point-matching-algorithm","title":"Computing F (8 Point matching algorithm)","text":"<p>Consider \\(8\\) matching points, \\(p_{l1}, p_{r1},...,p_{l8}, p_{r8}\\). Then, we have 8 equations \\(p_{li}Fp_{lr} = 0\\). We can flatten \\(F\\rightarrow \\vec f\\) and write the equations as vector products \\(a_i\\vec f = 0\\). Then, we can form system of equations</p> \\[A\\vec f = \\vec 0 \\Rightarrow \\begin{bmatrix} [\\vec a_1]\\\\ ...\\\\ [\\vec a_8] \\end{bmatrix}\\vec f = \\vec 0\\] <p>Note that \\(\\vec f\\) is the null space of \\(A\\). Using SVD, \\(A = UDV^T\\), and set \\(\\vec f\\) to the column of \\(V\\) corresponds to the smallest singular value in \\(D\\), and reshape \\(\\vec f\\) to \\(F\\). </p> <p>Then, we can impose \\(\\det F= 0\\) constraint by SVD on \\(F = UDV^T, D= diag(d_1,d_2,d_3)\\) where \\(d_1&gt;d_2&gt;d_3\\), then we can set \\(d_3=0\\), i.e. \\(\\hat D = diag(d_1,d_2,0)\\) and let output \\(\\hat F = U\\hat DV^T\\)</p>"},{"location":"csc420/stereo.html#computing-epipolar-lines","title":"Computing epipolar lines","text":"<p>Note that \\(F\\) is scale invariant, hence \\(p_l^TFp_r = 0\\Rightarrow l_l = Fp_r\\) is the epipolar line on the left image.  </p> <p>Similarly, \\(p_l^TFp_r = 0\\Rightarrow (p_l^TFp_r)^T = 0\\Rightarrow p_r^TF^Tp_l = 0\\Rightarrow l_r = F^Tp_l\\) is the eipolar line on the right image.</p>"},{"location":"csc420/stereo.html#computing-epipoles","title":"Computing epipoles","text":"<p>Wlog, using the left image.  Note that all epipolar lines \\(l_l\\) intersect at \\(e_l\\), hence \\(e_l^Tl_l = e_l^TF^Tp_l = 0\\), take \\(p_l=1\\), then we solve \\(e_l^TF^T = 0\\), and \\(e_l=\\) vector that is null space of \\(F\\) (rank 2). </p>"},{"location":"csc420/stereo.html#rectification-matrix","title":"Rectification Matrix","text":"<p>If \\(F\\) is knowing, we can compute the relative poses of the two cameras. </p> <p>Setting \\(P_l = [I_{3\\times 3}\\mid 0]\\) be the reference camera, \\(P_r = [[e_r]_xF\\mid e_r]\\), then \\(P_r\\) is the rectified projection matrix. </p>"},{"location":"mat334/arg.html","title":"Argument Principle and Rouche's Theorem","text":""},{"location":"mat334/arg.html#argument-principle","title":"Argument Principle","text":""},{"location":"mat334/arg.html#lemma-1","title":"Lemma 1","text":"<p>If \\(z_i\\) is a zero of order \\(n_i\\) of \\(f(z)\\), then </p> \\[\\frac{f'(z)}{f(z)} = \\frac{n_i}{z-z_i} + h_i(z)\\] <p>near \\(z_i\\) for some function \\(h_i(z)\\) analytic at \\(z_i\\). </p> <p>proof. Consider the Laurent expansion near \\(z_i\\), since \\(z_i\\) is a zero of order \\(n_i\\), we have </p> \\[\\begin{align*} f(z) &amp;= \\sum_{n=n_i}^{\\infty}a_n(z-z_i)^n\\\\ &amp;= (z-z_i)^{n_i} \\sum_{n=0}^\\infty a_{n+n_i}(z-z_i)^n\\\\ &amp;:= (z-z_i)^{n_i} g_i(z) \\end{align*}\\] <p>Note that we have pulled out all the zeros, hence \\(g_i(z)\\neq 0\\) near \\(z_i\\) and at \\(z_i\\). Thus, we can write </p> \\[\\frac{f'(z)}{f(z)} = \\frac{n_i(z-z_i)^{n_i - 1} g_i(z) + (z-z_i)^{n_i}g_i'(z)}{(z-z_i)^{n_i}g_i(z)} = \\frac{n_i}{z-z_i} + \\frac{g'_i(z)}{g_i(z)}\\] <p>Similarly, if \\(z_i\\) is a pole of order \\(p_i\\) near \\(z_i\\), then \\(f(z) = \\sum_{n=-n_i}^\\infty c_n(z-z_n)^n\\) so that </p> \\[\\frac{f'(z)}{f(z)} = \\frac{-p_i}{z-z_i} + \\frac{g'_i(z)}{g_i(z)}\\] <p>If \\(z_i\\) is of order \\(0\\), then \\(\\frac{f'(z)}{f(z)}\\) is already analytic using the same expansion. </p>"},{"location":"mat334/arg.html#theorem-2-argument-principle","title":"Theorem 2 Argument Principle","text":"<p>Suppose \\(f(z)\\) is analytic on a Jordon contour \\(C\\) and meromorphic (has finitely many singularities, and each of which are poles) inside of \\(C\\). Then</p> \\[N - P = \\frac{1}{2\\pi i} \\oint_C \\frac{f'(z)}{f(z)}dz = [\\arg(f(z))]_C=:w(f(C), 0)\\] <p>where \\(N\\) is the number of \\(0\\)'s in \\(C_{int}\\) counted with multiplicity, and \\(P\\) is thue number of poles counted with multiplicity. </p> <p>proof. For \\(N - P = \\frac{1}{2\\pi i} \\oint_C \\frac{f'(z)}{f(z)}dz\\),</p> \\[\\begin{align*} \\frac{1}{2\\pi i} \\oint_C \\frac{f'(z)}{f(z)}dz &amp;= \\sum_{k=1}^m Res(\\frac{f'}{f}, z_k)\\\\ &amp;= \\sum_{zeros} (\\lim_{z\\rightarrow z_i} (z-z_i)(\\frac{n_i}{z-z_i} + \\frac{g'_i(z)}{g_i(z)})) + \\sum_{poles} (\\lim_{z\\rightarrow z_i} (z-z_i)(\\frac{-p_i}{z-z_i} + \\frac{g'_i(z)}{g_i(z)})) \\\\ &amp;= \\sum n_i - \\sum p_i\\\\ &amp;= N- P \\end{align*}\\] <p>For \\(\\frac{1}{2\\pi i} \\oint_C \\frac{f'(z)}{f(z)}dz = w(f(C), 0)\\),</p> <p>Let \\(c:[a,b]\\rightarrow\\mathbb C\\) be some parameterization of \\(C\\), take \\(\\theta:[a,b]\\rightarrow \\mathbb R\\) s.t. \\(f(c(t)) = |f(c(t))|e^{i\\theta(t)} = r(c(t))e^{i\\theta(t)}\\) then, </p> \\[\\begin{align*} \\oint_C \\frac{f'(z)}{f(z)} dz &amp;= \\int_a^b \\frac{f'(c(t))}{f(c(t))}c'(t)dt\\\\ &amp;= \\int_a^b \\frac{r'(c(t))c'(t) e^{i\\theta(t)} + r(c(t))i\\theta'(t)e^{i\\theta(t)} }{r(c(t))e^{i\\theta(t)} }dt\\\\ &amp;= \\int_a^b \\frac{r'(c(t))c'(t)}{r(c(t))}dt + \\int_a^b \\theta'(t) dt \\\\ &amp;= \\int_{r(c(a))}^{r(c(b))} \\frac{dr}{r} + i\\int_{\\theta(a)}^{\\theta(b)} d\\theta\\\\ &amp;= \\log(r(c(b))) - \\log(r(c(a))) + i(\\theta(b) - \\theta(a))\\\\ &amp;= i(\\theta(b) - \\theta(a)) \\end{align*}\\] <p>So that \\(\\frac{1}{2\\pi i} \\oint_C \\frac{f'(z)}{f(z)}dz = \\frac{\\theta(b) - \\theta(a)}{2\\pi} = w(f(C), 0)\\)</p>"},{"location":"mat334/arg.html#applications-of-argument-principle","title":"Applications of Argument Principle","text":"<p>Example Determine the number of zeros of \\(f(z) = z^3 + 1\\) in the first quadrant.</p> <p>Consider \\(C = C_1\\cup C_2 \\cup C_3\\) parameterized by \\(c_1: [0, 1]\\rightarrow \\mathbb C:= Rt, c_2(t): [0, \\pi/2]\\rightarrow \\mathbb C= Re^{it}, c_3: [0, 1]\\rightarrow \\mathbb C = iR(1-t)\\). </p> <p>Using argument princple, note that \\(f\\) does not have any singularities, so that number of zeros inside \\(C\\) equals \\(w(f(C), 0)\\). Then, compute </p> \\[f(c_1(t)) = 1+R^3 t^3, f(c_2(t)) = 1+ R^3 e^{i3t} f(c_3(t)) = 1 - iR(1-3)^3\\] <p>Thus, for \\(R &gt; 1\\), \\(w(C, 0) =1\\) is the number of zeros.</p>"},{"location":"mat334/arg.html#rouches-theorem","title":"Rouche's Theorem","text":"<p>Theorem Let \\(f\\) and \\(g\\) by analytic on \\(\\partial C \\cup C_{int}\\) for a Jordon contour \\(C\\). If \\(|f(z)| &gt; |g(z)|\\) for all \\(z\\in C\\), then \\(f(z)\\) and \\(f(z) + g(z)\\) have the same number of zeros in \\(C_{int}\\). </p> <p>proof (informal). Since \\(|f(z)| &gt; |g(z)|\\) on \\(C\\), notably have \\(f(z)\\neq 0\\) on \\(C\\). Let \\(w(z) = \\frac{f(z)+g(z)}{f(z)}\\). Observe that \\(|w(z) - 1| = |\\frac{g(z)}{f(z)}| &lt; 1\\) on \\(C\\). i.e. \\([\\arg(w(z))]_C = 0\\). Thus, \\(w(z) \\in B_1(1)\\) has the same number of zeros and singularities. Furthermore, \\(N=P=0\\)</p>"},{"location":"mat334/arg.html#example-1","title":"Example 1","text":"<p>Show that \\(p(z) = z^4 + z^2 + 1\\) has 4 roots inside \\(C_2(0)\\).</p> <p>Let \\(f(z) = z^4, g(z) = z^2 + 1\\). \\(\\forall |z| = 2\\), obviously \\(|f(z)| = 16 &gt; 5 = |f(z)|\\) so that apply Rouche's Theorem, \\(p = f+g\\) has the same number of 0's as \\(f\\), which is 4.</p>"},{"location":"mat334/arg.html#example-2","title":"Example 2","text":"<p>Show that \\(h(z) = 3z^2 - \\cos z\\) has exactly \\(2\\) roots inside \\(C_1(0)\\).</p> <p>Let \\(f(z) = 3z^2, g(z) = -\\cos(z)\\). \\(\\forall |z| = 1. |f(z)| = 3\\) and \\(|g(z)| \\leq |\\cos z | \\leq e\\). so that by Rouche's Theorem, \\(p = f+g\\) has the 2 roots as of \\(f\\). </p>"},{"location":"mat334/arg.html#example-3","title":"Example 3","text":"<p>Show that \\(h(z) = z^5 + z + 3\\) has \\(5\\) roots in \\(\\bar A_{1,2}(0)\\)</p> <p>Note that \\(|z| = 2\\), \\(|z^5| =  32 &gt;  5 = |z|+ 3  = |z+3|\\) so that \\(h\\) has \\(5\\) zeros on \\(C_2(0)\\), also note that \\(|h(z)| &gt; 0\\) so that no zeros on the \\(\\partial C_2\\). In addition, note that on \\(C_1(0)\\),  \\(|z+3| \\geq 3 - |z| = 2 &gt; 1 = |z^5|\\) and \\(z+3\\) has on zeros inside \\(C_1(0)\\). Therefore, all \\(5\\) zeros must locate in \\(A_{1,2}(0)\\)</p>"},{"location":"mat334/arg.html#fundamental-theorem-of-algebra","title":"Fundamental Theorem of Algebra","text":"<p>Theorem For a polynomial \\(p(z) = \\sum_{k=0}^n a_kz^k\\), \\(p\\) has exactly \\(n\\) roots in \\(\\mathbb C\\). </p> <p>proof. Note that \\(p(z)\\) has the same number of roots as \\(\\frac{p(z)}{a_n}\\), so wlog assume \\(a_n = 1\\).  Let \\(f(z) = z^n, g(z) = \\sum_{k=0}^{n-1}a_k z^k\\) so that \\(p = f+g\\). Then, \\(\\forall |z| \\geq 1\\)</p> \\[\\begin{align*} |g(z)| &amp;\\leq \\sum_{k=0}^{n-1} |a_k||z|^k\\\\ &amp;\\leq |z|^{n-1}\\sum_{k=0}^{n-1}|a_k|\\\\ &amp;\\leq \\max\\{\\sum_{k=0}^{n-1}|a_k|, 1\\} |z|^{n-1} \\end{align*}\\] <p>Let \\(R = \\max\\{\\sum_{k=0}^{n-1}|a_k|, 1\\}\\), so that on \\(C_R(0)\\), i.e. \\(\\forall |z| = R\\) we have </p> \\[|f(z)| = R^n = R  |z|^{n-1}\\geq |g(z)|\\] <p>By Rouche's Theorem, the statement is proven. </p>"},{"location":"mat334/cie.html","title":"Cauchy's Integral Formula","text":""},{"location":"mat334/cie.html#cauchys-integral-formula_1","title":"Cauchy's Integral Formula","text":"<p>Lemma For \\(z_0 \\in \\mathbb C, C_r\\) be the circle of radius \\(r\\). Then, \\(\\oint_{C_r}\\frac{1}{z-z_0}dz = 2\\pi i\\). </p> <p>proof. We parameterize the circle by \\(c(t) = re^{it} + z_0\\)</p> \\[\\oint_{C_r}\\frac{1}{z-z_0}dz = \\int_0^{2\\pi}\\frac{1}{re^{it}-z_0+z_0}rie^{it}dt =i \\int_0^{2\\pi}dt = 2\\pi i\\] <p>Theorem Let \\(f:D\\rightarrow\\mathbb C\\) analytic and \\(D\\) simply connected. Then for any Jordon contour \\(C\\subset D\\). For \\(z\\in C_{int}\\), </p> \\[f(z) = \\frac{1}{2\\pi i}\\oint_C \\frac{f(w)}{w-z}dw\\] <p>proof. Let \\(z\\in D\\), define \\(C_\\delta\\) be a circle of radius \\(\\delta\\) centered at \\(z\\), we choose \\(\\delta\\) small enough so that \\(C_\\delta \\subset C_{int}\\). Then, we deform the integral over \\(C\\) to \\(C_\\delta\\).</p> \\[\\begin{align*} \\frac{1}{2\\pi i}\\oint_C \\frac{f(w)}{w-z}dw &amp;= \\frac{1}{2\\pi i}\\oint_{C_\\delta} \\frac{f(w) -f(z)}{w-z} + \\frac{f(z)}{w-z}dw\\\\ &amp;= \\frac{1}{2\\pi i}\\oint_{C_\\delta} \\frac{f(w) -f(z)}{w-z}dw + \\frac{f(z)}{2\\pi i}\\oint_C\\frac{1}{w-z}dw \\\\ &amp;= \\frac{1}{2\\pi i}\\oint_{C_\\delta} \\frac{f(w) -f(z)}{w-z}dw + f(z) &amp;\\text{lemma} \\end{align*}\\] <p>Thus, we need to show that \\(\\oint_{C_\\delta} \\frac{f(w) -f(z)}{w-z}dw  = 0\\). Since \\(C_\\delta \\subset C\\cup C_{int}\\) and it is compact. We can bound \\(\\frac{f(w) -f(z)}{w-z}\\) on \\(C_\\delta - \\{z\\}\\), in addition, since \\(f\\) is analytic, \\(\\lim_{w\\rightarrow z} \\frac{f(w) -f(z)}{w-z} = f'(z)\\). Therefore,  \\(\\frac{f(w) -f(z)}{w-z} \\leq M\\) for all \\(w\\in C_\\delta\\). By ML inequality, </p> \\[|\\frac{1}{2\\pi i}\\oint_{C_\\delta} \\frac{f(w) -f(z)}{w-z}dw| \\leq |\\frac1{2\\pi}||(2\\pi\\delta)M| = \\delta M\\] <p>Note that we can pick \\(\\delta\\) arbitrarily small so that the supremum of the integral approaches 0. </p>"},{"location":"mat334/cie.html#holomorphic-from-analyticity","title":"Holomorphic from Analyticity","text":"<p>Theorem (Analytic functions are infinitely complex-differentiable) If \\(f\\) in analytic on \\(C\\cup C_{int}\\) for some Jordon contour \\(C\\), then</p> \\[f^{(k)}(z) = \\frac{k!}{2\\pi i} \\oint_C \\frac{f(w)}{(w-z)^{k+1}}dw\\] <p>proof. First, consider \\(k = 1\\), we have </p> \\[\\begin{align*} \\frac{f(z+h) - f(z)}h &amp;= \\frac{1}{h2\\pi i}\\oint_C f(w)(\\frac{1}{w-(z+h)} - \\frac 1{w-z})dw\\\\ &amp;= \\frac{1}{2\\pi i}\\oint_C \\frac{f(w)}{(w-(z+h))(w-z)}dw\\\\ &amp;= \\frac{1}{2\\pi i}\\oint_C \\frac{f(w)}{(w-z)^2}dw + \\frac{h}{2\\pi i}\\oint_C \\frac{f(w)}{(w-z)^2(w-z-h)}dw \\end{align*}\\] <p>Then, let \\(I = \\oint_C \\frac{f(w)}{(w-z)^2(w-z-h)}dw\\), we need the \\(\\lim_{h\\rightarrow 0} \\frac{h}{2\\pi i}I = 0\\). </p> <p>Similar to the CIF proof, note that \\(f(w)\\) is bounded on \\(C\\), and since \\(C\\) can be taken arbitrarily small, \\(|I|\\) is then bounded by \\(\\frac M{(\\delta + h)^2\\delta} 2\\pi\\delta\\) so that</p> \\[\\lim_{h\\rightarrow 0} |\\frac{h}{2\\pi i}I| = \\frac{|h|M}{(\\delta + |h|)^2} = 0\\] <p>Then, we can repeat this process in our inductive step and prove this statement. </p>"},{"location":"mat334/cie.html#liouvilles-theorem","title":"Liouville's Theorem","text":"<p>Theorem If \\(f:\\mathbb C\\rightarrow\\mathbb C\\) is entire (analytic on \\(\\mathbb C\\)) and bounded, then \\(f\\) is a constant. </p> <p>proof. First note that \\(f\\) is analytic, take \\(R\\) be the radius of circle \\(C\\) around some \\(z\\), and \\(M\\) so that \\(|f(z)| &lt; M\\) for \\(z \\in C\\), using ML inequality and the theorem above, we can easily have</p> \\[\\begin{align*} |f^{(k)}(z)| &amp;= \\frac{k!}{2\\pi} |\\oint_C \\frac{f(w)}{(w-z)^{k+1}}dw| \\\\ &amp;= \\frac{k!}{2\\pi} \\frac{M}{R^{k+1}} 2\\pi R\\\\ &amp;= \\frac{k!M}{R^n} \\end{align*}\\] <p>Take \\(k=1\\), we have</p> \\[\\forall R &gt; 0, |f'(z)|\\leq \\frac{M}{R}\\] <p>Take \\(R\\) arbitrarily large, we have \\(|f'(z)| = 0\\) so that for any point \\(z\\in\\mathbb C\\)</p> \\[f(z) - f(0) = \\int_0^z f'(z)dz = 0\\implies f(z) = f(0)\\]"},{"location":"mat334/cie.html#fundamental-theorem-of-algebra","title":"Fundamental Theorem of Algebra","text":"<p>Theorem Let \\(p(z)\\) be some complex polynomials. Then \\(\\exists z\\in \\mathbb C\\) s.t. \\(p(z) = 0\\). </p> <p>proof. Let \\(p(z):= \\sum_{j=0}^n c_j z^j\\) be some n-degree complex polynomial. Assume that \\(\\forall z\\in\\mathbb C, p(z) \\neq 0\\). Then, \\(\\frac{1}{p(z)}\\) is entire and bounded. By Liouville's Theorem, it is a constant. This causes contradiction as \\(p\\) must be in degree of \\(n\\). </p>"},{"location":"mat334/cie.html#maximum-principles","title":"Maximum Principles","text":"<p>Lemma If \\(f\\) is analytic on some domain \\(D\\), then \\(|f(z)|\\) is unbounded unless \\(f(z)\\) is a constant. </p> <p>proof. Let \\(z\\in \\mathbb D\\), take some ball \\(B_{\\delta}(z) \\subset D\\). Then, we can establish </p> \\[\\begin{align*} f(z)\\frac{\\delta^2}{2} &amp;= \\int_0^\\delta f(z)\\tau d\\tau\\\\ &amp;= \\frac{1}{2\\pi}\\int_0^\\pi \\oint_{C_\\tau}\\frac{f(w)}{w-z}\\tau dw d\\tau\\\\ &amp;= \\frac1{2\\pi}\\int_0^\\pi \\oint_{C_\\tau}f(w)dwd\\tau &amp;\\forall w\\in C_\\tau.(w-z)=\\tau \\\\ &amp;= \\frac1{2\\pi}\\iint_{B_{\\delta}(z)}fdA \\end{align*}\\] <p>Suppose \\(z\\) is a maximum, then </p> \\[|f(z)|\\frac{\\delta^2}{2} = |\\frac1{2\\pi}\\iint_{B_{\\delta}(z)}fdA| \\leq \\frac{1}{2\\pi}|f(z)|A = \\frac{1}{2\\pi}|f(z)|\\pi\\delta^2 = \\frac{|f(z)|\\delta^2}{2}\\] <p>Implies that for all \\(z\\), \\(f(z)\\) is constant within its neighborhood, implies that \\(f\\) is a constant. </p> <p>Theorem If \\(f\\) is analytic and bounded on region \\(D\\) and \\(|f(z)|\\) is continuous in the closure \\(\\bar D\\), then \\(|f(z)|\\) must has its maximum on \\(\\partial D\\).  Note that first, by extreme value theorem, a closed and bounded region must have a maximum, then by lemma. Any open region cannot have a maximum, hence the maximum must be on some closure. </p>"},{"location":"mat334/conformal.html","title":"Conformal Map and Mobius Transformation","text":"<p>For some function \\(f:\\mathbb C\\rightarrow \\mathbb C\\), \\(f\\) is conformal at \\(z_0\\)  if for any differentiable curves \\(\\gamma_1\\) and \\(\\gamma_2\\) s.t. pass through some point \\(z_0\\). We have that the angle between \\(\\gamma_1\\) and \\(\\gamma_2\\) at \\(z_0\\) is the same as \\(f(\\gamma_1)\\) and \\(f(\\gamma_2)\\) at \\(z_0\\).</p> <p>Equivalently, a complex function \\(f\\) is conformal IFF \\(f\\) is analytic with non-zero derivative.</p> <p>Definition of conformal on \\(\\infty\\)</p> <ul> <li>\\(f(\\infty)\\) is conformal at \\(\\infty\\) if \\(g(z) = f(1/z)\\) is conformal at 0.</li> <li>If \\(f(z_0) = \\infty\\), then \\(f\\) is conformal at \\(z_0\\) if \\(f(z_0)^{-1}\\) is conformal at \\(z_0\\). </li> <li>Combine the two above, if \\(f(\\infty) = \\infty\\), then \\(f\\) is conformal at \\(\\infty\\) if \\(\\frac{1}{f(1/z)}\\) is conformal at \\(0\\). </li> </ul> Source code <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\n\ndef point_mapping(f, f_str, z, zb):\n    plt.axis(\"equal\")\n    plt.scatter(z.real, z.imag, s=1)\n    plt.plot(zb.real, zb.imag)\n    plt.scatter(f(z).real, f(z).imag, s=1)\n    plt.plot(f(zb).real, f(zb).imag)\n    plt.axhline(0, c=\"gray\", ls=\":\")\n    plt.axvline(0, c=\"gray\", ls=\":\")\n    plt.title(f_str)\n\nx = np.linspace(0, 1, 20)\nz = x.repeat(20) + 1j * np.tile(x, 20)\n\nplt.figure(figsize=(12, 4))\nplt.subplot(131)\nz1 = z[z.real + z.imag &gt;= 1]\nzb1 = np.concatenate((1+1j* x, x[::-1] + 1j))\nf1 = lambda x: (x * x)\npoint_mapping(f1, r\"$f(z) = z^2$\", z1, zb1)\n\nplt.subplot(132)\nz2 = z[z.real + (1 - z.imag) &gt;= 1]\nzb2 = np.concatenate((x, x + 1j * x))\nf2 = lambda x: (x ** 3)\npoint_mapping(f2, r\"$f(z) = z^3$\", z2, zb2)\n\nplt.subplot(133)\nz3 = 2 * z.real + np.pi * 2j * z.imag  \nzb3 = np.concatenate((2 * x, np.pi * 1j * x))\nf3 = lambda x: (np.exp(x))\npoint_mapping(f3, r\"$f(z) = e^z$\", z3, zb3)\nplt.savefig(\"../assets/conformal.jpg\")\n</code></pre> <p>\u200b </p>"},{"location":"mat334/conformal.html#theorem-1","title":"Theorem 1","text":"<p>If f(z) is analytic at \\(z_0\\) and \\(z_0\\) is a zero of order \\(n\\), then \\(f(z)\\) magnifies the angle between curves by a factor of \\(n\\). </p>"},{"location":"mat334/conformal.html#theorem-2-open-mappings-theorem","title":"Theorem 2. Open Mapping's Theorem","text":"<p>Let \\(f: D\\rightarrow \\mathbb C\\) be analytic with non-zero derivative and \\(D\\) is a domain. Then \\(f(D)\\) is a domain.  </p> <p>Theorem Let \\(D\\) be a simply connected domain whose boundary is parameterized by a differentiable curve \\(C\\). Let \\(f\\) be analytic on \\(D\\) and \\(C\\). If \\(f\\) is injective on \\(C\\) with non-zero derivative. Then it is injective on \\(D\\) and \\(f(C)\\) is the boundary of \\(f(D)\\).</p>"},{"location":"mat334/conformal.html#riemann-mappings-theorem","title":"Riemann Mapping's Theorem","text":"<p>Theorem (Riemann Mapping Theorem) If \\(D\\in\\mathbb C, D\\neq \\mathbb C\\) is a simply connected domain, then there exists a bijective, invertible, analytic conformal map \\(f:D\\rightarrow B_1(0)\\)</p> <p>Note that by Riemann Mapping Theorem, we can deform any simply connected domain to the unit circle, and then deform into another simply connected domain.</p>"},{"location":"mat334/conformal.html#example-1","title":"Example 1","text":"<p>\\(D = \\{z: Im(z) &gt; 0, |z| &gt; 1\\}, H = \\{z: Im(z) &gt; 0\\}, f(z) = z+ \\frac{1}{z}\\). Show that \\(f(D)=H\\) and \\(f\\) is bijective and conformal.</p> <p>First, check that \\(f\\) sends the boundary of \\(D\\) to \\(\\mathbb R\\).  - \\(\\forall a &lt; -1 , f(a) &lt; -1 - 1 = -2, f^{-1}(x) = -\\sqrt{x^2/4 - 1} + x/2\\)  - \\(\\forall a &gt; 1, f(a)  &gt; 1 + 1 &gt; 2, f^{-1}(x) = \\sqrt{x^2/4 - 1} + x/2\\)  - \\(\\forall |a| &gt; 1\\land Im(a) &gt; 0. c(t) = e^{it}, f(e^{it}) = e^{it}+e^{-it} = 2\\cos(t) \\in [-2, 2], f^{-1}(z) = \\cos^{-1}(z)\\)</p> <p>All of them are bijective in their given domain. </p> <p>Finally, note that for any \\(z = x+iy\\in D\\). \\(f(x+iy) = x+\\frac{1}{x} + i(y+\\frac{1}{y})\\), we have shown the case on the reals, only consider the imagine part, since \\(y =Im(z) &gt; 0, y + \\frac{1}{y} &gt; 0\\). Hence \\(f(z) \\in H\\)</p>"},{"location":"mat334/conformal.html#mobius-transformations","title":"Mobius Transformations","text":"<p>\\(f(z)\\) is a Mobius transformation if \\(f(z) = \\frac{az+b}{cz+d}, a,b,c,d\\in\\mathbb C\\) and \\(ad-bc\\neq 0\\).</p> <p>Example linear map \\(f(z) = az+b\\) with \\(a\\neq 0\\) are Mobius transformation since \\(az+b = \\frac{az+b}{0z+1}, a\\cdot 1 - b \\cdot 0 = a\\neq 0\\)</p> <p>Example When \\(c\\neq 0\\)</p> \\[f(\\infty) := \\lim_{z\\rightarrow \\infty} f(z) = \\lim_{z\\rightarrow \\infty}\\frac{az+b}{cz+d} = \\lim_{z\\rightarrow \\infty}\\frac{a + bz^{-1} }{c+dz^{-1} } = \\frac{a}{c}\\] <p>When \\(c= 0\\)</p> \\[f(\\infty) = \\lim_{z\\rightarrow \\infty}\\frac{az+b}{d} = \\infty\\] <p>When \\(c=0\\), \\(f(z)\\) has no pole in \\(\\mathbb C\\). When \\(c\\neq 0\\), consider \\(z_0 = -d/c\\), the denom is \\(0\\), hence \\(f\\) has a pole at \\(-d/c\\). Define \\(f(-d/c) = \\infty\\). </p> <p>Now, with the infinity points being defined, we can extend \\(f\\) to the Riemann sphere \\(f: \\bar{\\mathbb C}\\rightarrow \\bar{\\mathbb C}\\) and \\(f\\) then is analytic on \\(\\bar{\\mathbb C}\\). </p>"},{"location":"mat334/conformal.html#properties-of-mobius-transformations","title":"Properties of Mobius Transformations","text":"<p>(We will assume \\(c\\neq 0\\), otherwise Mobius transformations falls into a linear map and trivially have the properties list below). </p>"},{"location":"mat334/conformal.html#conformal","title":"Conformal","text":"<p>Mobius Transformations are conformal.</p> <p>proof. We need to consider \\(f(\\frac{-d}{c}) = \\infty\\) and \\(f(\\infty)\\) and all other points. </p> <ul> <li>First, consider \"all other points\", since \\(ad-bc\\neq 0\\) and \\(cz+d\\neq 0\\)</li> </ul> \\[f'(z) = \\frac{ad-bc}{(cz+d)^2} \\neq 0\\] <ul> <li>\\(z_0 = -d/c\\), then consider \\(g(z) = f(z)^{-1} = (\\frac{az+b}{cz+d})^{-1} = \\frac{cz+d}{az+b}\\) is still a Mobius Transformation. And it's \"special point\" is \\(-b/a\\), since \\(ad\\neq bc\\), \\(-b/a\\neq -d/c\\) and hence \\(f\\) is conformal at \\(-d/c\\). </li> <li>\\(z_0 = \\infty\\), then consider \\(g(z) = f(1/z) = \\frac{az^{-1} + b}{cz^{-1} + d} = \\frac{bz+a}{dz+c}\\) is still a Mobius Transformation, and \\(da-cb\\neq 0, -c/d\\neq 0\\) meaning that \\(g(z)\\) is conformal at 0, hence \\(f\\) conformal at \\(\\infty\\) </li> </ul>"},{"location":"mat334/conformal.html#finite-composition-of-linear-maps-and-inversion","title":"Finite composition of linear maps and inversion","text":"<p>Mobius Transformations is a finite composition of linear maps and inversion</p> <p>proof. Note that</p> \\[f(z) = \\frac{az+b}{dz+d} = \\frac{a}{c} + \\frac{bd-ad}{c}(cz+d)^{-1}\\]"},{"location":"mat334/conformal.html#closed-under-composition","title":"Closed under composition","text":"<p>Mobius transformation is closed under composition</p> <p>proof. Let \\(f_1(z) = \\frac{a_1z+b_1}{c_1z+d_1}, f_2(z) = \\frac{a_2z+b_2}{c_2z+d_2}\\). Let \\(g = f_1\\circ f_2\\).</p> \\[\\begin{align*} g(z) &amp;= \\frac{a_1(\\frac{a_2z+b_2}{c_2z+d_2}) + b_1}{c_1(\\frac{a_2z+b_2}{c_2z+d_2}) + d_1}\\\\ &amp;=  \\frac{a_1(a_2z+b_2) + b_1(c_2z+d_2)}{c_1(a_2z+b_2) + d_1(c_2z+d_2)}\\\\ &amp;= \\frac{(a_1a_2+b_1c_2)z + (a_1b_2 + b_1d_2)}{(c_1a_2+d_1c_2)z + (c_1b_2+d_1d_2)} \\end{align*}\\] <p>Also, note that if we let \\(g = \\frac{az+b}{cz+d}\\), the \\(a,b,c,d\\) can be obtained by</p> \\[\\begin{bmatrix}a&amp;b\\\\c&amp;d\\end{bmatrix} = \\begin{bmatrix}a_1&amp;b_1\\\\c_1&amp;d_1\\end{bmatrix} \\begin{bmatrix}a_2&amp;b_2\\\\c_2&amp;d_2\\end{bmatrix}\\]"},{"location":"mat334/conformal.html#invertible","title":"Invertible","text":"<p>Mobius transformation is invertible.  proof. Using the closure of composition, let \\(g = f^{-1} \\circ f, g(z) = \\frac{1z+0}{0z+1}, f(z) = \\frac{az+b}{cz+d}\\), </p> \\[\\begin{bmatrix}1&amp;0\\\\0&amp;1\\end{bmatrix} = \\begin{bmatrix}a&amp;b\\\\c&amp;d\\end{bmatrix} \\begin{bmatrix}a'&amp;b'\\\\c'&amp;d'\\end{bmatrix}\\] <p>Hence the coefs for \\(f^{-1}\\) is simply the inverse of coef matrix of \\(f\\). i.e.</p> \\[ \\begin{bmatrix}a&amp;b\\\\c&amp;d\\end{bmatrix}^{-1} =  \\frac{1}{ad-bc}\\begin{bmatrix}d&amp;-b\\\\-c&amp;a\\end{bmatrix}\\] <p>Since \\(ad-bc\\neq 0\\), also note that we can drop the \\(\\frac{1}{ad-bc}\\) part as it will be applied on both numurator and denominator, so that </p> \\[f^{-1}(z) = \\frac{dz-b}{-cz+a}\\]"},{"location":"mat334/conformal.html#cross-ratios","title":"Cross Ratios","text":"<p>Theorem Let \\(z_1,z_2,z_3,z_4 \\in \\mathbb C\\) be distinct. \\(f\\) is a Mobius transformation. Then </p> \\[\\frac{(z_1-z_4)(z_3-z_2)}{(z_1-z_2)(z_3-z_4)} = \\frac{(f(z_1)-f(z_4))(f(z_3)-f(z_2))}{(f(z_1)-f(z_2))(f(z_3)-f(z_4))}\\] <p>proof. omitted. simply expand the equation</p> <p>We define the ratio \\(\\frac{(z_1-z_4)(z_3-z_2)}{(z_1-z_2)(z_3-z_4)}\\) as cross ratio.</p> <p>If any of the points is \\(\\infty\\) (say \\(z_3\\)), then we write </p> \\[\\lim_{z_3\\rightarrow\\infty} \\frac{(z_1-z_4)(z_3-z_2)}{(z_1-z_2)(z_3-z_4)} = \\lim_{z_3\\rightarrow\\infty}\\frac{(z_1-z_4)(1-\\frac{z_2}{z_3})}{(z_1-z_2)(1-\\frac{z_4}{z_3})} = \\frac{z_1-z_4}{z_1-z_2}\\] <p>Cross ratios can be used to find the Mobius transoformation. </p>"},{"location":"mat334/conformal.html#example-1_1","title":"Example 1.","text":"<p>Let \\(z_1 = 0, z_2 = 1, z_3 = -1\\) maps to \\(w_1 = 0, w_2 = 1, w_3 = i\\). Find \\(f\\) be a Mobius transformation s.t. \\(f(z_i) = w_i\\). </p> <p>Set \\(z_4 = z, w_4 = 4\\), using the cross ratio formula, we have </p> \\[\\begin{align*} \\frac{(0-z)(-1-1)}{(0-1)(-1-z)} &amp;= \\frac{(0-w)(i-1)}{(0-1)(i-w)}\\\\ \\frac{2z}{1+z} &amp;= \\frac{w(i-1)}{i-w}\\\\ w(i-1) + wz(i-1) &amp;= 2iz - 2zw\\\\ w(z(i+1) + i-1) &amp;= 2iz\\\\ f(z) = w &amp;= \\frac{(2i)z}{2(1+i)+i-1} \\end{align*}\\]"},{"location":"mat334/conformal.html#preservation-of-angles-and-generalized-circles","title":"Preservation of angles and generalized circles","text":"<p>Note that Mobius transformations is conformal, we will show that lines and circles will be transformed to lines and circles. Also, note that Mobius transformation is a finite combination of linear maps and inversion. Hence we will show that lines and circles are preversed under inversion (linear map is trivial). </p> <p>Theorem lines are preserved under inversion. proof. First, we need to find equations for lines and circles under \\(\\mathbb C\\). For a 2D real line \\(ax+by+c = 0\\), write \\(z= x+iy\\) so that \\(x = \\frac{z+\\bar z}{2}, y = \\frac{z- \\bar z}{2i}\\). Then the equation becomes </p> \\[a(\\frac{z+\\bar z}{2}) + b(\\frac{z-\\bar z}{2i}) + c =(\\frac{a}{2} + \\frac{b}{2i})z + (\\frac{a}{2} - \\frac{b}{2i})\\bar z + c = Bz + \\bar B\\bar z + c =0\\] <p>set \\(B = \\frac{a}{2} + \\frac{b}{2i}\\)</p> <p>For circles centered at \\(z_0\\) with radius \\(r\\), write </p> \\[r^2 = (z - z_0)(z+z_0) = z\\bar z - z \\bar z_0 - \\bar z z_0 + z_0 \\bar z_0\\] <p>if we write \\(B = -z_0, C = z_0\\bar z_0 - r^2\\), </p> \\[z\\bar z +B\\bar z  +\\bar B z  + C = 0\\] <p>A circle only offs the line by \\(z\\bar z\\)</p> <p>Actucally, consider the inversion, note that \\(z^{-1} = \\frac{\\bar z}{z\\bar z}, \\overline{z^{-1} } = \\frac{z}{z\\bar z}\\), we have the transformed equations be</p> \\[B\\bar z + \\bar B z + C z\\bar z = 0\\] \\[1 + \\bar B\\bar z + Bz + C z\\bar z = 0\\] <p>Therefor  - If \\(C = 0\\), inversion maps lines and circles to lines.   - If \\(C\\neq 0\\), inversion maps lines and circles to circles. </p>"},{"location":"mat334/conformal.html#example-1_2","title":"Example 1","text":"<p>Let \\(f(z) = \\frac{z-i}{z+i}\\). Find the image of the line \\(y=x\\) under \\(f\\). </p> <p>Since a line can only be mapped to a line or a circle, it suffices to check 3 points.  We check that </p> <ul> <li>\\(f(0) = \\frac{-i}{i} = -1\\)</li> <li>\\(f(\\infty) = \\lim_{z\\rightarrow\\infty}\\frac{z-i}{z+i} =  \\lim_{z\\rightarrow\\infty}\\frac{1-i/z}{1+i/z} = 1\\)</li> <li>\\(f(1+i) = \\frac{1+i-i}{1+i+i} = \\frac{1}{2i+1}\\)</li> </ul> <p>Obviously the 3 points aren't on a line, hence they form a circle. and we can find the circle via regular circle equations with 3 equations and 3 unknowns. </p>"},{"location":"mat334/cont_diff.html","title":"Limit, Continuity, and Differentiability","text":"<p>The definitions for continuity and differentiability of Complex Functions are very similar to the definition in \\(\\mathbb R\\). However, note that if we consider complex numbers as on the complex plane, it's actually a ball instead of a segment, it's somewhat similar to \\(\\mathbb R^2\\), but take special note that they are not identical to each other. </p>"},{"location":"mat334/cont_diff.html#limits","title":"Limits","text":"<p>Example evaluate limits</p> \\[\\begin{align*} \\lim_{z\\rightarrow i} z+z^{-1} &amp;= \\lim_{z\\rightarrow i} z+\\frac{\\bar z}{|z|} = i + \\frac{-i}{1} = 0\\\\ \\lim_{z\\rightarrow i}\\sinh z&amp;= \\sinh i = i\\sin 1 \\end{align*}\\]"},{"location":"mat334/cont_diff.html#continuity","title":"Continuity","text":"<p>Known that any \\(f:\\mathbb C\\rightarrow \\mathbb C\\) can be decomposed into real and imaginary parts \\(f = u+iv, u:\\mathbb R\\rightarrow \\mathbb R, v:\\mathbb R\\rightarrow \\mathbb R\\) with \\(z = x+iy\\). Therefore, if \\(u,v\\) is continuous at \\((x_0, y_0)\\), then f is continuous at \\(z_0=x_0+iy_0\\) with limit</p> \\[\\lim_{z\\rightarrow z_0}f(z) = \\lim_{(x,y)\\rightarrow (x_0, y_0)} u(x, y) + iv(x, y) = u(x_0, y_0) + iv(x_0, y_0)\\] <p>Conversely, we can also prove \\(f\\) continuous implies \\(u\\) and \\(v\\) are both continuous. </p>"},{"location":"mat334/cont_diff.html#differentiation-laws","title":"Differentiation Laws","text":"<p>Differentiation laws on sum, product, quotient, composition, power, trigonometric all hold on complex numbers. The proof is similar to real number ones, hence some are omitted. </p>"},{"location":"mat334/cont_diff.html#example-lhospitals-rules-for-both-0","title":"Example. LHospitals Rules for both 0","text":"<p>If \\(f(z)\\) and \\(g(z)\\) have formal power series about \\(a\\) and \\(f(a)=f'(a) = \\cdots = f^{(k)}=g(a)=g'(a) = \\cdots = g^{(k)} = 0\\) with \\(f^{(k+1)}(a)\\) and \\(g^{(k+1)}(a)\\) note simultaneously \\(0\\). Then, \\(\\lim_{z\\rightarrow a}f(z)/g(z) = f^{(k+1)}(a)/g^{(k+1)}(a)\\). </p> <p>proof For every \\(k\\), </p> \\[\\begin{align*} \\lim_{z\\rightarrow a}\\frac{f^{(k)}(z)}{g^{(k)}(z)} &amp;= \\lim_{z\\rightarrow a}\\frac{f^{(k)}(z)-f^{(k)}(a)}{g^{(k)}(z)-g^{(k)}(a)}\\\\&amp;= \\lim_{z\\rightarrow a}\\frac{\\frac{f^{(k)}(z)-f^{(k)}(a)}{z-a}}{\\frac{g^{(k)}(z)-g^{(k)}(a)}{z-a}} \\end{align*}\\] <p>Note that \\(f^{(k)}(z), g^{(k)}(z)\\) are differentiable, so that </p> \\[\\lim_{z\\rightarrow a}\\frac{\\frac{f^{(k)}(z)-f^{(k)}(a)}{z-a}}{\\frac{g^{(k)}(z)-g^{(k)}(a)}{z-a}} = \\lim_{z\\rightarrow a}\\frac{f^{(k+1)}(z)}{g^{(k+1)}(z)}\\] <p>If \\(\\lim_{z\\rightarrow a}\\frac{f^{(k+1)}(z)}{g^{(k+1)}(z)}\\) exist to be \\(\\frac{f^{(k+1)}(a)}{g^{(k+1)}(a)}\\), then all limits ends to be \\(\\frac{f^{(k+1)}(a)}{g^{(k+1)}(a)}\\).</p>"},{"location":"mat334/cont_diff.html#cauchy-riemann-equations","title":"Cauchy Riemann Equations","text":"<p>Claim  Consider some complex function \\(f:\\mathcal R\\rightarrow \\mathbb C, f(z)=u(z)+iv(z)\\) defined on some regions \\(\\mathcal R\\subseteq \\mathbb C\\).  \\(f\\) is differentiable IFF </p> \\[\\partial_x u = \\partial_yv \\land \\partial_x v = -\\partial_yu\\] <p>Note that this automatically implies that \\(u,v\\) need to have derivative, hence also are continuous. </p> <p>Define \\(\\tilde f: \\tilde{\\mathcal R}\\rightarrow \\mathbb R^2: f(x, y) = (\\tilde u(x, y), \\tilde v(x, y))\\). Since \\(f\\) is differentiable, the limit will exist from any approach, and the derivate is \\(f'(z) = a(x)+ib(y)\\) for \\(a(x), b(y)\\in\\mathbb R\\). Consider approach only from the real line, let \\(t\\in\\mathbb R\\)</p> \\[\\begin{align*} &amp;\\lim_{t\\rightarrow 0} \\frac{f(z_0+t) - f(z_0)}{t}\\\\ = &amp;\\lim_{t\\rightarrow 0} \\frac{u(z_0+t) + iv(z_0+t)}t - \\frac{u(z_0) + iv(z_0)}{t}\\\\ = &amp;a(x_0) + ib(y_0) \\end{align*}\\] <p>On the projected \\(\\mathbb R^2\\) function, </p> \\[\\lim_{t\\rightarrow 0}\\frac{\\tilde u(x_0+t, y_0) - u(x_0,y_0)}t +\\frac{\\tilde v(x_0+t, y_0) - v(x_0,y_0)}{t} = \\partial_x\\tilde u + \\partial_x\\tilde v\\] <p>So that </p> \\[(\\partial_x\\tilde u ,\\partial_x\\tilde v) = (a, b)\\] <p>Similarly, if we approach only from the imaginary line, we have </p> \\[\\begin{align*} \\lim_{t\\rightarrow 0}\\frac{f(z_0+it) - f(z_0)}{it} &amp;= \\lim_{t\\rightarrow 0}-i\\frac{f(z_0+it) - f(z_0)}{t}\\\\ &amp;= \\lim_{t\\rightarrow 0} \\frac{v(z_0+it) - v(z_0)}t - i\\frac{u(z_0+it) - u(z_0)}t \\end{align*}\\] <p>So that </p> \\[(\\partial_y\\tilde v, - \\partial_y\\tilde u) = (a, b)\\] <p>Therefore, we have the Jacobian as </p> \\[D_{\\tilde f}(x_0, y_0) = \\begin{bmatrix}a(z_0)&amp;-b(z_0)\\\\b(z_0)&amp;a(z_0)\\end{bmatrix}\\] <p>Therefore, we may notice that \\(\\mathbb R^2\\)-differentiable only approach from \\(2\\) lines, while \\(\\mathbb C\\)-differentiable can approach from other curves. So \\(C\\)-differentiable implies \\(\\mathbb R^2\\)-differentiable, but ont the converse. For example</p> \\[f(z) = \\bar z = x -iy, u = x, v = -y\\] <p>However, </p> \\[\\lim_{\\Delta z\\rightarrow 0}\\frac{\\overline{(z_0 + \\Delta z)} - \\bar z_0}{\\Delta z} = \\lim_{\\Delta z\\rightarrow 0}\\frac{\\overline{\\Delta z}}{\\Delta z}\\] <p>Taking \\(\\Delta z = re^{i\\theta}, \\overline{\\Delta z} = re^{-i\\theta}\\), the limit becomes \\(\\lim_{\\Delta z\\rightarrow 0}e^{-2i\\theta}\\), note that \\(\\Delta z = r(\\cos\\theta + i\\sin\\theta)\\) so that if we approach with fixed \\(\\theta\\) and \\(r\\rightarrow 0\\), it gives different limits. Hence the limit does not exist. </p>"},{"location":"mat334/cont_diff.html#derivative-from-cauchy-riemann-equations","title":"Derivative from Cauchy Riemann Equations","text":"<p>Note that from the proof above, we have shown that if Cauchy Riemann conditions hold, then the derivative of some complex function \\(f\\) is</p> \\[f'(z) = u_x + iv_x = v_y - iu_y\\]"},{"location":"mat334/cont_diff.html#cauchy-riemann-equation-in-polar-form","title":"Cauchy Riemann Equation in Polar Form","text":"<p>Theorem \\(f = u+iv\\) is analytic at \\(z = re^{i\\theta}, r&gt;0, \\theta\\in[0, 2\\pi)\\) IFF</p> \\[u_r = r^{-1}v_\\theta, v_r = -r^{-1}u_\\theta\\] <p>and the partial derivatives are continuous. </p> <p>proof. Note that \\(re^{i\\theta} = r\\cos(\\theta) + ri\\sin(\\theta)\\) so that \\((x, y) = (r\\cos\\theta, r\\sin\\theta)\\). Follow chain rule</p> \\[\\begin{align*} u_r &amp;= u_x x_r + u_yy_r= u_x\\cos\\theta + u_y\\sin\\theta\\\\ u_\\theta &amp;= u_x x_\\theta + u_yy_\\theta= -r(u_x\\sin\\theta + u_yr\\cos\\theta)\\\\ v_r &amp;= v_x x_r + v_yy_r= v_x\\sin\\theta + u_y\\cos\\theta\\\\ v_\\theta &amp;= v_x x_\\theta + v_yy_\\theta= r(v_x\\cos\\theta - u_y\\sin\\theta) \\end{align*}\\] <p>Then, using CR equations, we can establish the equalities</p>"},{"location":"mat334/cont_diff.html#derivative-in-polar-form","title":"Derivative in Polar Form","text":"<p>Using the relationship above, we can then apply chain rule to compute \\(f'\\) as </p> \\[f'(z) = (\\cos\\theta - i\\sin\\theta)(u_r + iv_r) = e^{-i\\theta}(u_r + iv_r)\\] <p>Corollary The level set of \\(u\\), \\(C = \\{(x, y) : u(x, y) = c_1, c_1\\in\\mathbb R\\}\\), are orthogonal to the level set of \\(v\\), at point \\(z_0\\). </p> <p>proof. \\(\\nabla u\\cdot \\nabla v = u_xv_x + u_yv_y = -u_xu_y + u_yu_x = 0\\), so that their gradient is orthogonal at \\(z_0\\), and then each of the level-set is orthogonal to their gradient, hence also orthogonal to each other. </p> <p>Example Show that \\(Re(z)\\) and \\(Im(z)\\) are nowhere differentiable. Let \\(\\tilde f(x, y) = x\\) so that its Jacobian is \\(\\begin{bmatrix}1&amp;0\\\\0&amp;0\\end{bmatrix}\\) hence Cauchy Riemann equations does not hold, hence not differentiable. (\\(Im(z)\\) is similar). </p>"},{"location":"mat334/cont_diff.html#analytic-function","title":"Analytic Function","text":"<p>\\(f:\\mathcal R\\rightarrow \\mathbb C\\) is analytic at \\(z_0\\) IFF it is differentiable within the neighborhood of \\(z_0\\). \\(f\\) is an analytic (or sometimes holomorphic) is it is analytic on each point on the domain. </p> <p>Singular point is some point \\(z_0\\) that \\(f\\) fails to be analytic. </p> <p>In fact, a complex function is analytic, then it is also infinitely differentiable (as of a power series function on the reals). </p> <p>Also, take a note that the Laplacians of u and v, given CR conditions, satisfy that</p> \\[\\nabla^2 u = \\frac{\\partial^2u}{\\partial x^2} + \\frac{\\partial^2u}{\\partial y^2} = \\frac{\\partial}{\\partial x}\\frac{\\partial v}{\\partial y} - \\frac{\\partial}{\\partial y}\\frac{\\partial v}{\\partial x} = 0, \\nabla^2v = 0\\] <p>So that \\(u,v\\) are harmonic conjugate of each other. We can obtain \\(v\\) given \\(u\\), then as well as \\(f\\). </p> <p>Example Take \\(u(x,y) = y^2 - x^2\\). Then \\(u_x = -2x = v_y, u_y = 2y = -v_x\\), implies that \\(v = -2xy + c\\) for some constant \\(c\\). So that </p> \\[f(z) = y^2-x^2 + i(-2xy+c) = -z^2 + ic\\]"},{"location":"mat334/cont_diff.html#differentiability-at-infinity","title":"Differentiability at Infinity","text":"<p>The limit at infinity \\(\\lim_{z\\rightarrow\\infty}f(z) = L\\) is defined as \\(\\forall \\epsilon &gt; 0. \\exists N &gt; 0. \\forall z\\in\\mathbb C. |z|&gt; N\\implies |f(z)-L| &lt; \\epsilon\\).</p> <p>Define \\(g:R\\rightarrow\\mathbb C, g(z):=\\begin{cases}f(z^{-1})&amp;z\\neq 0\\\\L &amp;z=0\\end{cases}\\). Then, \\(f\\) is differentiable/analytic at \\(\\infty\\) is \\(g(z)\\) is differentiable/analytic at \\(0\\). </p> <p>Example Consider \\(f(z) = z^{-1}\\), then \\(g(z) = z\\) is differentiable at 0, so that \\(z^{-1}\\) is differentiable at \\(\\infty\\). \\(z\\) is not differentiable at \\(\\infty\\), since \\(z^{-1}\\) is not differentiable at \\(0\\).</p>"},{"location":"mat334/ct.html","title":"Cauchy's Theorem","text":""},{"location":"mat334/ct.html#cauchys-theorem_1","title":"Cauchy's Theorem","text":"<p>Theorem Let \\(f:D\\rightarrow \\mathbb C\\) be analytic on simply connected domain \\(D\\). For any Jordon contour \\(C\\subset D\\), \\(\\oint_C f(z)dz = 0\\) </p> <p>proof (simpler ver.). We will prove the statement with the assumption that \\(f'\\) continuous on \\(D\\), </p> <p>Remark that Greens theorem states that for real functions \\(f:\\mathbb R^2 \\rightarrow\\mathbb R^2, f(x, y) = \\begin{pmatrix}u(x,y\\\\v(x,y)\\end{pmatrix}\\) and a simple closed contour \\(C\\), If the partial derivatives \\(u_x, u_y, v_x, v_y\\) are continuous on the region \\(R = C\\cup C_{int}\\), then</p> \\[\\oint_C (udx + vdy) = \\iint_{R}(v_x - u_y)dxdy\\] <p>Then, consider its connection to \\(f = u+iv\\) on the same region. </p> \\[\\begin{align*} \\oint_C f(z)dz &amp;= \\oint_C u(x,y) + i(x,y) (dx+idy)\\\\ &amp;=\\oint_C udx -vdy + i\\oint_C udy + vdx\\\\ &amp;= \\iint_R (-v_x - u_y)dxdy + \\iint_R (u_x-v_y)dxdy &amp;\\text{Green} \\\\ &amp;= \\iint_R (u_y - u_y) dxdy + \\iint_R (u_x-u_x)dxdy&amp;\\text{CR}\\\\ &amp;= 0 \\end{align*}\\]"},{"location":"mat334/ct.html#deformation-of-curves","title":"Deformation of Curves","text":"<p>Note that for any two endpoints \\(z_0, z_1 \\in D\\), \\(D\\) is simply connected. Then, arbitrarily pick two curves \\(C_1, C_2\\) traverse from \\(z_0\\) to \\(z_1\\), then \\(C_1, \\neg C_2\\) will form a simple closed curve, so that \\(\\oint_{C_1\\cup\\neg C_2} = 0\\implies \\oint_{C_1} = \\oint_{C_2}\\)</p> <p>Example Define a branch of \\(\\log\\) as \\(D = \\{re^{i\\theta} : r &gt; 0, \\theta \\in (-\\pi, \\pi)\\}\\). Therefore, we can define its anti-derivative as \\(z^{-1}\\) on this branch by </p> \\[L(z_1) = \\int_1^{z_1}z^{-1}dz = \\log(r) + i\\theta - \\int_1^1 z^{-1} = \\log(r) + i\\theta \\]"},{"location":"mat334/ct.html#theorem-1","title":"Theorem 1","text":"<p>For \\(f:D\\rightarrow \\mathbb C\\) continuous. If \\(\\forall C\\subset D\\) be simple closed contours \\(\\oint_C f(z)dz = 0\\), then \\(\\exists F:D\\rightarrow\\mathbb C\\) s.t. \\(F\\) is analytic and \\(F' = f\\). </p> <p>proof. We will assume \\(D\\) is convex, otherwise, we need to construct path with multiple line segments within a union of convex regions and it will be unnecessarily messy. </p> <p>Fix some point \\(z_0\\in D\\), define \\(F(z_1) := \\int_{z_0}^{z_1} f(z)dz\\). Note that we can define the integral on any curve from \\(z_0\\) to \\(z_1\\), but here we'll specifically choose the line segment with the assumption that \\(D\\) is convex, so that the path is always within \\(D\\). </p> <p>Fix arbitrarily small \\(h\\), Note that \\(z_0\\rightarrow z_1+h\\rightarrow z_1\\rightarrow z_0\\) encloses a region within \\(D\\) so that </p> \\[F(z_1+h)  - F(z_1) = \\int_{z_0}^{z_1+h}f(z)dz - \\int_{z_0}^{z_1}f(z)dz = \\int_{z_1}^{z_1+h}f(z)dz\\] <p>Therefore, we can write</p> \\[\\begin{align*} \\lim_{h\\rightarrow 0}\\frac{F(z_1+h) - F(z_1)}h &amp;= \\lim_{h\\rightarrow 0} \\big(h^{-1}\\int_{z_1}^{z_1+h}f(z)dz - h^{-1}\\int_{z_1}^{z_1+h}f(z_1)dz + f(z_1)\\big)\\\\ &amp;= \\lim_{h\\rightarrow 0} \\big(h^{-1}\\int_{z_1}^{z_1+h}(f(z) - f(z_1))dz\\big) + f(z_1) \\end{align*}\\] <p>Then, since \\(f\\) is continuous on \\(D\\), for arbitrarily small \\(\\epsilon &gt; 0\\), we can take \\(\\delta &gt; 0\\) so that for any \\(z \\in B_\\delta(z_1)\\), \\(|f(z)-f(z_1)|\\leq \\epsilon\\).  Therefore, by ML inequality</p> \\[h^{-1}\\int_{z_1}^{z_1+h}(f(z)-f(z_1))dz \\leq h^{-1}h\\epsilon = \\epsilon\\] <p>By the definition of limit</p> \\[\\lim_{h\\rightarrow 0} h^{-1}\\int_{z_1}^{z_1+h}(f(z)-f(z_1))dz = 0\\] <p>So that we have </p> \\[F'(z_1) = \\lim_{h\\rightarrow 0}\\frac{F(z_1+h) - F(z_1)}h = 0 + f(z_1) = f(z_1)\\]"},{"location":"mat334/ct.html#corollary-an-easier-statement-of-ct","title":"Corollary (An easier statement of CT)","text":"<p>For \\(f:D\\rightarrow\\mathbb C\\) continuous and \\(F:D\\rightarrow \\mathbb C\\) analytic. If \\(\\forall z\\in D. F'(z) = f(z)\\) and \\(C\\subset D\\) is a Jordan Contour. Then \\(\\oint_C f(z)dz = 0\\)</p>"},{"location":"mat334/ct.html#deformation-of-contours","title":"Deformation of Contours","text":"<p>Consider \\(f:D\\rightarrow \\mathbb C\\) is analytic everywhere except for a points \\(z_0, z_1, ...\\) and \\(D\\) is simply connected. For a Jordan Contour \\(C\\) such that points \\(z_0, z_1,... \\in C_{int}\\). We can construct Jordan curves \\(C_0, C_1,...\\) that encloses \\(z_0, z_1,...\\), respectively so that </p> \\[\\oint_C f(z)dz = \\sum_{i=0}\\oint_{C_i}f(z)dz\\] <p>proof. For each \\(C_i\\), take line segments \\(L_i^1, L_i^2\\) connecting \\(C_i, C\\) and start and end points of \\(L_i^1, L_i^2\\) be arbitrarily close. Then, we construct path</p> \\[\\tilde C = C +\\sum_{i=0}-C_i + L_i^1 - L_i^2 = C -\\sum_{i=0}C_i\\] <p>Note that \\(\\tilde C\\) does not enclose any of \\(z_o, z_1,...\\) so that by Cauchy's Theorem, \\(\\oint_{\\tilde C} = 0\\), so that \\(\\oint_C = \\sum \\oint_{C_i}\\)</p> <p>\u200b </p>"},{"location":"mat334/functions.html","title":"Elementary Functions","text":""},{"location":"mat334/functions.html#topology-of-complex-space","title":"Topology of Complex Space","text":"<p>Note that with the definition as \\(a+bi\\), the complex space can be seen as a \\(\\mathbb R^2\\) space, and have all topology definitions defined as reals. </p>"},{"location":"mat334/functions.html#example-1","title":"Example 1","text":"<ul> <li>\\(|z|\\leq 1\\) not open, closed, bounded, compact  </li> <li>\\(|2z+1+i| &lt; 4\\) open, not closed, bounded  </li> <li>\\(Re(z) \\geq 4\\) not open, closed, unbounded  </li> <li>\\(|z| \\leq |z + 1|\\) not open, closed, unbounded  </li> <li>\\(0 &lt; |2z-1|\\leq 2\\) not open, not closed, bounded</li> </ul> <p>\u200b </p>"},{"location":"mat334/functions.html#example-2","title":"Example 2","text":"<p>determine connectedness and the closure  \\(0 &lt; \\arg z \\leq \\pi\\) not connected, \\(\\bar S = \\{z=x+iy: x=0\\lor y =0\\} = \\{z = re^{i\\theta}:\\theta = 0 + 2n\\pi\\lor \\theta = \\pi + 2n\\pi\\}\\) </p> <p>\\(0 \\leq \\arg z &lt; 2\\pi\\) connected, \\(\\bar S = \\emptyset\\) </p> <p>\\(Re(z) &gt;0\\land Im(z) &gt; 0\\) connected, \\(\\bar S = \\{z=x+iy: (x=0\\land y \\geq 0)\\lor (y=0\\land x\\geq 0)\\} = \\{z=re^{i\\theta}: r \\geq 0\\land (\\theta=0\\lor\\theta = \\pi)\\}\\) </p> <p>\\(Re(z-z_0) &gt; 0\\land Re(z-z_1) &lt; 0, z_0,z_1 \\in \\mathbb C\\). If \\(Re(z_0) \\leq Re(z_1)\\), then connected, otherwise unconnected.\\(\\bar S = \\{z:Re(z) = Re(z_0)\\lor Re(z) = Re(z_1)\\}\\)</p> <p>\\(|z| &lt; 1/2 \\land |2z-4| &lt; 2\\) note this is \\(\\emptyset\\), hence connected and closure is \\(\\emptyset\\)</p> <p>\u200b </p>"},{"location":"mat334/functions.html#elementary-functions_1","title":"Elementary Functions","text":"<p>A function of the complex variable is some function that maps to a subset of \\(\\mathbb C\\). </p> <p>Define polynomials of degree \\(n\\) as \\(P_n(z) = \\sum^n_0 a_iz^i\\). Define rational as a ratio of two polynomials, given the quotient is not \\(0\\). Generally, a complex function can be written as \\(f(z) = u(x, y) + iv(x, y)\\), i.e. the real part and the imaginary part.  </p>"},{"location":"mat334/functions.html#example-1_1","title":"Example 1","text":"<p>For the transformation \\(w=z+1/z=u+iv\\), show that \\(\\forall z\\in\\mathbb C. (Im(z) &gt; 0\\land |z| &gt; 1)\\Rightarrow \\exists v &gt; 0\\). proof. </p> \\[w = z + z^{-1} = z + \\frac{\\bar z}{|z|^2}=x+iy + \\frac{x-iy}{x^2+y^2}\\] <p>so we have \\(u = x + \\frac{x}{x^2+y^2}, v = y - \\frac{y}{x^2+y^2}\\), since \\(x^2 + y^2 &gt; 1, y &gt; 0\\), \\(v&gt;0\\)</p>"},{"location":"mat334/functions.html#exponential-functions","title":"Exponential functions","text":"<p>Define the sum of exponent as</p> \\[e^z = e^{x+iy} = e^xe^{iy} = e^x(\\cos y + i\\sin y)\\] <p>Noting that this equation plus trigonometric identities imply that </p> \\[e^{z_1+z_2} = e^{z_1}e^{z_2}, (e^z)^n = e^{nz}, n\\in\\mathbb N^+\\] \\[|e^z| = |e^x||\\cos y + i\\sin y| = e^x\\sqrt{\\cos^2 y + \\sin^2 y} = e^x\\] \\[\\overline{e^z} = e^x(\\cos y - i\\sin y) \\ e^{\\bar z} = e^{x-iy}\\]"},{"location":"mat334/functions.html#trigonometric-functions","title":"Trigonometric functions","text":"<p>Define the trigonometric functions on complex numbers as </p> \\[\\sin z = \\frac{e^{iz} - e^{-iz}}{2i}, \\cos z = \\frac{e^{iz} + e^{-iz}}{2}\\] <p>Then, other trigonometric functions are taken from this definition. </p> <p>Define the hyperbolic functions as </p> \\[\\sinh z = \\frac{e^{z} - e^{-z}}{2}, \\cosh z = \\frac{e^{z} + e^{-z}}{2}\\] <p>The usual trigonometric identities hold on these definitions</p> <p>It's worth noting that </p> \\[\\sinh(iz) = i\\sin z , \\sin(iz) = i\\sinh z\\] \\[\\cosh(iz) = \\cos z , \\cos(iz) = \\cosh z\\]"},{"location":"mat334/functions.html#example-2_1","title":"Example 2","text":"<p>Prove angle sum identities using complex numbers. proof For \\(a,b\\in\\mathbb R\\), </p> \\[\\begin{align*} \\sin a \\cos b + \\cos a \\sin b &amp;= \\frac{e^{ia} - e^{-ia}}{2i} \\frac{e^{ib} + e^{-ib}}{2}+ \\frac{e^{ib} - e^{-ib}}{2i} \\frac{e^{ia} + e^{-ia}}{2}\\\\ &amp;= \\frac{1}{4i}\\big(2e^{ia}e^{ib} - 2e^{-ia}e^{-ib} \\big)\\\\ &amp;= \\frac{e^{i(a+b)} - e^{-i(a+b)}}{2i}\\\\ &amp;= \\sin(a+b)\\\\ \\cos a\\cos b - \\sin a \\sin b &amp;= \\frac{e^{ia} + e^{-ia}}{2}\\frac{e^{ib} + e^{-ib}}{2} - \\frac{e^{ia} - e^{-ia}}{2i}\\frac{e^{ib} - e^{-ib}}{2i}\\\\ &amp;= \\frac{(e^{ia} + e^{-ia})(e^{ib} + e^{-ib})}{4} + \\frac{(e^{ia} - e^{-ia})(e^{ib} - e^{-ib})}{4}\\\\ &amp;= \\frac{e^{i(a+b)} + e^{-i(a+b)}}2\\\\ &amp;= \\cos(a+b) \\end{align*}\\]"},{"location":"mat334/functions.html#power-series","title":"Power Series","text":"<p>Define a power series of \\(f(z)\\) about the point \\(z=z_0\\) as </p> \\[f(z) = \\sum_{0}^\\infty a_i(z-z_0)^i\\] <p>Ratio test (motivated by reals) also holds on complex numbers even though we won't prove it here: If \\(\\lim_\\infty \\lvert \\frac{a_{n+1}}{a_n}\\rvert |z - z_0| &lt; 1\\), then the power series converges, and the radius of convergence is </p> \\[R = \\lim_\\infty\\lvert \\frac{a_{n}}{a_{n+1}}\\rvert\\] <p>\\(R=\\infty\\), then the series converges for all finite \\(z\\), \\(R = 0\\), then the series converges only for \\(z=z_0\\). </p> <p>Some common power series representations:  </p> \\[\\begin{align*} e^z &amp;= \\sum \\frac{z^k}{k!}\\\\ \\sin z &amp;= \\sum (-1)^k \\frac{z^{2k+1}}{(2k+1)!}\\\\ \\cos z &amp;= \\sum (-1)^k \\frac{z^{2k}}{(2k)!}\\\\ \\sinh z &amp;= \\sum\\frac{z^{2k+1}}{(2k+1)!}\\\\ \\cosh z &amp;= \\sum\\frac{z^{2k}}{(2k)!}\\end{align*}\\] <p>We will prove \\(e^z = \\sum \\frac{z^j}{j!}\\) from Euler's formula and series expansion from reals</p> \\[\\begin{align*} e^z &amp;= e^x(\\cos y + i\\sin y)\\\\     &amp;= \\sum\\frac{x^n}{n!}(\\sum(-1)^n\\frac{y^{2n+1}}{(2n+1)!} + \\sum(-1)^n\\frac{iy^{2n}}{(2n)!})\\\\     &amp;= \\sum\\frac{x^m}{m!}\\cdot \\sum i^n\\frac{y^n}{n!}\\\\     &amp;= \\sum_{k=0}^\\infty \\sum_{m=0}^{k}\\frac{x^m(iy)^{k-m}}{m!(k-m)!} &amp;\\text{diagonal sum}\\\\     &amp;= \\sum_{k=0}^\\infty \\frac{1}{k^!}\\sum_{m=0}^{k}k!\\frac{x^m(iy)^{k-m}}{m!(k-m)!}\\\\     &amp;= \\sum_{k=0}^\\infty \\frac{(x+iy)^k}{k^!}\\\\     &amp;= \\sum \\frac{z^k}{k!} \\end{align*}\\] <p>Using the series above, we can then derive for \\(\\sin z\\) as </p> \\[\\begin{align*} \\sin z &amp;= \\frac{1}{2i}(e^{iz} - e^{-iz})\\\\ &amp;= \\frac1{2i}\\big(\\sum\\frac{(iz)^n}{n!} - \\sum\\frac{(-iz)^n}{n!}\\big)\\\\ &amp;= \\frac{1}{2i} \\sum\\frac{2(iz)^{2k+1}}{(2k+1)!}\\\\ &amp;= \\sum\\frac{(i^2)^kz^{2k+1}}{(2k+1)!}\\\\ &amp;= \\sum\\frac{(-1)^kz^{2k+1}}{(2k+1)!} \\\\ \\cos z &amp;= \\frac{1}{2}(e^{iz} + e^{-iz})\\\\ &amp;= \\frac1{2}\\big(\\sum\\frac{(iz)^n}{n!} + \\sum\\frac{(-iz)^n}{n!}\\big)\\\\ &amp;= \\frac{1}{2} \\sum\\frac{2(i^2)^kz^{2k}}{(2k)!}\\\\ &amp;= \\sum\\frac{(-1)^kz^{2k}}{(2k)!}\\\\ \\end{align*}\\]"},{"location":"mat334/functions.html#stereographic-projection","title":"Stereographic Projection","text":"<p>Consider a unit sphere sitting on top of the complex plane with the south pole of the sphere located at the origin of the z plane. Connect the point \\(z\\) in the plane with the north pole using a straight line. This line intersects the sphere at the point \\(P\\). In this way each point \\(z= x +i y\\) on the complex plane corresponds uniquely to a point \\(P\\) on the surface of the sphere. This construction is called stereographic projection. The extended complex plane is sometimes referred as the compactified (closed) complex plane. </p> <p>Consider the 3 points \\(N = (0, 0, 2)\\) is the north pole of the ball, \\(P=(X,Y,Z)\\) be the points on the sphere, and \\(C = (x, y, 0)\\) be on the complex plane. Since they are on the same line, we have </p> \\[P-N = s(C-N)\\] <p>and note that \\(P\\) is on the sphere so that \\(X^2 + Y^2 + (Z-1)^2 = 1\\). Now we have 4 unknowns \\(X,Y,Z,s\\) and 4 equations, and it solves to be </p> \\[s = \\frac{4}{|z|^2 + 4}, X = \\frac{4x}{|z|^2 + 4}, Y = \\frac{4y}{|z|^2 + 4}, Z= \\frac{2|z|^2}{|z|^2 + 4}\\] <p>Similarly, if we have \\(X,Y,Z\\), we can solve to have</p> \\[s = \\frac{2-Z}2, x = \\frac{2X}{2-Z}, y = \\frac{2Y}{2-Z}\\] <p>Note that \\(P=(0, 0, 0)\\) maps to \\(z=0\\) and \\(\\lim_{|z|\\rightarrow\\infty}(X = \\frac{4x}{|z|^2 + 4}, Y = \\frac{4y}{|z|^2 + 4}, Z= \\frac{2|z|^2}{|z|^2 + 4}) = (0, 0, 2)\\) (all \\(z\\) with \\(|z|=\\infty\\) maps to the north pole). Any bounded region on the complex plane can be projected on the sphere, and any region, excluding the north pole, can be mapped to the complex plane. </p> <p>Example Consider the line \\(Re z = x = 0\\), the projected line on the sphere is </p> \\[X = 0, Y = \\frac{4y}{y^2 + 4}, Z = \\frac{2y^2}{y^2+4}\\]"},{"location":"mat334/integration.html","title":"Complex Integration","text":""},{"location":"mat334/integration.html#path-integrations","title":"Path Integrations","text":"<p>First, consider some \\(f: [a,b]\\rightarrow\\mathbb C, f(t) := u(t) + iv(t)\\). Assuming that \\(u,v\\) are real-valued functions and integrable on their domain, then \\(f\\) is integrable on \\([a,b]\\) as </p> \\[\\int_a^b f(t)dt = \\int_a^bu(t)dt + i\\int_a^b v(t)dt\\] <p>Note that with this definition, \\(f\\) follows fundamental theorem of calculus.</p> <p>Also, the basic rules for scaling by a constant real factor and summation still follows, namely For \\(f,g\\) be complex functions and \\(\\beta_0, \\beta_1\\) be real constant. </p> \\[\\int_a^b \\beta_0 f(t) + \\beta_1 g(t)dt = \\beta_0\\int_a^b f(t)dt + \\beta_1\\int_a^b g(t)dt\\]"},{"location":"mat334/integration.html#curves","title":"Curves","text":"<p>Let \\(c\\) be some parameterization, \\(c:[a,b]\\rightarrow \\mathbb C\\). Define a curve \\(C = \\{c(t): t\\in [a,b]\\}\\) as the image of some parameterization   - \\(C\\) is simple if there's no intersection \\(\\forall t_1, t_2\\in (a,b), t_1\\neq t_2\\implies c(t_1)\\neq c(t_2)\\).   - \\(C\\) is closed if \\(c(a) = c(b)\\)  - \\(C\\) is a Jordan Curve if it is simple and closed  - \\(C\\) is smooth if \\(c\\) is differentiable on \\([a,b]\\) and \\(c'\\) is continuous on \\([a,b]\\). </p> <p>Theorem (Jordan Curve Theorem) Let \\(C\\in\\mathbb C\\) be a Jordan curve. Then \\(\\mathbb C-C\\) consists of precisely 2 connected component, an interior component bounded by \\(C\\) and an exterior component unbounded. </p> <p>A function \\(f\\) is continuous on \\(C\\) if \\(f(z(t))\\) is continuous, \\(f\\) is piecewise continuous if \\([a,b]\\) if \\([a,b]\\) can be broken up into a finite number of subintervals in which \\(f(z)\\) is continuous on each interval. </p> <p>A contour is a curve consisting of finite number of connected smooth curves, i.e. a union of smooth pieces. A Jordan Contour is a closed simple contour. </p> <p>\\(A\\subseteq \\mathbb C\\)  is a simply connected area if for any simple closed curve \\(C\\subseteq A\\), \\(C_{int}\\subseteq A\\) (intuitively, no holes within \\(A\\)). </p>"},{"location":"mat334/integration.html#contour-integral","title":"Contour Integral","text":"<p>For a smooth contour \\(C\\) parameterized by \\(c\\) on \\([a,b]\\), the contour integral is defined as </p> \\[\\int_C f(z)dz = \\int_a^b f(c(t))c'(t)dt\\] <p>simply through substitution rule. </p>"},{"location":"mat334/integration.html#invariance-of-parameterization","title":"Invariance of Parameterization","text":"<p>Theorem The integral is invariant of the parameterization of \\(C\\) as long as orientation is preserved.  proof. Consider two orientation preserved parameterizations \\(c_1:[a,b]\\rightarrow \\mathbb C, c_2:[c,d]\\rightarrow \\mathbb C\\) of \\(C\\).  assume that   - \\(c_1([a,b]) = c_2([c,d])\\): their image is the same   - \\(c_1(a)=c_2(c), c_1(b) = c_2(d)\\): their orientation is the same </p> <p>Note that the two conditions above will give that there exists an increasing differentiable function \\(h:[c,d]\\rightarrow [a,b]\\) s.t. \\(c_2 = c_1\\circ h\\) (won't prove).  Therefore, we can check that </p> \\[\\begin{align*} \\int_c^d f(c_2(t))c_2'(t) &amp;= \\int_c^d f(c_1(h(t)))c_1'(h(t))h'(t)dt\\\\ &amp;= \\int_a^b f(c_1(u))c_1'(u)du &amp;u=h(t), du = h'(t)dt\\\\ \\end{align*}\\] <p>Theorem If \\(C_1, C_2\\) traces out the same curve but in opposite direction, then </p> \\[\\int_{C_1}f(z)dz = -\\int_{C_2}f(z)dz\\] <p>proof. From the above theorem, we can pick any parameterizations that is convenient. Let \\(c_1, c_2: [0,1]\\rightarrow\\mathbb C\\) be the parameterization of \\(C\\), but in different orientations, i.e. \\(c_2(t) = c_1(1-t)\\). so that </p> \\[\\begin{align*} \\int_0^1f(c_2(t))c_2'(t)dt &amp;= \\int_{0}^1 f(c_1(1-t))c_1'(1-t)(-1)dt \\\\ &amp;= \\int_1^0f(c_1(u))c_1'(u)du &amp;u=1-t, du=-tdt\\\\ &amp;= -\\int_0^1f(c_1(u))c_1'(u)du \\end{align*}\\] <p>Note that we can fix an orientation for Jordan contours \\(C\\) once for all, so that there's no ambiguity for the path integral defined on some curve. </p> <p>We fix the orientation as \"counterclockwise\", or more formally, as the curve is traversed, the interior is always on the left. With such convention, we write \\(\\oint_C\\) to specify such orientation. </p>"},{"location":"mat334/integration.html#ftc-ii-on-complex","title":"FTC II on Complex","text":"<p>Theorem (Fundamental Theorem of Calculus II) Let \\(f:R\\rightarrow\\mathbb C\\) be continuous on \\(R\\). If exists \\(F:R^0\\rightarrow\\mathbb C\\) s.t. \\(F\\) is analytic on \\(R\\) and \\(f(z) = F'(z)\\). Then, for a contour \\(C\\subseteq R^0\\) with endpoints \\(z_1,z_2\\), then </p> \\[\\int_C f(z)dz = F(z_2) - F(z_1)\\] <p>proof. (Note that the proof is almost identical to the proof in real number calculus)</p> \\[\\begin{align*} \\int_C f(z)dz &amp;= \\int_C F'(z)dt\\\\ &amp;= \\int_a^b F'(c(t))c'(t)dt\\\\ &amp;= \\int_a^b \\frac{dF}{dt}dt\\\\ &amp;= F(z(b)) - F(z(a)) \\end{align*}\\]"},{"location":"mat334/integration.html#corollary-1","title":"Corollary 1","text":"<p>If \\(C\\) is a closed, then \\(\\int_C f(z)dz = 0\\) for all \\(f\\) that satisfies the assumptions above. </p>"},{"location":"mat334/integration.html#example","title":"Example","text":"<p>Evaluate \\(\\int_C z^ndz\\) over \\(C:\\{e^{it}: t\\in[0, 2\\pi]\\}\\) counterclockwise. </p> \\[\\begin{align*} \\int_C z^n dz = \\int_0^{2\\pi} (e^{it})^nie^{it} dt= i\\int_0^{2\\pi}e^{i(n+1)t}dt \\end{align*}\\] <p>Note that when \\(n \\neq -1\\), we have</p> \\[\\int_0^{2\\pi}e^{i(n+1)t}dt = \\int_0^{2\\pi} cos((n+1)t)dt + i\\int_0^{2\\pi}\\sin((n+1)t)dt = 0\\] <p>However, when \\(n = -1\\), then </p> \\[i\\int_0^{2\\pi}e^{i(-1+1)t}dt = i\\int_0^{2\\pi}1 = 2\\pi i\\neq 0\\] <p>Note that the anti-derivative of \\(z^{-1}\\) is \\(\\log z\\) is not analytic unless we pick a branch of \\(\\log z\\). </p>"},{"location":"mat334/integration.html#ml-inequality","title":"ML inequality","text":"<p>Theorem Let \\(C = c([a,b])\\) be a contour, \\(f\\) be a function continuous on \\(C\\), \\(L\\) be the arc-length of \\(C\\) and \\(M&gt; 0\\) be the upper bound of \\(|f|\\), formally \\(\\forall t\\in[a,b]. M\\geq |f(c(t))|\\).  Then, \\(|\\int_C f(z)dz | \\leq ML\\). </p> <p>proof. Take some parameterizations of \\(C\\)</p> \\[\\begin{align*} |\\int_C f(z)dz| &amp;= |\\int_a^b f(c(t))c'(t)dt|\\\\ &amp;\\leq \\int_a^b |f(c(t))||c'(t)| dt\\\\ &amp;\\leq \\int_a^b M|c'(t)|dt\\\\ &amp;= M\\int_a^b|c'(t)|dt\\\\ &amp;= ML \\end{align*}\\]"},{"location":"mat334/integration.html#example-1","title":"Example 1","text":"<p>Let \\(R &gt; 1\\), let \\(C\\) be an arc of radius \\(R\\) and of angle \\(\\pi/3\\). </p> <p>(a) Show that \\(|\\int_C (z^3+1)^{-1}dz \\leq \\frac{\\pi}3\\frac{R}{R^3-1}\\). Note that \\(L = \\pi R/3\\), with ML inequality, it's sufficient to show that \\(|(z^3+1)^{-1}| \\leq \\frac{1}{R^3-1}\\) for all \\(z\\in C\\). By triangular inequality </p> \\[|z^3 + 1| \\geq ||z^3| - |1|| = |R^3 - 1| = R^3 - 1\\] <p>Therefore, \\(|(z^3 + 1)^{-1}| \\leq (R^3-1)^{-1}\\)</p>"},{"location":"mat334/integration.html#example-2","title":"Example 2","text":"<p>For \\(C_\\epsilon\\) be a circle of radius \\(\\epsilon\\) around the origin and \\(f\\) be analytic insider \\(C\\cup C_{int}\\), show that </p> \\[\\lim_{\\epsilon \\rightarrow 0} \\oint_{C_{\\epsilon}}z^a f(z)dz= \\lim_{\\epsilon\\rightarrow} I_\\epsilon = 0, \\forall a &gt; -1\\] <p>proof. Note that \\(C\\cup C_{int}\\) is a compact set, since \\(f\\) is continuous, the image of \\(f\\) is bounded. Take \\(M &gt; 0\\) s.t. \\(\\forall z \\in C\\cup C_{int}. M\\geq |f(z)|\\). Then, note that \\(\\forall z \\in C_{\\epsilon}. |z^a| = \\epsilon^a &gt; 0\\). By ML inequality, \\(|I_\\epsilon| \\leq \\epsilon^{a+1}M\\), define \\(b=a+1 &gt; 0\\), \\(|I_\\epsilon| \\leq \\epsilon^{b}M\\) Let \\(\\epsilon &gt; 0\\), take \\(\\delta = (\\frac{\\epsilon}M)^{1/b}\\) so that \\(I_\\delta \\leq \\epsilon\\)</p>"},{"location":"mat334/integration.html#example-3","title":"Example 3","text":"<p>\\(I_R = \\int_{C_R} \\frac{e^{iz}}{z^2}dz\\), \\(C_R = \\{e^{it}: t\\in[0, \\pi]\\}\\). Show that \\(\\lim_{R\\rightarrow \\infty}I_R = 0\\). </p> \\[\\begin{align*} |\\int_{C_R} \\frac{e^{iz}}{z^2}dz| &amp;= |\\int_0^\\pi \\frac{\\exp(iRe^{it})}{(Re^{it})^2}iRe^{it}dt|\\\\ &amp;= |i||\\int_0^\\pi\\frac{\\exp(iRe^{it})}{Re^{it}}dt|\\\\ &amp;\\leq \\int_0^\\pi |R^{e^{it}}|^{-1} |\\exp(iRe^{it})|dt\\\\ &amp;= \\int_0^\\pi R^{-1}|e^{iR\\cos t}e^{-R\\sin t}|dt\\\\ &amp;= R^{-1}\\int_0^\\pi e^{-R\\sin t}dt \\end{align*}\\] <p>Note that \\(e^{-R\\sin t}=(e^{-\\sin t})^{R} \\leq 1\\) for \\(0\\leq t\\leq \\pi, R &gt; 1\\), so that </p> \\[|\\int_{C_R} \\frac{e^{iz}}{z^2}dz| \\leq R^{-1}\\int_0^\\pi dt = \\frac{\\pi}{R}\\] <p>Therefore, </p> \\[\\lim_{R\\rightarrow\\infty}I_R = \\lim_{R\\rightarrow\\infty}\\frac{\\pi}R = 0\\]"},{"location":"mat334/intro.html","title":"Introducing Complex Numbers","text":""},{"location":"mat334/intro.html#imaginary-unit-number-and-complex-plane","title":"Imaginary Unit Number and Complex plane","text":"<p>Define the imaginary unit number (Euler's notation) as \\(i^2 = -1\\).  Then a complex number is an expression of the form \\(z=x+iy\\) where </p> <ul> <li>\\(\\text{Re}(z):=x\\) is the real part of \\(z\\) </li> <li>\\(\\text{Im}(z):=y\\) is the imaginary part of \\(z\\). </li> </ul> <p>If \\(y=0\\), then \\(z\\) is real, if \\(x=0\\), then \\(z\\) is pure imaginary. </p> <p>Therefore, we can define the set of complex numbers as </p> \\[\\mathbb C = \\{x+iy:x,y\\in\\mathbb R\\}\\] <p>Geometrically, each complex number can be placed on a 2-D coordinate system \\((x, y)\\), such system is defined as a complex plane. </p> <p>\u200b </p> <p>Note that \\(z\\) can also be represented by the polar coordinate</p> \\[x = r\\cos\\theta, y = r\\sin\\theta\\] <p>so that the complex number \\(z\\) can also reside in the polar coordinate </p> \\[z = x+iy = r(\\cos\\theta+i\\sin\\theta)\\] <p>And we can define the modulus of \\(z\\) as its absolute value</p> \\[|z|:= r = \\sqrt{x^2+y^2}\\] <p>and the argument of \\(z\\) is defined by its angle when \\(z\\neq 0\\)</p> \\[\\arg z := \\theta = \\arctan(y/x), z\\neq 0\\] <p>Note that \\(\\theta\\) is periodic with period \\(\\pi\\), which means it's multivalued. </p>"},{"location":"mat334/intro.html#geometry-of-arithmetic","title":"Geometry of Arithmetic","text":"<p>Using the complex plane, we can better understand the \"addition\" and \"multiplication\". Let \\(z, w\\) be two complex variables. Then, addition is the addition of their vectors in the complex plane. </p> <p>\u200b </p> <p>Multiple \\(z\\) by \\(i\\) will rotate \\(z\\) counterclockwise by 90 degrees.</p> \\[iz = i(a+ib) = -b + ai\\] <p>\u200b </p> <p>Multiple \\(z\\) by \\(c\\in\\mathbb R\\) simply scales the vector. </p> \\[cz = ca+icb\\] <p>Multiple \\(z=a+bi, w=c+di\\in\\mathbb C\\) gives</p> \\[zw = (a+bi)w = aw+biw\\] <p>Geometrically, it scales \\(z\\) by \\(|w|\\) and rotates from \\(w\\) counterclockwise for \\(\\theta\\). </p> <p>\u200b </p>"},{"location":"mat334/intro.html#polar-exponential","title":"Polar Exponential","text":"<p>Define the polar exponential by </p> \\[\\cos\\theta + i\\sin\\theta = e^{i\\theta}\\] <p>So that \\(z\\) can be written in the polar exponential form as </p> \\[z = r(\\cos\\theta+i\\sin\\theta) = re^{i\\theta}\\] <p>Note that this notation is so convenient as we can prove exponential properties from trigonometric identities. </p> <p>Theorem</p> \\[\\begin{align*} e^{i\\theta_1} e^{i\\theta_2} &amp;= (\\cos\\theta_1 + i\\sin\\theta_1)(\\cos\\theta_2 + i\\sin\\theta_2)\\\\ &amp;= \\cos\\theta_1\\cos\\theta_2+i(\\cos\\theta_1\\sin\\theta_2 + \\cos\\theta_2\\sin\\theta_1) + i^2\\sin\\theta_1\\sin\\theta_2\\\\ &amp;= (cos\\theta_1\\cos\\theta_2 - \\sin\\theta_1\\sin\\theta_2) + i\\sin(\\theta_1+\\theta_2) + (i^2+1)\\sin\\theta_1\\sin\\theta_2\\\\ &amp;= \\cos(\\theta_1 + \\theta_2) + i\\sin(\\theta_1+\\theta_2) + 0\\\\ &amp;= e^{(\\theta_1+\\theta_2)i}\\\\ (e^{i\\theta})^m &amp;= e^{im\\theta}\\\\ (e^{i\\theta})^{1/n} &amp;= e^{\\frac{i\\theta}n} \\end{align*}\\] <p>Example complex number to polar exponential form (since \\(\\theta\\) is periodic, we will only take values in \\(0&lt;\\theta\\leq 2\\pi\\))</p> \\[\\begin{align*} 1 &amp;\\Rightarrow r = \\sqrt{1^2+0^2} = 1, \\theta = \\arctan(0) = 2\\pi \\Rightarrow e^{i2\\pi}\\\\ -i &amp;\\Rightarrow r = \\sqrt{0^2+(-1)^2} = 1, \\theta = \\arctan(-\\infty) = \\frac{3\\pi}2 \\Rightarrow e^{i\\frac32\\pi}\\\\ 1+i &amp;\\Rightarrow r = \\sqrt{1^2+1^2} = \\sqrt2, \\theta = \\arctan(1) = \\frac{\\pi}4 \\Rightarrow \\sqrt2e^{i\\frac\\pi4}\\\\ \\frac12+\\frac{\\sqrt3}2i &amp;\\Rightarrow r = \\sqrt{(1/2)^2+(\\sqrt{3}/2)^2} = 1, \\theta = \\arctan(\\sqrt3) = \\frac{\\pi}3 \\Rightarrow e^{i\\frac\\pi3}\\\\ \\frac12-\\frac{\\sqrt3}2i &amp;\\Rightarrow r = \\sqrt{(1/2)^2+(-\\sqrt{3}/2)^2} = 1, \\theta = \\arctan(-\\sqrt3) = \\frac{\\pi}3 \\Rightarrow e^{i\\frac{5\\pi}3}\\\\ \\end{align*}\\] <p>Example polar exponential form to \\(x+yi\\)</p> \\[\\begin{align*} e^{2+i\\pi/2}&amp;\\Rightarrow x^2 + y^2  = e^4, \\frac{y}x=\\tan(\\frac{\\pi}2)=\\infty\\Rightarrow e^2i\\\\  &amp;\\text{or} \\Rightarrow e^2e^{i\\frac\\pi2} = e^2i\\\\ (1+i)^{-1}&amp;= (e^{i2\\pi} + e^{i\\pi/2})^{-1} = e^{-i\\frac52\\pi}=e^{i\\frac32\\pi} = -i\\\\ (1+i)^3 &amp;= e^{i\\frac{15}{2}\\pi} = e^{i\\frac32\\pi} = -i\\\\ |3+4i| &amp;= \\sqrt{3^2+4^2} = 5\\\\ \\end{align*}\\] <p>Example Define \\(\\cos z = \\frac{e^{iz} + e^{-iz}}2, e^z = e^{x}e^{iy}\\), evaluate \\(\\cos(c+i\\frac{\\pi}4)\\) where \\(c\\in\\mathbb R\\)</p> \\[\\begin{align*} \\cos(c+i\\frac{\\pi}4) &amp;= \\frac12\\big(\\exp(i(c+i\\frac\\pi4)) + \\exp(-i(c+i\\frac\\pi4))\\big)\\\\ &amp;= \\frac{1}2\\big(e^{ic + e^{i^2}\\frac{\\pi}4} + e^{-ic-i^2\\frac\\pi4}\\big)\\\\ &amp;= \\frac12\\big(e^{ic-\\pi/4} + e^{\\pi/4 - ic}\\big)\\\\ &amp;= \\frac12\\big(e^{ic}e^{-\\pi/4} + e^{-ic}e^{\\pi/4}\\big)\\\\ &amp;= \\frac12\\big((\\cos c+i\\sin c)e^{-\\pi/4} + (\\cos(-c) +i \\sin(-c))e^{\\pi/4}\\big)\\\\ &amp;= \\frac12\\big(e^{-\\pi/4}\\cos c+ e^{\\pi/4}\\cos(-c)\\big)+ i\\frac12\\big(e^{-\\pi/4}\\sin c + e^{\\pi/4}\\sin(-c)\\big) \\end{align*}\\]"},{"location":"mat334/intro.html#powers-of-complex-numbers","title":"Powers of Complex Numbers","text":"<p>Consider an equation of the form for \\(z, w\\in\\mathbb C, n\\in\\mathbb N^+\\)</p> \\[z^n = w = |w|e^{i\\theta} = |w|e^{i(\\theta + 2\\pi m)}, m\\in\\mathbb N\\] <p>so that </p> \\[z = |w|^{1/n} e^{i(\\theta + 2\\pi m)/n}, m\\in\\mathbb N\\] <p>Since the period is \\(2\\pi\\), this will yield \\(n\\) unique roots. </p> <p>Example </p> \\[\\begin{align*} z^3 &amp;= 4 \\\\ z^3 &amp;= 4e^{i2\\pi} \\\\ z &amp;= \\{4^{1/3}e^{i\\frac{2\\pi}3}. 4^{1/3}e^{i\\frac{4\\pi}3}, 4^{1/3}e^{i2\\pi}\\} \\end{align*}\\] \\[\\begin{align*} z^4 &amp;= -1 \\\\ z^4 &amp;= e^{i\\pi} \\\\ z &amp;= \\{e^{i\\frac{\\pi}{4}}, e^{i\\frac{3\\pi}{4}}, e^{i\\frac{5\\pi}{4}}, e^{i\\frac{7\\pi}{4}}\\} \\end{align*}\\] \\[\\begin{align*} (az+b)^3 &amp;= c;\\:a,b,c \\in\\mathbb R^+\\\\ (az+b)^3 &amp;= |c|e^{i2\\pi}\\\\ az+b &amp;= \\{c^{1/3}e^{i\\frac{2k\\pi}{3}}: k = 0, 1, 2\\}\\\\ z &amp;= \\{\\frac{1}a(c^{1/3}e^{i\\frac{2k\\pi}{3}} -b): k = 0, 1, 2\\} \\end{align*}\\] \\[\\begin{align*} z^4+2z^2+2 =0\\\\ (z^2+1)^2&amp;=-1\\\\ z^2+1 &amp; =\\pm i\\\\ z^2_1 &amp;= -1 + i\\\\ z_1^2 &amp;= 2^{1/2}e^{i\\frac{3\\pi}4}\\\\ z_1 &amp;= \\{2^{1/4}e^{i\\frac{3\\pi}{8}}, 2^{1/4}e^{i\\frac{11\\pi}{8}} \\}\\\\ z_2^2 &amp;= -1-i\\\\ z_2 &amp;= \\{2^{1/4}e^{i\\frac{\\pi}{8}}, 2^{1/4}e^{i\\frac{9\\pi}{8}} \\}\\\\ \\Rightarrow z &amp;= \\{2^{1/4}e^{i\\frac{\\pi}8}, 2^{1/4}e^{i\\frac{3\\pi}8}, 2^{1/4}e^{i\\frac{9\\pi}8}, 2^{1/4}e^{i\\frac{11\\pi}8}\\} \\end{align*}\\]"},{"location":"mat334/intro.html#arithmetic-operations","title":"Arithmetic Operations","text":"<p>Equivalence \\(z_1 = z_2\\) IFF their real and imaginary parts are respectively equal, i.e. </p> \\[z_1 x_1+iy_1 = x_2 + iy_2=z_2 \\iff x_1=x_2\\land y_1=y_2\\] <p>Also note that given the definition as \\(\\mathbb C = \\{a+ib:a,b\\in\\mathbb R\\}\\), Addition, subtraction, multiplication, division follows the same rules as real numbers. and the commutative, associative, distributive laws of addition and multiplication hold.</p>"},{"location":"mat334/intro.html#complex-conjugate","title":"Complex Conjugate","text":"<p>For \\(z = x+iy =re^{i\\theta}\\), define its complex conjugate as \\(\\bar z = x - iy = re^{-i\\theta}\\).  Consequently, we can have the following results,</p> \\[z\\bar z = \\bar z z= (x+iy)(x-iy) =|z|^2\\] <p>and the division can be better viewed as</p> \\[\\begin{align*} \\frac{z_1}{z_2} &amp;= \\frac{z_1\\bar{z_2}}{z_2\\bar{z_2}} =  \\frac{z_1\\bar{z_2}}{|z_2|^2} = \\frac{x_1x_2+y_1y_2}{x_2^2+y_2^2} + i\\frac{x_2y_1-x_1y_2}{x_2^2+y_2^2} \\end{align*}\\]"},{"location":"mat334/intro.html#results-of-elementary-functions","title":"Results of Elementary Functions","text":"<p>Let \\(z=a+ib, w =c+id\\in\\mathbb C\\),</p> <p>Theorem \\(\\overline{z+w} = \\bar z + \\bar w\\) proof \\(z+w=(a+c)+i(b+d), \\overline{z+w}= (a+c) -i(b+d) = (a-ib)+(c-id) =  \\bar z + \\bar w\\)</p> <p>Theorem \\(|z-w| \\leq |z|+|w|\\) proof \\(|z-w|^2 = (a-c)^2 + (b-d)^2 \\leq a^2+b^2 + c^2 + d^2 = |z|^2 + |w|^2\\), since the absolute values are all real numbers, it follows triangle inequality that \\(|z|^2 + |w|^2 \\leq (|z|+|w|)^2\\) Given \\(|z-w| \\geq 0, |z|+|w|\\geq 0, |z-w|\\leq |z|+|w|\\)</p> <p>Theorem \\(z-\\bar z = 2i Im(z)\\) proof \\(z-\\bar z = a+ib-(a-ib) = 2ib\\)</p> <p>Theorem \\(Re(z)\\leq |z|\\) proof \\(a \\leq \\sqrt{a^2+b^2}\\)</p> <p>Theorem \\(|wz| = |w||z|\\) proof We will equivalently prove \\(|wz|^2 = |w|^2|z|^2\\) \\(|wz|^2 = (ac-bd)^2 + (ad+bc)^2 = \\cdots = (a^2+b^2)(c^2+d^2)\\)</p>"},{"location":"mat334/intro.html#triangular-inequality","title":"Triangular Inequality","text":"\\[|z+w|\\leq |z|+|w|\\] <p>proof First note that </p> \\[|z+w|^2 = (z+w)(\\bar z+ \\bar w) = |z|^2+|w|^2 + z\\bar w + \\bar z w\\] <p>Then, </p> \\[\\begin{align*} z\\bar w + \\bar z w &amp;= (ac+bd) + i(ad-bc) + (ac+bd) + i(bc-ad) \\\\&amp;= 2(ac+bd)\\\\ &amp;= 2Re(z\\bar w) \\end{align*}\\] <p>Using the results above, </p> \\[Re(z\\bar w) \\leq |z\\bar w | = |z||w|\\] <p>So that \\(|z+w|^2 \\leq |z|^2 + 2|z||w| + |w|^2 = (|z| + |w|)^2\\), and take a square root, we have the right inequality. </p> <p>Note that the reverse triangular inequality is a corollary of triangular inequality, which </p> \\[||z|-|w||\\leq |z+w|\\leq |z|+|w|\\] <p>Theorem \\(|w\\bar z + \\bar w z| \\leq 2|wz|\\) proof First note that \\(|z| = |\\bar z|\\) as \\(a^2+b^2 = a^2 + (-b)^2\\), then by trig-inequality and the theorem above,  \\(|w\\bar z + \\bar w z| \\leq |w\\bar z| + |\\bar w z| = 2|w||z| = 2|wz|\\)</p>"},{"location":"mat334/jl.html","title":"Jordon's Lemma","text":""},{"location":"mat334/jl.html#claim-1","title":"Claim 1","text":"<p>If \\(zf(z)\\rightarrow^{unif.} 0\\) as \\(z\\rightarrow\\infty\\), then </p> \\[\\lim_{R\\rightarrow\\infty} \\int_{C(R,0,\\pi)} f(z)dz = 0\\] <p>where \\(C(R,0,\\pi)\\) is the top semicircle arc of radius \\(R\\) centered at \\(0\\), rotated ccw. </p> <p>proof. Note that on the arc, \\(|z| = R\\) so that \\(|zf(z)| = R |f(z)| \\leq \\kappa(R)\\) so that  \\(|f(z)| \\leq \\frac{\\kappa(R)}{R}\\). </p> <p>Let \\(\\epsilon &gt; 0\\). By definition of \\(\\kappa\\), take \\(S\\) s.t. \\(\\forall R &gt; S. \\kappa(R) &lt; \\frac{\\epsilon}{\\pi}\\). Then, with ML inequality we have</p> \\[|\\int_{C(R,0,\\pi)} f(z)dz| \\leq \\pi R \\frac{\\kappa(R)}{R} &lt; \\epsilon\\]"},{"location":"mat334/jl.html#example-1","title":"Example 1","text":"<p>Compute \\(\\int_{-\\infty}^{\\infty} (x^2 + 1)^{-1}dx\\)</p> <p>\u200b </p> <p>For any \\(R &gt; 0\\), expanding \\(f\\) to the complex domain, and consider </p> <p>$$\\int_{C} \\frac{1}{z^2 + 1}dz = 2\\pi i Res(f, i) = 2\\pi i \\frac{1}{2 i} = \\pi $$ Also we have </p> \\[\\int_{C} f(z)dz = \\int_{C^{line}_R} f(z)dz + \\int_{C(R,0,\\pi)}f(z)dz\\] <p>Therefore, we can write</p> \\[\\lim_{R\\rightarrow\\infty} \\int_{C} \\frac{1}{z^2 + 1}dz = \\lim_{R\\rightarrow\\infty} \\int_{C^{line}_R} f(z)dz + \\lim_{R\\rightarrow\\infty} \\int_{C(R,0,\\pi)}f(z)dz = \\pi\\] <p>Consider the latter term, because \\(zf(z) = \\frac{z}{z^2+1}\\rightarrow^{unif} 0\\) as \\(z\\rightarrow\\infty\\), so that \\(\\lim_{R\\rightarrow\\infty} \\int_{C(R,0,\\pi)}f(z)dz = 0\\) Note that </p> \\[\\lim_{R\\rightarrow\\infty} \\int_{C^{line}_R} f(z)dz = \\lim_{R\\rightarrow\\infty} \\int_{-R}^R f(x)dx = \\int_{-\\infty}^\\infty f(x)dx\\] <p>so that </p> \\[\\pi = \\int_{-\\infty}^\\infty \\frac{1}{x^2 + 1}dx + 0\\]"},{"location":"mat334/jl.html#jordons-lemma_1","title":"Jordon's Lemma","text":"<p>IF \\(f(z)\\rightarrow^{unif} 0\\) as \\(z\\rightarrow \\infty\\) and \\(k &gt; 0\\) then </p> \\[\\lim_{R\\rightarrow\\infty} \\int_{C(R, 0, \\pi) } e^{ikz}f(z)dz = \\lim_{R\\rightarrow\\infty} \\int_{C(R, -\\pi, 0) }e^{ikz}f(z)dz = 0\\] <p>proof. parameterize \\(C(R,0,\\pi)\\) with \\(c(t):[0,\\pi] \\rightarrow \\mathbb C, c(t)= Re^{it}\\)</p> \\[\\begin{align*} I_R&amp;:=|\\int_{C(R,0,\\pi)} e^{ikz} f(z)dz|\\\\  &amp;= |\\int_0^\\pi e^{ikRe^{it}} f(Re^{it}) iRe^{it}dt|\\\\ &amp;= R|\\int_0^\\pi e^{kR(i\\cos t- \\sin t)} f(Re^{it}) ie^{it}dt|\\\\ &amp;\\leq R \\int_0^\\pi |e^{kR(i\\cos t)}|e^{-kR\\sin t}||ie^{it}| |f(Re^{it})| dt\\\\ &amp;= R \\int_0^\\pi e^{-kR\\sin t}|f(Re^{it})| dt\\\\ &amp;\\leq   R \\kappa(R) \\int_0^\\pi e^{-kR\\sin t}  dt\\\\ &amp;= R \\kappa(R) 2 \\int_0^{\\pi/2} e^{-kR\\sin t}  dt\\\\ &amp;\\leq R \\kappa(R) 2 \\int_0^{\\pi/2} e^{-kR\\frac{2t}{\\pi}}  dt &amp;\\sin t\\geq \\frac{2t}{\\pi}. \\forall t \\in [0, \\pi/2]\\\\ &amp;= 2R\\kappa(R) \\frac{\\pi (1-e^{-\\pi R})}{2Rk}\\\\ &amp;\\leq \\frac{\\pi}{k} \\kappa(R) \\end{align*}\\] <p>Since \\(\\lim_{R\\rightarrow\\infty} \\kappa(R) = 0, \\lim_{R\\rightarrow\\infty} |I_R|  = 0\\)</p> <p>The proof for the second equation is similar</p>"},{"location":"mat334/jl.html#example","title":"Example","text":"<p>\\(\\int_{-\\infty}^\\infty \\frac{\\cos x}{x^2+1}\\)</p> <p>First, expand to complex domain, </p> \\[\\frac{\\cos z}{z^2+1} = \\frac{1}{2}\\frac{e^{iz}}{x^2+1} + \\frac{1}{2}\\frac{e^{-iz}}{x^2+1}\\] <p>Let \\(f_1(z) = \\frac{e^{iz}}{x^2+1}, f_2(z) = \\frac{e^{-iz}}{x^2+1}\\). We can compute \\(f_1\\) on the top semicircle, and \\(f_2\\) on the bottom semicircle. </p> \\[\\begin{align*} \\lim_{R\\rightarrow\\infty}\\oint_C f_1(z)dz &amp;= \\lim_{R\\rightarrow\\infty}\\int_{C^{line}_R} f_1(z)dz + \\lim_{R\\rightarrow\\infty}\\int_{C(R, 0, \\pi)} f_1(z)dz\\\\ Res(f_1, i) &amp;= \\int_{-\\infty}^\\infty \\frac{e^{iz}}{x^2+1}dx + \\lim_{R\\rightarrow\\infty}\\int_{C(R, 0, \\pi)} e^{iz}\\frac{1}{x^2+1}dz\\\\ 2\\pi i\\lim_{z\\rightarrow i} \\frac{e^{iz}}{z+i} &amp;= \\int_{-\\infty}^\\infty \\frac{e^{iz}}{x^2+1}dx + 0&amp;\\text{Jordon's Lemma}\\\\ \\frac{\\pi}{e} &amp;=  \\int_{-\\infty}^\\infty \\frac{e^{iz}}{x^2+1}dx \\end{align*}\\] <p>Simiarly,</p> \\[\\begin{align*} \\lim_{R\\rightarrow\\infty}\\oint_C f_2(z)dz &amp;= \\lim_{R\\rightarrow\\infty}\\int_{-C^{line}_R} f_2(z)dz + \\lim_{R\\rightarrow\\infty}\\int_{C(R, -\\pi, 0)} f_2(z)dz\\\\ Res(f_2, -i) &amp;= -\\int_{-\\infty}^\\infty \\frac{e^{-iz}}{x^2+1}dx + \\lim_{R\\rightarrow\\infty}\\int_{C(R, -\\pi, 0)} e^{-iz}\\frac{1}{x^2+1}dz\\\\ 2\\pi i\\lim_{z\\rightarrow -i} \\frac{e^{-iz}}{z-i} &amp;= -\\int_{-\\infty}^\\infty \\frac{e^{-iz}}{x^2+1}dx + 0&amp;\\text{Jordon's Lemma}\\\\ \\frac{\\pi}{e} &amp;=  \\int_{-\\infty}^\\infty \\frac{e^{-iz}}{x^2+1}dx \\end{align*}\\] <p>Therefore, </p> \\[\\frac{\\cos z}{z^2+1} = \\frac{1}{2}(2\\frac{\\pi}{e}) = \\frac{\\pi}{e}\\] <p>Another convinient way is to notice </p> \\[\\int_{-\\infty}^\\infty \\frac{e^{ix}}{x^2+1}dx =\\int_{-\\infty}^\\infty \\frac{\\cos x}{x^2+1}dx + i\\int_{-\\infty}^\\infty \\frac{\\sin x}{x^2+1}dx \\] <p>Because \\(\\frac{\\sin x}{x^2+1}\\) is odd around 0, \\(\\int_{-\\infty}^\\infty \\frac{\\sin x}{x^2+1}dx = 0\\) and we are left with </p> \\[\\frac{\\pi}{e} = \\int_{-\\infty}^\\infty \\frac{e^{ix}}{x^2+1}dx =\\int_{-\\infty}^\\infty \\frac{\\cos x}{x^2+1}dx\\]"},{"location":"mat334/multi_val_func.html","title":"Multivalued Functions","text":""},{"location":"mat334/multi_val_func.html#square-root-function","title":"Square Root Function","text":"<p>Consider the simplest example, for \\(z = w^2\\), we can write its inverse as \\(w = z^{1/2}\\), we know that its \\(z^{1/2}\\) have values as </p> \\[z^{1/2} \\equiv \\sqrt r e^{i\\frac{\\theta + 2\\pi n }{2}}, n = 0, 1, 2, ...\\] <p>Which takes to be 2 distinct values \\(\\pm \\sqrt r e^{i\\theta/2}\\). </p> <p>Therefore, if we define some function f as one root of the square root function, say \\(f(re^{i\\theta}) = \\sqrt{r} e^{i\\theta/2}\\). Consider a path traverse through the circle of radius \\(r\\) around \\(0\\), </p> \\[c:[0, 2\\pi)\\rightarrow \\mathbb C, c(t) = re^{it}, c(0) = c(2\\pi) = r\\] <p>Then, consider any \\(t\\in(0, 2\\pi)\\), \\(f(c(t))\\) is continuous. However, when \\(f\\) goes along the path and approaches \\(2\\pi\\), we have </p> \\[\\lim_{t\\rightarrow 2\\pi} f(c(t)) = \\sqrt re^{i\\pi} = -\\sqrt r\\neq \\sqrt r = f(c(0))\\] <p>\\(f\\) does not return to its original value. </p> <p>If we look at another example (fig.2), consider a path traverse through the circle around \\(1.1+1.1i\\). \\(f\\) goes back to the original value. </p> Source code <pre><code>import matplotlib.pyplot as plt\nimport numpy as np\n\ndef angle(z):\n    # since np.angle uses y-axis as branch cut, \n    # we need to change it to x-axis\n    theta_pos = np.angle(z) * (z.imag &gt; 0)\n    theta_neg = (np.angle(z) + 2 * np.pi) * (z.imag &lt; 0)\n    return theta_pos + theta_neg\ndef sqrt(z):\n    # define our sqrt function with branch cut y=0\n    return np.abs(z)**.5 * np.exp(1j * (angle(z)/2))\n\nt = np.arange(0, 2 * np.pi, np.pi/100)\nc = np.array([\n    1.2 * np.exp(1j * t[:, np.newaxis]),\n    1.1 + np.exp(1j * t[:, np.newaxis]),\n    1.1j + np.exp(1j * t[:, np.newaxis]),\n    1.1 + 1.1j + np.exp(1j * t[:, np.newaxis]),\n])\nc_func = [\n    '$c(t) = 1.2e^{it}$',\n    '$c(t) = 1.1 + e^{it}$',\n    '$c(t) = 1.1i + e^{it}$',\n    '$c(t) = 1.1 + 1.1i + e^{it}$'\n]\nfc = sqrt(c)\nfig = plt.figure(figsize=(16, 4))\nfig.suptitle(r\"f defined as $\\sqrt{r} e^{i\\theta/2}$\", y=0)\nfor i in range(c.shape[0]):\n    plt.subplot(1, 4, 1+i)\n    plt.xlim(-3, 3); plt.ylim(-3, 3)\n    plt.gca().set_aspect('equal')\n    plt.axhline(c=\"grey\", ls=\"--\"); plt.axvline(c=\"grey\", ls=\"--\"); \n    plt.scatter(c[i].real, c[i].imag, s=.5); plt.scatter(fc[i].real, fc[i].imag, s=.5)\n    plt.title(f\"fig{i}. \" + c_func[i])\nplt.savefig(\"../assets/multi_val_func_01.jpg\")\n</code></pre> <p>\u200b </p> <p>Note that for the curve traversed by \\(c(t)= 1.1+e^{it}\\), there exists some other definition of \\(f\\) as square root function such that the image of \\(f\\) can be closed. </p>"},{"location":"mat334/multi_val_func.html#branch","title":"Branch","text":"<p>Therefore, we define a branch point for a multivalued function \\(f\\) as   - (intuitively) \\(f\\) is discontinuous upon traversing a small circuit around the point  - (more formally) \\(z_0\\) is a branch point if there is no domain \\(D\\) on which a continuous single value functions that is defined which contains \\(B_\\delta(z_0) - \\{z_0\\}\\) for any \\(\\delta &gt; 0\\). </p> <p>For example, the square root function have branch points \\(0, \\infty\\). </p> <p>A branch of a multivalued function is a single valued continuous function defined on a restricted region. </p> <p>For functions with a single branch point, \\(z_0\\), a branch cut is a curve \\(p:[0,\\infty)\\rightarrow \\mathbb C\\), s.t. \\(p(0) = z_0, p(\\infty) = \\infty\\), so that a branch can be defined on \\(\\mathbb C - p([0,\\infty))\\)</p>"},{"location":"mat334/multi_val_func.html#logarithm-function","title":"Logarithm Function","text":"<p>Consider the inverse of exponential function \\(w = f(z)\\) s.t. \\(e^w = z\\). </p> \\[\\begin{align*} e^w &amp;= z\\\\ e^{u+iv} &amp;= re^{i\\theta} &amp;u,v\\in\\mathbb R, r &gt; 0, \\theta\\in [0, 2\\pi)\\\\ e^ue^{iv} &amp;= re^{i\\theta}\\\\ e^u = r&amp;\\Rightarrow u = \\log(r)\\\\ v &amp;= \\theta + 2\\pi n\\\\ \\Rightarrow w &amp;= log(r) + i(\\theta + 2\\pi n) &amp;n\\in\\mathbb Z \\end{align*}\\] <p>This is multivalued, and in fact, it has infinite distinct values for each \\(n\\) chosen. </p> <p>Conveniently, we take \\(n = 0\\) and defines the principal branch of \\(\\log\\) on the domain \\(D = \\{re^{i\\theta} :r &gt; 0, \\theta \\in [0, 2\\pi)\\}\\) as </p> \\[\\log: D\\rightarrow \\mathbb C, \\log(re^{i\\theta}):= \\log r + i\\theta\\]"},{"location":"mat334/multi_val_func.html#power-function","title":"Power Function","text":"<p>Note that the power function can be defined through </p> \\[z^a = (e^{\\log z})^a = e^{a\\log z}\\] <p>Consider \\(a = n\\in\\mathbb Z\\), </p> \\[\\begin{align*} \\exp({n\\log(re^{i\\theta}))} &amp;\\equiv \\exp(n(\\log r + i\\theta + i2\\pi k)) \\\\ &amp;= e^{n\\log r} \\cdot e^{in\\theta} \\cdot e^{i2\\pi (nk)} \\\\ &amp;= r^n\\cdot e^{in\\theta}\\cdot 1 \\\\ &amp;= r^n (e^{i\\theta})^n \\\\ &amp;= (re^{i\\theta})^n \\end{align*}\\] <p>it is uniquely defined </p> <p>However, for \\(a = n^{-1}\\), </p> \\[\\begin{align*} \\exp({n^{-1}\\log(re^{i\\theta}))} &amp;\\equiv \\exp(n^{-1}(\\log r + i\\theta + i2\\pi k)) \\\\ &amp;= e^{\\frac{\\log r}{n}} \\cdot e^{i\\frac{\\theta}n} \\cdot e^{i \\frac{2\\pi k}{n}} \\\\ &amp;= r^{1/n}\\cdot e^{i\\frac{\\theta}n}\\cdot e^{i \\frac{2\\pi k}{n}}  \\\\ &amp;= (re^{i\\theta})^{1/n}e^{i \\frac{2\\pi k}{n}} \\end{align*}\\] <p>it has \\(n\\) different values</p> <p>Finally, for \\(a = i\\), </p> \\[\\begin{align*} \\exp(i\\log z) &amp;\\equiv \\exp(i(\\log r + i\\theta + i2\\pi k)) \\\\ &amp;=e^{i\\log r} e^{-\\theta}e^{-2\\pi k} \\end{align*}\\] <p>it has infinitely many values</p>"},{"location":"mat334/multi_val_func.html#example","title":"Example","text":"<p>For \\(a\\in\\mathbb R\\). Show that the set of all values of \\(\\log(z^a)\\) is not necessarily the same as \\(a\\log(z)\\). </p> <p>Consider \\(z = re^{i\\theta}\\) for \\(r &gt; 0, \\theta \\in [0, 2\\pi)\\). </p> \\[\\begin{align*} \\log(z^a) &amp;= \\log(r^ae^{ia\\theta}) = a\\log(r) + i(a\\theta + 2\\pi k)\\\\ a\\log(z) &amp;=  a\\log(re^{i\\theta}) = a\\log(r) + ia(\\theta + 2\\pi k) \\end{align*}\\] <p>Note that \\(\\{(a\\theta + 2\\pi ak):k\\in\\mathbb Z\\} \\neq \\{a\\theta + 2\\pi k):k\\in\\mathbb Z\\}\\)</p>"},{"location":"mat334/power_series.html","title":"Power Series and Larent Series","text":""},{"location":"mat334/power_series.html#series","title":"Series","text":"<p>For definitions of series, check real analysis notes.</p>"},{"location":"mat334/power_series.html#power-series-taylor-series","title":"Power Series (Taylor Series)","text":"<p>Let \\(f(z) = \\sum^\\infty_{n=0} a_n(z-z_0)^n\\) for all \\(z\\in\\mathbb C\\), if the RHS converges, then we call such \\(f(z)\\) a power series centered at \\(z_0\\). </p>"},{"location":"mat334/power_series.html#commonly-used-power-series","title":"Commonly used Power Series","text":"<p>The power series for complex is almost the same as for reals (same proof). </p> \\[S_n = \\frac{1-c^{n+1}}{1-c} = \\sum_{k=0}^n c^k, \\forall |c| &lt; 1.  S_\\infty = \\frac{1}{1-c} = \\sum_{k=0}^\\infty c^k\\] \\[e^z = \\sum_{k=0}^\\infty  \\frac{z^k}{k!}\\] \\[\\cos z = \\frac{e^{iz} + e^{-iz}}{2} = \\frac{1}{2}(\\sum_{k=0}^\\infty  \\frac{i^kz^k}{k!} + \\sum_{k=0}^\\infty  \\frac{(-i)^kz^k}{k!}) = \\sum_{k=0}^\\infty \\frac{(-1)^kz^{2k}}{(2k)!}\\] \\[\\sin z = \\sum_{k=0}^\\infty \\frac{(-1)^kz^{2k+1}}{(2k+1)!}\\]"},{"location":"mat334/power_series.html#theorem-1","title":"Theorem 1","text":"<p>Let \\(f(z) = \\sum^\\infty a_n z^n\\), if \\(f(z_1)\\) converges absolutely for some \\(z_1\\in \\mathbb C\\), then \\(f(z)\\) converges on \\(B_{|z_1|}(0)\\) and converges uniformly on \\(\\overline{B_r(0)}\\) for all \\(r \\in (0, |z_1|)\\).</p> <p>proof. First note that \\(\\lim_\\infty |a_n z_1^n| = 0\\) since the series converges absolutely at \\(z_1\\).  Let \\(r \\in (0, z_1), z \\in \\overline{B_r(0)}\\).  take \\(N &gt; 0\\) s.t. \\(\\forall n\\geq N. |a_n z_1^n| &lt; 1\\). Therefore,</p> \\[|a_nz^n| = |a_nz_1^n||\\frac z{z_1}|^n &lt; |\\frac z{z_1}|^n \\leq (\\frac r{|z_1|})^n\\] <p>Therefore, by M-test, \\(f(z)\\) is uniformly convergent on \\(\\overline{B_r(0)}\\). </p>"},{"location":"mat334/power_series.html#example-1","title":"Example 1","text":"<p>\\(f(z) = \\sum^\\infty \\frac{z^n}{n!}\\) converges on \\(\\mathbb C\\) and is continuous.   </p> <p>proof. Let \\(z\\in\\mathbb C\\), take \\(r &gt; |z|\\), since \\(r\\in\\mathbb R\\), we know that \\(f(r) = e^r\\). Thus, by the above theorem, \\(f(z)\\) converges for all \\(z\\in\\mathbb C\\). In addition, we have uniform converges of \\(f(z)\\) on any closed ball, hence it is also continuous. </p>"},{"location":"mat334/power_series.html#existence-of-power-series-expansion","title":"Existence of Power Series Expansion","text":"<p>Theorem If \\(f(z)\\) is analytic on \\(B_r(z_0)\\), then on \\(B_r(z_0)\\) we have </p> \\[f(z) = \\sum_{n=0}^\\infty \\frac{f^{(n)}(z_0)}{n!}(z-z_0)^n\\] <p>and the series is absolutely convergent and uniformly convergent on \\(\\overline{B_\\delta(z_0)}\\) for all \\(\\delta \\in (0,r)\\), also convergent on \\(B_r(z_0)\\).</p> <p>proof. Let \\(z\\in B_r(0)\\), let \\(\\delta\\) where \\(|z| &lt; \\delta &lt; r_1 &lt; r\\). Take \\(C\\) be the circle of radius \\(r_1\\) centered at 0. Then </p> \\[\\begin{align*} f(z) &amp;= \\frac1{2\\pi i}\\oint_C\\frac{f(w)}{w-z}dw &amp;\\text{CIF}\\\\ &amp;=  \\frac1{2\\pi i}\\oint_C\\frac{f(w)}{w}\\frac1{1-z/w}dw \\\\ &amp;=  \\frac1{2\\pi i}\\oint_C\\frac{f(w)}w\\sum^\\infty (\\frac zw)^ndw \\\\ &amp;=  \\frac1{2\\pi i}\\oint_C\\sum^\\infty \\frac{f(w)}{w^{n+1}}z^ndw \\\\ &amp;= \\sum^\\infty \\frac1{2\\pi i}\\oint_C \\frac{f(w)}{w^{n+1}}z^ndw\\\\ &amp;=  \\sum^\\infty \\frac{f^{(n)}(0)}{n!}z^n &amp;\\text{CIF} \\end{align*}\\] <p>Therefore, we can use M-test to prove the claim. </p>"},{"location":"mat334/power_series.html#radius-of-convergence","title":"Radius of Convergence","text":"<p>Theorem (existence) Let \\(f(z) = \\sum^\\infty a_nz^n\\), then \\(\\exists R \\in [0, \\infty)\\) s.t. \\(f(z)\\rightarrow z\\in B_r(0)\\), does not converge for \\(z\\in \\mathbb C - \\overline{B_R(0)}\\) and converge uniformly on \\(\\overline{B_r(0)}\\) for all \\(r\\in(0,R)\\). </p> <p>And we have  1. ratio test If \\(\\lim_{n\\rightarrow\\infty} \\frac{|a_{n+1}|}{|a_n|} = S, R = S^{-1}\\) 2. root test. If \\(\\lim_\\infty |a_n|^{1/n} = S, R = S^{-1}\\) 3. Hadamard's Theorem. \\(S = \\lim_\\infty\\sup |a_n|^{1/n} = S, R = S^{-1}\\)</p> <p>Lemma \\(\\lim_\\infty |a_n|^{1/n} = \\lim_\\infty\\sup \\big((n+1)|a_{n+1}|\\big)^{1/n}\\).  proof. Part of Hadamard's Theorem.</p> <p>Theorem Let \\(f(z) = \\sum^\\infty a_nz^n\\) with radius of convergence \\(R&gt;0\\), then \\(f(z)\\) is analytic on \\(B_R(0)\\). </p> <p>proof. Let \\(g(z) = \\sum^\\infty (n+1)a_{n+1}z^n\\), by lemma, \\(g\\) also has radius of convergence \\(R\\). </p> <p>Then, we need to show that there exists some anti-derivative of \\(g\\) and g is continuous on \\(B_R(0)\\), so we need to show that all Jordon contour \\(C \\subset B_R(0)\\), \\(\\oint_C g(z)dz = 0\\). Note that this holds since there is some \\(0 &lt;\\delta &lt; R\\) s.t. \\(C\\subset \\overline{B_\\delta(0)}\\) and g is uniformly convergent on \\(\\overline{B_\\delta(0)}\\), and thus on \\(C\\). Therefore, </p> \\[\\oint_C g(z)dz= \\oint_C \\sum^\\infty (n+1)a_{n+1} z^n dz = \\sum^\\infty \\oint_C  (n+1)a_{n+1} z^n dz = \\sum^\\infty 0\\] <p>Therefore, take \\(G\\) be some anti-derivative s.t. </p> \\[\\begin{align*} G(z_1) &amp;= \\int_0^{z_1} g(z)dz \\\\ &amp;= \\int_0^{z_1} \\sum^\\infty_{n=0} (n+1)a_{n+1} z^n dz\\\\ &amp;= \\sum^\\infty_{n=0}  \\int_0^{z_1} (n+1)a_{n+1} z^n dz&amp;\\text{u.c. on} B_{|z_1|}(0)\\\\ &amp;= \\sum^\\infty_{n=1}  \\frac{na_n}{n}z_1^n\\\\ &amp;= \\sum^\\infty_{n=1}  a_nz_1^n\\\\ &amp;= f(z_1) - a_0 \\end{align*}\\] <p>Therefore, \\(f(z) = G(z) + a_0\\) is analytic.</p>"},{"location":"mat334/power_series.html#laurent-series","title":"Laurent Series","text":"<p>Define an annulus \\(A_{r_1, r_2}(z_0) = \\{z: r_1 &lt; |z-z_0| &lt; r_2\\}\\) and \\(A_{r_1, \\infty} (z_0) = \\{z : |z-z_0| &gt; r_1\\}\\). </p> <p>Theorem Let \\(f(z)\\) be analytic on \\(A_{r_1,r_2}(0)\\), then \\(\\forall z \\in A_{r_1,r_2}(0)\\), </p> \\[f(z) = \\sum_{n=-\\infty}^{\\infty}\\bigg(\\frac1{2\\pi i}\\oint_C \\frac{f(w)}{w^{n+1}}dw\\bigg) z^n\\] <p>where \\(C \\subset A_{r_1,r_2}(0)\\) is a Jordon contour s.t. \\(0\\in C_{int}\\). Moreover, \\(f\\) converges uniformly on \\(\\overline{A_{s_1,s_2}(0)}\\) for all \\(r_1&lt;s_1&lt;s_2&lt;r_2\\). </p> <p>proof. Let \\(z \\in A_{r_1,r_2}(0)\\), let \\(s_1, s_2, r_1', r_2'\\) s.t. \\(r_1&lt; r_1' &lt; s_1 \\leq |z| \\leq s_2 &lt; r_2' &lt; r_2\\). Then, CIF and CT gives </p> \\[f(z) = \\frac1{2\\pi i}\\oint_{C_{r_2'}}\\frac{f(w)}{w-z}dw - \\frac1{2\\pi i}\\oint_{C_{r_1'}}\\frac{f(w)}{w-z}dw  = I_2 - I_1\\] <p>As from power series proof. </p> \\[I_2 =\\sum_{n=0}^\\infty\\frac{1}{2\\pi i}\\oint_C \\frac{f(w)}{w^{n+1}} dw z^n\\] <p>And similarly, </p> \\[-I_1 =\\frac1{2\\pi i} \\oint_C\\frac{f(w)}{z}\\frac1{1-w/z} dw = \\sum_{n=-\\infty}^{-1} \\frac1{2\\pi i}\\oint_C \\frac{f(w)}{w^{n+1}} dw z^n\\]"},{"location":"mat334/power_series.html#lemma-1","title":"Lemma 1","text":"<p>If \\(f(z) = \\sum_0^\\infty a_n z^n\\) and \\(g(z) = \\sum_0^\\infty b_nz^n\\) on \\(B_r(0)\\), then either \\(f=g\\) or \\(\\exists r &gt; \\delta &gt; 0\\) s.t. \\(f\\neq g\\) on \\(A_{0, \\delta}(0)\\). </p> <p>proof. If \\(f(0)\\neq g(0)\\), then by continuity, \\(\\exists \\delta &gt; 0\\) s.t. \\(f(z)\\neq g(z)\\) on \\(B_\\delta(0)\\). Now suppose that \\(f(0)= g(0)\\) i.e. If \\(f\\neq g\\), then there must exist some \\(n\\) s.t. \\(a_n \\neq b_n\\). Take \\(n_0\\) be the least \\(n\\) s.t. \\(a_{n_0}\\neq b_{n_0}\\). Therefore,</p> \\[\\begin{align*} f(z) &amp;= &amp;\\sum_{n=0}^{n_0-1} a_nz^n + &amp;z^{n_0} \\sum_{n=0}^\\infty a_{n - n_0}z^n\\\\  &amp;= &amp;p(z) + &amp;z^{n_0}\\tilde f(z)\\\\ g(z) &amp;= &amp;\\sum_{n=0}^{n_0-1} a_nz^n + &amp;z^{n_0} \\sum_{n=0}^\\infty b_{n - n_0}z^n\\\\ &amp;= &amp;p(z) + &amp;z^{n_0}\\tilde g(z) \\end{align*}\\] <p>Notablly, \\(\\tilde f(0) = a_{n_0} \\neq b_{n_0} = \\tilde g(0)\\), so exists \\(\\delta &gt; 0\\) s.t. \\(\\tilde f\\neq \\tilde g\\) on \\(B_\\delta(0)\\). Therefore, on \\(z^{n_0}\\tilde f\\neq z^{n_0}\\tilde g\\) on \\(A_{0, \\delta}(0)\\)</p>"},{"location":"mat334/power_series.html#theorem-2","title":"Theorem 2","text":"<p>Let \\(f,g\\) be analytic on a domain \\(D\\). Let \\(A\\subset D\\) s.t. \\(f(z) = g(z)\\) for all \\(z\\in A\\). If \\(A\\) has a limit point contained in \\(D\\) then \\(f=g\\) on \\(D\\). </p> <p>proof. Let \\(z_0\\) be a limit point of \\(A \\subseteq D\\). Write the functions as their power series expansion \\(f(z) = \\sum_{n=0}^\\infty a_n(z-z_0)^n\\) and \\(g(z)=\\sum_{n=0}^\\infty b_n(z-z_0)^n\\) on \\(B_r(z_0)\\subset D\\). We want to show \\(f=g\\) on such \\(B_r(z_0)\\). </p> <p>By the lemma above,   - Either \\(f=g\\) on \\(B_r(z_0)\\)  - Or exists \\(\\delta &gt; 0\\) s.t. \\(f\\neq g\\) on \\(A_{0, \\delta}(z_0)\\). Take such \\(\\delta\\), however, \\(z_0\\) is a limit point of \\(A\\) so that this is a contradiction. </p> <p>Now, take \\(D'\\subseteq A \\subseteq D\\) be the largest open set s.t. \\(f=g\\). If \\(D'\\neq D\\), let \\(z_1 \\in \\partial D'\\cap D\\), since \\(z_1\\) is a limit point, we can expand \\(D'\\) with \\(B_r(z_1)\\). \\(D'\\cup B_r(z_1)\\) is larger than \\(D'\\) and \\(f=g\\), hence contradiction. </p>"},{"location":"mat334/pvt.html","title":"Principal Value Integrals","text":""},{"location":"mat334/pvt.html#principal-value-integrals_1","title":"Principal Value Integrals","text":"<p>Suppose \\(f:[a, c)\\cup (c,b]\\rightarrow \\mathbb C\\) is integrable on \\([a, c-\\epsilon)\\cup (c+\\epsilon, b]\\) for all \\(\\epsilon &gt; 0\\). Define the principal value integral as </p> \\[p.v.\\int_a^b f(x)dx = \\lim_{\\epsilon\\rightarrow 0} \\big(\\int_a^{c-\\epsilon} f(x)dx + \\int_{c+\\epsilon}^b f(x)dx\\big)\\] <p>Example Consider \\(x^{-1}\\), which is not defined on \\(x=0\\), but </p> \\[\\begin{align*} p.v. \\int_{-1}^1 x^{-1}dx &amp;= \\lim_{\\epsilon\\rightarrow 0} \\big(\\int_{-1}^{-\\epsilon} x^{-1}dx + \\int_{\\epsilon}^1 x^{-1}dx\\big) \\\\ &amp;= \\lim_{\\epsilon\\rightarrow 0} [\\log(x)]^{-\\epsilon}_{-1} + [\\log(x)]^{1}_{\\epsilon}\\\\ &amp;= 0 \\end{align*}\\] <p>Note that \\(p.v.\\int_a^b f(x)dx = \\int_a^b f(x)dx\\) if the integral exists. And the principal value integral at infinity is defined as </p> \\[\\begin{align*} p.v.\\int_{-\\infty}^\\infty f(x)dx &amp;= \\lim_{R\\rightarrow\\infty}p.v.\\int_{-R}^R f(x)dx\\\\ &amp;=\\lim_{R\\rightarrow\\infty} \\lim_{\\epsilon\\rightarrow 0} \\big(\\int_{-R}^{c-\\epsilon} f(x)dx + \\int_{c+R}^\\infty f(x)dx\\big) \\end{align*}\\]"},{"location":"mat334/pvt.html#solving-integrals-with-pvi","title":"Solving Integrals with PVI","text":"<p>Then, for some real functions, we can extend to complex, and take a way around its undefined point.</p> <p>\u200b </p>"},{"location":"mat334/pvt.html#lemma-1","title":"Lemma 1","text":"<p>\\(\\lim_{z\\rightarrow z_0}(z-z_0)f(z) = 0\\implies \\lim_{\\epsilon\\rightarrow 0^+} \\int_{C(\\epsilon, \\theta_1, \\theta_2)_{z_0} }f(z)dz = 0\\). </p> <p>proof. First, take \\(\\epsilon\\) small enough s.t. \\(z_0\\) is the only singularity in \\(C_\\epsilon\\). By residue theorem, </p> \\[\\int_{C_\\epsilon} f(z)dz = 2\\pi i Res(f, z_0) = 2\\pi i \\lim_{z\\rightarrow z_0}(z-z_0)f(z)  = 0\\] <p>Then, let \\(a, b\\) be the two endpoints of \\(C(\\epsilon, \\theta_1, \\theta_2)_{z_0}\\), so that \\(C^{1} = C(\\epsilon, \\theta_1, \\theta_2)_{z_0}\\) is the upper arc of \\(C_{\\epsilon}\\), then, let \\(C_{2}\\) be the lower arc from \\(a\\) to \\(b\\). Because \\(\\int_{C_\\epsilon} = 0\\), by deformation of curve, \\(\\int_{C_{1} } = \\int_{C_{2} }\\). Also, note that \\(\\int_{C_2} + (\\int_{-C_1}) = 0\\) so that \\(\\int_{C_1} = \\int_{-C_1} = 0\\)</p>"},{"location":"mat334/pvt.html#lemma-2","title":"Lemma 2","text":"<p>If \\(f(z)\\) has a pole of order 1 at \\(z_0\\) then</p> \\[\\lim_{\\epsilon\\rightarrow 0^+} \\int_{C(\\epsilon, \\theta_1, \\theta_2)_{z_0} }f(z)dz = i(\\theta_2 - \\theta_1)Res(f, z_0)\\] <p>proof.  Consider the Laurent expansion of \\(f\\),</p> \\[\\begin{align*} \\lim_{\\epsilon\\rightarrow 0^+} \\int_{C(\\epsilon, \\theta_1, \\theta_2)_{z_0} }f(z)dz &amp;= \\lim_{\\epsilon\\rightarrow 0^+} \\int_{C(\\epsilon, \\theta_1, \\theta_2)_{z_0} }\\sum_{n=-1}^\\infty c_n (z-z_0)^ndz\\\\ &amp;=  \\lim_{\\epsilon\\rightarrow 0^+} \\int_{C(\\epsilon, \\theta_1, \\theta_2)_{z_0} } c_{-1} (z-z_0)^{-1}dz&amp;\\text{lemma 1}\\\\ &amp;= \\lim_{\\epsilon\\rightarrow 0^+}i\\int_{\\theta_1}^{\\theta_2} c_{-1} (z_0 + \\epsilon e^{it} - z_0)^{-1} \\epsilon e^{it}dt\\\\ &amp;= i(\\theta_2 - \\theta_1) c_{-1} \\end{align*}\\]"},{"location":"mat334/pvt.html#example-1","title":"Example 1","text":"<p>Consider \\(p.v. \\frac{\\sin^{x} }{x}dx\\) First note that </p> \\[p.v. \\int_{-\\infty}^\\infty \\frac{e^{ix} }{x}dx = p.v. \\int_{-\\infty}^\\infty \\frac{\\cos x}{x}dx + i  \\:\\:p.v.\\int_{-\\infty}^\\infty \\frac{\\sin x}{x}dx \\] <p>Because $\\cos x / x $ is odd, $ p.v. \\int_{-\\infty}^\\infty \\frac{\\cos x}{x}dx = 0$, we are left with</p> \\[ \\:\\:p.v.\\int_{-\\infty}^\\infty \\frac{\\sin x}{x}dx  = i^{-1}p.v. \\int_{-\\infty}^\\infty \\frac{e^{ix} }{x}dx\\] <p>Define \\(f(z) = e^{iz}/z\\) and consider the integrals</p> \\[\\begin{align*} I_C &amp;= \\lim_{R\\rightarrow\\infty}\\lim_{\\epsilon\\rightarrow 0} \\oint_{C_{R,\\epsilon} } f(z)dz\\\\ &amp;= \\lim_{R\\rightarrow\\infty}\\lim_{\\epsilon\\rightarrow 0} (\\int_{C_{R, \\epsilon}^{line} } f(z)dz + \\int_{C(R,0,\\pi)} f(z)dz - \\int_{C(\\epsilon,0,\\pi)} f(z)dz )\\\\ &amp;= \\lim_{R\\rightarrow\\infty}\\lim_{\\epsilon\\rightarrow 0} \\int_{C_{R, \\epsilon}^{line} } f(z)dz +  \\lim_{R\\rightarrow\\infty}\\int_{C(R,0,\\pi)} f(z)dz - \\lim_{\\epsilon\\rightarrow 0}\\int_{C(\\epsilon,0,\\pi)} f(z)dz \\\\ &amp;= I_{line} + I_R - I_\\epsilon \\end{align*}\\] <p>Consider each integral, we have  - By Residue Theorem, since \\(I_C\\) goes around the undefined point, \\(I_C = 0\\)  - By definition, \\(I_{line} = \\lim_{R\\rightarrow\\infty}\\lim_{\\epsilon\\rightarrow 0}p.v.\\int_{-R}^R f(z)dz\\) is exactly what we want.  - By Jordon's Lemma, \\(I_{R} = 0\\). Thus, we are left with \\(I_{line} = I_\\epsilon =  \\lim_{\\epsilon\\rightarrow 0}\\int_{C(\\epsilon,0,\\pi)} f(z)dz\\) is what we want. By lemma 2, </p> \\[\\lim_{\\epsilon\\rightarrow 0}\\int_{C(\\epsilon,0,\\pi)} f(z)dz = i(\\pi - 0) Res(f, 0) = i\\pi (\\lim_{z\\rightarrow 0} z \\frac{e^{iz} }{z}) = i\\pi\\] <p>Therefore, </p> \\[p.v. \\frac{\\sin x }{x}dx = i^{-1}I_{line} = \\pi\\]"},{"location":"mat334/pvt.html#example-2","title":"Example 2","text":"<p>Similarly, if the principal value integral has multiple undefined points, we can go around each of them. Consider \\(p.v. \\int_{-\\infty}^\\infty (x^2 - 1) dx\\)</p> <p>Similarly, \\(I_C = I_{R} + I_{line} - I_{\\epsilon, -1} - I_{\\epsilon, 1}\\) - \\(I_C = 0\\) by residue theorem - \\(I_{line} =  \\lim_{R\\rightarrow\\infty}\\lim_{\\epsilon\\rightarrow 0} \\int_{C_{R, \\epsilon}^{line} } (z^2 - 1)^{-1}dz = p.v. \\int_{-\\infty}^\\infty (x^2-1)^{-1}dx\\) is what we want - $ I_{\\epsilon, -1} + I_{\\epsilon, 1} = I_{line}$ is what we calculate. By lemma 2, since \\(z=-1\\) and \\(z=1\\) are both poles of order 1, </p> \\[I_{\\epsilon, 1} = \\pi i Res(f, 1) = \\lim_{z\\rightarrow 1}(z+1)^{-1} = \\pi i /2\\] \\[I_{\\epsilon, -1} = \\pi i Res(f, -1) = \\lim_{z\\rightarrow -1}(z-1)^{-1} = -\\pi i/2\\] \\[ \\int_{-\\infty}^\\infty (x^2-1)^{-1}dx = \\pi i /2 - \\pi i /2 = 0\\]"},{"location":"mat334/pvt.html#integrals-with-branch-cuts","title":"Integrals with Branch Cuts","text":"<p>Consider functions on \\(\\mathbb C\\) with branch points at \\(0\\), and we only defined the funciton on some domain \\(D = \\{re^i\\theta: r&gt; 0, \\theta\\in (0, 2\\pi)\\}\\) with a branch cut.</p> <p>For function \\(f(z)\\), define \\(f_+, f_-: (0,\\infty)\\rightarrow\\mathbb C\\) as </p> \\[f_+(r) = \\lim_{\\theta\\rightarrow 0^+} f(re^{i\\theta}), f_-(r) = \\lim_{\\theta\\rightarrow 2\\pi^-} f(re^{i\\theta})\\]"},{"location":"mat334/pvt.html#example-1_1","title":"Example 1","text":"<p>Consider the sqrt function \\(f(re^{i\\theta}) = r^{1/2} e^{i\\theta/2}\\), then</p> \\[f_+(r) = r^{1/2}, f_-(r) = -r^{1/2}\\]"},{"location":"mat334/pvt.html#example-2_1","title":"Example 2","text":"<p>evaluate \\(\\int_0^\\infty \\frac{\\sqrt x}{x^3+1}dx\\)</p> <p>\u200b </p> <p>First, note that </p> \\[\\lim_{R\\rightarrow\\infty}\\lim_{\\epsilon\\rightarrow 0}\\lim_{\\theta\\rightarrow0^+}I_C =   \\lim_{R\\rightarrow\\infty}I_{C_R} - \\lim_{\\epsilon\\rightarrow 0}I_{C_\\epsilon} +  \\lim_{R\\rightarrow\\infty}\\lim_{\\epsilon\\rightarrow 0}\\lim_{\\theta\\rightarrow0^+} I_{C^{+}_{R,\\epsilon}} + \\lim_{R\\rightarrow\\infty}\\lim_{\\epsilon\\rightarrow 0}\\lim_{\\theta\\rightarrow0^+}  I_{C^{-}_{R,\\epsilon}}\\] <p>Then, note that \\(\\lim_{z\\rightarrow\\infty} z f(z) = \\lim_{z\\rightarrow\\infty}\\frac{z^{3/2}}{z^3+1} = 0, \\lim_{z\\rightarrow 0} z f(z) = \\frac{0^{3/2}}{0^3+1} = 0\\), so that </p> \\[\\lim_{R\\rightarrow\\infty}I_{C_R} = 0, \\lim_{\\epsilon\\rightarrow 0}I_{C_\\epsilon} = 0\\] <p>Then, consider the left two terms</p> \\[\\lim_{R\\rightarrow\\infty}\\lim_{\\epsilon\\rightarrow 0}\\lim_{\\theta\\rightarrow0^+} I_{C^{+}_{R,\\epsilon}} = \\lim_{R\\rightarrow\\infty}\\lim_{\\epsilon\\rightarrow 0}\\int_{\\epsilon}^R f_+(x)dx = \\int_{0}^\\infty f_+(x)dx\\] \\[\\lim_{R\\rightarrow\\infty}\\lim_{\\epsilon\\rightarrow 0}\\lim_{\\theta\\rightarrow0^+} I_{C^{-}_{R,\\epsilon}} = \\lim_{R\\rightarrow\\infty}\\lim_{\\epsilon\\rightarrow 0}\\int_{R}^{\\epsilon} f_-(z)dz = -\\int_{0}^\\infty f_-(z)dz\\] <p>via branch cut of square root functions, we have \\(-f_-(z) = f_+(z) = f(x)\\) so that the whole integral </p> \\[\\lim_{R\\rightarrow\\infty}\\lim_{\\epsilon\\rightarrow 0}\\lim_{\\theta\\rightarrow0^+}I_C = \\int_{0}^\\infty f_+(x) -\\int_{0}^\\infty f_-(x) = 2 \\int_0^\\infty \\frac{\\sqrt x }{x^3+1} dx\\] <p>Now, using residue theorem, </p> \\[I_C = \\int_{C} f(z)dz = 2\\pi i (Res(f, -1) + Res(f, e^{i\\pi/3}) +  Res(f, e^{-i\\pi/3}))\\] <p>so that the original integral</p> \\[\\int_0^\\infty \\frac{\\sqrt x }{x^3+1} dx = \\pi i (Res(f, -1) + Res(f, e^{i\\pi/3}) +  Res(f, e^{-i\\pi/3}))\\]"},{"location":"mat334/residue.html","title":"Sigularities and Residue Theorem","text":""},{"location":"mat334/residue.html#singularity","title":"Singularity","text":"<p>\\(z_0\\) is an isolated singularity of \\(f\\) if \\(f(z)\\) is not differentiable or is undefined at \\(z_0\\), but \\(f(z)\\) is analytic on \\(A_{0, r_0}(z_0)\\) for some \\(r_0 &gt; 0\\) </p>"},{"location":"mat334/residue.html#classes-of-singularities","title":"Classes of Singularities","text":"<p>Suppose \\(f\\) has a isolated singularity at \\(z_0\\), and its Larent expansion is \\(f(z) = \\sum_{-\\infty}^{\\infty} c_n (z-z_0)^n\\) on \\(A_{0, r_0}(z_0)\\) for some \\(r_0 &gt; 0\\), then   - \\(z_0\\) is a removable singularity if \\(c_n = 0\\) for all \\(n &lt; 0\\).   - \\(z_0\\) is a pole of order \\(N\\) if \\(c_{-N}\\neq 0\\) abd \\(c_n = 0\\) for all \\(n &lt; -N\\).  - \\(z_0\\) is a essential singularity if \\(c_n\\neq 0\\) for all \\(n &lt; 0\\).</p> <p>Another definition   - \\(z_0\\) is a removable singularity if \\(\\exists s, 0 &lt; s &lt; r\\) s.t. \\(f(z)\\) is bounded on \\(A_{0, s}(z_0)\\), i.e. \\(\\lim_{z\\rightarrow z_0} f(z)\\) exist.   - \\(z_0\\) is a pole of order \\(N\\) if \\(\\exists s, 0 &lt; s &lt; r\\) s.t. \\((z-z_0)^N f(z)\\) is bounded on \\(A_{0, s}(z_0)\\), i.e. \\(\\lim_{z\\rightarrow z_0} (z-z_0)^N f(z)\\) exist but \\(\\lim_{z\\rightarrow z_0} (z-z_0)^{N-1} f(z)\\) DNE.   - \\(z_0\\) is a essential singularity if \\(\\forall N, \\forall s, 0 &lt; s &lt; r\\) \\((z-z_0)^N f(z)\\) is unbounded, or \\(\\lim_{z\\rightarrow z_0} (z-z_0)^N f(z)\\) DNE.</p> <p>The two definitions are equivalent</p>"},{"location":"mat334/residue.html#claim-1","title":"Claim 1","text":"<p>Let \\(f(z)\\) have a removable singularity at \\(z_0\\), and analytic on \\(A_{0, r}(z_0)\\) for some \\(r &gt; 0\\). Then, \\(\\exists g(z)\\) analytic on \\(B_r(z_0)\\) and \\(f=g\\) on \\(A_{0,r}(z_0)\\).</p> <p>proof. Note that for removable sinularity, we have the Laurent series \\(f(z) = \\sum_{n=0}^\\infty c_n(z-z_0)^n\\) since for \\(\\forall m \\leq  -1, c_m = 0\\) on the anulus. Then, simply write as the power series on the ball. </p>"},{"location":"mat334/residue.html#example-1","title":"Example 1","text":"<p>consider \\(\\frac{\\sin z}{z}\\) on \\(A_{0,r}(0)\\). its Laurent series is given by </p> \\[\\frac{\\sin z}{z} = \\sum_{n=0}^\\infty \\frac{(-1)^n}{(2n+1)!}\\frac{z^{2n+1}}{z} = \\sum_{n=0}^\\infty \\frac{(-1)^n}{(2n+1)!}z^{2n}\\] <p>Therefore, take \\(z=0\\), we have the series equals to \\(1\\). we can write \\(g(z) = \\begin{cases}\\sin z / z &amp;z\\neq 0\\\\ 1&amp;z=0\\end{cases}\\)</p>"},{"location":"mat334/residue.html#residue","title":"Residue","text":"<p>Note that using Cauchy Theorem we can do contour deformation to solve singularity in \\(C_{int}\\), and each integral around singularity can be related to CIF. Therefore, we can unify them together. </p> <p>For \\(f\\) analytic, for some singularity \\(z_0\\) on \\(f\\) s.t. \\(f\\) is analytic on \\(A_{o,r}(z_0)\\) for \\(r&gt;0\\). The residue is defined as </p> \\[Res(f, z_0) = c_{-1} = \\frac{1}{2\\pi i} \\oint_{C}\\frac{f(w)}{(w - z_0)^{-1+1} }dw = \\frac{1}{2\\pi i} \\oint_{C_s}f(w)dw\\] <p>for some circle \\(C_s\\) of radius \\(0 &lt; s &lt; r\\) centered at \\(z_0\\). Note that \\(c_{-1}\\) is the coefficient of Larent series expansion on \\(A_{0,r}(z_0)\\). </p>"},{"location":"mat334/residue.html#theorem-1-residue-theorem","title":"Theorem 1. Residue Theorem","text":"<p>Let \\(D\\) e a domain, Let \\(C\\subset D\\) be a Jordon contour,  let the set of singularities \\(Z = \\{z_1,..., z_N\\} \\subset C_{int}, f(z)\\) analytic on \\(D - Z\\). Then, </p> \\[\\oint_C f(z)dz = 2\\pi i \\sum_{k=1}^N Res(f, z_k)\\] <p>proof. First, by contour deformation, we only need to show that </p> \\[\\int_{C_k} f(z)dz = 2\\pi i Res(f, z_k)\\] <p>where \\(C_k \\subset A_{0, r_k}(z_k)\\) s.t. \\(f\\) is analytic on each anulus. From the definition of residue above, </p> \\[2\\pi i Res(f, z_k) = 2\\pi i \\frac{1}{2\\pi i} \\oint_{C_k} f(w)dw = \\oint_{C_k} f(z)dz\\]"},{"location":"mat334/residue.html#claim-2","title":"Claim 2","text":"<p>\\(f(z)\\) has a pole of order 1 at \\(z_0\\) IFF \\(Res(f, z_0) = \\lim_{z\\rightarrow z_0} (z-z_0)f(z)\\)</p> <p>proof. \\(f\\) has pole of order 1 IFF the Larent series starts from \\(-1\\), so that</p> \\[(z-z_0)f(z) = \\sum_{n=0}^{\\infty} c_{n-1} (z-z_0)^n\\] <p>Therefore, consider the limit \\(\\lim_{z\\rightarrow z_0} (z-z_0)f(z)\\), if the limit exists, all terms with \\((z-z_0)\\) must approach \\(0\\) as \\(z\\rightarrow z_0\\), the only term left can be the 0th term, i.e. \\(c_{-1}\\) </p>"},{"location":"mat334/residue.html#claim-3","title":"Claim 3","text":"<p>For \\(z_0\\) is a pole of order \\(k\\) singularity, or a removable singularity, or an analytic point, (i.e. \\(z_0\\) has at most order \\(k\\))take any \\(M\\geq k\\), we have </p> \\[Res(f, z_0) = \\frac{1}{(m-1)!} \\lim_{z\\rightarrow z_0} \\frac{d^{M-1}}{dz^{M-1}} (z-z_0)^M f(z)\\] <p>proof. </p> \\[\\begin{align*} (z-z_0)^M f(z) &amp;= \\sum_{n=0}^\\infty c_{n-M} (z-z_0)^n\\\\ \\frac{d^{M-1}}{dz^{m-1}}(z-z_0)^M f(z) &amp;= \\sum_{n=M-1}^\\infty (\\prod_{k=0}^{M-2} (n-k))c_{n-M} (z-z_0)^{n-M-1} \\end{align*}\\] <p>Therefore, the coefficient at \\((z-z_0)^0\\) is when \\(n = M-1\\)</p> \\[(\\prod_{k=0}^{M-2} (M-1 - k))c_{-1} = (M-1)! c_{-1}\\]"},{"location":"mat334/residue.html#residue-at-infinity","title":"Residue At Infinity","text":"<p>If \\(f(z)\\) analytic on \\(A_{s,\\infty}(0)\\) for some \\(s&gt;0\\), define the residue at inifinity as</p> \\[Res(f,\\infty) = \\lim_{R\\rightarrow\\infty} \\frac{1}{2\\pi i} \\oint_{C_R} f(z)dz\\] <p>Note that this definition makes sense because on \\(A_{s,\\infty}(0)\\), consider Laurent series expansion \\(f(z) = \\sum_{-\\infty}^\\infty c_nz^n\\) we have </p> \\[\\begin{align*} \\lim_{R\\rightarrow\\infty} \\frac{1}{2\\pi i} \\oint_{C_R} f(z)dz &amp;= \\frac{1}{2\\pi i}\\lim_{R\\rightarrow\\infty}  \\oint_{C_R} \\sum_{n=-\\infty}^\\infty c_n z^ndz\\\\ &amp;\\rightarrow^{u.c} \\frac{1}{2\\pi i}\\lim_{R\\rightarrow\\infty} \\sum_{n=-\\infty}^\\infty \\oint_{C_R}  c_n z^ndz\\\\ &amp;= \\frac{1}{2\\pi i} \\oint_{C_R} c_{-1}z^{-1}dz&amp;\\text{CT}\\\\ &amp;= c_{-1} \\end{align*}\\]"},{"location":"mat334/residue.html#claim-1_1","title":"Claim 1","text":"<p>If \\(f\\) has finitely many singularities \\(z_1,...,z_n \\in \\mathbb C\\), then </p> \\[Res(f,\\infty) = \\sum_{k=1}^n Res(f, z_k)\\] <p>proof. </p> \\[\\begin{align*} Res(f,\\infty) &amp;= \\lim_{R\\rightarrow\\infty} \\frac{1}{2\\pi i} \\oint_{C_R} f(z)dz\\\\ &amp;= \\sum_{k=1}^n Res(f, z_k) &amp;\\text{Residue Thrm} \\end{align*}\\]"},{"location":"mat334/residue.html#claim-2_1","title":"Claim 2","text":"<p>\\(Res(f,\\infty) = -Res(\\frac{1}{z^2} f(\\frac{1}{z}), 0)\\)</p> <p>proof. By our definition</p> \\[\\begin{align*} Res(f,\\infty) &amp;= \\lim_{R\\rightarrow\\infty} \\frac{1}{2\\pi i} \\oint_{C_R} f(z)dz\\\\ &amp;= \\lim_{R\\rightarrow\\infty} \\frac{1}{2\\pi i} \\oint_{C_{R^{-1}}} f(u^{-1})(-u^{-2})du &amp;z=u^{-1}, dz=-u^{-2}du\\\\ &amp;= \\lim_{\\epsilon\\rightarrow0} -\\frac{1}{2\\pi i} \\oint_{C_{\\epsilon}} \\frac{1}{u^2}f(\\frac{1}{u})du\\\\ &amp;= -Res(\\frac{1}{z^2} f(\\frac{1}{z}), 0) \\end{align*}\\]"},{"location":"mat334/residue.html#claim-3_1","title":"Claim 3","text":"<p>For \\(p, q\\) polynomials, if some \\(C\\) large enough circle to contain all the roots of polynomial \\(q(z)\\) and \\(deg(q)\\geq deg(p) + 2\\), then </p> \\[\\oint_C \\frac{p(z)}{q(z)} dz = 0\\] <p>proof. Consider  \\(\\lim_{z\\rightarrow\\infty} z\\frac{p(z)}{q(z)}, deg(zp(z)) = deg(p(z)) + 2\\leq deg(q)\\) so that the limit exists and \\(\\lim_{z\\rightarrow\\infty} z\\frac{p(z)}{q(z)} = 0\\). Therefore,</p> \\[Res(p/q,\\infty) = \\frac{1}{2\\pi i}\\oint_C \\frac{p(z)}{q(z)} dz = 0\\]"},{"location":"mat334/residue.html#winding-number","title":"Winding Number","text":"<p>For closed curve \\(C\\), define for the winding number of \\(C\\) at \\(z_0\\) (not on \\(C\\)) as </p> \\[w(C, z_0) = \\frac{1}{2\\pi i}\\int_C \\frac{1}{z-z_0} dz\\] <p>Graphically, winding number is the number of circles of \\(C\\) oriented c.c.w. around \\(z_0\\). </p> <p>Theorem For curves \\(C\\) that does not go across \\(0\\), any parameterization \\(c(t)\\) of \\(C\\) can be decomposed into </p> \\[c(t) = r(t)e^{i\\theta(t)}\\] <p>Theorem \\(w(C, 0) = \\frac{\\theta(b) - \\theta(a)}{2\\pi i}\\)</p>"},{"location":"mat334/residue.html#generalized-residue-theorem","title":"Generalized Residue Theorem","text":"<p>Let \\(D\\) be a simply connected domain, \\(\\mathbf z = \\{z_1,...,z_n\\} \\in D\\) be singularities. \\(C \\subset D - \\mathbf z\\) be a closed curve and \\(f\\) is analytic on \\(D-\\mathbf z\\). Then, </p> \\[\\int_C f(z)dz = 2\\pi i \\sum_{i=1}^n w(C, z_k) Res(f, z_k)\\]"},{"location":"mat337/AAT.html","title":"Arzela-Ascoli Theorem","text":""},{"location":"mat337/AAT.html#defn-equicontinuous","title":"Def'n. Equicontinuous","text":"<p>A family of function \\(\\mathcal F \\subset C(K,\\mathbb R^m)\\) is equicontinuous at \\(a\\in K\\) </p> \\[\\forall f \\in \\mathcal F. \\forall \\epsilon &gt; 0. \\exists \\delta &gt; 0. \\|x-a\\|&lt;\\delta \\Rightarrow \\|f(x)-f(a)\\|&lt;\\epsilon\\] <p>Then \\(\\mathcal F\\) is equicontinuous on \\(K\\) is \\(\\mathcal F\\) is equicontinuous \\(\\forall a\\in K\\). </p>"},{"location":"mat337/AAT.html#defn-uniformly-equicontinuous","title":"Def'n. Uniformly equicontinuous","text":"<p>\\(\\mathcal F\\) is uniformly equicontinuous if </p> \\[\\forall \\epsilon &gt; 0. \\exists \\delta &gt; 0. \\forall f\\in\\mathcal F. \\forall x,y\\in K. \\|x-y\\|&lt;\\delta \\Rightarrow \\|f(x)-f(y)\\|&lt;\\epsilon\\]"},{"location":"mat337/AAT.html#theorem-1","title":"Theorem 1","text":"<p>\\(\\mathcal G := \\{g_n\\}_{n\\geq 1}\\cup \\{g\\}\\) where \\(g_n \\in C(K,\\mathbb R^m), g_n\\rightarrow^{u.c.}g\\), then \\(\\mathcal G\\) is equicontinuous.</p> <p>proof. Let \\(a\\in K, \\epsilon &gt; 0\\), By completeness and uniform continuous, \\(g\\) is also continuous, take \\(\\delta' &gt; 0\\) s.t. \\(\\forall x\\in K. \\|x-a\\|&lt;\\delta' \\Rightarrow \\|g(x)-g(a)\\|&lt;\\epsilon/3\\). By uniform continuous, take \\(N\\geq 0\\) s.t. \\(\\forall n\\geq N. \\|g_n-g\\|_\\infty &lt;\\epsilon/3\\). Then, \\(\\forall n\\geq N. \\forall x, \\|x-a\\|&lt;\\delta'\\) \\(\\|g_n(x)-g_n(a)\\| \\leq \\|g_n(x)-g(x)\\| + \\|g(x)-g(a)\\| + \\|g(a)-g_n(a)\\| \\leq 3(\\epsilon/3)=\\epsilon\\)</p> <p>Then, for each \\(g_k \\in \\{g_1,g_2,...,g_N\\}\\), take \\(\\delta_k\\) by continuity of each \\(g_k\\),  take \\(\\delta = \\min\\{\\delta_1,..,\\delta_k, \\delta'\\}\\) </p> \\[\\forall f \\in \\mathcal F. \\forall \\epsilon &gt; 0. \\exists \\delta &gt; 0. \\|x-a\\|&lt;\\delta \\Rightarrow \\|f(x)-f(a)\\|&lt;\\epsilon\\]"},{"location":"mat337/AAT.html#example-1","title":"Example 1","text":"<p>\\(f_n(x)=x^n, x\\in [0,1], \\mathcal F = \\{f_n\\}_{n\\geq 1}\\) is not equicontinuous at 1.</p> <p>proof. Take \\(\\epsilon =1/2\\), let \\(\\delta &gt; 0\\), wlog, \\(\\delta &lt; 1\\). Take \\(y = 1-\\delta/2&lt;1\\), then \\(y^n \\rightarrow 0\\), hence we can take \\(N\\) s.t. \\(\\forall n\\geq N, 1-y^n &gt; 1/2\\) Therefore, \\(|1-y|=\\delta/2 &lt; \\delta\\) but \\(|f_n(1)-f_n(y)|= 1-y^n &gt; 1/2\\)</p>"},{"location":"mat337/AAT.html#lemma-1-compact-implies-equicontinuous","title":"Lemma 1.   Compact Implies Equicontinuous","text":"<p>If \\(\\mathcal F\\) compact, then \\(\\mathcal F\\) equicontinuous on \\(K\\).  </p> <p>proof. Suppose \\(\\mathcal F\\) is not equicontinuous,  take \\(a\\in K, \\epsilon &gt; 0\\) s.t. \\(\\forall n\\geq 1. \\exists f_n\\in\\mathcal F. \\exists x_n\\in K\\) s.t. \\(\\|x_n-a\\|&lt;1/n\\) but \\(\\|f_n(x_n) - f_n(a)\\| \\geq \\epsilon\\), hence we construct sequences \\(\\{x_n\\}, \\{f_n\\}\\) Then, any subset of \\(\\{f_n\\}\\) cannot be equicontinuous \\((i)\\) However, since \\(\\mathcal F\\) is compact, take \\(\\{f_{n_k}\\}\\) converges uniformly to some \\(f\\in\\mathcal F\\), and \\(\\{f_{n_k}\\}\\cup \\{f\\}\\) is equicontinuous, this contradicts with \\((i)\\)</p>"},{"location":"mat337/AAT.html#lemma-2-equicontinuous-implies-uniformly-equicontinuous","title":"Lemma 2.  Equicontinuous implies uniformly equicontinuous","text":"<p>proof. Suppose \\(\\mathcal F\\) not u.e.c. Take \\(\\epsilon &gt; 0\\) s.t. \\(\\forall n\\geq 1, \\exists x_n,y_n \\in K. \\exists f_n\\in \\mathcal F. \\|x_n-y_n\\| &lt; 1/n \\land \\|f_n(x_n)-f_n(y_n)\\|\\geq \\epsilon\\) hence we construct sequence \\(\\{x_n\\}, \\{y_n\\}, \\{f_n\\}\\) Since \\(K\\) is compact, take \\(x_{n_k}\\rightarrow a \\in K\\), then \\(y_{n_k} = x_{n_k}-(x_{n_k}-y_{n_k})\\rightarrow a\\)</p> <p>\\(\\mathcal F\\) is equicontinous at \\(a\\in K\\Rightarrow \\exists \\delta &gt; 0, \\|f_n(x)-f_n(a)\\|\\leq \\epsilon/2\\) for all \\(\\|x-a\\|&lt;\\delta, f\\in\\mathcal F\\). Since \\(x_{n_k}\\rightarrow a, y_{n_k}\\rightarrow a, \\exists M\\in \\mathbb N, \\forall m \\geq M. \\|x_{n_m}-a\\|&lt;\\delta\\land \\|y_{n_m}-a\\|&lt;\\delta\\) Therefore, \\(\\forall m\\geq M\\) </p> \\[\\begin{align*} \\|f_{n_m}(x_{n_m})- f_{n_m}(y_{n_m})\\|&amp;\\leq \\|f_{n_m}(x_{n_m})- f_{n_m}(a)\\| + \\|f_{n_m}(a)- f_{n_m}(y_{n_m})\\|\\\\&amp;&lt; 2(\\epsilon/2)\\\\&amp;=\\epsilon \\end{align*}\\] <p>contradicts with assumption</p>"},{"location":"mat337/AAT.html#defn-totally-bouded","title":"Def'n. Totally bouded","text":"<p>\\(S\\subseteq K\\) is an \\(\\epsilon\\)-net of \\(K\\) if \\(K\\subseteq \\cup_{a\\in S}B_\\epsilon(a)\\) \\(K\\) is totally bounded if it has a finite \\(\\epsilon\\)-net \\(\\forall \\epsilon &gt; 0\\)</p>"},{"location":"mat337/AAT.html#lemma-3-bounded-implies-totally-bounded","title":"Lemma 3. Bounded Implies Totally Bounded","text":"<p>If \\(K\\subseteq \\mathbb R^m\\) bounded, then totally bounded. </p> <p>proof. Let \\(\\epsilon &gt; 0\\), choose \\(N, N \\leq \\min\\{\\epsilon, \\frac{1}{\\sqrt m}\\}\\) \\(K\\) bounded, hence \\(\\exists L &gt; 0, \\forall x = (x_1,...,x_m)\\in K, |x_i|\\leq L, \\forall i\\) Let \\(F = \\{\\frac{k_i}{2N^2}\\}_{k_i\\in \\mathbb Z}\\subseteq [-L,L]\\), then \\(F\\) is a finite \\(\\frac{1}{2N^2}\\)-net for \\([-L,L]\\). Let \\(A = \\{x_1,...,x_m\\}\\subseteq \\mathbb R^m\\) s.t. \\(x_i \\in F, \\forall i\\) Let \\(\\tilde A = \\{a\\in A: B_{\\epsilon/2}(x)\\cap K \\neq \\emptyset\\}\\). Then, for each \\(x\\in \\tilde A\\), choose \\(x_a \\in B_{\\epsilon/2}(a)\\cap K\\). Take \\(x = (x_1,...,x_m)\\in K\\) for each \\(i = 1,...,m, \\exists a_i \\in F\\) s.t. \\(|x_i-a_i|&lt;\\frac{1}{2N^2}\\) Then, \\(\\|x-a\\| = \\sqrt{\\sum |x_i-a_i|^2} &lt; \\sqrt{\\frac{m}{4N^4}} = \\sqrt M/2N^2 \\leq N/2N^2 \\leq (2N)^{-1} &lt; \\epsilon/2\\) Also, \\(B_{\\epsilon/2}(a)\\cap K\\neq \\emptyset, a\\in \\tilde A\\), so that \\(x_a\\) is defined. Then, \\(\\|x-x_a\\|\\leq \\|x-a\\|+\\|x-x_a\\|&lt; \\epsilon/2+\\epsilon/2 = \\epsilon\\)</p>"},{"location":"mat337/AAT.html#lemma-4","title":"Lemma 4","text":"<p>If \\(K\\) bounded, then \\(K\\) contains a sequence \\(\\{x_i\\}_{i\\geq 1}\\) dense in \\(K\\). Moreover, \\(\\forall \\epsilon &gt; 0. \\exists N\\in\\mathbb N\\) s.t. \\(\\{x_i\\}_{i\\leq N}\\) is an \\(\\epsilon\\)-net for \\(K\\). </p> <p>proof. For each \\(k\\geq i\\), let \\(B_k\\) be a finite \\(k^{-1}\\)-net. Take \\(\\{x_i\\}\\) be the sequence which lists the \\(B_k\\) consecutively, i.e. \\(x_0,...,x_{N_0}\\in B_0, x_{N_0+1}, ...,x_{N_1} \\in B_1,..\\) Let \\(x\\in K\\), then \\(\\forall k\\geq 1. \\exists n_k\\) s.t. \\(x_{n_k}\\in B_k\\) and \\(\\|x-x_{n_k}\\|&lt; k^{-1}\\), hence dense. Also, given \\(\\epsilon &gt; 0\\), choose \\(k &gt; \\epsilon^{-1}, \\{x_i\\}_{i\\leq N_k}\\) is a \\(\\epsilon\\)-net. </p>"},{"location":"mat337/AAT.html#thrm-arzela-ascoli-theorem","title":"Thrm. Arzela-Ascoli Theorem","text":"<p>\\(\\mathcal F \\subseteq C(K,\\mathbb R^m)\\) is compact IFF closed, bounded, euicontinuous</p> <p>\\(\\Rightarrow\\) proof. Suppose not closed, then take \\(\\{f_n\\}\\subseteq \\mathcal F\\) s.t. \\(f_n\\rightarrow f\\not\\in \\mathcal F\\) contradicts with compactness. Suppose not bounded, take \\(\\{f_n\\}\\subseteq \\mathcal F\\) that \\(\\|f_n\\|_\\infty\\rightarrow\\infty\\) contradicts and Lemma 1</p> <p>\\(\\Leftarrow\\) proof. Fix \\(\\{f_n\\} \\subseteq \\mathcal F\\), by the Lemma 4, \\(\\exists \\{x_i\\}\\subseteq K\\) s.t. \\(\\forall \\epsilon &gt; 0. \\exists \\{x_i\\}_{i\\leq N}\\) is a \\(\\epsilon\\)-net. WTF \\(\\{f_{n_k}\\} \\subseteq \\{f_n\\}\\) s.t. \\(f_{n_k}(x_i)\\rightarrow^{k}L_i, \\forall 1\\leq i\\leq N\\). Let \\(A_0=\\mathbb N\\), since \\(\\{f_n(x_1)\\}_{n\\in A_0}\\) bounded, by Bolzano-Weierstrass Theorem, take the convergent subsequence, i.e. \\(A_1\\subseteq A_0, \\lim_{n\\in A_1}f_n(x_1) =L_1\\). Inductively take \\(\\mathcal A = A_0\\supseteq A_1\\supseteq A_2\\supseteq ...\\) be a decreasing sequence, s.t. \\(\\lim_{n\\in A_i}f_n(x_i)=L_i\\) Then, for each \\(k\\geq 1\\), let \\(n_k\\) be the \\(k\\)th element of \\(A_k\\), i.e. </p> \\[\\begin{matrix} &amp;A_1 &amp;n_1 &amp; &amp; \\\\ &amp;A_2 &amp; &amp; n_2 &amp;\\\\ &amp;A_3 &amp; &amp; &amp;n_3 \\\\ &amp;... \\end{matrix}\\] <p>Since \\(A_n\\) is decreasing, for each \\(i\\geq 1\\), there are at most \\(i-1\\) elements are not in \\(A_i\\). In particular, this implies \\(\\lim_k f_{n_k}(x_i) = \\lim_{n\\in A_i} f_n(x_i) = L_i\\) Let \\(g_k = f_{n_k}\\), let \\(\\epsilon &gt; 0\\), since \\(\\mathcal F\\) is equicontinuous, i.e. uniform equicontinuous. Take \\(\\delta &gt; 0. \\|x-y\\|&lt;\\delta\\Rightarrow\\|f(x)-f(y)\\|&lt;\\epsilon/3, \\forall f \\in \\mathcal F\\) By definition of \\(\\{x_i\\}\\), take \\(N\\in\\mathcal N, \\{x_1,...,x_n\\}\\) is a \\(\\delta\\)-net. Since \\(\\lim_{k\\rightarrow\\infty} g_k(x_i)\\) exists for all \\(i\\geq 1\\). \\(\\exists M\\in \\mathbb N\\) s.t. \\(\\forall k,l\\geq M. \\forall 1\\leq i\\leq N\\Rightarrow \\|g_n(x_i)-g_l(x_i)\\|&lt;\\epsilon/3\\) Since \\(\\{x_1,...,x_N\\}\\) is a \\(\\delta\\)-net, \\(\\exists i, \\|x_i-x\\|&lt;\\delta\\)</p> \\[\\|g_k(x)-g_l(x)\\|\\leq \\|g_k(x)-g_k(x_i)\\| + \\|g_k(x_i)-g_l(x_i)\\| + \\|g_l(x_i)-g_l(x)\\|&lt;\\epsilon\\] <p>Since \\(\\{g_k\\}\\) is uniform Cauchy, and \\(C(K,\\mathbb R^m)\\) is complete, \\(g_k\\rightarrow g\\in C(K,\\mathbb R^m)\\) Since \\(\\mathcal F\\) closed, \\(g\\in\\mathcal F\\), therefore, compact. </p> <p>Note that we only used closed at the very end. Therefore, If \\(\\{f_n\\}\\subseteq C([a,b])\\) and is bounded and equicontinuous, then it has a convergent subsequence.</p>"},{"location":"mat337/ODE.html","title":"Fixed Point and ODE","text":""},{"location":"mat337/ODE.html#discrete-dynamical-systems","title":"Discrete Dynamical Systems","text":"<p>Discrete Dynamical Systems Let \\(X\\subseteq (V,\\|\\cdot\\|), T:X\\rightarrow X\\) continuous. \\((X,T)\\) is a discrete dynamical system Forward orbit \\(\\forall x\\in X\\), forward orbit of x is the sequence \\(O(x):=\\{T^n_x\\}_{n\\geq 0}\\), \\(T^n_x=T(T...T(x))\\)</p> <p>Fixed Point \\(x^*\\) is  attractive fixed point (sink) if \\(\\exists (a,b)\\ni x^*\\) s.t. \\(\\forall x \\in (a,b). T^N_x\\rightarrow x^*\\) repelling fixed point (source) if \\(\\forall x\\neq x^*\\), \\(O(x)\\) leaves \\((a,b)\\)</p> <p>Contraction \\(X\\subseteq (V,\\|\\cdot\\|),T:X\\rightarrow X\\) is a contraction on \\(X\\) if </p> \\[\\exists c &lt; 1. \\forall x,y\\in X. \\|Tx-Ty\\|\\leq c\\|x-y\\|\\] <p>i.e. \\(T\\) is \\(c\\)-Lipschitz \\(c&lt;1\\)</p>"},{"location":"mat337/ODE.html#banach-contraction-principle","title":"Banach Contraction Principle","text":"<p>Let \\(X\\subseteq (V,\\|\\cdot\\|)\\) be a closed subset of a complete normed vector space, IF \\(T:X\\rightarrow X\\) is a contraction on \\(X\\) THEN  (i) \\(T\\) has unique fixed point \\(x^*\\) (ii) \\(\\forall x\\in X. x^* = \\lim_{n\\rightarrow\\infty} T^nx\\) (iii) \\(\\|T^nx-x^*\\|\\leq c^n\\|x-x^*\\|\\leq \\frac{c^n}{1-c}\\|x-Tx\\|, c\\) is the Lipschitz constant </p> <p>proof. Let \\(x_0\\in X\\), recursively define \\(x_{n+1} = Tx_n\\), hence form a sequence \\(\\{x_n\\}\\). Then, we can show that \\(\\{x_n\\}\\) is Cauchy by observing</p> \\[\\begin{align*} \\|x_{n+m} - x_n\\| &amp;\\leq \\sum_0^{m-1}\\|x_{n+i-1}-x_{n+i}\\| &amp;\\text{triangle inequality} \\\\ &amp;= \\sum_{i=0}^{m-1} \\|Tx_{n+i}-Tx_{n+i-1}\\| \\\\ &amp;\\leq \\sum_{i=0}^{m-1} c\\|x_{n+i}-x_{n+i-1}\\|&amp;\\text{by contraction}\\\\ &amp;\\leq \\sum^{m-1}c^{n+i}\\|x_1-x_0\\| &amp;\\text{recursively repeat such process} \\\\ &amp;&lt;\\sum^\\infty c^{n+i}\\|x_1-x_0\\| \\\\ &amp;= \\frac{c^n\\|x_1-x_0\\|}{1-c} \\end{align*}\\] <p>Since \\(c^n\\rightarrow 0\\), we can choose \\(N\\) sufficiently large to have \\(\\|x_{n+m}-x_n\\|\\rightarrow 0\\) </p> <p>Existence By completeness of the normed space, Cauchy implies convergent to some \\(x^*\\in V\\), by \\(X\\) closed, \\(x^*\\in X\\). By Lipshitz, hence continuous of \\(T\\), \\(Tx^* = T(\\lim x_n) = \\lim Tx_n = x^*\\Rightarrow x^*\\) is the fixed point.  </p> <p>Uniqueness Suppose \\(y\\in X\\) is also fixed point, \\(\\|x^*-y\\| = \\|Tx^* - Ty\\| \\leq c\\|x^*-y\\|\\Rightarrow \\|x^*-y\\| = 0\\Rightarrow x^*=y\\)</p> \\[\\|T^Nx-x^*\\| = \\|T^n x-T^nx^*\\|\\leq c^n\\|x-x^*\\|= c^n lim \\|x-x_m\\|\\leq \\frac{c^n}{1-c}\\|Tx-x\\|\\]"},{"location":"mat337/ODE.html#solving-ode","title":"Solving ODE","text":"<p>Example given the initial value problem \\(f'(x)=1+x-f(x), |x|\\leq 1/2\\) and \\(f(0)=1\\)</p> <p>First convert to the integral problem  </p> \\[f(x) = 1+ \\int_0^x 1 + t - f(t)dt = 1+ x + \\frac{x^2}{2} - \\int_0^x f(t)dt, f\\in C[-1/2,1/2]\\] <p>Define \\(T\\) on \\(C[-1/2,1/2]\\) and \\(Tf = f\\) and the solution of the integral equation is a fixed point of \\(T\\) Then, to show \\(T\\) is a contraction </p> \\[\\begin{align*} |Tf(x)-Tg(x) &amp;= |\\int_0^x f(t)-g(t)dt| \\\\ &amp;\\leq \\big|\\int_0^x |f(t)-g(t)|dt\\big| &amp;\\text{tri. ineq.}\\\\ &amp;\\leq \\int_0^x \\|f-g\\|dt \\\\ &amp;=\\|f-g\\|_\\infty \\int_0^{|x|}dt\\\\ &amp;\\leq \\frac{\\|f-g\\|_\\infty}{2} \\end{align*}\\] <p>Thus, by Banach Contraction Principle, choose \\(f_0 := 1\\in C[-1/2,1/2]\\), then </p> \\[\\begin{align*} f_1(x) &amp;= Tf_0(x) = 1+x+\\frac{x^2}{2} - \\int_0^x dt = 1+\\frac{x^2}{2} \\\\ f_2(x) &amp;= Tf_1(x) = 1+x+\\frac{x^2}{2} - \\int_0^x 1+\\frac{x^2}{2} dt = 1 + \\frac{x^2}{2} - \\frac{x^3}{6}\\\\ &amp;...\\\\ f_n(x) &amp;= \\sum \\frac{(-x)^{n+1}}{(n+1)!} \\end{align*}\\] <p>Since \\(|x|\\leq 1/2\\), using M-test, this power series is uniform convergence, and note that \\(f_n(x):=e^{-x} + x\\)</p>"},{"location":"mat337/ODE.html#thrm-existence-of-ode","title":"Thrm.  Existence of ODE","text":"<p>If \\(\\Phi:[a,b]\\times \\mathbb R^n\\rightarrow \\mathbb R^n\\) is Lipschitz on the second coordinate \\(y\\in\\mathbb R^n\\). Then \\(\\exists F_*\\in C([a,b],\\mathbb R^n)\\) s.t. \\(F_*=TF_*=\\Gamma + \\int_a^x \\Phi(t,F_*(t))dt\\) </p> <p>proof. (Picard iteration) </p> <p>Recursively define \\(F_0(x)=\\Gamma, F_{k+1}(x)=TF_k = \\Gamma + \\int_a^x \\Phi(t, F_k(t))dt\\)</p> <p>Consider \\(F_{k+1}-F_k\\), let \\(M = \\|\\Phi(t,\\Gamma)\\|_\\infty\\), by EVT, since \\([a,b]\\) compact, \\(M\\) is the maximum. Then for any \\(x\\in [a,b]\\). \\(\\|F_1(x) - F_0(x)\\|  = \\int_a^x \\Phi(t,\\Gamma)dt\\leq M(x-a)\\) </p> \\[\\begin{align*} \\|F_{k+1}(x) - F_k(x)\\| &amp;= \\int_a^x \\Phi(t, F_{k}(t))-\\Phi(t, F_{k-1}(t))dt \\\\ &amp;\\leq L\\int_a^x\\|F_k - F_{k-1}\\|_\\infty dt &amp;L \\text{ is the Lipschitz constant} \\\\ &amp;\\leq L \\int_a^x \\frac{L^{k-1}M(x-a)^k}{k!} &amp;\\text{induction hypothesis}\\\\ &amp;= L^k \\frac{M(x-a)^{k+1}}{(k+1)!} \\end{align*}\\] <p>Then, for \\(n\\geq N\\), for some large \\(N\\)</p> \\[\\begin{align*} \\|F_n - F_0\\| &amp;\\leq \\sum^{n-1} \\|F_{k+1}-F_k\\| \\\\ &amp;\\leq \\sum^{n-1} \\frac{L^k M(x-a)^{k+1}}{(k+1)!} \\\\ &amp;\\leq \\frac{M}{L} \\sum^\\infty \\frac{(L(x-a))^{k+1}}{(k+1)!}\\\\ &amp;\\leq \\frac{M}{L} \\sum^\\infty \\frac{(L(b-a))^{k+1}}{(k+1)!} &lt; \\infty \\end{align*}\\] <p>Since the infinite sum is finite, the series is Cauchy. so that \\(F_n\\rightarrow^{u.c.}F_*\\in C([a,b], \\mathbb R^n)\\) by compactness and closeness. </p> <p>Therefore, \\(TF_* = T\\lim F_n = \\lim TF_n = F_*\\) using continuity</p> <p>Uniqueness Suppose exists \\(G_*\\), then </p> \\[\\begin{align*} TF_* - G_* &amp;= TTF_* - TG_* &amp;\\text{by fixed point assumption} \\\\ &amp;= T^kF_* - T^kG_* \\\\ &amp;\\leq \\frac{(L(x-a))^{k+1}M}{(k+1)!L} \\\\ &amp;\\sim 0 &amp;\\text{Sterling's formula} \\end{align*}\\]"},{"location":"mat337/ODE.html#example","title":"Example","text":"<p>\\(f'(x)=f(x), f(0)=1, x\\in [0,1]\\)</p> <p>proof. Let \\(F_0(x)=1, F_1(x) = 1 + \\int_0^x F_0(t)dt = 1 + \\int_0^x dt = 1 + x\\) Assume \\(F_n(x) = \\sum_0^k \\frac{x^m}{m!}\\), then </p> \\[F_{k+1}(x) = 1 + \\int_0^x F_n(t)dt = 1 + \\int_0^x \\sum_0^k \\frac{x^m}{m!} = \\sum_0^{k+1}\\frac{x^m}{m!}\\] <p>Therefore by induction, \\(F_*(x) = \\lim_nF_n(x) = \\sum^\\infty x^m/m! = e^x\\)</p>"},{"location":"mat337/approximations.html","title":"Function Approximations","text":""},{"location":"mat337/approximations.html#taylor-series","title":"Taylor Series","text":"<p>\\(f\\in C^n(\\mathbb R)\\), then \\(P_{n,a}(x):= \\sum_{i=0}^n \\frac{f^{(i)}(a)(x-a)^i}{i!}\\) is the Taylor polynomial around \\(a\\)</p>"},{"location":"mat337/approximations.html#defn-taylor-form-remainder","title":"Def'n. Taylor Form remainder","text":"<p>\\(f\\in C^n(\\mathbb R)\\) and \\(f^{(n+1)}\\) exists, then \\(f(x) = P_{n,a}(x) + \\frac{f^{(n+1)}(c)}{(n+1)!}(x-a)^{n+1}, c\\in [a,x]\\)</p>"},{"location":"mat337/approximations.html#thrm-1-uniqueness","title":"Thrm 1. Uniqueness","text":"<p>Claim. Taylor polynomial is unique  </p> <p>proof. Suppose \\(f(x) = \\sum a_k(x-a)^k + \\epsilon_1(x-a)^n = \\sum b_k(x-a)^k + \\epsilon_2(x-a)\\), then \\(\\sum(a_k-b_k)(x-a)^k + (\\epsilon_1-\\epsilon_2)(x-a)^n = 0\\), by independence of the system of equations, \\(a_n = b_n, \\epsilon_1 = \\epsilon_2\\)</p> <p>Claim. For \\(x,a\\in [-R,R], P_{a,n}\\rightarrow^{u.c.}f\\) </p> <p>proof. </p> \\[\\begin{align*} \\|f(x)-P_{a,n}(x)\\| &amp;\\leq \\frac{|f^{(n+1)(c)}|}{(n+1)!}|x-a|^{n+1}\\\\ &amp;\\leq \\frac{|f^{(n+1)(c)}|}{(n+1)!}|2R|^{n+1} \\\\ &amp;\\leq \\frac{M}{(n+1)!}(2R)^{n+1} &amp;\\text{by EVT}\\\\ &amp;\\leq \\frac{M}{n^ne^{-n}\\sqrt{2\\pi n}}(2R)^{n+1} &amp;\\text{Stirling's formula}\\\\ &amp;\\leq M(\\frac{2Re}{n+1})^{n+1}\\rightarrow 0 \\text{ for large }n \\end{align*}\\]"},{"location":"mat337/approximations.html#thrm-2weierstrass-theorem","title":"Thrm 2.Weierstrass Theorem","text":"<p>For any \\(a&lt;b, f\\in C[a,b]\\), exists \\(p_n\\rightarrow^{u.c.}f\\) </p> <p>proof. Define \\(g(x):[0,1]\\rightarrow \\mathbb R := f(a+x(b-a))\\). Take \\(q_n\\rightarrow^{u.c.}g\\), consider \\(y=a+x(b-a)\\), i.e. \\(x = \\frac{y-a}{b-a}\\) Define \\(p_n(y):=q_n(\\frac{y-a}{b-a}) = q_n(x)\\rightarrow^{u.c.}f(y)\\)</p>"},{"location":"mat337/approximations.html#thrm-2","title":"Thrm 2.","text":"<p>If \\(f\\in C^1[0,1]\\), then \\(\\exists \\{p_n\\}\\) over \\([0,1]\\), \\(p_n\\rightarrow^{u.c}f\\land p_n'\\rightarrow^{u.c}f'\\)</p> <p>proof. Let \\(f\\in C^1[0,1]\\), then \\(f'\\in C[0,1]\\), take \\(q_n\\rightarrow^{u.c}f'\\), then let \\(p_n = \\int_0^x q_n(x)dx + f(0)\\), </p> \\[\\begin{align*} p_n &amp;= \\int_0^x q_n(x)dx + f(0) + f(0) \\\\ &amp;\\rightarrow^{u.c.}\\int_0^x f'(x)dx +f(0)&amp;\\text{ICT} \\\\ &amp;=f(x)-f(0) + f(0)&amp;\\text{FTC II} \\\\ &amp;=f(x) \\end{align*}\\]"},{"location":"mat337/approximations.html#thrm-3","title":"Thrm 3.","text":"<p>If \\(f\\in C[0,1]\\) and \\(f(0)=0\\), then you can find \\(p_n\\rightarrow^{u.c.}f\\) s.t. \\(p_n(0)=0, p'_n(0)=0\\) </p> <p>lemma 1 If \\(f\\in C^1[-1,1]\\) is even, then exists \\(\\{p_n\\}\\) is even and \\(p_n\\rightarrow^{u.c.}f\\) </p> <p>proof. Take \\(q_n \\rightarrow^{u.c.}f\\) over \\([-1,1]\\) by Weierstrass Theorem. Define \\(p_n(x):= \\frac{q_n(x)+q_n(-x)}{2}\\), and by \\(f\\) even, known \\(f(x)=\\frac{f(x)+f(-x)}{2}\\)</p> \\[\\begin{align*} \\|p_n-f\\|_\\infty &amp;= \\sup_{x\\in[-1,1]}|\\frac{q_n(x)+q_n(-x)}{2} - \\frac{q_n(x)+q_n(-x)}{2}|\\\\ &amp;= \\frac{1}{2}\\sup|q_n(x)-f(x)+(q_n(-x)-f(-x))| \\\\ &amp;\\leq \\frac{1}{2}(\\sup|q_n(x)-f(x))| + \\sup|q_n(x)-f(x))|) &amp;\\text{tri.ineq}\\\\ &amp;\\leq \\frac{\\epsilon + \\epsilon}{2} = \\epsilon \\end{align*}\\] <p>proof. extend \\(f\\) to \\(F=\\begin{cases}f(x) &amp;[0,1]\\\\f(-x)&amp;[-1,0]\\end{cases}, F\\) is even. by lemma 1, take even \\(q_n\\rightarrow F\\). Then, take \\(p_n(x) := q_n(x)-q_n(0)\\) and (i) by \\(f(0)=0, q_n\\rightarrow F\\Rightarrow q_n(0) &lt; \\epsilon/2\\), \\(\\|p_n-f\\| \\leq \\|q_n-F\\|+ \\epsilon/2 = \\epsilon\\) (ii) \\(p_n(0)=q_n(0)-q_n(0)=0\\) (iii) \\(p_n\\) is even, hence \\(p_n(x)=\\frac{p_n(x)+p_n(-x)}{2}\\), \\(p_n'(x)=\\frac{p_n'(x)-p_n'(-x)}{2}\\Rightarrow p'_n(0)=0\\)</p>"},{"location":"mat337/approximations.html#thrm-4","title":"Thrm 4.","text":"<p>If \\(f\\in C(\\mathbb R), p_n\\rightarrow f\\) over \\(\\mathbb R\\), then \\(f\\) is a polynomial  </p> <p>proof. Since \\(p_n\\) uniform converge, \\(p_n\\) is also Cauchy. Take \\(N\\) large so that \\(\\forall n,m \\geq N. \\|p_m-p_n\\|_\\infty \\leq 1\\) Then \\(f(x)=(f(x)-p_m(x))+(p_m(x)-p_n(x))+p_n(x)\\). Consider \\(q_{n,m}(x) = p_m(x)-p_n(x)\\), it will also be a polynomial, and since \\(\\|q_{n,m}\\|\\leq 1\\), it must converges to some \\(a_{n,m}\\in[-1,1]\\).  By BW Theorem, take subsequence \\(a_{n,m_k}\\rightarrow a_n\\) for each \\(n\\). \\(f(x)=\\lim_{m_k\\rightarrow\\infty}f(x) = \\lim_k(f(x)-p_{m_k}(x)) + \\|p_m - p_n\\| + p_n(x) = 0 + a_n + p_n(x)\\)</p>"},{"location":"mat337/extremum.html","title":"Extremum","text":""},{"location":"mat337/extremum.html#defn-boundedness","title":"Def'n. Boundedness","text":"<p>Bound  A set \\(S\\subset\\mathbb{R}\\) is  </p> <ul> <li>bounded above \\(\\exists M\\in\\mathbb{R}. \\forall s \\in S. s\\leq M\\).  </li> <li>bounded below \\(\\exists L\\in\\mathbb{R}. \\forall s \\in S. s\\geq L\\).</li> </ul>"},{"location":"mat337/extremum.html#defn-extremum","title":"Def'n. Extremum","text":"<p>Supremum \\(\\sup(S)\\in\\mathbb{R}\\) is a upper bound and \\(\\forall v. v\\geq \\sup(S)\\land v\\) is upper bound. Infimum \\(\\inf(S)\\in\\mathbb{R}\\) is a lower bound and \\(\\forall v. v\\leq \\inf(S)\\land v\\) is lower bound. </p> <p>Proposition \\(u\\) is a supremum of \\(S\\) IFF \\(u\\) is a upper bound and \\(\\forall \\epsilon &gt; 0. \\exists s_\\epsilon \\in S. u-\\epsilon &lt; s_\\epsilon\\)</p>"},{"location":"mat337/extremum.html#least-upper-bound-principle","title":"Least upper bound principle","text":"<p>\\(\\forall S\\neq \\emptyset, S\\) bounded above. \\(S\\) has a supremum.  </p> <p>proof.  Since bounded above, take \\(M\\in\\mathbb{R}\\) where \\(\\forall s \\in S. M\\geq s\\).  Pick \\(s'=s_0\\),</p> \\[s_1 \\in S\\Rightarrow M\\geq s'\\Rightarrow m_0 + 1 &gt; s_0\\] <p>Find lowest \\(a_0\\in \\{s_0, ..., m_0 + 1\\}\\) that is an upper bound for \\(S\\).  </p> <p>Induction \\(y_n = \\sum_0^n a_i / 10^i\\) where \\(a_i=\\min\\{a \\in \\{0, 1, ..., 9\\}\\mid y_i \\geq S\\}\\). Take \\(x_n \\in S\\) s.t. \\(a_0.a_1...a_n - 10^{-n}\\leq x_n \\leq y_n\\)</p> <p>WTS \\(L = a_0.a_1... = sup(S)\\) </p> <p>Upper bound property: start with any \\(s_0.s_1,... \\in S\\) either 1. \\(\\forall i. s_i = a_i\\Rightarrow S = L\\) 2. \\(\\exists k\\) be the first different digit and \\(s_k &gt; a_k\\) Construct \\(y_k^* = a_0.a_1...a_{k-1}s_k\\). then \\(L &lt; y_k^*\\leq s_0.s_1\\), while by the ordering this cannot happen. </p> <p>Subsequence Property: For each \\(\\epsilon\\), you can pick \\(n &gt; 0\\) s.t. \\(L-\\epsilon \\leq L - 10^{-n} \\leq x_n \\leq y_n &lt; L\\)</p>"},{"location":"mat337/extremum.html#thrm-1-uniqueness-of-supremum","title":"Thrm 1. Uniqueness of supremum","text":"<p>proof Assume \\(\\exists u,v\\) be two supremums, \\(v &lt; u\\). Take \\(\\epsilon = u -v, \\forall \\epsilon &gt; 0. \\exists s_\\epsilon \\in S \\Rightarrow u - (u-v) &lt; s_\\epsilon \\Rightarrow v &lt; s_\\epsilon\\). </p>"},{"location":"mat337/extremum.html#thrm-2","title":"Thrm 2.","text":"<p>For all bounded above set \\(A\\) and \\(c\\geq 0\\). \\(\\sup(cA) = c \\sup(A)\\) </p> <p>proof. Let \\(M = \\sup(A)\\). Upper bound property: \\(\\forall s. s\\leq M \\Rightarrow cs \\leq cM\\) Subsequence property: Let \\(\\epsilon \\geq 0\\), take \\(s_{\\epsilon/c}\\), then \\(cs_{\\epsilon/c}\\geq u - \\epsilon\\)</p>"},{"location":"mat337/function_convergence.html","title":"Functions Convergence","text":""},{"location":"mat337/function_convergence.html#convergence","title":"Convergence","text":""},{"location":"mat337/function_convergence.html#defn-pointwise-convergence","title":"Def'n. Pointwise convergence","text":"<p>Given \\(\\{h_k\\}\\subset C(S,\\mathbb R)\\). We say that they converge pointwise if \\(\\forall x\\in S. h_k(x)\\rightarrow h(x)\\). </p> \\[\\forall \\epsilon &gt; 0. \\forall x \\in S. \\exists N &gt; 0. \\forall k \\geq N. |f_k(x)-f(x)|&lt;\\epsilon\\]"},{"location":"mat337/function_convergence.html#defn-convergence-uniformly","title":"Def'n. Convergence Uniformly","text":"<p>\\(f_k\\rightarrow^{u.c.}f\\) if \\(\\|f_k - f\\|_{\\infty}\\rightarrow 0\\). </p> \\[\\forall \\epsilon &gt; 0. \\exists N &gt; 0. \\forall k\\geq N. \\sup_{x\\in S}|f_k(x)-f(x)|\\leq \\epsilon\\]"},{"location":"mat337/function_convergence.html#example","title":"Example","text":"<p>consider  \\(f_k(x):=     \\begin{cases}         k^2 x &amp; 0\\leq x\\leq 1/k \\\\        k^2 x (\\frac{2}{k}-x) &amp; 1/2 \\leq x \\leq 2/k \\\\        0 &amp;2/k \\leq x \\leq 1     \\end{cases}\\) </p> <p>Claim \\(f_k \\rightarrow^{p.w.} 0\\) </p> <p>proof. Let \\(x\\in[0,1]\\). take \\(N &gt; 0\\) s.t. \\(2/N \\leq x\\). Therefore, \\(\\forall k \\geq N, |f_k(x)-0|&lt;\\epsilon\\)</p> <p>Claim \\(f_k\\not\\rightarrow^{u.c.} 0\\) </p> <p>proof. \\(\\sup_{x\\in[0,1]}|f_k(x)-0|=k\\rightarrow\\infty\\)</p> <p>Claim \\(\\lim_{k\\rightarrow\\infty}\\int_0^1 f_k(x)dx \\neq \\int_0^1 \\lim_{k\\rightarrow\\infty}f_k(x)dx\\) </p> <p>proof. \\(LHS=\\lim_{k\\rightarrow\\infty}\\int_0^{1/k} k^2 x dx + \\int_{1/k}^{2/k} k^2(\\frac{2}{k}-x)dx + 0 = \\lim_{k\\rightarrow\\infty}1=1\\) \\(RHS = 0 \\neq 1\\)</p>"},{"location":"mat337/function_convergence.html#example_1","title":"Example","text":"<p>Let \\(f_n:[0,A]\\rightarrow \\mathbb R, f(x):= \\frac{\\sin(nx)}{n}\\rightarrow^{u.c.}0\\).  </p> <p>proof. \\(\\sup_{x\\in[0,A]}|\\frac{\\sin(nx)}{x} - 0| \\leq 1/n \\rightarrow 0\\)</p>"},{"location":"mat337/function_convergence.html#thrm-1","title":"Thrm 1.","text":"<p>Claim u.c. does not preserve derivatives  </p> <p>proof. \\(f'(x)=\\frac{n\\cos(nx)}{n} = cos(nx)\\) doesn't converge to any function. </p>"},{"location":"mat337/function_convergence.html#thrm-2-dinis-theorem","title":"Thrm 2. Dini's Theorem","text":"<p>If \\(f,f_n\\in C([a,b]). (\\forall n \\geq 1. f_n \\leq f_{n+1} \\land f_n(x)\\rightarrow f(x))\\Rightarrow f_n\\rightarrow^{u.c.} f\\)</p> <p>proof. Since \\(f_n \\leq f_{n+1}\\land f_n \\rightarrow f \\Rightarrow g(x):=f(x)-f_n(x)\\geq 0\\) </p> <p>(i) By continuity at \\(x_0\\), take \\(\\delta_1 &gt; 0, \\forall y\\in\\mathbb R. |x_0 - y|\\Rightarrow |g(x_0) - g(y)|\\leq \\epsilon/2\\) </p> <p>(ii) By \\(f_n(x_0)\\rightarrow^{p.w.}f(x_0)\\), take \\(N&gt;0, \\forall n &gt; N. |f_n(x_0)-f(x_0)|\\leq \\epsilon/2\\) </p> \\[\\begin{align*} |g(y)| &amp;= |f(y)-f_n(y)| \\\\ &amp;\\leq |f(x_0)-f_n(x_0)| + |g(x_0) - g(y)| \\\\ &amp;\\leq \\epsilon /2 + \\epsilon /2 = \\epsilon. &amp;\\forall y\\in (x_0-\\delta, x_0 + \\delta) \\end{align*}\\] <p>Then, suppose that we don't have u.c. \\(\\Rightarrow \\sup_{x\\in[a,b]}|g(x)|=\\sup_{x\\in[a,b]} |f(x)-f_n(x)|\\geq d, d &gt; 0\\). \\(\\|g\\|_{\\infty}&gt;d\\Rightarrow \\forall n &gt; 1. \\exists x_n \\in [a,b]. g_n(x_n)\\geq d\\) By \\(\\{x_n\\}\\in[a,b]\\), with bolzano weierstrass theorem, \\(x_{n_k}\\rightarrow x_0 \\in [a,b]\\). But we show for \\(\\epsilon &lt; d. \\exists \\delta &gt; 0. |x_0 - y|&lt;\\delta\\Rightarrow |g(y)|&lt; \\epsilon &lt; d\\). For \\(\\delta, \\exists N&gt;0\\) so that \\(\\forall k\\geq N.|x_{n_k}-x_0|\\leq \\delta\\). Take \\(y = x_{n_k}\\Rightarrow |g(x_{n_k})|&lt;\\epsilon &lt; d\\) causes contradiction</p>"},{"location":"mat337/function_convergence.html#uniform-convergence-and-completeness","title":"Uniform convergence and completeness","text":""},{"location":"mat337/function_convergence.html#thrm-3","title":"Thrm 3.","text":"<p>If \\(f_k\\in C([a,b]),f_k \\rightarrow^{u.c.}f\\), then \\(f\\in C([a,b])\\) </p> <p>proof. Let \\(\\epsilon &gt; 0\\), and \\(x_0\\in [a,b]\\). By uniform continuous, take \\(N_{\\epsilon/3} &gt; 0, \\forall k &gt; N_{\\epsilon/3}. \\sup_{x\\in[a,b]}|f_k(k)-f(x)|\\leq \\epsilon/3\\) By continuity of \\(f_k\\), take \\(\\delta_{\\epsilon/3} &gt; 0, |x_0 - y|\\leq \\delta \\Rightarrow |f_{N_{\\epsilon/3} }(x_0) - f_{N_{\\epsilon/3} }(y)|\\leq \\epsilon/3\\) Take \\(\\delta = \\delta_{\\epsilon/3}\\),  </p> \\[\\begin{align*} |f(x_0) - f(y)| &amp;\\leq |f(x_0) - f_{N_{\\epsilon/3} }(x_0)| &amp;\\text{by uniform continuous} \\\\ &amp;\\quad+ |f_{N_{\\epsilon/3} }(x_0) - f_{N_{\\epsilon/3} }(y)|&amp;\\text{by continuity}  \\\\ &amp;\\quad+ |f_{N_{\\epsilon/3} }(y) - f(y)|&amp;\\text{by uniform continuous}  \\\\ &amp;= \\epsilon/3 + \\epsilon/3 + \\epsilon/3 = \\epsilon \\end{align*}\\]"},{"location":"mat337/function_convergence.html#example_2","title":"Example","text":"<p>\\(f_n(x) = (1 + \\frac{x}{n})^n\\rightarrow^{u.c.} e^x := \\sum^\\infty \\frac{x^k}{k!}\\) over any interval \\([a,b]\\)</p> <p>proof. </p> \\[\\begin{align*} \\sup_{x\\in [a,b]} |(1+\\frac{x}{n})^n - e^x| &amp;=\\|\\sum_{k=1}^n {n\\choose k}\\frac{x^k}{n^k} - \\sum\\frac{x^k}{k!}\\|_\\infty \\\\ &amp;\\leq |\\sum^\\infty \\frac{x^k}{k!}| &amp;{n\\choose k}/n^k \\leq \\frac{n}{k!}\\\\ &amp;\\leq |\\sum \\frac{b^k}{k!}| &amp;x\\in [a,b] \\end{align*}\\] <p>By big-oh, Take \\(N_\\epsilon &gt; 0, \\forall n \\geq N. \\sum \\frac{b^k}{k!} \\leq \\epsilon\\)</p>"},{"location":"mat337/function_convergence.html#thrm-4","title":"Thrm 4.","text":"<p>Claim \\(\\lim_{n\\rightarrow\\infty}\\int_0^1 f_n(x)dx = \\int_0^1 \\lim_{n\\rightarrow\\infty}f_n(x)dx = \\int_0^1 e^x dx = e-1\\) </p> <p>proof. </p> \\[\\begin{align*} \\int_0^1 (1 + \\frac{x}{n})^n dx &amp;= \\big(\\frac{ {n+x}^{n+1} }{(n+1)n^n}\\big)\\big|_0^1  \\\\ &amp;= \\frac{(n+1)^{n+1} }{(n+1)n^n} - \\frac{n^{n+1} }{(n+1)n^n}\\\\  &amp;= (1 + \\frac{1}{n})^n - \\frac{n}{n+1}\\\\  \\lim_{n\\rightarrow \\infty} (1 + \\frac{1}{n})^n - \\frac{n}{n+1} &amp;= e - 1\\\\ \\end{align*}\\]"},{"location":"mat337/function_convergence.html#example_3","title":"Example","text":"<p>Point-wise convergence does not imply uniform continuous  </p> <p>proof. Take \\(f_n:[0,1]\\rightarrow \\mathbb{R} : = x^n\\). \\(f_n(x_0) \\rightarrow^{p.w.} \\mathbb I (x_0 = 1)\\) But \\(\\sup_{x\\in[0,1]}|x_n - \\mathbb I(x = 1)| = \\sup_{x\\in(0,1)}|x^n|\\). Therefore, for \\(x_0 = 0.5^{1/n}, f_n(x_0) = 1/2 &gt; 0\\). You will lose u.c. when you arbitrarily close to 1. </p>"},{"location":"mat337/function_convergence.html#example_4","title":"Example","text":"<p>\\(\\forall f,g\\in C(K). K\\) compact, if \\(f_n \\rightarrow^{u.c.} f, g_n \\rightarrow^{u.c.} g\\), then \\(f_n g_n \\rightarrow^{u.c.} f g\\) </p> <p>proof. Let \\(\\epsilon &gt; 0\\), for all \\(x\\in K\\) </p> \\[\\begin{align*} |f_n(x)g_n(x) - f(x)g(x)| &amp;= |f_n(x)g_n(x) - g(x)f_n(x) + g(x)f_n(x) - f(x)g(x)| \\\\ &amp;\\leq |f_n(x)||g_n(x)-g(x)| + |g(x)||f_n(x)-f(x)| \\\\ &amp;&lt; B_1 |g_n(x)-g(x)| + B_2 |f_n(x)-f(x)|&amp;\\text{By EVT on $f_n, g_n$, since $K$ compact} \\\\ &amp;\\leq B_1 \\|g_n - g\\|_\\infty + B_2 \\|f_n-f\\|_{\\infty}&amp;\\text{By u.c. of $f,g$} \\\\ \\text{take } N_1&amp;\\implies \\forall n \\geq N_1 . \\|g_n-g\\|_\\infty \\leq |\\frac{\\epsilon}{2B_1}|, N_2\\\\&amp;\\implies \\forall n \\geq N_2 . \\|f_n-f\\|_\\infty \\leq |\\frac{\\epsilon}{2B_2}| \\\\  &amp;\\leq \\epsilon \\end{align*}\\]"},{"location":"mat337/function_convergence.html#integral-convergence-theorem","title":"Integral Convergence Theorem","text":"<p>If \\(f_n \\in C([a,b])\\land f_n \\rightarrow^{u.c.}f\\), then \\(\\forall [c,d]\\subseteq [a,b]. \\lim_{n\\rightarrow\\infty}\\int_c^d f_n(t)dt = \\int_c^d \\lim_{n\\rightarrow\\infty}f_n(t)dt\\)</p> <p>proof. </p> \\[\\begin{align*} \\Big|\\int_c^d f_n(t) - f(t)dt\\Big| &amp;\\leq \\int_c^d |f_n(t)-f(t)|dt  \\\\ &amp;\\leq \\int_c^d \\sup_{s\\in[a,b]}|f_n(s)-f(s)|dt \\\\ &amp;= \\sup_{s\\in[a,b]} |f_n(s)-f(s)| \\int_c^d d_t \\\\ &amp;= \\|f_n-f\\|_\\infty(d-c) \\\\ &amp;\\text{by u.c., take }\\tilde\\epsilon = \\frac{\\epsilon}{b-a} s.t. \\exists N_{\\tilde \\epsilon} \\geq 0. \\forall n\\geq N_{\\tilde\\epsilon}, \\|f_n-f\\|_\\infty \\leq \\tilde\\epsilon  \\\\ &amp;\\leq \\frac{d-c}{a-b}\\epsilon \\end{align*}\\] <p>Note that if \\([a,b]=[0,\\infty)\\), then the proof breaks down. because \\(\\int_c^d dt\\) can be arbitrarily large, so one fix is to have a density \\(\\rho(x)\\) so that \\(\\int_c^d \\rho(s)ds\\leq B\\)</p>"},{"location":"mat337/function_convergence.html#thrm-5-leibrizs-rule-differentiation-under-the-integral","title":"Thrm 5. Leibriz's Rule (differentiation under the integral)","text":"<p>\\(\\forall f(x,t), d_x f(x,t) \\in C([a,b]\\times[c,d])\\). If \\(f(x) = \\int_c^d f(x,t)dt\\), then \\(\\frac{dF}{dx} = \\int_c^d d_x f(x,t)dt\\) </p> <p>proof. Fix \\(x_0\\in[a,b]\\),</p> \\[F'(x_0) = \\lim_{h\\rightarrow 0}\\frac{F(x_0 + h) - F(x_0)}{h}\\] <p>Consider the RHS, </p> \\[\\begin{align*} \\lim_{h\\rightarrow 0}\\frac{F(x_0 + h) - F(x_0)}{h} &amp;= \\frac{1}{h}\\int_c^d f(x_0+h,t)-f(x_0, t)dt \\\\ \\text{by MVT, take }\\xi(t,h)\\in [x_0, x_0+h], &amp;\\frac{F(x_0 + h) - F(x_0)}{h} = d_x f(\\xi(t,h), t) \\\\ &amp;= \\int_c^d d_xf(\\xi(t,h),t)dt \\end{align*}\\] <p>WTS given \\(\\epsilon\\), can find \\(h\\) small enough so that \\(|\\int_c^d d_xf(\\xi(t,h), t) - d_xf(x_0, t)dt|\\leq \\epsilon\\) Using continuity of \\(d_xf(x_0, t)\\), take \\(\\delta &gt; 0. |x_0 - y|&lt;\\delta \\Rightarrow |d_xf(y,t) - d_xf(x_0,t)|\\leq \\frac{\\epsilon}{d-c}\\) </p> <p>Since \\(|\\xi(t,h)-x_0|&lt;h\\), take \\(\\delta &lt; h\\), then \\(|d_xf(\\xi(t,h), t) - d_xf(x_0, t)|\\leq \\frac{\\epsilon}{d-c}\\),  </p> \\[\\begin{align*}  |\\int_c^d d_xf(\\xi(t,h), t)-d_xf(x_0, t)dt| &amp;\\leq \\int_c^d |d_xf(\\xi(t,h), t)-d_xf(x_0, t)|dt \\\\  &amp;\\leq \\frac{\\epsilon}{d-c}\\int_c^d dt = \\epsilon \\end{align*}\\] <p>Therefore, \\(\\epsilon&gt;0, \\exists h\\) small enough so that </p> \\[|\\frac{F(x_0 + h, t) - F(x_0)}{h} - \\int_c^d d_xf(x_0,t)dt|\\leq \\epsilon\\] <p>Therefore, </p> \\[F'(x_0)=\\lim_{h\\rightarrow 0} \\frac{F(x_0+h)-F(x)}{h} = \\int_c^d d_xf(x_0, t)dt\\]"},{"location":"mat337/inner_product_and_fourier.html","title":"Inner Product Space and Fourier Series","text":""},{"location":"mat337/inner_product_and_fourier.html#inner-products-spaces","title":"Inner Products Spaces","text":"<p>An inner product \\(\\langle \\cdot, \\cdot\\rangle :V\\rightarrow \\mathbb R\\) is a function that satisfies </p> <ul> <li>positive definite \\(\\langle x,x\\rangle &gt; 0\\) and \\(\\langle x,x\\rangle =0\\) IFF \\(x\\equiv0\\)</li> <li>symmetry \\(\\langle x,y\\rangle =\\langle y,x\\rangle\\)</li> <li>bilinear \\(\\langle ax+cy,z\\rangle =a\\langle x,z\\rangle +c\\langle y,z\\rangle\\)</li> </ul>"},{"location":"mat337/inner_product_and_fourier.html#example-1","title":"Example 1","text":"<p>On \\(\\mathbb R^n, x\\cdot y = \\sum_1^n x_i y_i\\) </p> <ul> <li>\\(x\\cdot x =\\sum^n x_i^2 \\geq 0\\)</li> <li>\\(x\\cdot y = \\sum x_i y_i = \\sum y_i x_i = y\\cdot x\\)</li> <li>\\((ax+cy)\\cdot z = \\sum (ax_i + cy_i) z_i = a\\sum x_i z_i + c\\sum y_i z_i = ax\\cdot z + cy\\cdot z\\) </li> </ul>"},{"location":"mat337/inner_product_and_fourier.html#example-2","title":"Example 2","text":"<p>Let \\(A_{n\\times n}\\) be a positive definite symmetric matrix, i.e. all its eigenvalues are strictly positive. Then \\(\\langle x\\cdot y\\rangle _A = x A y^T\\) is an inner product on \\(\\mathbb R^n\\)</p> <p>Note that such \\(A\\) can be diagonalized into \\(A=PDP^T\\)</p> <ul> <li>\\(\\langle x,x\\rangle _A = xAx^T = xPDPx^T = vDv^T = \\sum v_k^2 \\lambda_k \\rangle 0\\) (since \\(A\\) is definite, \\(\\lambda_k \\rangle 0\\))  </li> <li>\\(\\langle x,x\\rangle =0\\) IFF \\(\\sum v_k^2 \\lambda_k = 0\\Rightarrow v_k = 0 \\Rightarrow xP = 0\\), since \\(P\\) is orthogonal, \\(x=0\\)</li> </ul>"},{"location":"mat337/inner_product_and_fourier.html#thrm-1-cauchy-schwarz-inequality","title":"Thrm 1. Cauchy Schwarz inequality","text":"<p>For \\(x,y\\in (V,\\langle ,\\rangle ), |\\langle x,y\\rangle |\\leq \\langle x,x\\rangle ^2\\langle y,y\\rangle ^2\\) </p> <p>proof. By positive definite, for \\(t\\in\\mathbb R\\), \\(\\langle x-ty, x-ty\\rangle  \\geq 0\\). Take \\(t = \\frac{\\langle x,y\\rangle }{y,y}\\),  </p> \\[\\begin{align*} \\langle x,x\\rangle  - 2t |\\langle x,y\\rangle | + t^2 \\langle y,y\\rangle &amp;= \\langle x,x\\rangle  - 2\\frac{\\langle x,y\\rangle ^2}{\\langle y,y\\rangle} + \\frac{\\langle x,y\\rangle ^2}{\\langle y,y\\rangle }\\\\ =&amp;\\langle x,x\\rangle  - \\frac{\\langle x,y\\rangle ^2}{\\langle x,y\\rangle } \\geq 0\\\\ \\implies &amp;\\langle x,y\\rangle ^2 \\leq \\langle x,x\\rangle \\langle y,y\\rangle\\\\ \\end{align*}\\]"},{"location":"mat337/inner_product_and_fourier.html#example-3","title":"Example 3","text":"<p>Over \\(C[a,b]\\), \\(\\langle f,g\\rangle _{L^2} = \\int_a^b f g dx\\)</p> <ul> <li>\\(\\langle f,f\\rangle _{L^2} = \\int_a^b f^2 dx \\geq 0\\) </li> <li>\\(\\langle f,f\\rangle _{L^2}=0\\) IFF \\(\\int_a^b |f|^2 dx = 0\\) IFF \\(f(x)=0\\) because \\(f\\) is continuous.</li> </ul> <p>Assume \\(f(x_0) &gt;  0\\), then by continuity, take \\(I_\\delta(x_0):=[x_0-\\delta, x_0+\\delta], \\forall x\\in I_\\delta(x_0). f(x) &gt;  \\epsilon &gt; 0\\) so that \\(\\int_a^b |f|^2 dx &gt;  \\int_{I_\\delta(x_0)} |f(x)|^2 dx &gt;  \\epsilon^2\\) contradicts with \\(\\langle f,f\\rangle _{L^2}=0\\) (other details to be filled)</p> <p>By Cauchy Schwartz inequality, </p> \\[\\begin{align*}   &amp;\\int fg\\leq \\sqrt{\\int f^2}\\sqrt{\\int g^2}\\\\ \\implies &amp;\\langle f,g\\rangle _{L^2} \\leq \\|f\\|_{L^2}\\|g\\|_{L^2}\\\\ \\implies &amp;f\\in L^2([a,b])\\\\ \\implies &amp;f\\in L^1([a,b])\\\\ \\end{align*}\\] <p>Therefore, every inner product gives rise to a norm \\(\\|x\\|_v := \\sqrt{\\langle x,x\\rangle }\\)</p>"},{"location":"mat337/inner_product_and_fourier.html#defn-hilbert-space","title":"Def'n. Hilbert space","text":"<p>a complete inner product space is called Hilbert space</p> <p>Completeness over inner product space A inner product space is complete if every Cauchy sequence \\(x_n\\) has a limit in the space. i.e.  </p> \\[\\|x_n - x\\|_V^2 = \\langle x_n - x, x_n - x\\rangle \\rightarrow 0\\] \\[L^2([a,b]):= \\{f:[a,b]\\rightarrow \\mathbb R: \\int_a^b |f|^2 \\langle  \\infty\\}\\] <p>Remark The space \\(S:=(C[a,b], \\|\\cdot\\|_{L^2})\\) is not complete, but \\(L^2([a,b])\\) is the completion of \\(S\\). which means taking any Cauchy sequence, then it has a limit in \\(L^2\\). In another words, continuous functions can approximate any square integrable function in \\(L^2\\)-norm.</p>"},{"location":"mat337/inner_product_and_fourier.html#thrm-2-monotone-convergence-theorem","title":"Thrm 2. Monotone Convergence Theorem","text":"<p>Take nonnegative \\(f_n(x)\\geq 0\\), if  \\(f_1(x)\\leq f_2(x) \\leq \\cdots\\)  then point-wise limit \\(f_n(x)\\rightarrow f(x)\\) i.e. \\(|f_n(x)-f(x)|\\rightarrow 0\\)</p> <p>Corollary By MCT, \\(\\lim \\int f_n = \\int \\lim f_n = \\int f\\)</p>"},{"location":"mat337/inner_product_and_fourier.html#thrm-3-dominatedintegral-convergence-theorem","title":"Thrm 3. Dominated/Integral Convergence Theorem","text":"<p>If \\(\\{h_n\\}\\) satisfy </p> <ul> <li>\\(|h_n(x)|\\leq B(x)\\), which \\(B\\) is an integrable function</li> <li>\\(h_n(x)\\rightarrow^{\\text{point-wise}} h(x)\\)</li> </ul> <p>Then, \\(\\lim \\int f_n = \\int \\lim f_n = \\int f\\)</p>"},{"location":"mat337/inner_product_and_fourier.html#thrm-4-completeness","title":"Thrm 4. Completeness","text":"<p>The space \\((L^2, \\|\\cdot\\|_{L^2})\\) is complete</p> <p>proof. Take Cauchy sequence \\(f_n \\in L^2([a,b])\\). By Cauchy, take subsequence \\(f_{n,k}\\) s.t. \\(\\|f_{n,k} - f_{n,k+1}\\| \\leq 2^{-k}\\). Let \\(f(x):=f_{n,1}(x) + \\sum (f_{n, k+1} - f_{n,k}(x))\\). WTS \\(f\\in L^2 (i)\\), \\(f_{n,k}\\rightarrow^{L^2} f (ii)\\). Define \\(g:= |f_n,1| + \\sum|f_{n,k+1}- f_{n,k}|\\), \\(S_M(g) = |f_n,1| + \\sum^M|f_{n,k+1}- f_{n,k}|\\), then \\(f\\leq g, S_M(f) \\leq S_M(g)\\) Apply MCT to \\(\\{S_M(g)\\}, 0\\leq |f_{n,1}| \\leq |f_{n,2}-f_{n,1}|\\leq ..., S_1(g) \\leq S_2(g) \\leq S_3(g)\\) Then, WTS \\(S_M(g)\\rightarrow g(x)\\), which suffices to show that the sequence \\(g\\) converges, a.k.a. \\(g\\in L^2\\) </p> \\[\\begin{align*} \\|S_M(g)\\|_{L^2} &amp;\\leq \\|f_{n,1}\\|_{}L^2 + \\sum^M \\|f_{n,k+1} - f_{n,k}\\| \\\\ &amp;\\leq \\|f_{n,1}\\| + \\sum^M 2^{-k} &amp;\\text{tri. ineq.}\\\\ &amp;&lt;\\infty \\end{align*}\\] <p>Thus we have that </p> \\[\\begin{align*} \\int |g^2| dx &amp;= \\int \\lim_{M\\rightarrow \\infty} |S_M(g)^2|dx\\\\ &amp;=\\lim_{n\\rightarrow\\infty} \\int |S_M(g)|^2 dx &amp;\\text{MCT}\\\\ &amp;\\leq \\lim_{M\\rightarrow \\infty} \\|f_{n,1}\\| + \\sum^M 2^{-k} \\leq \\|f_{n,1}\\| + 2 \\\\ &amp;&lt; \\infty \\end{align*}\\] <p>We have Cauchy seuqnce \\(f_n \\in L^2\\), WTS \\(\\|f_n -f\\|_{L^2}\\rightarrow 0\\) </p> <p>(i) Since \\(|f|\\leq g\\) and \\(g\\in L^2, \\int |f|^2 \\leq \\int |g|^2 &lt; \\infty\\implies f\\in L^2\\) </p> <p>(ii) \\(\\|f_n - f\\|_{L^2}^2 = \\int |f_n - f|^2 dx\\rightarrow 0\\).</p> <p>Let \\(h_k:=|f_{n,k}- f|^2\\), then  </p> \\[h_k \\leq |S_k(g)| + |g| \\leq 2|g|\\in L^2\\] <p>so that \\(f_{n,k}\\rightarrow^{\\text{p.w.}} f\\) since \\(g\\in L^2\\) so \\(\\lim_{k\\rightarrow \\infty} f_{n,k}(x)=f(x)\\) exists BY DCT, \\(\\lim_{k\\rightarrow \\infty} \\int |h_k|^2 = \\int \\lim_{k\\rightarrow \\infty} |h_k|^2 =0\\),  </p> <p>Therefore, \\(f_{n,k}\\rightarrow^{L^2}f\\)</p> <p>Then, expansion \\(f_{n,k}\\) to \\(f_n\\). i.e. \\(f_n \\rightarrow^{L^2} f\\). By Cauchy sequence of \\(f_n\\), if the sub-sequence converges to some limit, so that the whole sequence will do so. </p>"},{"location":"mat337/inner_product_and_fourier.html#fourier-series","title":"Fourier Series","text":""},{"location":"mat337/inner_product_and_fourier.html#defn-orthogonal","title":"Def'n. Orthogonal","text":"<p>Given \\(x,y\\ in (V,\\langle\\cdot, \\cdot\\rangle)\\), x,y are orthogonal if \\(\\langle x,y\\rangle  = 0\\) </p>"},{"location":"mat337/inner_product_and_fourier.html#example","title":"Example","text":"<p>\\(x\\circ y = \\cos(\\theta) |x||y|\\), so when \\(\\theta = \\pi /2, x\\circ y = 0\\)</p>"},{"location":"mat337/inner_product_and_fourier.html#defn-orthogonal-set","title":"Def'n. Orthogonal set","text":"<p>A collection of \\(\\{e_k\\}\\) is called orthonormal if \\(\\langle e_k,e_m\\rangle  = \\mathbb I(k=m)\\)</p>"},{"location":"mat337/inner_product_and_fourier.html#thrm-4","title":"Thrm 4.","text":"<p>Every Hilbert space has orthonormal basis, i.e. \\(\\forall x\\in H. x = \\sum^\\infty \\langle x,e_k\\rangle  e_k\\)</p>"},{"location":"mat337/inner_product_and_fourier.html#example_1","title":"Example","text":"<p>Take \\(e_k = (0,...,0,1,0)\\) where \\(1\\) is at \\(k\\)th position, \\(\\{e_k\\}\\) is orthonormal set. </p> <p>proof. \\(e_k \\cdot e_m = \\sum^n (e_k)_i (e_m)_i = 1\\mathbb I(k=m)\\)</p>"},{"location":"mat337/inner_product_and_fourier.html#example_2","title":"Example","text":"<p>\\(\\{1, \\sqrt2\\cos(n\\theta), \\sqrt2\\sin(n\\theta)\\}\\) is orthogonal for \\((C([-\\pi, \\pi]), \\|\\cdot\\|_{L^2})\\)</p> <p>proof. We have the followings hold</p> \\[\\begin{align*} &amp;\\frac{\\sqrt2}{\\pi}\\int_{-\\pi}^\\pi \\cos(n\\theta)\\cos(k\\theta) = 1\\mathbb I(n=k)\\\\ &amp;\\frac{\\sqrt2}{\\pi}\\int_{-\\pi}^\\pi \\sin(n\\theta)\\sin(k\\theta) = 1\\mathbb I(n=k)\\\\ &amp;\\frac{\\sqrt2}{\\pi}\\int_{-\\pi}^\\pi \\sin(n\\theta)\\cos(k\\theta) = 0 \\end{align*}\\]"},{"location":"mat337/inner_product_and_fourier.html#defn-fourier-series","title":"Def'n. Fourier Series","text":"<p>For function \\(f\\in L^2([a,b])\\), the Fourier series is defined to be </p> \\[f(x):= a_0 + \\sum a_n \\frac{\\cos(n\\pi x)}{L} + b_n \\frac{\\sin(n\\pi x)}{L}\\] <p>where</p> \\[a_n = \\langle f, \\frac{\\cos(n\\pi x)}{L})\\rangle _{L^2}, b_n = \\langle f, \\frac{\\sin(n\\pi x)}{L}\\rangle _{L^2}\\]"},{"location":"mat337/inner_product_and_fourier.html#thrm-carleson-hunt-theorem","title":"Thrm. Carleson Hunt Theorem","text":"<p>If \\(f\\in L^2([-L,L])\\), then \\(f\\) equals its Fourier series almost everywhere. almost everywhere outside a measure zero set</p>"},{"location":"mat337/mct_bwt.html","title":"MCT and BWT","text":""},{"location":"mat337/mct_bwt.html#monotone-convergence-theorem","title":"Monotone Convergence Theorem","text":"<p>Monotone increasing and bounded above \\(\\Rightarrow\\) converges to supremum. </p>"},{"location":"mat337/mct_bwt.html#defn-monotonic","title":"Def'n. Monotonic","text":"<p>A sequence \\((a_n)\\) is (strictly) monotone increasing if</p> \\[a_n\\leq a_{n+1}(a_n&lt;a_{n+1}), \\forall n\\geq 1\\]"},{"location":"mat337/mct_bwt.html#thrm-1","title":"Thrm 1.","text":"\\[\\lim_{n\\rightarrow\\infty}a_n = \\inf(a_n)\\] <p>proof. We'd like to show that </p> \\[\\forall \\epsilon. \\exists N. \\forall n \\geq N. |a_n - L| \\leq \\epsilon\\] <p>Let \\(L=\\inf(a_n)\\). Let \\(\\epsilon &gt; 0\\), take \\(a_N \\in (a_n)\\) s.t. \\(L-\\epsilon &lt; a_N\\). Then \\(L-\\epsilon &lt; a_N \\leq a_n \\leq L\\) for all \\(n\\geq N. \\Rightarrow \\forall n\\geq N. |a_n-L|&lt;\\epsilon\\)</p>"},{"location":"mat337/mct_bwt.html#corollary-2","title":"Corollary 2.","text":"<p>Consider closed, non-empty nested intervals \\(I_n\\supseteq I_{n+1}\\supseteq I_{n+2}...\\), then then \\(\\cap^\\infty_n I_n\\neq \\emptyset\\)</p> <p>proof. Let \\(I_n=[a_n,b_n]\\), then we have two monotone sequence \\((a_n)\\) is increasing, \\((b_n)\\) is decreasing. Then, \\(\\forall k\\geq 1. a_k \\leq \\lim_\\infty a_n\\leq \\lim_\\infty b_n \\leq b_k\\)</p> <p>Then, we can show \\(\\Rightarrow [lim_\\infty a_n, \\lim_\\infty b_n]=[\\sup(a_n),\\inf(b_n)]\\subseteq \\cap_{n\\geq 1} I_n\\)</p>"},{"location":"mat337/mct_bwt.html#example-1","title":"Example 1","text":"<p>(D&amp;D 2.6.B) Let \\(a_1 = 0, a_{n+1}=\\sqrt{5 + a_n}\\), find whether it's convergent and if convergent what's the limit. </p> <p>Monotone </p> \\[\\begin{align*} a_2 &amp;= \\sqrt 5 &gt; 0 = a_1\\\\ a_{n+1} &amp;= \\sqrt{5 + a_n} &gt; \\sqrt{5 + a_{n-1}}=a_n \\end{align*}\\] <p>Bounded above \\(a^2_{n+1} =5 + a_n &lt; 5 + a_{n+1}\\). Let \\(x = a_{n+1}\\Rightarrow x^2 &lt; 5 + x\\Rightarrow x^2 -x -5 &lt; 0\\). \\(x\\in [\\frac{1-\\sqrt{21}}{2}, \\frac{1+\\sqrt{21}}{2}]\\Rightarrow a_{n+1}=x\\) is bounded above.  </p> <p>In fact, \\(L = \\lim\\sqrt{5 + a_{n-1}}=\\sqrt{5+L} \\Rightarrow L^2 = L - 5\\). solve and take \\(L &gt; 0\\) since monotone increasing, then \\(L=\\frac{1+\\sqrt{21}}{2}\\)</p>"},{"location":"mat337/mct_bwt.html#example-2","title":"Example 2","text":"<p>(D&amp;D 2.6.I) Let \\((a_n)\\) be bounded, define \\(\\lim\\sup a_n = b_n = \\sup\\{a_k: k\\geq n\\}\\) for \\(n\\geq 1\\). Prove that \\((b_n)\\) converges. </p> <p>Monotone \\(b_n\\leq b_{n+1}\\) since \\(b_n = \\max(a_n, \\sup\\{a_k: k\\geq n+1\\})=\\max(a_n,b_{n+1})\\geq b_{n+1}\\)</p> <p>Bounded below \\(\\forall n \\geq 1. a_n\\geq M, b_n = \\sup(a_k:k\\geq n )\\geq M\\)</p> \\[\\exists L&lt;\\infty.  \\lim b_n = \\lim_{n\\rightarrow\\infty}\\sup_{k\\geq n} a_nL\\] <p>Remark \\(L_u = \\lim\\sup a_k \\geq \\lim\\inf a_k = L_l\\), if \\(L_u = L_l = L\\Rightarrow \\lim a_k = L\\)</p>"},{"location":"mat337/mct_bwt.html#bolzano-weierstrass-theorem","title":"Bolzano-Weierstrass Theorem","text":"<p>Every bounded sequence of real numbers has a convergent subsequence</p>"},{"location":"mat337/mct_bwt.html#defn-subsequence","title":"Def'n Subsequence","text":"<p>A subsequence of \\((a_n)\\) is a sequence \\((a_{n_k})\\) where \\(n_1&lt;n_2&lt;...\\)</p> <p>proof. Let \\((a_n)\\subseteq [-M, M]\\). Construct the subsequence by  - picking \\(I_1\\subset [-M,M]\\) that contains infinitely many \\(a_n\\) s.t. \\(|I_1|\\leq M/2\\) - ... - picking \\(I_n\\subset I_{n-1}\\) that contains infinitely many \\(a_n\\) s.t. \\(|I_n| \\leq |I_{n-1}|/2 \\leq M/2^n\\). </p> <p>By Nested interval lemma, \\(\\cap_{n\\geq 1}I_n \\neq\\emptyset\\), take \\(L\\in \\cap I_n\\). </p> <p>Pick \\(a_n\\in I_n, \\forall n \\geq 1\\) Since \\(L\\in I_n\\Rightarrow |a_n - L|\\leq |I_n|&lt;M/2^n\\). </p> <p>Then \\(\\forall \\epsilon &gt; 0\\), take \\(N_\\epsilon\\) s.t. \\(\\epsilon &gt; M/2^{N_\\epsilon} &gt; |a_{N_\\epsilon} - L|\\) but \\(|a_n - L | \\leq |I_n|\\leq |I_{N_\\epsilon}| / 2^{n-N_\\epsilon}, \\forall n\\geq N_\\epsilon\\)</p> \\[\\epsilon &gt; M / 2^{N_\\epsilon} &gt; |a_n - L|. \\forall n\\geq N_\\epsilon\\Rightarrow \\lim{a_n} = L\\]"},{"location":"mat337/normed_space.html","title":"Normed Space","text":""},{"location":"mat337/normed_space.html#defn-norm","title":"Def'n. Norm","text":"<p>Let \\(V\\) be a vector space over \\(\\mathbb{R}\\), a norm \\(\\|\\cdot\\|\\) over \\(V\\) is a function \\(V\\rightarrow \\mathbb{R}^+\\) s.t. </p> <ul> <li>positive definite: \\(\\|x\\|\\geq 0 \\land \\|x\\| = 0 IFF x\\equiv 0 \\in V\\) </li> <li>homogeneous \\(\\|ax\\| = |a|\\|x\\|. \\forall a\\in \\mathbb{R}\\)</li> <li>triangular inequality \\(\\|x+y\\| = \\|x\\| + \\|y\\|\\)</li> </ul>"},{"location":"mat337/normed_space.html#example-euclidean-space-over-mathbbrn","title":"Example: Euclidean space over \\(\\mathbb{R}^n\\)","text":"<p>\\(\\|x\\|_2 = \\sqrt{\\sum |x_k|^2}\\)   - positive definite \\(|x_k|^2 \\geq 0 \\Rightarrow \\sqrt{\\sum |x_k|^2} \\geq 0\\) \\(\\|x\\| = 0\\Rightarrow x = 0\\)  - homogeneous \\(\\|ax\\| = \\sqrt{\\sum |ax_k|^2} = \\sqrt{|a|^2\\sum |x_k|^2} = |a|\\|x\\|\\)  - Tri-ineq  By Cauchy Swartz Inequality \\(\\|x+y\\|^2 = \\sum |x_k + y_k|^2 \\leq \\sum |x_k|^2 + |y_k|^2 = \\sum |x_k|^2 + \\sum|y_k|^2 = \\|x\\|^2 + \\|y\\|^2\\)</p>"},{"location":"mat337/normed_space.html#example-some-well-known-norms","title":"Example: Some well-known norms","text":"<p>Some norms are  </p> <ul> <li>p-norm \\(\\|x\\|_p:=(\\sum |x_k|^p)^{p^{-1}}, p\\geq 1\\)</li> <li>Lp-norm \\(\\|f\\|_{L_p} := (\\int_S f(x)^p dx)^{p^{-1}}, p\\geq 1\\) is a norm over \\(C:=\\) the set of all continuous functions</li> <li>sup-norm \\(\\|f\\|_\\infty:= sup\\{|f(x)|:x\\in S\\}\\) is a norm over \\(C_b(S):=\\) the set of all continuous bounded functions or over \\(C(S)\\) if \\(S\\) is compact. </li> </ul>"},{"location":"mat337/normed_space.html#example-sup-norm","title":"Example: sup-norm","text":"<p>Claim. sup-norm is a norm proof.   </p> <ol> <li>\\(|f(x)| \\geq 0\\), \\(f(x):= 0 \\Rightarrow \\|f\\|_\\infty = 0, \\|f\\|_\\infty=0\\Rightarrow |f(x)|\\leq 0\\Rightarrow f(x)=0\\)</li> <li>\\(\\|af\\|_\\infty = sup|af(S)| = |a|sup|f(S)|= |a|\\|f\\|_\\infty\\) </li> <li>\\(\\|f+g\\|=sup(|f+g|) \\leq sup(|f| + |g|) \\leq sup|f| + sup|g| = \\|f\\| + \\|g\\|\\)</li> </ol> <p>\\(C^k(S):=\\) The set of all real number functions whose k-first derivative exists and continuous</p> <p>Some norms are defined on \\(C^k\\), such as \\(\\|f\\|':= \\max\\|f'\\|_\\infty; \\|f\\|_{C^k} = \\sum \\|f^{(n)}\\|_\\infty\\)</p> <p>Remark \\(C^\\infty\\) a.k.a. smooth are normed space that obey completeness</p>"},{"location":"mat337/normed_space.html#topology-of-normed-spaces","title":"Topology of normed-spaces","text":""},{"location":"mat337/normed_space.html#thrm-1","title":"Thrm 1.","text":"<p>The set \\(C:=\\{f:[0,1]\\rightarrow \\mathbb{R}: f(0)=1\\}\\) is closed in \\((C([0,1]), \\|f\\|_\\infty:= sup_{x\\in [0,1]}f(x))\\).  </p> <p>proof. Take a sequence \\(g_n \\in C\\) s.t. \\(\\|g_n - g\\| \\rightarrow 0\\). Then, consider </p> \\[\\begin{align*} |g(0) - 1| &amp;\\leq |g(0) - g_n(0)| + |g_n(0) -1|\\\\ &amp;\\leq |g(0)-g_n(0)| \\leq \\|g_n-g\\|=0\\\\ &amp;\\Rightarrow g(0)\\rightarrow 1\\Rightarrow g(0) = 1\\Rightarrow g\\in C \\end{align*}\\]"},{"location":"mat337/normed_space.html#thrm-2","title":"Thrm 2.","text":"<p>Let \\(A:=\\{f\\in [0,1]: f(x) &gt; 0, \\|f'\\|_\\infty &lt; 2\\}\\) is open in \\(C^1([0,1])\\) wrt \\(\\|f\\|_{\\infty, C^1}:=\\|f\\|_\\infty + \\|f'\\|_\\infty\\) </p> <p>proof. Take any \\(g\\in A\\). Since \\(g(x) &gt; 0\\), by EVT on \\([0,1]\\), take \\(\\delta_1\\) s.t. \\(g(x)&gt; \\delta_1 &gt; 0\\). Since \\(g'(x) &lt; 2\\), by EVT on \\([0,1]\\), take \\(\\delta_2\\) s.t. \\(g'(x)&lt; 2-\\delta_2 &lt; 2\\). Take \\(\\delta = \\min(\\delta_1, \\delta_2)/ 2\\).  </p> \\[\\begin{align*}     \\|h-g\\|_\\infty &lt; \\delta &amp;\\implies |h(x)-g(x)|&lt; \\delta \\\\     &amp;\\implies h(x) &gt; g(x)-\\delta &gt; \\delta_1/2 &gt; 0\\\\     \\|h'-g'\\|_\\infty &lt; \\delta &amp;\\implies |h'(x) - g'(x)| &lt; \\delta\\\\      &amp;\\implies h'(x) &lt; g'(x)+\\delta &lt; 2-\\delta_2 + \\delta_2/2 &lt; 2 \\end{align*}\\]"},{"location":"mat337/normed_space.html#thrm-3","title":"Thrm 3.","text":"<p>\\(C_c(\\mathbb{R}):=\\) space of compactly supported function on reals, \\(C_c(\\mathbb R), f\\in C_c(\\mathbb R)\\) if \\(\\exists M &gt; 0\\) s.t. \\(f(x)=0, \\forall |x| &gt; M\\)</p> <p>Claim \\(C_c(\\mathbb R)\\) is not complete wrt \\(\\|f\\|_\\infty\\)</p> <p>proof. WTF a Cauchy sequence \\(f_n \\in C_c(\\mathbb R)\\) s.t. \\(f_m \\rightarrow f\\not\\in C_c(\\mathbb R)\\). Take \\(f_n(x) = \\frac{1-(x/n)^2}{1+x^2}\\mathbb I (|x|\\leq n)\\) and we can show that \\(f_n\\) is Cauchy wrt \\(\\|\\cdot\\|_\\infty\\) </p> <p>wlog, assume \\(n &gt; m\\), Consider </p> \\[\\|f_n - f_m\\| = \\sup_{x\\in [-n,n]} |\\frac{1-(x/n)^2}{1+x^2} - f_m(x)|\\] <p>Suppose that \\(x\\in [-m,m]\\),</p> \\[\\sup_{x\\in [-n,n]} |\\frac{1-(x/n)^2}{1+x^2} - \\frac{1-(x/m)^2}{1+x^2}| = |\\frac{x^2}{1+x^2}||n^{-2} - m^{-2}| \\rightarrow |1||0|=0\\] <p>Suppose that \\(x\\in [-n, -m)\\cup (m, n]\\),</p> \\[\\sup_{x\\in [-n,n]} |\\frac{1-(x/n)^2}{1+x^2}|\\leq |\\frac{1}{1+x^2}|\\leq |\\frac{1}{1+m^2}|\\rightarrow 0\\] <p>Therefore, take \\(N = \\epsilon^{1/2}\\) we can prove Cauchy. </p> <p>However, \\(f_n(x)=\\frac{1-(x/n)^2}{1+x^2}\\rightarrow \\frac{1}{1+x^2}\\rightarrow 0\\) is not compactly supported</p> <p>Therefore, only compact in metric space \\(\\rightarrow\\) closed and bounded but not the converse. </p>"},{"location":"mat337/real_number_construction.html","title":"Real Number Construction","text":""},{"location":"mat337/real_number_construction.html#decimal-expansion","title":"Decimal expansion","text":"<p>Given \\(r\\in\\mathbb{R}^+\\) </p> <ol> <li>find \\(q\\in\\mathbb{N}^+, q\\leq r\\le q+1\\) </li> <li>So we find next decimal place \\(d_1/10 \\leq r- q &lt; d_1 / 10 + 1/10\\) </li> <li>repeat \\(\\frac{d_k}{10^k} \\leq r - q - \\sum _1^{k-1} \\frac{d^m}{10^m} &lt; \\frac{d_k}{10^k} + 10^{-k}\\)</li> </ol> <p>So that \\(r = q.d_1d_2d_3...\\)</p>"},{"location":"mat337/real_number_construction.html#defn-terminating-and-repeating","title":"Def'n. Terminating and repeating","text":"<p>terminating decimal expansion \\(q.d_1d_2...d_{m_0}\\) repeating decimal expansion \\(q.d_1...d_k\\overline{d_{k+1}...d_n}\\)(ex. \\(1/35 = 0.0287154\\overline{287154}\\))</p>"},{"location":"mat337/real_number_construction.html#thrm-1","title":"Thrm. 1","text":"<p>Claim \\(\\forall x\\in \\mathbb{R}^+\\) is rational IFF \\(x\\) has a decimal expansion that is either terminating or repeating  </p> <p>proof.  \\(\\Leftarrow\\) Assume \\(x\\) is terminating, </p> \\[x = q.d_1...d_{n_0} = q + \\sum_1^n \\frac{d_m}{10^m} \\in \\mathbb{Q}\\] <p>Assume \\(x\\) is repeating, </p> \\[q.d_1...d_k\\overline{d_{k+1}...d_m}\\] <p>Known that \\(d_1...d_k\\) part is rational, the remaining \\(0.0...0\\overline{d_{k+1}...d_m}\\) equals</p> \\[10^{-k}(\\sum_{m=1}^n \\sum_{l=0}^\\infty \\frac{d'_m}{10^{nl + m}})\\] <p>since the number is repeating, we denote \\(d'_0,...,d'_n\\) be the repeated digits  </p> \\[10^{-k}\\sum_{m=1}^n d'_m 10^{-m} (\\sum_{l=0}^\\infty 10^{-nl})\\] <p>Using geometeric series, we have </p> \\[10^{-k}\\sum_{m=1}^n d'_m 10^{-m} (1 - 10^{-n})^{-1} = \\sum_{m=1}^n\\frac{d'_m 10^n}{10^{m+k}(10^n - 1)}\\] <p>\\(\\Rightarrow\\) Take \\(l, m \\in\\mathbb{Z}^+\uff0c x = l/m\\). By Euclidean division, \\(l = d_0m + r_0/m\\) where \\(d_0\\in\\mathbb{Z}^+\\) is the quotient, \\(r_0\\in\\mathbb{Z}^+\\) is the remainder, \\(r_0&lt;m\\).  </p>"},{"location":"mat337/real_number_construction.html#lemma-2","title":"Lemma 2","text":"\\[\\forall n\\in \\mathbb{N}. l/m = \\sum_{m=0}^n \\frac{d_m}{ 10^{m}} + \\frac{r_n} {10^{n}m}\\] <p>proof. (induction by further Euclidean division)</p> <p>Suppose \\(\\exists i, r_i = 0\\), then is terminated Suppose \\(\\forall k\\) WTS repeating.  </p> <ul> <li>since \\(r_k\\) is a remainder, it can only choose from \\(r_k \\in \\{1, ..., m - 1\\}\\). Then \\(\\exists k_1, k_2. r_{k_1} = r_{k_2}\\).  </li> </ul> <p>Then by uniqueness of Euclidean division, it is repeated. </p>"},{"location":"mat337/real_number_construction.html#defn-irrational-numbers","title":"Def'n. Irrational Numbers","text":"<p>\\(x\\) is irrational if \\(x\\in\\mathbb{Q}^c\\), i.e. \\(\\not\\exists l, m\\in\\mathbb{Z}^+ s.t. x = l/m\\)</p>"},{"location":"mat337/real_number_construction.html#claim-3","title":"Claim 3","text":"<p>\\(\\forall x, y \\in \\mathbb{R}, x &lt; y \\Rightarrow \\exists r\\in\\mathbb{R}. x &lt; r &lt; y\\) and \\(r\\) is terminating.  </p> <p>proof. consider the decimal expansions of \\(x = x_0.x_1..., y = y_0.y_1...\\), then exists the smallest \\(k\\) where \\(x_k + 1 \\leq y_k\\), then find the next \\(m&gt;k\\) where \\(x_m \\neq 9\\). Then construct \\(r = x_0.x_1...[x_m + 1]y_{m+1}y_{m+2}...\\)</p>"},{"location":"mat337/real_number_construction.html#construction-from-cauchy-sequence","title":"Construction From Cauchy Sequence","text":"<p>Consider the space of Cauchy sequence \\(C_Q = \\{(y_n): y_n\\in \\mathbb{Q}\\}\\) </p> <p>\\(x_n, y_n\\) Cauchy, then</p> <p>Proposition 1 \\(x_n + y_n\\) Cauchy.  proof. Let \\(\\epsilon &gt; 0, N_\\epsilon = \\max(N^x_{\\epsilon/2}, N^y_{\\epsilon/2})\\)</p> <p>Proposition 2 \\(x_ny_n\\) Cauchy proof. </p> \\[\\begin{align*} |x_ny_n - x_my_m| &amp;= |x_ny_m - x_my_n + x_my_n - x_my_m| \\\\ &amp;\\leq |y_n(x_n-x_m)| + |x_m(y_n - y_m)|\\\\ &amp;\\leq B_2|x_n - x_m|  B_1|y_n - y_m| &amp;\\text{Cauchy implies bounded}\\\\ \\end{align*}\\] <p>Want this to be less than \\(\\epsilon &gt; 0\\).  Therefore, take \\(|x_n - x_m| \\leq \\epsilon / 2B_2, |y_n - y_m|\\leq \\epsilon/2B_1\\)</p> <p>Proposition 3 Additive identity \\(r_0=(0,0,0,...)\\), multiplicative identity \\(r_1 = (1,1,1,...)\\) </p> \\[a+r_o = \\{a_1 + 0, a_2 + 0,...\\}=\\{a_n\\} = a\\] \\[ar_1 = \\{a_11, a_21,...\\} = \\{a_n\\} = a\\]"},{"location":"mat337/real_number_construction.html#defn-equivalence-class","title":"Def'n. Equivalence Class","text":"<p>\\((x_n)\\sim (y_n)\\) IFF \\(\\lim_{n\\rightarrow \\infty}|x_n -y_n| = 0\\) </p> <p>Example \\(x_n = 1/n, y_n = 0\\)</p> <p>Example Let \\(\\pi=p_0.p_1p_2...\\), take \\(x_n = p_0.p_1...p_n, y_n = p_0.p_1...p_n1 = x_n + 10^{-(n+1)}\\). </p> <p>Let \\(\\mathbb{R}:=\\)equivalence class on \\(C_Q\\), so for any \\(x\\in\\mathbb{R}\\) you can find a Cauchy sequence \\((x_n)\\) in \\(\\mathbb{Q}\\) s.t. \\(\\lim_{n\\rightarrow\\infty}|x_n - x| = 0\\)</p>"},{"location":"mat337/series.html","title":"Series","text":""},{"location":"mat337/series.html#thrm-1","title":"Thrm 1.","text":"<p>Claim For \\(S_n(x)= \\sum_{k=1}^n a_k(x)\\), \\((S_n,\\|\\cdot\\|_\\infty)\\) Cacuhy IFF \\(S_n \\rightarrow^{u.c.}S\\) </p> <p>proof. \\(\\Rightarrow\\) Find \\(S(x)\\) s.t. \\(S_k(x)\\rightarrow^{p.w.}S(x)\\).  </p> \\[\\begin{align*} |S_k(x)-S(x)| &amp;\\leq |S_k(x)-s_n(x)| + |s_n(x)-S(x)| \\\\ &amp;\\leq \\epsilon + |S_n(x)-S(x)| &amp;\\text{by p.w. convergence} \\\\ &amp;\\leq \\epsilon + \\sup_{x\\in\\mathbb{R}} |S_n(x)-S(x)|\\\\ \\end{align*}\\] <p>Apply limit on \\(n\\rightarrow\\infty\\), since \\(n\\) is independent on \\(k\\),  \\(|S_k(x)-S(x)|\\leq \\epsilon + \\lim_{n\\rightarrow\\infty}\\sup_{x\\in\\mathbb{R}} |S_n(x)-S(x)| = \\epsilon + 0\\Rightarrow \\|S_k-S\\|_\\infty \\leq \\epsilon\\)</p>"},{"location":"mat337/series.html#example-1","title":"Example 1","text":"<p>\\(S_n :=\\sum_{k=1}^n \\frac{x^k}{k!}\\rightarrow^{u.c.}\\sum^\\infty \\frac{x^k}{k!},x\\in [-R,R]\\) </p> \\[\\begin{align*} \\|S_k - S_n\\|_\\infty &amp;= \\sup_{|x|\\leq R} |\\sum^{n+m}\\frac{x^k}{k!} - \\sum^n \\frac{x^k}{k!}| \\\\ &amp;= \\sup|\\sum_{n+1}^{n+m}\\frac{x^k}{k!}| \\\\ &amp;\\leq \\sum \\frac{R^k}{k!} = e^R - \\sum^n \\frac{R^k}{k!}\\rightarrow 0 \\end{align*}\\]"},{"location":"mat337/series.html#example-2","title":"Example 2","text":"<p>\\(S_n = \\sum^n \\frac{x^2}{k(k+x^2)}, x\\in \\mathbb R\\) </p> <p>proof. \\(S_n\\rightarrow^{p.w.}S\\) but \\(S_n \\not\\rightarrow^{u.c.}S\\) Fix \\(x_0\\), \\(S_n \\leq x^2 \\sum_{k=1}^n k^{-2} \\leq x^2\\pi^2/6 &lt; \\infty\\)</p> \\[\\begin{align*} \\|S_n-S\\|_\\infty &amp;= \\|\\sum_{k\\geq n}\\frac{x^2}{k(k+x^2)}\\|_\\infty \\\\ &amp;= \\sup_{x\\in\\mathbb R}|\\sum_{k\\geq n}\\frac{x^2}{k(k+x^2)}| \\\\ &amp;\\geq \\sup_{|x|\\leq N}|\\sum_{k\\geq n} \\frac{1}{k}\\frac{x^2}{k+x^2}|\\\\ \\text{take N large so that }\\frac{x^2}{k+x^2}&amp;\\geq 1-\\epsilon \\text{ and k is fixed} \\\\ &amp;\\geq \\sum_{k\\geq n}\\frac{1-\\epsilon}{k} = \\infty \\end{align*}\\]"},{"location":"mat337/series.html#thrm-2-m-test","title":"Thrm 2. M-Test","text":"<p>For \\(S_n(x) = \\sum_{k=1}^n a_k(x), \\|a_k\\|_\\infty \\leq M_k \\land \\sum M_k &lt; \\infty \\Rightarrow S_n \\rightarrow^{u.c.}S &lt; \\infty\\).  </p> <p>proof. We'll show Cauchy so that uniform convergent.  </p> \\[\\begin{align*} \\|S_n - S_{n+m}\\|_\\infty &amp;= \\sup|\\sum_{n+1}^{n+m}a_k(x)| \\\\  &amp;\\leq \\sum_{n+1}^{n+m} \\sup|a_k(x)| \\\\ &amp;\\leq \\sum_{n+1}^{n+m} M_k &lt; \\epsilon \\text{ for N large} \\end{align*}\\]"},{"location":"mat337/series.html#example-1_1","title":"Example 1","text":"<p>\\(S_n = \\sum_{k=1}^n (-x^2)^k\\rightarrow^{u.c.}S, x\\in [-r,r]\\subset (-1,1)\\) but not for \\(x\\in(-1,1)\\) </p> <p>proof. For \\(x\\in [-r,r]\\).  </p> \\[\\|(-x^2)^k\\| \\leq r^{2k} =:M_k\\] <p>Since \\(\\sum^\\infty r^{2k} = (1-r^2)^{-1}-1 &lt; \\infty\\), apply M-test, \\(S_n\\rightarrow^{u.c.}S\\)</p> <p>However, take \\(x\\in(-1,1)\\)</p> \\[\\begin{align*} \\|S_n - S_{m+n}\\|_\\infty &amp;= \\sup_{x\\in(-1,1)}|\\sum_n^{n+m}(-x^2)^k| \\\\ \\text{take }x_0 = 0.5^{1/2N} &amp;\\in (-1,1) \\text{ and }n + m\\leq N \\\\ &amp;\\geq |\\sum_n^{n+m} (-1)^k 0.5^{k/N}| \\\\ &amp;= |0.5^{n/N} + (-1)^{n+1}0.5^{\\frac{n+1}{N}}| \\\\ &amp;\\sim 0.5^{n/N} &amp;\\text{ for N large} \\\\ &amp;&gt; \\epsilon &amp;\\text{since }N\\rightarrow\\infty, 0.5^{n/N}\\rightarrow 1 \\end{align*}\\] <p>Alternatively, take \\(\\lim_{N\\rightarrow\\infty} = |-1+1-1+1...| = 1\\) when \\(n+m\\) is odd</p>"},{"location":"mat337/series.html#example-2_1","title":"Example 2","text":"<p>For the series for \\(\\arctan(x)\\)</p> \\[\\begin{align*} \\arctan(x) &amp;= \\int_0^x \\frac{1}{1+t^2}dt \\\\ &amp;= \\int_0^x \\sum (-t^2)^k dt &amp;|x|&lt;1 \\\\ &amp;= \\sum \\int_0^x (-t^2)^k dt &amp;\\text{ICT} \\\\ &amp;= \\sum (-1)^k (\\frac{t^{2k+1}}{2k+1}\\Big|_0^x) \\\\ &amp;= \\sum (-1)^k (\\frac{x^{2k+1}}{2k+1}) \\end{align*}\\] <p>\\(\\arctan(1)\\) is defined and \\(\\arctan(1) = \\pi/4\\) </p> <p>if \\(a_k \\geq a_{k+1} &gt; 0, |\\sum^\\infty a_k(-1)^k| \\leq a_n\\) </p> <p>proof. </p> \\[\\begin{align*} \\sup_{x\\in[-1,1]}|S_n(x)-S(x)| &amp;= \\sup_{|x|\\leq 1}|\\sum^\\infty_n (-1)^k \\frac{x^{2k+1}}{2k+1}| \\\\ &amp;= \\sup_{|x|\\leq 1} |\\frac{x^{2n+1}}{2n+1}(-1)^n)|&amp;\\text{since }\\frac{x^{2k+1}}{2k+1} \\text{ decreasing} \\\\ &amp;= \\frac{1}{2n+1} \\leq \\epsilon \\end{align*}\\]"},{"location":"mat337/series.html#claim","title":"Claim","text":"<p>For \\(f_n \\in C([a,b])\\) If \\(\\exists c \\in [a,b], f_n(c)\\rightarrow \\gamma, f'_n \\rightarrow^{u.c.}g\\), then \\(f_n \\rightarrow f\\) where \\(f'=g\\) </p> <p>proof. By FTC, </p> \\[\\begin{align*} f_n(x)&amp;=f_n(c) + \\int_c^x f'_n(t)dt \\\\ &amp;\\rightarrow\\gamma + \\int_c^x g(t)dt =:f(x) \\\\ |f_n(x)-f(x)| &amp;= |f_n(x)-\\gamma -\\int_c^x g(t)dt| \\\\ &amp;\\leq |f_n(x)-\\gamma| + |\\int_c^x f'_n(t)-g(t)dt| \\\\ &amp;\\leq \\epsilon/2 + \\|f'_n - g\\|_\\infty \\int_c^x dt \\\\ &amp;= \\epsilon/2 + \\|f'_n-g\\|(x-c)\\\\ &amp;\\leq \\epsilon/2 + \\frac{\\epsilon}{b-a} (x-c)\\\\ &amp;leq\\epsilon \\end{align*}\\] <p>Corollary Swap derivative and sum  </p> \\[S'_n \\rightarrow^{u.c.}S\\land\\exists c. S_n(c)\\rightarrow S(c) \\Rightarrow d_x \\sum a_k(x) = \\sum d_x a_k(x)\\]"},{"location":"mat337/series.html#defn-power-series","title":"Def'n. Power Series","text":"<p>\\(\\sum a_k x^k\\)</p>"},{"location":"mat337/series.html#thrm-3-ratio-test","title":"Thrm 3. Ratio test","text":"<p>\\(\\lim_\\infty \\frac{a_k}{a_{k+1}}&lt; 1\\Rightarrow \\sum a_k &lt; \\infty\\)</p>"},{"location":"mat337/series.html#thrm-4-root-test","title":"Thrm 4. Root test","text":"<p>\\(\\lim\\sup(|b_k|)^{1/k} &lt; 1\\Rightarrow \\sum b_k &lt; \\infty\\)</p>"},{"location":"mat337/series.html#example","title":"Example","text":"<p>\\(\\sum \\frac{x^n}{n}\\) is u.c. for \\(x\\in[-1,r], r&lt; 1\\)</p> \\[\\begin{align*} \\sup_{x\\in[-1,1)}|S_N - S| &amp;= \\sup_{x\\in[-1,1)}|\\sum_{n\\geq N} x^n/n| \\\\ (i) \\sup_{x\\in[-1,0]}|\\sum_{n\\geq N}x_n /n | = \\sup_{y\\in[0,1]}&amp;|\\sum (-1)^n y^n/n| \\leq \\sup|y^N/N| \\leq 1/N \\quad\\text{for large }N \\\\ (ii) \\sup_{x\\in[0,r]}|\\sum_{n\\geq N}x_n /n | = \\sup_{x\\in[0,r]}&amp;\\sum |x^n/n| \\leq \\sum r^n/n \\leq \\epsilon \\quad\\text{for large }N \\end{align*}\\]"},{"location":"mat337/series.html#thrm-5-hadamards-theorem","title":"Thrm 5. Hadamard's Theorem","text":"<p>Let \\(S(x)=\\sum a_k x^k\\) and \\(R^{-1} = \\lim\\sup(|a_n|)^{1/n}\\), then \\(S_k \\rightarrow^{u.c.}S, x\\in (-R,R)\\). </p> <p>proof. By root test, \\(\\lim\\sup (a_kx^k)^{1/k} &lt; 1\\) \\(|a_k x^k|^{1/k} = |x|\\lim\\sup |a_k|^{1/k} = |x|R^{-1} &lt; 1\\Rightarrow |x|&lt; R\\) Let \\(M_k := |a_k|r^k\\) by root test, \\(\\sum M_k &lt; \\infty\\), then by M-test, \\(S_k \\rightarrow^{u.c.}S, \\forall x\\in[-r,r]\\)</p> <p>Ratio test can do the same work</p>"},{"location":"mat337/series.html#example-1_2","title":"Example 1","text":"<p>\\(\\sum \\frac{2^{2k}}{k^2} x^{2k}\\) Let \\(y := x^2\\),  </p> \\[\\begin{align*} \\frac{1}{R} &amp;= \\lim\\sup (|\\frac{2^{2k}}{k^2}|^{1/k}) \\\\ &amp;= \\lim\\sup 2^2 k^{-2/k} \\\\ &amp;= 4 \\lim\\sup k^{-2/k} &amp;(i)\\\\ &amp;= 4 \\\\ \\Rightarrow R &amp;= 1/4 \\\\ \\\\ (i) \\log L  &amp;= \\lim\\sup\\log(k^{-2/k}) \\\\ \\frac{2}{k}\\log(\\frac{1}{k}) \\rightarrow 0 &amp;\\Rightarrow k^{-2/k}\\rightarrow 1 \\\\ |y| \\leq R = 1/4 &amp;\\Rightarrow |x|\\leq 1/2 \\end{align*}\\] <p>So the interval of convergence \\((-1/2, 1/2)\\)</p>"},{"location":"mat337/series.html#example-2_2","title":"Example 2","text":"<p>Claim \\(\\sum a_k x^k\\) with radius \\(R_1\\), \\(\\sum b_k x^k\\) with radius \\(R_2\\). Then  (i) \\(\\sum (a_k + b_k)x^k\\) has radius \\((R_1^{-1}+R_2^{-1})^{-1}\\) (ii) \\(\\sum_{n=1}^\\infty (\\sum_{k=1}^n a_k b_{n-k})x^n\\)</p> <p>Fix \\(\\epsilon &gt; 0\\), take \\(N = \\max(N_1, N_2)\\) s.t. \\(\\forall n \\geq N_1. |a_n|^{1/n}\\leq R_1^{-1} + \\epsilon, \\forall n \\geq N_2. |b_n|^{1/n} \\leq R_2^{-1}\\). </p> <p>(i) proof. </p> \\[\\begin{align*} R^{-1} &amp;= \\lim\\sup (|a_n| + |b_n|)^{1/n} \\\\ &amp;\\leq \\lim\\sup ((R_1^{-1}+\\epsilon)^n + (R_2^{-1}+\\epsilon)^n)^{1/n} \\\\ &amp;\\leq \\lim\\sup ((R_1^{-1}+\\epsilon) + (R_2^{-1}+\\epsilon))^{n/n} &amp;\\text{by }|a|^q + |b|^q \\leq (|a|+|b|)^q, \\forall q &gt; 1 \\\\ &amp; = R_1^{-1} + R_2^{-1} + 2\\epsilon \\\\ &amp;\\rightarrow^{\\epsilon\\rightarrow 0} R_1^{-1} + R_2^{-1} \\\\ \\Rightarrow R &amp;= (R_1^{-1} + R_2^{-1})^{-1} \\end{align*}\\] <p>(ii) proof.  Let \\(M_1 = \\max(\\{1\\}\\cup \\{\\frac{|a_k|}{(R_1^{-1} + \\epsilon)^k}\\}), M_2 = \\max(\\{1\\}\\cup \\{\\frac{|b_k|}{(R_2^{-1} + \\epsilon)^k}\\})\\). Therefore, \\(|a_n|\\leq M_1(R_1^{-1}+\\epsilon)^n\\land|b_n|\\leq M_2(R_2^{-1}+\\epsilon)^n. \\forall n\\geq 1\\)</p> \\[\\begin{align*} R^{-1}&amp;\\leq \\lim\\sup (\\sum^n M_1(R_1^{-1}+\\epsilon)^kM_2(R_2^{-1} +\\epsilon)^{n-k})^{1/n} \\\\ &amp;= \\lim\\sup (M_1M_2)^{1/n}\\lim\\sup ((R_2^{-1}+\\epsilon)^n \\sum^n r^k)^{1/n} &amp;r := \\frac{R_1^{-1}+\\epsilon}{R_2^{-1}+\\epsilon} \\\\ \\end{align*}\\] <p>Take log limt for each lim sup, \\(\\lim\\sup(M_1M_2)^{1/n} = 1\\). \\(\\lim\\sup (R_2^{-1}+\\epsilon)^{n/n}\\lim\\sup (\\frac{1-r^{n+1}}{1-r})^{1/n} = R_2^{-1}+\\epsilon\\) If we swap \\(R_1,R_2\\), similar conclusion can be made \\(=R_1^{-1}+\\epsilon\\), therefore \\(R^{-1} \\leq \\max(R_1^{-1}+\\epsilon, R_2^{-1}+\\epsilon)\\)</p>"},{"location":"mat337/swt.html","title":"Extra. Stone Weierstrass Theorem","text":""},{"location":"mat337/swt.html#defn-function-algebra","title":"Def'n. Function Algebra","text":"<p>For \\(\\mathcal A\\subset C([a,b])\\), that follows  </p> <ul> <li>(linearity) \\(\\forall f,g\\in \\mathcal A. \\forall a,b\\in\\mathbb R. af + bg\\in \\mathcal A\\)</li> <li>(closed under product) \\(\\forall f,g\\in\\mathcal A. f\\cdot g\\in \\mathcal A\\)</li> </ul>"},{"location":"mat337/swt.html#thrm-1","title":"Thrm 1.","text":"<p>For \\(\\mathcal A\\subset C([a,b])\\) be a function algebra, IF  </p> <ul> <li>(vanishes nowhere) \\(\\forall p\\in [a,b]. \\exists f\\in \\mathcal A. f(p)\\neq 0\\)</li> <li>(separates points) \\(\\forall x\\neq y \\in [a,b]. \\exists h\\in\\mathcal A. h(x)\\neq h(y)\\) </li> </ul> <p>THEN \\(\\mathcal A\\) is dense in \\(C[a,b]\\), i.e. \\(\\forall f\\in C[a,b]. \\exists \\{f_n\\}\\in \\mathcal A. \\{f_n\\}\\rightarrow^{u.c.} f\\)</p>"},{"location":"mat337/swt.html#thrm-2","title":"Thrm 2.","text":"<p>\\(\\mathcal A_{trig}:= \\{a_0 + \\sum_{k=1}^N a_k\\cos(kn) + \\sum_{k=1}^N b_k \\sin(kn) | a_k, b_k\\in\\mathbb R. N\\in\\mathbb N\\}\\) is dense in \\(C[-\\pi, \\pi]\\)</p> <p>proof. Consider the assumptions of SWT Let \\(f\\) have coefficients \\(a_k, b_k, M\\), \\(g\\) have \\(a'_k, a'_k, N\\), let \\(c,d\\in\\mathbb R\\). wlog, assume \\(M\\geq N\\), extend \\(a'_k, b'_k = 0. \\forall k &gt; N\\)</p> <p>(linearity) \\(ca_0 + da'_0 + \\sum (ca_k + da'_k)\\cos(kn) + \\sum(cb_k + db'_k)\\sin(kn)\\in \\mathcal A_{trig}\\) </p> <p>(product) Using half angle formula \\(\\cos(a)\\cos(b) = \\frac{1}{2}cos(a+b) + cos(a-b)\\) and other similar ones, we can break all the products of trig functions back to a sum, hence rearrange the summation coefficients. </p> <p>(vanish nowhere) For any \\(a_0 + \\sum a_k\\cos(pn) + \\sum b_k\\sin(pn) = 0\\), take \\(a_0' = a_0 + 1\\) and others remain unchanged. </p> <p>(separates points) Take \\(a\\neq b\\in[-\\pi,\\pi]\\). Using periodicity of trig functions If \\(a\\neq -b. \\cos(a)\\neq cos(b)\\), if \\(a = -b. \\sin(a)\\neq [sin(-a) = \\sin(b)]\\)</p> <p>Therefore, we can apply SWT</p>"},{"location":"mat337/topology_reals.html","title":"Topology of real number space","text":""},{"location":"mat337/topology_reals.html#defn-cauchy-sequence","title":"Def'n. Cauchy sequence","text":"\\[\\forall \\epsilon &gt; 0. \\exists N &gt;0 .\\forall m,n \\geq N. |a_m - a_n| \\leq \\epsilon\\] <p>Any convergent sequence is Cauchy Every Cauchy sequence is bounded</p>"},{"location":"mat337/topology_reals.html#defn-metric-space","title":"Def'n. Metric space","text":"<p>a space X plus a notion of distance</p>"},{"location":"mat337/topology_reals.html#defn-completeness","title":"Def'n. Completeness","text":"<p>a metric space \\(S\\) is complete if Cauchy sequence in \\(S\\) converges to some point in that space.</p>"},{"location":"mat337/topology_reals.html#claim-dd-285","title":"Claim (D&amp;D 2.8.5)","text":"<p>every Cauchy sequence of reals converges. i.e. \\(\\mathbb{R}\\) is complete.  </p> <p>proof. Let \\((x_n)\\in\\mathbb{R}\\) be Cauchy and \\(x_n\\rightarrow L\\). WTS \\(L\\in\\mathbb{R}\\) </p> <p>We'll use diagonalization argument: For fixed \\(x_n\\in\\mathbb{R}, \\exists (r_{n,k})\\) be rational Cauchy s.t. \\(\\lim_{k\\rightarrow\\infty}r_{n,k}=x_n\\).  Let \\(a_n\\) be s.t. \\(|a_n - x_n|\\leq 1/n\\) by choosing a candidate for each \\(n\\). </p> \\[\\implies |a_n-L|\\leq |a_n-x_n| + |x_n-L|\\leq 1/n + \\epsilon_n \\implies \\lim_{n\\rightarrow\\infty} a_n = L \\in\\mathbb{R}\\]"},{"location":"mat337/topology_reals.html#thrm-mean-value-theorem","title":"Thrm. Mean Value Theorem","text":"\\[\\exists \\xi\\in(x,y).|f(x)-f(y)| = |f'(\\xi)||x-y| \\]"},{"location":"mat337/topology_reals.html#example","title":"Example","text":"<p>\\(a_1 = 1, a_{n+1} = cos(a_n)\\) show that \\(a_n\\rightarrow L\\).  proof. WTS \\((a_n)\\) is Cauchy. Take some  \\(\\xi\\in(a_n,a_{n-1})\\), then</p> \\[\\begin{align*} |\\cos(a_{n - 1})-\\cos(a_n)|&amp;\\leq |\\sin(\\xi)||a_{n-1}-a_n|\\\\  \\implies |a_n - a_{n+1}|&amp;\\leq r|a_{n-1}-a_n|\\\\ &amp;\\leq r^n|a_2 - a_1|  \\end{align*}\\] <p>If \\(r &lt; 1\\), then \\(\\lim|a_n - a_{n-1}|=0\\)</p>"},{"location":"mat337/topology_reals.html#close-and-open","title":"Close and Open","text":"<p>Limit point For a sequence \\((x_n)\\in\\mathbb{R}\\), its limit point \\(x\\in\\mathbb{R}\\) satisfies \\(\\lim_{n\\rightarrow\\infty}x_n = x\\).  </p> <p>Closed if it contains all its limit points</p> <p>Example \\(\\mathbb{R}^n\\) is closed because \\(\\mathbb{R}\\) is complete </p> <p>Claim Finite union of closed sets is closed  </p> <p>Open if \\(\\forall x\\in U.\\exists r_{x,v}&gt;0. B_r(x)\\subseteq U\\)</p> <p>\\(U\\) open, \\(f\\) continuous \\(\\Rightarrow f^{-1}(U)\\) is open</p> <p>Claim (D&amp;D 4.3.8) \\(U\\) open IFF \\(\\mathbb{R}-U\\rightarrow U^c\\) is closed </p>"},{"location":"mat337/topology_reals.html#theorem","title":"Theorem","text":"<p>finite intersection of open sets is open.  </p> <p>proof. \\((\\cap^M U_i)^{c^c} = (\\cup^M U_i^c)^c\\) Since \\(U_i\\) open, \\(U_i^c\\) closed, \\(\\cup^M U_i^c\\) closed. </p>"},{"location":"mat337/topology_reals.html#compactness","title":"Compactness","text":"<p>Compact Defn 1 \\(C\\subseteq\\mathbb{R}\\) is compact if all sequence \\(\\{x_n\\}\\subseteq C\\) has a converging subsequence \\(\\{x_{n_k}\\}\\subseteq C\\) \\(\\forall \\{x_n\\}\\subseteq C. \\exists \\{x_{n_k}\\}\\subseteq \\{x_n\\}. x_{n_k}\\rightarrow x\\in C\\)</p> <p>Compact Defn 2 A set \\(C\\) is compact if every open cover of \\(C\\) has a finite subcover that covers \\(C\\). </p> <p>Open cover A collection of open balls \\(\\{U_a\\}_{a\\geq 1}\\) is an open cover if \\(c\\subseteq \\cup^\\infty U_a\\)</p> <p>Claim \\([a,b]\\subseteq \\mathbb{R}\\) is compact.  </p> <p>proof. By Bolzano Weierstrass Theorem. every bounded sequence has a convergent subsequence. Since \\([a,b]\\) closed, such limit falls in \\([a,b]\\). </p> <p>Continuous mapping theorem For continous \\(f, x_n\\rightarrow x\\Rightarrow f(x_n)\\rightarrow f(x)\\). </p> <p>Corollary \\(f:U\\rightarrow V\\), if \\(C\\subseteq U\\) is compact, then \\(f(C)\\) is also compact.  </p>"},{"location":"mat337/topology_reals.html#thrm-heine-borel-theorem","title":"Thrm. Heine-Borel Theorem","text":"<p>\\(C\\subseteq \\mathbb{R}\\) is compact IFF closed and bounded. </p> <p>Claim 1 compact \\(\\Rightarrow\\) bounded</p> <p>proof. Suppose compact but not bounded. Then take \\(\\{x_n\\}\\subset C\\), wlop, \\(x_n\\rightarrow \\infty\\). Then, we can take a monotone increasing subsequence \\(y_n\\) of \\(x_n\\). Then by compactness, take \\(y_{n_k}\\rightarrow y\\in C\\). But a monotone divergent sequence cannot have convergent subsequence. </p> <p>Claim 2 compact \\(\\Rightarrow\\) closed  </p> <p>proof. Let \\(\\{x_n\\}\\subseteq C. x_n\\rightarrow x\\), WTS \\(x\\in C\\). By compactness, take \\(\\{x_{n_k}\\}\\rightarrow L\\). We can show that  \\(|x-L| = |x - x_{n_k} + x_{n_k} - L| \\leq |x - x_{n_k}| + |x_{n_k} - L| \\rightarrow 0\\)</p> <p>Claim 3 closed &amp; bounded \\(\\Rightarrow\\) compact  </p> <p>proof. Let \\(\\{x_n\\}\\subseteq C\\). Since bounded, by BWT, take \\(\\{x_{n_k}\\}\\) where \\(x_{n_k}\\rightarrow L\\in\\mathbb{R}\\). Since \\(C\\) closed, its limit point is in \\(C\\). </p>"},{"location":"mat337/topology_reals.html#thrm-extreme-value-theorem","title":"Thrm. Extreme value theorem","text":"<p>If a function \\(f:K\\rightarrow R\\) is continous and \\(K\\) is compact, then \\(f(K)\\) is compact and \\(\\exists a,b\\in K. \\forall k\\in K. f(a)\\leq f(k)\\leq f(b)\\)</p> <p>Part1 \\(f(K)\\) compact proof. Let \\((y_k)\\subseteq f(K)\\), take \\((x_k)\\subseteq K s.t. f(x_k)=y_k\\). By compactness of \\(K\\), take \\((x_{k_n})\\subseteq (x_k)\\) and \\(x_k\\rightarrow x\\in K.\\) Then by continuous mapping theorem, \\(f(x_{k_n})\\rightarrow f(x)\\in f(K)\\)</p> <p>Part2 \\(f(K)\\) has max and min proof. By compactness and least bound principle, take \\(M,m\\) be the supremum and infimum of \\(f(X)\\). WTS thats contained in \\(f(K)\\). Construct a sequence \\((d_n)\\subset K\\) s.t. \\(M-\\frac{1}{n}\\leq f(d_n)\\leq M\\) (since M is the supremum we can do such construction). Since \\(K\\) is compact, by Bolz. Weis. Theorem, take \\((d_{n_k})\\subseteq (d_n), d_{n_k}\\rightarrow d\\) is convergent. </p> <p>Then by squeeze theorem, \\(M-\\frac{1}{n}\\leq f(d_{n_k})\\leq M\\) is squeezed by \\(M\\), \\(\\lim f(d_{n_k}) = f(\\lim f(d_{n_k})) = M\\). By compactness of \\(f(K), M\\in f(K)\\).</p> <p>Example \\(f\\) continuous on \\([a,b]\\subseteq \\mathbb{R}\\) has no local extremum, then \\(f\\) is monotone. local maxmimum/minimum \\(x_0\\) is local max/min if \\(\\exists \\epsilon &gt; 0. \\forall x\\in [x_0-\\epsilon, x_0+\\epsilon], f(x)\\leq | \\geq f(x_0)\\).</p>"},{"location":"mat337/topology_reals.html#uniform-continuilty","title":"Uniform Continuilty","text":"<p>\\(f:S\\rightarrow R\\) is uniformly continuous if \\(\\forall \\epsilon &gt; 0. \\exists \\delta &gt; 0. \\forall x,y. |x-y|\\leq \\delta \\Rightarrow |f(x)-f(y)|&lt;\\epsilon\\)</p> <p>Sequential criterion A function \\(f:S\\rightarrow R\\) is not uniform continuous if \\(\\exists (x_n), (y_n)\\subseteq S\\) s.t. \\(|x_n - y_n|\\rightarrow 0\\) but \\(\\exists \\epsilon_0 &gt;0 s.t. |f(x_n)-f(y_n)|\\geq \\epsilon_0 &gt; 0\\)</p> <p>proof. negation of the uniform continuous definition </p> <p>Example \\(f(x):=x^2\\) Let \\(x_n = n + n^{-1}, y_n = n\\). Then \\(|x_n-y_n|= n^{-1}\\rightarrow 0\\), but \\(|(n+n^{-1})^2 - n^2| = |-2|+n^{-2}\\rightarrow 2 &gt; 0\\)</p> <p>\\(\\alpha\\)-Holder \\(f:S\\rightarrow R\\) is a-Holder if \\(|f(x)-f(y)|\\leq c |x-y|^\\alpha, \\alpha\\in[0,1]. \\forall x,y\\in S\\).  </p> <p>Example \\(f(x)=x^p, p\\in(0,1)\\) is call p-Holder and is uniform continuous.  </p> <p>proof. Let \\(\\epsilon &gt; 0\\), take \\(\\delta = (\\epsilon / c)^{a^{-1}} &lt; 1\\) because \\(\\alpha \\leq 1\\) \\(|f(x)-f(y)|\\leq c|x-y|^\\alpha \\leq c\\delta^\\alpha =\\epsilon\\)</p> <p>When \\(\\alpha = 1, i.e. |f(x)-f(y)|\\leq c|x-y|\\), they are called Lipschitz.</p> <p>Example Any differentiable functions are Lipschitz (bounded derivative)  </p> <p>proof. by MVT, \\(|f(x)-f(y)| = |f'(c)||x-y|\\leq B|x-y|\\)</p>"},{"location":"mat337/topology_reals.html#thrm-heine-cantar-theorem","title":"Thrm. Heine-Cantar Theorem","text":"<p>if \\(K\\) compact, and \\(f:K\\rightarrow R\\) is continuous, then \\(f\\) is also uniform continuous.  </p> <p>proof. Suppose \\(f\\) is not uniform continuous Take \\((x_n), (y_n)\\) and for \\(\\epsilon &gt; 0\\) s.t. \\(|x_n - y_n|\\leq 1/n\\) but \\(|f(x_n)-f(y_n)|\\geq \\epsilon &gt; 0\\). By \\(K\\) compact, take \\((x_{n_k})\\rightarrow x\\in K\\)\u3002 However, \\(|y_{n_k}-x|\\leq |y_{n_k}-x_{n_k}| + |x_{n_k}-x| \\leq n_k^{-1} + \\epsilon_k \\rightarrow 0\\) i.e. \\(y_{n_k}\\rightarrow x\\) \\(\\forall n_k &gt; N_{\\epsilon_0/4}|f(x_n)-f(y_n)|\\leq |f(x_n)-f(x)| + |f(x)-f(y)| \\leq \\epsilon_0/2\\) causes contradiction</p>"},{"location":"mat337/topology_reals.html#thrm-intermediate-value-theorem","title":"Thrm. Intermediate value theorem","text":"<p>If \\(f:[a,b]\\rightarrow \\mathbb{R}\\) is continuous and \\(z\\in\\mathbb{R}\\) satisfy \\(f(a)\\leq z\\leq f(b)\\), then \\(\\exists c \\in [a,b]\\) s.t. \\(f(c)=z\\). </p> <p>proof. Let \\(A = \\{x\\in[a,b]: f(x) &lt; z\\}\\), then \\(A\\) is upper bounded. Then, take \\(c = sup(A)\\).</p> <p>\\(\\forall \\epsilon_n = n^{-1}\\), take \\(a_n \\in A\\) s.t. \\(c - n^{-1} \\leq a_n \\leq c\\) and  \\(f(a_n)\\leq z\\) so \\(f(c) = \\lim_{n\\rightarrow \\infty} f(a_n)\\leq z\\) (1). Explicitly assume \\(c &lt; b\\) (otherwise we've done), then take \\(\\exists b_n\\) s.t. \\(c &lt; b_n \\leq b, \\lim_\\infty b_n =c\\). Since \\(z\\) is upper bound for \\(A, b_n\\not\\in A, f(b_n)\\leq z\\) (2).  By (1)(2) \\(f(c)=z\\)</p>"},{"location":"mat337/topology_reals.html#connected","title":"Connected","text":"<p>Disconnected \\(A\\subseteq \\mathbb{R}\\) is disconnected \\(U,V\\) open cover s.t. \\(A = U\\sqcup V\\) Connected \\(A\\) is connected if \\(\\forall U,V. A\\neq U\\sqcup V\\) Path connected of \\(\\forall a,b \\in A. \\exists \\gamma:[0,1]\\rightarrow A\\) be a continuous path i.e. \\(\\gamma(0)=a, \\gamma(1)=b, \\gamma([0,1])\\subseteq A\\) </p>"},{"location":"mat363/arc_length.html","title":"Arc Length","text":""},{"location":"mat363/arc_length.html#defining-arc-length","title":"Defining arc Length","text":"<p>For a vector \\(v\\), the length is defined as \\(\\|v\\| = \\sqrt{v_1^2 + \\cdots + v_n^2}\\).  For vector \\(u, v\\), \\(\\|u-v\\|\\) is the length of the line segment joining \\(u, v\\). </p> <p>Consider a parameterized curve \\(\\gamma\\), and an arc from \\(\\gamma(t_0)\\) to \\(\\gamma(t_1)\\). We can partition the curve into many points, and then approximate the length by summing the length of the line segments joining the points. </p> <p>One easy partition is to partition by adding a small step \\(\\delta t\\) from previous time step \\(t\\). Hence for an arbitrary segment of the curve between \\(\\gamma(t)\\) and \\(\\gamma(t+\\delta t)\\), its length is then approximated by \\(\\|\\gamma(t+\\delta t) - \\gamma(t)\\|\\). Since \\(\\delta t &gt; 0\\) and it's taken arbitrarily small, </p> \\[\\lim_{\\delta t\\rightarrow 0}\\|\\gamma(t+\\delta t) - \\gamma(t)\\| = \\|\\gamma'(t)\\delta t\\| = \\|\\gamma'(t)\\|\\delta t\\] <p>Therefore, the arc is then approximated as </p> \\[l(t_0, t_1) = \\lim_{\\delta_t \\rightarrow 0} \\sum_{n = 0}^{(t_1 - t_0)/\\delta_t} \\|\\gamma(t+\\delta t) - \\gamma(t)\\|= \\int_{t_0}^{t_1} \\|\\gamma'(u)\\|du\\] <p>Therefore, the arc-length of a curve \\(\\gamma\\) starting at \\(\\gamma(t_0)\\) is a function \\(s(t)\\) given by </p> \\[s(t) =\\int_{t_0}^{t} \\|\\gamma'(u)\\|du\\] <p>Note that   - \\(s(t_0) = \\int_{t_0}^{t_0} = 0\\)  - \\(\\forall t &gt; t_0. s(t) &gt; 0\\)  - \\(\\forall t &lt; t_0. s(t) = \\int_{t_0}^{t}  \\|\\gamma'(u)\\|du = -\\int_{t}^{t_0}  \\|\\gamma'(u)\\|du &lt; 0\\)</p> <p>Also, if we choose a different starting point \\(t_1\\), the resulting arc-length \\(s_1\\) differs from \\(s\\) by a constant \\(\\int_{t_0}^{t_1} \\|\\gamma'(u)\\| du\\)</p>"},{"location":"mat363/arc_length.html#example-logarithmic-spiral","title":"Example: Logarithmic spiral","text":"<p>Parameterize the a logarithmic spiral as </p> \\[\\gamma(t) = (e^{kt}\\cos t, e^{kt}\\sin t), k\\neq 0\\] Source code <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\n\nt = np.arange(-20 * np.pi, 20 * np.pi, 0.1)\ndef log_spiral(t, k):\n    plt.gca().set_aspect('equal')\n    plt.plot(np.exp(k * t) * np.cos(t), np.exp(k * t) * np.sin(t))\n    plt.xlim(-3, 3)\n    plt.ylim(-3, 3)\n    plt.title(rf\"k={k}\")\nplt.figure(figsize=(12, 4))\nplt.subplot(131); log_spiral(t, 0.01)\nplt.subplot(132); log_spiral(t, 0.02)\nplt.subplot(133); log_spiral(t, -0.05)\nplt.savefig(\"../assets/arc_length.jpg\")\n</code></pre> <p>\u200b </p> <p>Then, we can find its arc length by </p> \\[\\begin{align*} \\gamma'(t) &amp;= (ke^{kt}\\cos t - e^{kt}\\sin t, ke^{kt}\\sin t + e^{kt}\\cos t)\\\\ \\|\\gamma'(t)\\|^2 &amp;= k^2 e^{2kt}\\cos^2 t -ke^{2kt}\\cos t \\sin t + e^{2kt}\\sin^2 t + k^2 e^{2kt}\\sin^2 t +ke^{2kt}\\cos t \\sin t + e^{2kt}\\cos^2 t\\\\ &amp;= k^2e^{2kt} + e^{2kt}\\\\ s(t) &amp;= \\int_0^t (k^2e^{2ku} + e^{2ku})^{1/2}du\\\\ &amp;= \\int_0^t ((k^2 + 1)^{1/2}e^{ku})du\\\\ &amp;= \\int_0^t \\sqrt{k^2+1} u du \\\\ &amp;= \\sqrt{k^2+1}{k}(e^{kt} - 1) \\end{align*}\\]"},{"location":"mat363/arc_length.html#example-catenary","title":"Example: Catenary","text":"<p>The catenary parameterized with \\(\\gamma(t) = \\cosh t\\). Determine the arc-length starting at \\((0, 1)\\). </p> \\[\\|\\gamma'(t)\\| = \\sqrt{1 + \\sinh^2 t} = \\cosh t\\] \\[s(t) = \\int_0^t \\cosh x dx = \\sinh t\\]"},{"location":"mat363/arc_length.html#speed","title":"Speed","text":"<p>Treat \\(\\gamma(t)\\) as the position of a particle at time \\(t\\), then the arc-length \\(s(t)\\) can be seen as the distance traveled by the particle. Therefore, we can define speed as \\(\\frac{ds}{dt}\\) since \\(s\\) is definitely differentiable. </p> <p>For a parameterized curve \\(\\gamma\\), its speed at \\(t\\) is defined as \\(\\|\\gamma'(t)\\|\\). \\(\\gamma\\) is said to be a unit-speed curve if \\(\\forall t \\in (a, b). \\|\\gamma'(t)\\| = 1\\).</p> <p>Lemma For some function \\(\\mathbf n: (a, b)\\rightarrow\\mathbb R\\) s.t. \\(\\mathbf n\\) is smooth and \\(\\forall t. \\|\\mathbf n(t)\\| = 1\\). Then, \\(\\mathbf n'(t)\\cdot\\mathbf n(t) = 0\\).</p> <p>proof. Assume \\(\\mathbf n\\) is smooth, then \\(\\mathbf n\\) and \\(\\mathbf n'\\) are both differentiable.  Therefore, consider </p> \\[\\frac{d}{dt}(\\mathbf n\\cdot \\mathbf n) = \\mathbf n'\\cdot \\mathbf n + \\mathbf n\\cdot \\mathbf n' = 2\\mathbf n'\\cdot \\mathbf n\\] <p>Also we know that </p> \\[\\mathbf n\\cdot \\mathbf n = \\|n\\| = 1\\implies \\frac{d}{dt} (\\mathbf n\\cdot \\mathbf n ) = 0\\] <p>Therefore, </p> \\[2\\mathbf n'\\cdot \\mathbf n = 0 \\Rightarrow \\mathbf n'\\cdot \\mathbf n = 0\\] <p>Corollary If \\(\\gamma\\) is a unit-speed curve, then \\(\\gamma'' = 0\\) or perpendicular to \\(\\gamma'\\). proof. Take \\(\\mathbf n = \\gamma'\\), \\(\\gamma''\\cdot \\gamma' = 0\\)</p>"},{"location":"mat363/arc_length.html#example","title":"Example","text":"<p>Show that \\(\\gamma(t) = (\\frac{1}{3}(1+t)^{3/2}, \\frac{1}{3}(1-t)^{3/2}, \\frac{t}{\\sqrt{2}})\\) is unit-speed. </p> \\[\\begin{align*} \\gamma'(t) &amp;= (\\frac{1}{2}(1+t)^{1/2}, -\\frac{1}{2}(1-t)^{1/2}, \\frac{1}{\\sqrt{2}})\\\\ \\|\\gamma'(t)\\|^2 &amp;= (\\frac{1}{2}(1+t)^{1/2})^2 + (-\\frac{1}{2}(1-t)^{1/2})^2 + \\frac{1}{2}\\\\ &amp;= \\frac{1}{4}(1+t+ 1-t) + \\frac{1}{2}\\\\ &amp;= 1 \\end{align*}\\] <p>Show that \\(\\gamma(t) = (\\frac{4}{5}\\cos t, 1 - \\sin t, \\frac{-3}{5}\\cos t)\\) is unit-speed. </p> \\[\\begin{align*} \\gamma'(t) &amp;= (-\\frac{4}{5}\\sin t, -\\cos t, -\\frac{-3}{5}\\sin t)\\\\ \\|\\gamma'(t)\\|^2 &amp;= \\frac{16}{25}\\sin^2 t + \\cos^2 t + \\frac{9}{25}\\sin^2 t)\\\\ &amp;= 1 \\end{align*}\\]"},{"location":"mat363/arc_length.html#example_1","title":"Example","text":"<p>Claim A straight line is the shortest curve joining two given points. </p> <p>proof. Let \\(\\mathbf p, \\mathbf q\\) be two points, \\(\\gamma\\) be an arbitrary curve passing through with \\(\\gamma(a) = \\mathbf p, \\gamma(b) = \\mathbf q, a &lt; b\\).  Take \\(\\mathbf u = \\mathbf p - \\mathbf q, \\hat{\\mathbf u} = \\frac{\\mathbf u}{\\|\\mathbf u\\|}\\) so that \\(\\|\\hat{\\mathbf u}\\| = 1\\). Note that </p> \\[\\dot\\gamma\\cdot \\hat {\\mathbf u} = \\|\\dot\\gamma\\| \\|\\hat{\\mathbf u}\\|\\cos(\\theta) =  \\|\\dot\\gamma\\| \\cos(\\theta)\\leq \\|\\dot\\gamma\\| \\] <p>So that </p> \\[\\|\\mathbf u\\| = \\mathbf u\\cdot \\hat {\\mathbf u} = (\\gamma(b) - \\gamma(a)) \\cdot \\hat{\\mathbf u} = \\int_a^b \\gamma'(t)dt \\cdot \\hat{\\mathbf u} \\leq \\int_a^b \\|\\gamma'(t)\\|dt\\]"},{"location":"mat363/curvature.html","title":"Curvature","text":"<p>Intuitively, curvature measures how \"curved\" a curve is. Suppose that \\(\\gamma(t)\\) is a pamameterized curve. As \\(t\\) changes to \\(t+\\Delta t\\), the curve moves away from its tangent \\(\\gamma'(t)\\) by </p> \\[(\\gamma(t+\\Delta t) - \\gamma(t))\\cdot \\hat{\\mathbf n}, \\|\\hat{\\mathbf n}\\| = 1, \\hat{\\mathbf n}\\cdot \\gamma' = 0\\] <p>Then, Taylor's theorem gives </p> \\[\\gamma(t+\\Delta t) = \\gamma(t) + \\gamma'(t)\\Delta t + \\frac{1}{2}\\gamma''(t)\\Delta t^2 + rem\\] <p>By Taylor's remainder theorem, \\(rem\\) vanishes faster than \\(\\Delta t^2\\). </p> <p>Therefore, we have </p> \\[(\\gamma(t) + \\gamma'(t)\\Delta t + \\frac{\\Delta t^2}{2}\\gamma''(t)-\\gamma(t))\\cdot\\hat{\\mathbf n} = \\frac{\\Delta t^2}{2}\\gamma''(t)\\cdot\\hat{\\mathbf n} \\] <p>Then, note that \\(\\gamma'\\cdot \\gamma'' = 0\\) and known that \\(\\gamma\\) is unit-speed, hence \\(\\gamma'' \\parallel \\hat{\\mathbf n}\\). So that </p> \\[ \\frac{\\Delta t^2}{2}\\gamma''(t)\\cdot\\hat{\\mathbf n}  =  \\frac{\\Delta t^2}{2}\\|\\gamma''(t)\\| \\] <p>Therefore, we can derive the definition:</p> <p>If \\(\\gamma\\) is a unit-speed curve, its curvature \\(\\kappa(t)\\) at the point \\(\\gamma(t)\\) is defined as \\(\\|\\gamma''(t)\\|\\).</p>"},{"location":"mat363/curvature.html#cross-product","title":"Cross Product","text":"<p>For 3D vectors \\(\\mathbf a, \\mathbf b \\in \\mathbb R^3\\), cross product is a 3D vector s.t. \\(\\mathbf a\\times \\mathbf b \\in \\mathbb R^3\\) s.t. \\(\\forall w\\in\\mathbb R^3. (\\mathbf a\\times \\mathbf b)\\cdot \\mathbf w = \\det(\\begin{bmatrix}\\mathbf a&amp;\\mathbf b&amp;\\mathbf w\\end{bmatrix})\\).  </p> <p>Note that \\(\\det(A) = \\det(A^T)\\), and for \\(3\\times3\\) matrices, switching two adjacent rows will flip the sign, then switch twice won't change the determinant. </p> \\[\\det(\\begin{bmatrix}\\mathbf a&amp;\\mathbf b&amp;\\mathbf w\\end{bmatrix}) =  \\det(\\begin{bmatrix}a_1&amp;b_1&amp;w_1\\\\a_2&amp;b_2&amp;w_2\\\\a_3&amp;b_3&amp;w_3\\end{bmatrix}) =  \\det(\\begin{bmatrix}w_1&amp;w_2&amp;w_1\\\\a_1&amp;a_2&amp;a_3\\\\b_1&amp;b_2&amp;b_3\\end{bmatrix})\\] <p>Therefore, we can rewrite the cross product as</p> \\[\\begin{align*} \\mathbf a\\times \\mathbf b &amp;=  \\begin{bmatrix}     \\det(\\begin{bmatrix}a_2&amp;a_3\\\\b_2&amp;b_3\\end{bmatrix})\\\\     -\\det(\\begin{bmatrix}a_1&amp;a_3\\\\b_1&amp;b_3\\end{bmatrix})\\\\     \\det(\\begin{bmatrix}a_1&amp;a_2\\\\b_1&amp;b_2\\end{bmatrix}) \\end{bmatrix} =   \\begin{bmatrix} a_2b_3-a_3b_2\\\\ a_3b_1-a_1b_3\\\\ a_1a_2-a_2b_1 \\end{bmatrix} \\\\ &amp;= (a_2b_3-a_3b_2)\\mathbf e_1 - (a_1b_3-a_3b_1)\\mathbf e_2 + (a_1a_2-a_2b_1)\\mathbf e^3 \\end{align*}\\]"},{"location":"mat363/curvature.html#properties-of-cross-product","title":"Properties of Cross Product","text":""},{"location":"mat363/curvature.html#linearity","title":"Linearity","text":"<ul> <li>\\(\\mathbf a\\times (\\mathbf b + \\mathbf c) = \\mathbf a \\times \\mathbf b + \\mathbf a \\times \\mathbf c\\)</li> <li>\\(\\lambda \\mathbf a \\times \\mathbf b = \\lambda (\\mathbf a \\times \\mathbf b) = \\mathbf a \\times \\lambda\\mathbf b\\)</li> <li>\\(\\mathbf a \\times \\mathbf b + \\mathbf c \\times \\mathbf d = (\\mathbf a - \\mathbf c) \\times (\\mathbf b - \\mathbf d) + \\mathbf a \\times \\mathbf d + \\mathbf c \\times \\mathbf b\\)</li> </ul>"},{"location":"mat363/curvature.html#plane-normal","title":"Plane Normal","text":"<p>Claim Cross product is perpendicular to both of its vectors. </p> \\[(\\mathbf a \\times \\mathbf b) \\cdot \\mathbf a = (\\mathbf a \\times \\mathbf b) \\cdot \\mathbf b = 0\\] <p>proof. Note that </p> \\[(\\mathbf a \\times \\mathbf b) \\cdot \\mathbf a = \\det(\\begin{bmatrix}\\mathbf a&amp;\\mathbf b&amp;\\mathbf a\\end{bmatrix}), (\\mathbf a \\times \\mathbf b) \\cdot \\mathbf b = \\det(\\begin{bmatrix}\\mathbf a&amp;\\mathbf b&amp;\\mathbf b\\end{bmatrix})\\] <p>\\(\\begin{bmatrix}\\mathbf a&amp;\\mathbf b&amp;\\mathbf a\\end{bmatrix}, \\begin{bmatrix}\\mathbf a&amp;\\mathbf b&amp;\\mathbf b\\end{bmatrix}\\) are both linearly dependent, hence their determinant are both 0. </p>"},{"location":"mat363/curvature.html#geometric-meaning","title":"Geometric Meaning","text":"<p>Lemma \\((\\mathbf a \\times \\mathbf b)\\cdot (\\mathbf c \\times \\mathbf d) = \\det(\\begin{bmatrix}\\mathbf a\\cdot\\mathbf c&amp;\\mathbf b\\cdot\\mathbf c\\\\\\mathbf a\\cdot\\mathbf d&amp;\\mathbf b\\cdot\\mathbf d\\end{bmatrix})\\).</p> <p>proof. By linearity, it is sufficient to show only when \\(\\mathbf a, \\mathbf b, \\mathbf c, \\mathbf d \\in \\{\\mathbf e_1, \\mathbf e_2, \\mathbf e_3\\}\\). Then, we can list all possible combinations and prove this claim. </p> <p>Corollary (Lagrange's Identity in 3D). \\(\\|\\mathbf a\\times \\mathbf b\\|^2 = \\det(\\begin{bmatrix}\\mathbf a\\cdot\\mathbf a&amp;\\mathbf a\\cdot\\mathbf b\\\\\\mathbf b\\cdot\\mathbf a&amp;\\mathbf b\\cdot\\mathbf b\\end{bmatrix})\\). </p> <p>proof. By the lemma above. </p> <p>Corollary (Geometric Meaning of cross product in 3D). \\(\\|\\mathbf a\\times \\mathbf b\\| = \\|\\mathbf a\\|\\|\\mathbf b\\|\\sin\\theta\\).  </p> <p>proof. Using Lagrange's Identity in 3D, we can then have</p> \\[\\begin{align*} \\|\\mathbf a\\times \\mathbf b\\|&amp;= (\\mathbf a\\times \\mathbf b)\\cdot(\\mathbf a\\times \\mathbf b)\\\\ &amp;= \\det(\\begin{bmatrix}\\mathbf a\\cdot\\mathbf a&amp;\\mathbf a\\cdot\\mathbf b\\\\\\mathbf b\\cdot\\mathbf a&amp;\\mathbf b\\cdot\\mathbf b\\end{bmatrix})\\\\ &amp;= (\\mathbf a\\cdot\\mathbf a) (\\mathbf b\\cdot\\mathbf b) - (\\mathbf a\\cdot\\mathbf b)(\\mathbf b\\cdot\\mathbf a)\\\\ &amp;= \\|\\mathbf a\\|^2 \\|\\mathbf b\\|^2 - (\\mathbf a\\cdot\\mathbf b)^2\\\\ &amp;= \\|\\mathbf a\\|^2 \\|\\mathbf b\\|^2 - \\|\\mathbf a\\|^2 \\|\\mathbf b\\|^2 \\cos^2\\theta\\\\ &amp;= \\|\\mathbf a\\|^2 \\|\\mathbf b\\|^2 (1 - \\cos^2\\theta)\\\\ &amp;= \\|\\mathbf a\\|^2 \\|\\mathbf b\\|^2 \\sin^2\\theta\\\\ \\|\\mathbf a\\times \\mathbf b\\| &amp;= \\|\\mathbf a\\| \\|\\mathbf b\\|\\sin \\theta \\end{align*}\\] <p>Therefore, \\(\\|\\mathbf a\\times \\mathbf b\\|\\) can be understood are the area of the parallelogram the area of the parallelogram that \\(\\mathbf a, \\mathbf b\\) span. Also, \\(\\mathbf a\\times \\mathbf b, \\mathbf a, \\mathbf b\\) forms a signed basis. </p>"},{"location":"mat363/curvature.html#curvature-for-3d","title":"Curvature for 3D","text":"<p>Note that our definition for curvature depends on \\(\\gamma\\) being unit-speed. However, find a unit-speed parameterization is not always easy even though we know it exist. </p> <p>Claim Let \\(\\gamma(t)\\) be a regular curve in \\(\\mathbb R^3\\), then its curvature is \\(\\kappa = \\frac{\\|\\gamma''\\times \\gamma'\\|}{\\|\\gamma'\\|^3}\\). </p> <p>proof. Since \\(\\gamma\\) is regular, take \\(s\\) be a unit-speed map for \\(\\gamma\\). </p> \\[\\begin{align*} \\gamma' &amp;= \\frac{d\\gamma}{dt} = \\frac{d\\gamma}{ds}\\frac{ds}{dt}&amp;\\text{chain rule}\\\\ \\implies \\frac{d\\gamma}{ds} &amp;= \\frac{d\\gamma}{dt}/\\frac{ds}{dt}\\\\ \\kappa &amp;= \\|\\frac{d^2\\gamma}{ds^2}\\|\\\\ &amp;= \\|\\frac{d}{ds}(\\frac{d\\gamma}{dt}/\\frac{ds}{dt})\\|\\\\ &amp;= \\|\\frac{d}{dt}(\\frac{d\\gamma}{dt}/\\frac{ds}{dt}) / \\frac{ds}{dt}\\|\\\\ &amp;= \\| (\\frac{ds}{dt}\\frac{d^2\\gamma}{dt^2} - \\frac{d^2s}{dt^2}\\frac{d\\gamma}{dt})(\\frac{ds}{dt})^{-3}\\| \\end{align*}\\] <p>Note that \\(s\\) is the unit-length map, so that \\((\\frac{ds}{dt})^2 = (s')^2 = \\|\\gamma'\\|^2 = \\gamma'\\cdot \\gamma'\\), differentiating both side gives </p> \\[\\frac{d}{dt}((\\frac{ds}{dt})^2) = \\frac{ds}{dt} \\frac{d^2s}{dt^2} = \\gamma'\\cdot\\gamma''\\] <p>Then, we can insert these back to get</p> \\[\\begin{align*} \\kappa &amp;= \\| (\\frac{ds}{dt}\\frac{d^2\\gamma}{dt^2} - \\frac{d^2s}{dt^2}\\frac{d\\gamma}{dt})(\\frac{ds}{dt})^{-3}\\|\\\\ &amp;=  \\| ((\\frac{ds}{dt})^2\\frac{d^2\\gamma}{dt^2} - \\frac{d^2s}{dt^2}\\frac{ds}{dt}\\frac{d\\gamma}{dt})(\\frac{ds}{dt})^{-4}\\|\\\\ &amp;= \\|(\\gamma'\\cdot \\gamma')\\gamma'' - (\\gamma'\\cdot \\gamma'')\\gamma'\\|\\|\\gamma'\\|^{-4}\\\\ &amp;= \\frac{\\|(\\gamma'\\cdot \\gamma')\\gamma'' - (\\gamma'\\cdot \\gamma'')\\gamma'\\|}{\\|\\gamma'\\|^{4} } \\end{align*}\\] <p>Using vector triple product identity</p> \\[\\mathbf a\\times (\\mathbf b\\times \\mathbf c) = (\\mathbf a\\cdot \\mathbf c)\\mathbf b - (\\mathbf a \\cdot \\mathbf b)\\mathbf c\\] \\[\\|(\\gamma'\\cdot \\gamma')\\gamma'' - (\\gamma'\\cdot \\gamma'')\\gamma'\\| = \\|\\gamma'\\times (\\gamma''\\times \\gamma')\\| = \\|\\gamma'\\|\\|\\gamma''\\times \\gamma'\\|\\sin\\theta\\] <p>Note that \\(\\gamma'\\) and \\(\\gamma''\\times \\gamma'\\) are orthogonal by definition of cross product, so that \\(\\sin\\theta = 1\\)</p> \\[\\kappa = \\frac{\\|\\gamma'\\|\\|\\gamma''\\times \\gamma'\\|}{\\|\\gamma'\\|^4} = \\frac{\\|\\gamma''\\times \\gamma'\\|}{\\|\\gamma'\\|^3}\\]"},{"location":"mat363/curvature.html#example-compute-curvature","title":"Example: Compute curvature","text":"<p>(1) \\(\\gamma(t) = (\\frac{(1+t)^{3/2} }{3},\\frac{(1-t)^{3/2} }{3}, \\frac{t}{\\sqrt{2} })\\)</p> \\[\\begin{align*} \\gamma'(t) &amp;= (\\frac{1}{2}\\sqrt{1+t}, -\\frac{1}{2}\\sqrt{1-t}, \\frac{1}{\\sqrt{2} })\\\\ \\|\\gamma'\\| &amp;= \\sqrt{\\frac{1+t}{4} + \\frac{1-t}{4} + \\frac{1}{2} } = 1\\\\ \\gamma''(t) &amp;= (\\frac{1}{4}(1+t)^{-1/2}, \\frac{1}{4}(1-t)^{-1/2}, 0)\\\\ \\kappa(t) = \\|\\gamma''(t)\\| &amp;= \\sqrt{(16(1+t))^{-1} + (16(1-t))^{-1} } \\\\ &amp;= \\sqrt{\\frac{1-t + 1+t}{16(1+t)(1-t)} } \\\\ &amp;= (8(1-t^2))^{-1/2} \\end{align*}\\] <p>(2) \\(\\gamma(t) = (\\frac{4\\cos t}{5}, 1-\\sin t, \\frac{-3\\cos t}{5})\\)</p> \\[\\begin{align*} \\gamma'(t) &amp;= (-\\frac{4}{5}\\sin t, -\\cos t, \\frac{3}{5}\\sin t)\\\\ \\|\\gamma'(t)\\| &amp;= (\\frac{16}{25}\\sin^2 t + cos^2 t + \\frac{9}{25}\\sin^2 t)^{1/2} = 1\\\\ \\gamma''(t) &amp;= (-\\frac{4}{5}\\cos t, \\sin t, \\frac{3}{5}\\cos t)\\\\ \\kappa(t) = \\|\\gamma''(t)\\| &amp;= (\\frac{16}{25}\\cos^2 t + \\sin^2 t + \\frac{9}{25}\\cos^2 t)^{1/2} = 1 \\end{align*}\\] <p>(3) \\(\\gamma(t) = (t, \\cosh t)\\)</p> \\[\\begin{align*} \\gamma'(t) &amp;= (1, \\sinh t)\\\\ \\|\\gamma'(t)\\| &amp;= \\sqrt{1 + \\sinh^2} \\neq 1\\\\ \\gamma''(t) &amp;= (0, \\cosh t)\\\\ \\kappa(t) &amp;= \\frac{\\|\\gamma''(t)\\times \\gamma'(t)\\|}{\\|\\gamma'(t)\\|^3}\\\\ &amp;= (1 + \\sinh^2)^{-3/2}\\begin{vmatrix} i&amp;j&amp;k\\\\ 1&amp;\\sinh t&amp;0\\\\ 0&amp;\\cosh t&amp;0 \\end{vmatrix} &amp;= (1+\\sinh^2)^{-3/2} \\cosh t \\end{align*}\\] <p>(4) \\(\\gamma(t) = (\\cos^3 t, \\sin^3 t)\\)</p> \\[\\begin{align*} \\gamma'(t) &amp;= (-3\\cos^2 t \\sin t, 3\\sin^2 t \\cos t)\\\\ \\|\\gamma'(t)\\| &amp;= (9\\cos^4 t sin^2 t + 9 \\sin^4 t \\cos^2 t)^{1/2} = |3\\sin t \\cos t|\\\\ \\gamma''(t) &amp;= (6\\cos t\\sin^2 t - 3cos^3 t, 6\\sin t\\cos^2 t - 3\\sin^3 t))\\\\ \\|\\gamma''(t) \\times \\gamma'(t)\\| &amp;= (-3\\cos^2 t \\sin t)(6\\sin t\\cos^2 t - 3\\sin^3 t) - (3\\sin^2 t \\cos t)(6\\cos t\\sin^2 t - 3cos^3 t)\\\\ &amp;= |-18\\sin^2 t\\cos^4 t + 9\\sin^4 t \\cos^2 t - 18\\sin^4 t \\cos^2 t + 9\\sin^2 t \\cos^4 t|\\\\ &amp;= 9\\sin^2 t\\cos^2 t = |3\\sin t \\cos t|^2\\\\ \\kappa(t) &amp;= \\frac{ |3\\sin t \\cos t|^2}{ |3\\sin t \\cos t|^3} =  |3\\sin t \\cos t|^{-1} \\end{align*}\\] Source code <pre><code>import plotly.graph_objects as go\nimport numpy as np\nt = np.arange(-1, 1, 0.1)\nd1 = go.Scatter3d(\n        x=0.333333 * (1 + t)**1.5, y=0.333333 * (1 - t)**1.5, z=2**(-0.5) * t,\n        mode='lines', name='(1)'\n    )\nd3 = go.Scatter3d(\n        x=t, y=np.cosh(t), z=np.zeros(t.shape),\n        mode='lines', name='(3)'\n    )\nt = np.arange(-1.5 * np.pi, 1.5 * np.pi, 0.01)\nd2 = go.Scatter3d(\n        x=0.8 * np.cos(t), y=1 - np.sin(t), z=-0.6 * np.cos(t),\n        mode='lines', name='(2)'\n    )\nd4 = go.Scatter3d(\n        x = np.cos(t)**3, y=np.sin(t)**3, z=np.zeros(t.shape),\n        mode='lines', name='(4)'\n    )\nfig = go.Figure(data=[d1, d2, d3, d4])\nfig.update_layout(margin=dict(l=0, r=0, b=0, t=0))\nwith open(\"../assets/curvature.json\", \"w\") as f:\n    f.write(fig.to_json())\n</code></pre>"},{"location":"mat363/curvature.html#claim-1","title":"Claim 1","text":"<p>For some regular curve \\(\\gamma\\), and its curvature o\\(\\kappa\\), if \\(\\forall t,\\kappa(t) &gt; 0\\), then \\(\\kappa\\) is smooth. </p> <p>proof. Let \\(\\gamma\\) be regular, wlog assume \\(\\gamma\\) is unit-length so that \\(\\kappa = \\|\\gamma''\\|\\).  Note that \\(\\gamma\\) is smooth, hence all of its components are smooth, add is smooth, and square root is smooth on \\((0, \\infty)\\). Therefore, \\(\\kappa = \\|\\gamma''\\|\\) is smooth on \\((0,\\infty)\\).</p>"},{"location":"mat363/examples_smooth.html","title":"More Examples of Surface Regularity and Smoothness","text":""},{"location":"mat363/examples_smooth.html#examples-of-surface-patches","title":"Examples of Surface Patches","text":"<ul> <li>\\(\\sigma(u,v) = (u,v,uv)\\) is a regular surface patch proof. \\(\\sigma\\) is clearly injective, and \\(\\sigma_u = (1, 0, u), \\sigma_v = (0, 1, v)\\) is clearly linearly independent. </li> <li>\\(\\sigma(u,v) = (u, v^2, v^3)\\) is injective, but \\(\\sigma_u = (1, 0, 0), \\sigma_v = (0, 2v, 3v^2)\\) is not linearly independent when \\(v= 0\\), hence not regular</li> <li>\\(\\sigma(u,v) = (u+u^2 , v, v^2)\\) is not injective, for example \\(\\sigma(0, 0) = \\sigma(-1, 0)\\). Also, \\(sigma_u = (1+2u, 0, 0), \\sigma_v = (0, 1, 2v)\\) is not linearly independent when \\(u=-1/2\\). </li> </ul>"},{"location":"mat363/examples_smooth.html#example-ellipsoid","title":"Example: Ellipsoid","text":"<p>A ellipsoid is defined as </p> \\[\\frac{x^2}{p^2} + \\frac{y^2}{q^2} + \\frac{z^2}{r^2} = 1\\] <p>Similar to how we parameterize a sphere, using the long-lat coordinate, we have </p> \\[\\sigma\\begin{pmatrix}x\\\\y\\\\z\\end{pmatrix} = \\begin{pmatrix} p\\cos\\theta\\cos\\psi\\\\ q\\cos\\theta\\sin\\psi\\\\ r\\sin\\theta \\end{pmatrix}\\] <p>We will still use the same 2 surface patches to cover the surface. </p> <p>Then, consider regularity, we have that </p> \\[\\sigma_\\theta = \\begin{pmatrix} -p\\sin\\theta\\cos\\psi\\\\ -q\\sin\\theta\\sin\\psi\\\\ r\\cos\\theta \\end{pmatrix}, \\sigma_\\psi = \\begin{pmatrix} -p\\cos\\theta\\sin\\psi\\\\ q\\cos\\theta\\cos\\psi\\\\ 0 \\end{pmatrix}\\] <p>The two vectors are linearly independent for </p> \\[U_1 = \\{(\\theta, \\psi): \\theta\\in(-\\pi/2, \\pi/2), \\psi\\in(0, 2\\pi)\\}\\] <p>In addition, surface patches are still regular if taken rotations, hence the ellipsoid is smooth and regular. </p>"},{"location":"mat363/examples_smooth.html#example-torus","title":"Example: Torus","text":"<p>A torus is obtained by rotating a circle \\(C\\) in plane \\(\\Pi\\) around a line \\(l \\subset \\Pi\\) and \\(l\\) does not intersect \\(C\\). Let \\(\\Pi = \\Pi_{XZ}\\), \\(l(t) = (0, t, 0)\\), take \\(a &gt; 0\\) be the distance from the center of \\(C\\) to \\(l\\) and \\(b\\) be the radius of \\(C\\).  Then, the torus is a smooth surface with parameterization </p> \\[\\sigma(\\theta, \\varphi) = \\begin{pmatrix} (a+b\\cos\\theta)\\cos\\varphi\\\\ (a+b\\cos\\theta)\\sin\\varphi\\\\ b\\sin\\theta \\end{pmatrix}\\] <p>proof. WLOG assume that the center of the circle always reside on the \\(XY\\) plane \\((z=0)\\). </p> <p>Consider the circle defined on the XZ plane as \\(\\gamma(\\theta) = (a + b\\cos\\theta, 0, b\\sin\\theta)\\).   Then, rotate \\(\\varphi\\) degrees along the \\(z\\)-axis, the xy coordinate will be </p> \\[\\begin{bmatrix}\\cos\\varphi&amp;-\\sin\\varphi\\\\\\sin\\varphi&amp;\\cos\\varphi\\end{bmatrix}\\begin{bmatrix}a + b\\cos\\theta\\\\0\\end{bmatrix}\\] <p>Therefore we can obtain the parameterization. </p> <p>Then, note that the torus can be cover by 4 patches. By taking each of \\(\\theta, \\varphi\\) in \\((0, 2\\pi), (-\\pi, \\pi)\\), and we check the regularity conditions by </p> \\[\\sigma_\\theta = (-b\\sin\\theta\\cos\\varphi, -b\\sin\\theta\\sin\\varphi, b\\cos\\theta)\\] \\[\\sigma_\\varphi = (-(a+b\\cos\\theta)\\sin\\varphi, (a+b\\cos\\theta)\\cos\\varphi, 0)\\] <p>We can easily verify that for each domain, the vectors are linearly independent. </p> Source code <pre><code>import plotly.graph_objects as go\nimport numpy as np\n\ntheta, phi = np.mgrid[0:2*np.pi:20j, 0:2*np.pi:20j]\n\nfig = go.Figure(data=[go.Surface(\n    x=(2 + np.cos(theta)) * np.cos(phi), \n    y=(2 + np.cos(theta)) * np.sin(phi), \n    z=np.sin(theta)\n    )])\nfig.update_traces(showscale=False)\nfig.update_layout(margin=dict(l=0, r=0, b=0, t=0))\nwith open(\"../assets/torus.json\", \"w\") as f:\n    f.write(fig.to_json())\n</code></pre>"},{"location":"mat363/examples_smooth.html#example-helicoid","title":"Example: Helicoid","text":"<p>A helicoid can be parameterized as </p> \\[\\sigma(u, v) = (v\\cos u, v\\sin u, \\lambda u)\\] <p>Then, we have that </p> \\[\\sigma_u = (-v\\sin u, v\\cos u, \\lambda), \\sigma_v = (\\cos u, \\sin u, 0)\\] \\[N = \\sigma_u \\times \\sigma_v = (-\\lambda \\sin u, \\lambda \\cos u, -v)\\] \\[\\hat N = \\frac{N}{\\|N\\|} = \\frac{1}{\\sqrt{\\lambda^2 + v^2}} (-\\lambda \\sin u, \\lambda \\cos u, -v)\\] Source code <pre><code>import plotly.graph_objects as go\nimport numpy as np\n\nu, v = np.mgrid[0:4*np.pi:20j, 0:4*np.pi:20j]\n\nfig = go.Figure(data=[go.Surface(x=v * np.cos(u), y=v * np.sin(u), z=u)])\nfig.update_traces(showscale=False)\nfig.update_layout(margin=dict(l=0, r=0, b=0, t=0))\nwith open(\"../assets/helicoid.json\", \"w\") as f:\n    f.write(fig.to_json())\n</code></pre>"},{"location":"mat363/examples_smooth.html#example-tube-along-some-curve","title":"Example: Tube along some curve","text":"<p>Let \\(\\gamma\\) be a unit-speed space curve with \\(\\kappa \\neq 0\\) everywhere. The tube is parameterized by </p> \\[\\sigma(s, \\theta) = \\gamma(s) + a(\\mathbf n(s)\\cos\\theta, +\\mathbf b(s)\\sin \\theta)\\] <p>The tube is hence obtained by a circle of radius \\(a\\), where the center of the circle moves through \\(\\gamma\\) and the circle is always on the plane perpendicular to the curve. </p> <p>Claim \\(\\sigma\\) is regular if \\(\\kappa &lt; a^{-1}\\) everywhere</p> <p>proof. For \\(s\\in\\mathbb R, \\theta \\in (0, 2\\pi)\\) (or any open interval of period \\(2\\pi\\))</p> \\[\\begin{align*} \\sigma_s &amp;= \\gamma'(s) + a(\\cos\\theta \\mathbf n'(s) + \\sin\\theta \\mathbf b'(s))\\\\ &amp;= \\mathbf t + a(\\cos\\theta (-\\kappa\\mathbf t+ \\tau \\mathbf b) - \\sin\\theta \\tau\\mathbf n)\\\\ &amp;= (1 - a\\cos(\\theta)\\kappa)\\mathbf t + a\\cos\\theta\\tau\\mathbf b - a\\sin\\theta\\tau \\mathbf n\\\\ \\sigma_\\theta &amp;= a\\cos\\theta \\mathbf b - a\\sin \\theta \\mathbf n\\\\ \\sigma_s \\times \\sigma_\\theta &amp;= (1 - a\\cos(\\theta)\\kappa)(a\\cos\\theta)(\\mathbf t\\times \\mathbf b) - (1 - a\\cos(\\theta)\\kappa)(a\\sin\\theta)(\\mathbf t\\times \\mathbf n)\\\\ &amp;= -a(1-a\\kappa\\cos\\theta)(\\cos\\theta\\mathbf n + \\sin\\theta\\mathbf b) \\end{align*}\\] <p>Then, \\(\\sigma\\) is regular if \\(1-a\\kappa\\cos\\theta\\neq 0\\), note that if \\(\\cos\\theta\\in (-1, 1)\\), so that \\(a\\kappa &lt; 1 \\implies 1-a\\kappa\\cos\\theta &gt; 0\\)</p>"},{"location":"mat363/examples_smooth.html#tangent-and-derivatives","title":"Tangent and Derivatives","text":"<p>Find the tangent plane at given point</p> <p>\\(\\sigma(u,v) = (u, v, u^2 - v^2), (1, 1, 0)\\)</p> \\[\\sigma_u(1, 1)= (1, 0, 2u) = (1, 0, 2), \\sigma_v(1, 1) = (0, 1, 2v) = (0, 1, 2)\\] \\[\\sigma_u\\times \\sigma_v(1, 1) = (-2, 2, 1)\\implies -2x+2y+z =0\\] <p>\\(\\sigma(r,\\theta) = (r\\cosh \\theta, r\\sinh\\theta, r^2), (1, 0, 1)\\)</p> \\[\\sigma_r(1, 0) = (\\cosh \\theta, \\sinh\\theta, 2r) = (1, 0, 2)\\] \\[\\sigma_\\theta(1, 0) = (r\\sinh\\theta, r\\cosh\\theta, 2) = (0, 1, 2)\\] \\[\\sigma_u\\times \\sigma_v(1, 1) = (-2, -2, 1)\\implies -2x-2y+z =0\\] <p>Claim If \\(f:\\Sigma_1\\rightarrow\\Sigma_2\\) is a local diffeomorphism and \\(\\gamma\\) is a regular curve on \\(\\Sigma_1\\). Then \\(f\\circ \\gamma\\) is regular on \\(\\Sigma_2\\). </p> <p>proof. Let \\(\\tilde \\gamma := f\\circ\\gamma : \\mathbb R\\rightarrow \\Sigma_2\\)</p> \\[\\frac{d\\tilde\\gamma}{dt} = D_\\gamma f\\frac{d\\gamma}{dt}\\] <p>where \\(D_\\gamma f\\) is locally intertible and \\(\\gamma' \\neq 0\\) by regularity assumption.  Therefore, \\(\\tilde \\gamma\\) is also regular. </p>"},{"location":"mat363/examples_smooth.html#surface-of-revolution-and-ruled-surfaces","title":"Surface of Revolution and Ruled Surfaces","text":""},{"location":"mat363/examples_smooth.html#example-catenoid","title":"Example: Catenoid","text":"<p>The surface is obtained by rotating \\(x=\\cosh z\\) in xz-plane around z-axis. </p> <p>Take \\(\\gamma(t) = (\\cosh t, 0, t)\\) so that </p> \\[\\sigma(t,\\theta) = (\\cosh t\\cos \\theta, \\cosh t\\sin\\theta, t)\\] <p>Note that we need at least 2 patches \\(t\\in\\mathbb R, \\theta\\in (0,2\\pi)\\) or \\(\\theta\\in(-\\pi,\\pi)\\)</p> Source code <pre><code>import plotly.graph_objects as go\nimport numpy as np\n\nt, theta = np.mgrid[-1:1:10j, 0:1.5 * np.pi:50j]\n\nfig = go.Figure(data=[go.Surface(\n    x=np.cosh(t) * np.cos(theta), \n    y=np.cosh(t) * np.sin(theta), \n    z=t)])\nfig.update_traces(showscale=False)\nfig.update_layout(margin=dict(l=0, r=0, b=0, t=0))\nwith open(\"../assets/catenoid.json\", \"w\") as f:\n    f.write(fig.to_json())\n</code></pre>"},{"location":"mat363/examples_smooth.html#example-mercators-projection","title":"Example: Mercator's Projection","text":"<p>Show that \\(\\sigma(u,v) = (\\text{sech} u\\cos v, \\text{sech} u\\sin v, \\text{tanh} u)\\) is a regular surface patch for \\(\\Sigma = \\{x^2 + y^2 + z^2 = 1\\}\\). </p> \\[\\begin{align*} \\|\\sigma\\|^2 &amp;= \\text{sech}^2 u\\cos^2 v + \\text{sech}^2 u\\sin^2 v + \\text{tanh}^2 u\\\\ &amp;= \\text{sech}^2 u + \\text{tanh}^2 u\\\\ &amp;= 1\\\\ \\sigma_u &amp;= (-\\text{sech}(u)\\text{tanh}(u)\\cos v, -\\text{sech}(u)\\text{tanh}(u)\\sin v, \\text{sech}^2 u)\\\\ \\sigma_v &amp;= (-\\text{sech} u\\sin v, \\text{sech} u\\cos v, \\text{tanh} u)\\\\ \\sigma_u\\times \\sigma_v &amp;= -\\text{sech}^2 u (\\text{sech} u\\cos v, \\text{sech} u\\sin v, \\text{tanh} u) \\neq 0 \\end{align*}\\]"},{"location":"mat363/fff.html","title":"First Fundamental Form","text":""},{"location":"mat363/fff.html#first-fundamental-form_1","title":"First Fundamental Form","text":"<p>Let \\(E = \\|\\sigma_u\\|^2, F = \\sigma_u\\cdot\\sigma_v, G = \\|\\sigma_v\\|^2\\), consideer the matrix  \\(M = \\begin{bmatrix}E&amp;F\\\\F&amp;G\\end{bmatrix}\\), note that \\(E, G\\) are always positve, and since \\(\\det(M) = EG-F^2 &gt; 0, M\\) is always a postive definite symmetric matrix. </p> <p>Consider the inner product equipped with dot product and restricted to the tangent plane at \\(p\\). Then, for any \\(v\\in T_p\\Sigma\\), </p> \\[\\langle v, v\\rangle_{T_p\\Sigma} = \\|a\\sigma_u + b\\sigma_v\\|^2 = Ea^2 + 2Fab + Gb^2\\] <p>The First fundamentally form is the expression </p> \\[\\langle v, v\\rangle_{T_p\\Sigma} = Eu'^2 + 2Fu'v' + Gv'^2\\]"},{"location":"mat363/fff.html#arc-length-of-curve-on-surface","title":"Arc-length of curve on surface","text":"<p>Let \\(\\gamma: \\mathbb R\\rightarrow \\Sigma\\) be some smooth curve on the surface \\(\\Sigma\\), then for some parameterization \\(\\sigma\\) (assuming the surface is covered by one patch) we have that </p> \\[\\gamma(t) = (x(t), y(t), z(t)) = \\sigma(u(t), v(t))\\] <p>Since \\(\\sigma\\) is a homeomorphism, such \\(u,v\\) exists and are smooth. Therefore, for some segment of the curve, </p> \\[\\begin{align*} L &amp;= \\int \\|\\gamma'(t)\\| dt\\\\ &amp;= \\int \\|\\frac{\\partial \\sigma}{\\partial u}\\frac{du}{dt}+\\frac{\\partial \\sigma}{\\partial v}\\frac{dv}{dt}\\|dt\\\\ &amp;= \\int \\sqrt{(\\sigma_u\\cdot\\sigma_u) (u')^2+ 2(\\sigma_u\\sigma_v) u'v' + (\\sigma_v\\cdot\\sigma_v) (v')^2}dt\\\\ &amp;= \\int \\sqrt{E(u')^2 + 2F(u'v') + G(v')^2} dt \\end{align*}\\]"},{"location":"mat363/fff.html#area-of-surface","title":"Area of Surface","text":"<p>Assume doamin \\(\\Omega \\subseteq \\sigma(U)\\), say \\(\\Omega = \\sigma(U_1)\\). The area of \\(U_1\\) is then integrated by \\(\\iint_{U_1}dudv\\), i.e. integrate along each square \\((u, v), (u+du, v+dv)\\). For small \\(du,dv\\), since \\(\\sigma\\) is a local diffeomorphism, it is approximated by the tangent plane \\(\\sigma_u, \\sigma_v\\). Therefore, the area of the mapped square is \\(\\|\\partial_u\\times \\partial_v\\|\\), by change of variable, </p> \\[\\begin{align*} A(\\Omega) &amp;= \\iint_{U_1} \\|\\partial_u\\times \\partial_v\\| dudv\\\\ &amp;= \\iint_{U_1} \\sqrt{(\\sigma_u\\cdot \\sigma_u)(\\sigma_v\\cdot \\sigma_v) - (\\sigma_u\\cdot\\sigma_v)^2} dudv\\\\ &amp;= \\int_{U_1} \\sqrt{EG - F^2} dudv \\end{align*}\\]"},{"location":"mat363/fff.html#angle-between-curves","title":"Angle between Curves","text":"<p>Let \\(\\gamma_1: \\mathbb R\\rightarrow \\Sigma, \\gamma_2: \\mathbb R\\rightarrow\\Sigma\\) with \\(\\gamma_1(t_1) = \\gamma_2(t_2) = p\\in\\Sigma\\).  Then, at point \\(p, \\gamma'_1, \\gamma'_2 \\in T_p\\Sigma\\).  </p> <p>Define angle two 2 curves on the surface as the angle on \\(T_p\\Sigma\\) at intersection point \\(p\\). </p> <p>Note that for each \\(\\gamma = \\sigma(u(t), v(t))\\) for \\(u,v:\\mathbb R\\rightarrow\\mathbb R\\), its derivative is</p> \\[\\frac{d}{dt} (\\sigma(u(t), v(t))) = \\sigma_u\\cdot u' + \\sigma_v\\cdot v'\\] <p>Therefore, the angle is defined as </p> \\[\\begin{align*} \\angle(\\gamma_1,\\gamma_2) &amp;= \\arccos(\\frac{\\gamma_1'}{\\|\\gamma_1'\\|}\\cdot \\frac{\\gamma_1'}{\\|\\gamma_1'\\|})\\\\ &amp;= \\frac{(\\sigma_{u}\\cdot u_1' + \\sigma_{v}\\cdot v_1')\\cdot(\\sigma_{u}\\cdot u_2' + \\sigma_{v}\\cdot {v_2}')}{\\|\\gamma_1'\\|\\|\\gamma_2'\\|}\\\\ &amp;= \\frac{(\\sigma_u\\cdot\\sigma_u) (u_1'\\cdot u_2') + (\\sigma_u\\cdot\\sigma_v)(u_1'v_2'+u_2'v_1') + (\\sigma_v\\cdot\\sigma_v) (v_1'\\cdot v_2')}{(E(u'_1)^2 + 2F(u_1'\\cdot v_1') + G(v_1')^2)(E(u'_2)^2 + 2F(u_2'\\cdot v_2') + G(v_2')^2)}\\\\ &amp;= \\frac{E (u_1'\\cdot u_2') +F(u_1'v_2'+u_2'v_1') + G (v_1'\\cdot v_2')}{(E(u'_1)^2 + 2F(u_1'\\cdot v_1') + G(v_1')^2)(E(u'_2)^2 + 2F(u_2'\\cdot v_2') + G(v_2')^2)} \\end{align*}\\]"},{"location":"mat363/fff.html#example-parameter-curve","title":"Example: Parameter Curve","text":"<p>The parameter curves on a surface patch \\(\\sigma(u,v)\\) can be parametrized by </p> \\[\\gamma_1(t) = \\sigma(u_0, t), \\gamma_2(t) = \\sigma(t, v_0)\\] <p>so that \\(D\\gamma_1 = (1, 0), D\\gamma_2 = (0, 1)\\) \\(\\cos\\theta = \\frac{E(0) + F(1) + G(0)}{\\sqrt{EG} } = \\frac{F}{\\sqrt{EG} }\\)</p>"},{"location":"mat363/fff.html#examples","title":"Examples","text":"<p>\\(\\sigma = (\\sinh u\\sinh v, \\sinh u\\cosh v, sinh u)\\)</p> \\[\\begin{align*} \\sigma_u &amp;= (\\cosh u\\sinh v, \\cosh u\\cosh v, \\cosh u)\\\\ \\sigma_v &amp;= (\\sinh u\\cosh v, \\sinh u\\sinh v, 0)\\\\ E = \\sigma_u\\cdot\\sigma_v &amp;= \\cosh^2 u\\sinh^2 v + \\cosh^2 u\\cosh^2 v + \\cosh^2 u \\\\ &amp;= \\cosh^2 u((\\sinh^2 v + 1)+\\cosh^2 v)\\\\ &amp;= \\cosh^2 u (\\cosh^2 v + \\cosh^2 v)\\\\ &amp;= 2\\cosh^2u \\cosh^2 v\\\\ F = \\sigma_u\\cdot\\sigma_v &amp;=2\\cosh u\\cosh v\\sinh u\\sinh v\\\\ G = \\sigma_v\\cdot\\sigma_v &amp;=\\sinh^2 u (\\cosh^2 v + \\sinh^2 v) = \\sinh^2 u \\cosh (2v)\\\\ \\end{align*}\\] <p>\\(\\sigma = (u-v, u+v, u^2+v^2)\\)</p> \\[\\sigma_u = (1, 1, 2u)\\sigma_v = (-1, 1, 2v)\\] \\[(2 + 4u^2)du^2 + 8uv dudv + (2+4v^2)dv^2\\] <p>\\(\\sigma = (\\cosh u, \\sinh u, v)\\)</p> \\[\\sigma_u = (\\sinh u, \\cosh u, 0)\\sigma_v = (0, 0, 1)\\] \\[(\\sinh^2 u +\\cosh^2 u)du^2 + dv^2\\] <p>\\(\\sigma = (u, v, u^2+v^2)\\)</p> \\[\\sigma_u = (1, 0, 2u)\\sigma_v = (0, 1, 2v)\\] \\[(1 + 4u^2)du^2 + 8uv dudv + (1+4v^2)dv^2\\] Source code <pre><code>import plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nimport numpy as np\nfrom numpy import sinh, cosh\nfrom IPython.display import display, IFrame\n\ntheta, psi = np.mgrid[-.5 * np.pi:.5 * np.pi:10j, -np.pi: np.pi:10j]\nu, v = np.mgrid[-.5 :.5 :10j, -1: 1:10j]\nS1 = go.Surface(x=sinh(theta) * sinh(psi), y=sinh(theta) * cosh(psi), z=sinh(theta))\nS2 = go.Surface(x=u - v, y=u + v, z=u * u + v * v)\nS3 = go.Surface(x=cosh(theta), y=sinh(theta), z=v)\nS4 = go.Surface(x=u, y=v, z=u * u + v * v)\n\nfig = make_subplots(cols=2, rows=2, horizontal_spacing=0, vertical_spacing=0, \n                    specs=[[{\"type\": \"scene\"}, {\"type\": \"scene\"}], [{\"type\": \"scene\"}, {\"type\": \"scene\"}]])\nfig.add_trace(S1, row=1, col=1)\nfig.add_trace(S2, row=1, col=2)\nfig.add_trace(S3, row=2, col=1)\nfig.add_trace(S4, row=2, col=2)\nfig.update_traces(showscale=False)\nfig.update_layout(margin=dict(l=0, r=0, b=0, t=0), height=720)\nwith open(\"../assets/fff.json\", \"w\") as f:\n    f.write(fig.to_json())\n</code></pre>"},{"location":"mat363/fff.html#first-fundamental-forms-with-reparameterization","title":"First Fundamental Forms with Reparameterization","text":"<p>Theorem Let \\(\\tilde \\sigma(\\tilde u, \\tilde v)\\) be a reparameterization of some \\(\\sigma(u,v)\\) with transformation map \\(\\Phi\\). Then, the first fundamental form of the reparameterization is </p> \\[\\begin{bmatrix}\\tilde E &amp;\\tilde F\\\\\\tilde F&amp;\\tilde G\\end{bmatrix} = J(\\Phi)^T \\begin{bmatrix}E&amp;F\\\\F&amp;G\\end{bmatrix}J(\\Phi)\\] <p>where \\(J\\) is the Jacobian matrix of \\(\\Phi\\). </p> <p>proof. First, by the reparameterization, we have that \\((u, v) = \\Phi(\\tilde u, \\tilde v)\\), therefore </p> \\[\\frac{\\partial u}{\\partial t} = \\frac{\\partial u}{\\partial \\tilde u}\\frac{\\partial \\tilde u}{\\partial t}+\\frac{\\partial u}{\\partial \\tilde v}\\frac{\\partial \\tilde v}{\\partial t}, \\frac{\\partial v}{\\partial t} = \\frac{\\partial v}{\\partial \\tilde u}\\frac{\\partial \\tilde u}{\\partial t}+\\frac{\\partial v}{\\partial \\tilde v}\\frac{\\partial \\tilde v}{\\partial t}\\] <p>If we plug in this, we have that the first fundamental form of \\(\\tilde \\sigma\\) is</p> \\[E(\\partial_{\\tilde u} u\\tilde u' + \\partial_{\\tilde v} u\\tilde v')^2 + 2F(\\partial_{\\tilde u} u\\tilde u' + \\partial_{\\tilde v} u\\tilde v')(\\partial_{\\tilde u} v\\tilde u' + \\partial_{\\tilde v} v\\tilde v') + G(\\partial_{\\tilde u} v\\tilde u' + \\partial_{\\tilde v} v\\tilde v')^2\\] <p>Simplify the equation, we can have the formula as wanted</p> \\[\\begin{bmatrix}\\tilde E &amp;\\tilde F\\\\\\tilde F&amp;\\tilde G\\end{bmatrix} =  \\begin{bmatrix}\\partial_{\\tilde u} u &amp;\\partial_{\\tilde u} v\\\\\\partial_{\\tilde v} u&amp;\\partial_{\\tilde v} v\\end{bmatrix}  \\begin{bmatrix}E&amp;F\\\\F&amp;G\\end{bmatrix} \\begin{bmatrix}\\partial_{\\tilde u} u &amp;\\partial_{\\tilde v} u\\\\\\partial_{\\tilde u} v&amp;\\partial_{\\tilde v} v\\end{bmatrix}\\]"},{"location":"mat363/fff.html#example-surface-of-revolution","title":"Example: Surface of Revolution","text":"<p>For surface pf revolution </p> \\[\\sigma(u,v) = (f(u)\\cos v, f(u)\\sin v, g(u))\\] <p>with the assumption that \\(\\gamma(t) = (f(t), g(t))\\) resides on the plane is unit-speed.  Then, we have </p> \\[\\sigma_u = (f'\\cos v, f'\\sin v, g'), \\sigma_v = (-f\\sin v, f\\cos v, 0)\\] \\[E = f'^2+g'^2 = 1, F = 0. G = f^2\\] <p>Therefore, the first fundamental form is </p> \\[u'^2 + f(u)^2 v'^2\\] <p>For sphere, we have \\(f(u) = \\cos u\\), which implies the first fundamental form is </p> \\[u'^2 + \\cos^2 u v'^2\\]"},{"location":"mat363/fvt.html","title":"Four Vertex Theorem","text":""},{"location":"mat363/fvt.html#simple-closed-curves","title":"Simple Closed Curves","text":"<p>A parameterized curve \\(\\gamma\\) is closed with a period of \\(T\\), if \\(\\forall t\\in \\mathbb R, \\forall k\\in\\mathbb N. \\gamma(t) = \\gamma(t+kT)\\)</p> <p>A parameterized curve \\(\\gamma\\) is simple if it has no self-intersections.</p>"},{"location":"mat363/fvt.html#jordans-theorem","title":"Jordan's Theorem","text":"<p>Claim Any simple closed curve in the plane has a connected bounded \"interior\" and an connected unbounded \"exterior\".</p>"},{"location":"mat363/fvt.html#hopfs-umlaufsatz-theorem","title":"Hopf's Umlaufsatz Theorem","text":"<p>The total signed curvature of a simple closed curve in \\(\\mathbb R^2\\) is \\(\\pm 2\\pi\\).</p>"},{"location":"mat363/fvt.html#vertex","title":"Vertex","text":"<p>A vertex of a plane curve \\(\\gamma\\) is a point where \\(\\frac{d\\kappa_s}{dt} = 0\\). Note that this definition is independent of parameterizations.</p> <p>Example Every point on the circle \\(\\gamma(t) = (R \\cos t, R\\sin t)\\) is a vertex. </p> <p>proof. We known that \\(\\kappa_s = R^{-1}\\) for the circle \\(\\gamma\\), so that \\(\\frac{d\\kappa_s}{dt} = 0\\) everywhere.</p>"},{"location":"mat363/fvt.html#circumscribed-circle","title":"Circumscribed circle","text":"<p>For a simple closed curve \\(\\gamma:\\mathbb R\\rightarrow\\mathbb R^2\\), we can define a circumscribed circle \\(C(c, R) = \\{x\\in\\mathbb R^2: \\|x - c\\| = R\\}\\) that contains \\(\\gamma(\\mathbb R)\\) and attains the smallest possible radius. </p> <p>Claim existence</p> <p>proof. By Jordon's Theorem, since \\(\\gamma\\) is simple, closed. Its interior is bounded. There exists some circles \\(C(c,R)\\) that bounds \\(\\gamma(\\mathbb R)\\).  </p> <p>Then, we need to consider whether \\(\\text{inf}\\{R\\}\\) exists. </p> <p>Take a sequence \\(c_n, R_n\\) s.t. \\(R_n\\searrow \\text{inf}\\{R\\} = R_\\infty\\). Note that \\(\\|c-\\gamma(s)\\| \\leq R_n\\), take a convergent subsequence \\(c_{n_k}\\) s.t. \\(\\lim_{k\\rightarrow\\infty} c_{n_k} = c_\\infty\\). </p> <p>We claim that circle \\(C(c_\\infty, R_\\infty)\\) contains \\(\\gamma(\\mathbb R)\\). The idea of the claim is that if we assume \\(C_\\infty = C(c_\\infty, R_\\infty)\\) does not contain \\(\\gamma(\\mathbb R)\\), then there are some part of \\(\\gamma(\\mathbb R)\\) \"sticks out\" of \\(C_\\infty\\), i.e. </p> \\[\\exists s_0\\in\\mathbb R. \\|\\gamma(s_0) - c_\\infty\\| &gt; R_\\infty\\] <p>By triangle inequality we have </p> \\[\\|\\gamma(s_0) - c_\\infty\\| \\leq \\|\\gamma(s_0) - c_{n_k}\\| \\leq \\|\\gamma(s_0) - c_\\infty\\| + \\|c_\\infty - c_{n_k}\\|\\] <p>Let \\(D:= \\|\\gamma(s_0) - c_\\infty\\| + \\|c_\\infty - c_{n_k}\\|, \\epsilon_k := \\|c_\\infty - c_{n_k}\\|\\).  so that </p> \\[\\|\\gamma(s_0) - c_{n_k}\\| \\geq D - \\epsilon_{k}\\] <p>Take \\(k\\) large enough s.t. \\(\\epsilon_k &lt; \\frac{D-R_\\infty}{2}\\) and \\(R_{n_k} - R_\\infty &lt; \\frac{D-R_\\infty}{2}\\). Therefore, </p> \\[\\|\\gamma(s_0) - c_{n_k}\\| \\geq D - \\frac{D-R_\\infty}{2} = \\frac{D-R_\\infty}{2} + R_\\infty &gt; R_{n_k}\\] <p>Then, by the construction of our sequence, this is a contradiction. </p> <p>\u200b </p>"},{"location":"mat363/fvt.html#claim-uniqueness","title":"Claim. uniqueness","text":"<p>Assume not unique, then exists \\(C_1 = (c_1, R), C_2 = (c_2, R)\\) (\\(R\\) must be the same since minimum) s.t. \\(\\|\\gamma(s) - c_1\\| &lt; R\\) and \\(\\|\\gamma(s) - c_2\\| &lt; R\\). Therefore, \\(\\gamma(\\mathbb R) \\in \\text{int}(C_1)\\cap \\text{int}(C_2)\\). Then, we can construct new circle according to the picture, where \\(c = \\frac{c_1 + c_2}{2}\\) and by triangle inequality, \\(R' &lt; R\\)</p>"},{"location":"mat363/fvt.html#lemmas","title":"Lemmas","text":"<p>Fact 1 \\(C(c, R)\\) and \\(\\gamma(\\mathbb R)\\) has at least one intersection.  Assume not, then we can keep \\(c\\) unmoved, note that \\(\\forall s. \\|\\gamma(s) - c\\| &lt; R\\) so that we can shrink \\(R' &lt; R\\).</p> <p>Fact 2 \\(C(c, R)\\) and \\(\\gamma(\\mathbb R)\\) has at least two intersections. Assume not, by fact 1, we take \\(s_0\\) to be the only intersection. Then move \\(c\\) along the direction of \\(\\gamma(s_0)-c\\) by arbitrarily small \\(\\epsilon\\), then it has no intersection, which violates Fact 1.</p> <p>Fact 3 If \\(C(c, R)\\) and \\(\\gamma(\\mathbb R)\\) has exactly two intersections, then the line segment between intersection points is a diameter.  If not, then move \\(c\\) along the direction normal to the line.</p> <p>Fact 4 \\(\\gamma(\\mathbb R)\\) and \\(C\\) has the same tangent at their point of intersections.  Assume there are at least two intersections \\(p_1,...,p_n\\). First, we want to show that at all intersections, the orientation is the same. Assume \\(\\gamma\\) is oriented the same as \\(C\\) as intersection point \\(p_k\\), to show that all intersection points have the same intersection, it's sufficient to show that of \\(p_{k+1}\\). Note that the curve \\(\\gamma([p_k, p_{k+1}])\\) and the arc of circle between \\(p_k\\) and \\(p_{k+1}\\) will form a simple closed curve, by Jordon's theorem, it's bounded and connected. Then, consider any simple curve within the enclosed region from \\(p_{k+1}\\) to \\(p_k\\), and it forms a simple closed curve with  \\(\\gamma([p_k, p_{k+1}])\\). Therefore, they must oriented the same. </p> <p>Fact 5.1 If \\(\\gamma\\)'s image is within \\(\\text{int}(C)\\) in a neighborhood of the intersection point, then the curvature of \\(\\gamma, \\kappa_s(s_i) \\geq R^{-1}\\). Wlog assume \\(c = (0,0)\\). At point of intersection, we have that \\(p_k = \\gamma(s_k)\\), then note that \\(\\|\\gamma(s_k)\\|^2 = \\gamma(s_k)\\cdot\\gamma(s_k)= R^2\\) must attain its maximum (since other points are \\(\\|\\gamma(s)\\| \\leq R\\) by circumscribed circle). Then, we have </p> \\[\\frac{d}{ds}(\\gamma(s_k)\\cdot \\gamma(s_k)) = 2\\gamma'(s_k)\\cdot\\gamma(s_k) = 0\\] <p>Therefore, we have that \\(\\mathbf t(s_k)\\) is perpendicular to the radius at \\(p_k\\) and is the same tangent vector. </p> <p>In addition, we have that </p> \\[\\begin{align*} \\frac{d^2}{ds^2}(\\gamma(s_k)\\cdot \\gamma(s_k)) &amp;\\leq 0\\\\ 2(\\gamma''(s_k)\\cdot\\gamma(s_k) + \\gamma'(s_k)\\cdot\\gamma'(s_k))&amp;\\leq 0\\\\ 2(\\kappa_s(s_k)p_k \\cdot \\mathbf n_s(s_k) + 1) &amp;\\leq 0 \\end{align*}\\] <p>Then note \\(\\mathbf n_s(s_k)\\) is parallel and directly opposite to \\(p_k\\), since \\(\\|p_k\\| = R, p_k \\cdot \\mathbf n_s(s_k) = -R\\)</p> \\[\\begin{align*} \\frac{d^2}{ds^2}(\\gamma(s_k)\\cdot \\gamma(s_k)) &amp;\\leq 0\\\\ (-\\kappa_s(s_k)R + 1) &amp;\\leq 0\\\\ \\kappa_s(s_k) &amp;\\geq R^{-1} \\end{align*}\\] <p>Fact 5.2 If \\(\\gamma\\)'s image is within \\(\\text{ext}(C)\\) in a neighborhood of the intersection point, then the curvature of \\(\\gamma, \\kappa_s(s_i) \\leq R^{-1}\\).</p> <p>The idea is similar to Fact 5.1 while here \\(\\gamma(s_k)\\cdot \\gamma(s_k)\\) attains its local minimum, so that </p> \\[\\begin{align*} \\frac{d^2}{ds^2}(\\gamma(s_k)\\cdot \\gamma(s_k)) &amp;\\geq 0\\\\ \\kappa_s(s_k) &amp;\\leq R^{-1} \\end{align*}\\]"},{"location":"mat363/fvt.html#theorem-four-vertex-theorem","title":"Theorem (Four Vertex Theorem)","text":"<p>For a simple closed curve \\(\\gamma:\\mathbb R\\rightarrow\\mathbb R^2\\), it has at least 4 vertex. </p> <p>Let \\(\\gamma\\) be a simple closed curve, and \\(C\\) be its circumscribed curve, \\(p_k = \\gamma(s_k)\\) and \\(p_{k+1} = \\gamma(s_{k+1})\\) be two conservative intersection points. Connecting the two points with the line segment \\(l = \\overline{p_k p_{k+1} }\\) and move the the center of the circle along the direction normal to \\(l\\) until the last moment it touches \\(\\gamma\\). Then, the \\(\\gamma([s_k, s_{k+1}])\\) is all in \\(\\text{ext}(C)\\), by Fact 5.2, there exists at least one point \\(q_k\\) s.t. \\(\\kappa_s(q_k) \\leq R^{-1}\\). Note that there are two opposite directions to move \\(c\\).</p> <p>Therefore, we have that the curve passing through \\(p_{k}, q_k, p_{k+1}\\) must have a local minimum since it goes down from \\(\\geq R^{-1}\\) to \\(\\leq R^{-1}\\) and then back to \\(\\geq R^{-1}\\). Therefore, we have at least \\(n\\) local minimum.  Then, consider the curve passing through \\(q_k, p_k, q_{k+1}\\), similarly we can obtain \\(n\\) local maximum. </p> <p>Note that the only possible case is that \\(\\kappa_s\\) at \\(p_k, q, p_{k+1}\\) are all \\(R^{-1}\\). In this case, there is infinitely many points of \\(\\gamma\\) in the neighborhood of \\(p_k\\), that is coincided with the arc of the circle. Since \\(C\\) is the circumscribed circle, either \\(\\gamma([p_k, p_{k+1]}\\) coincides with the arc of the circle all the way, or \\(\\kappa(q_k) &lt; R^{-1}\\)</p>"},{"location":"mat363/fvt.html#example-limacon","title":"Example : Limacon","text":"<p>Let \\(\\gamma:\\mathbb R\\rightarrow\\mathbb R^2\\) be defined as </p> \\[\\gamma(t) = ((1+2\\cos t)\\cos t, (1 + 2\\cos t)\\sin t)\\] <p>Note that \\(\\gamma\\) is closed (but not simple) with a period of \\(2\\pi\\). </p> <p>Claim Limacon only has 2 vertices. </p> <p>proof. </p> \\[\\begin{align*} \\gamma'(t) &amp;=  (-\\sin t - 2\\sin 2t, \\cos t + 2\\cos 2t)\\\\ \\|\\gamma'(t) \\| &amp;= \\sqrt{\\sin^2 t + 4\\sin t\\sin 2t + 4\\sin^2 2t + \\cos^2 t + 4\\cos t\\cos 2t + 4\\cos^2 2t}\\\\ &amp;= \\sqrt{5 + 8\\cos t \\sin^2 t + 4\\cos t - 8\\cos t\\sin^2 t)}\\\\ &amp;= \\sqrt{5 + 4\\cos t} \\end{align*}\\] <p>Then, let \\(\\gamma'(t) = \\frac{1}{\\sqrt{5 + 4\\cos t} }(-\\sin t - 2\\sin 2t, \\cos t + 2\\cos 2t)\\) and define the turning angle by </p> \\[\\cos\\varphi = \\frac{-\\sin t - 2\\sin 2t}{\\sqrt{5 + 4\\cos t} }. \\sin\\varphi = \\frac{\\cos t + 2\\cos 2t}{ {\\sqrt{5 + 4\\cos t} } }\\] <p>Because we only interested in \\(\\kappa_s = \\varphi'\\), observe that </p> \\[\\begin{align*} \\frac{d}{dt}\\sin \\varphi &amp;= \\frac{d}{d\\varphi}\\sin\\varphi\\frac{d\\varphi}{dt}\\\\ \\frac{d}{dt}\\sin \\varphi &amp;= \\frac{(-\\sin t - 4\\sin 2t)\\sqrt{5+4\\cos t} - (\\cos t + 2\\cos 2t)(-4\\sin t)\\frac{1}{2\\sqrt{5+4\\cos t} } }{5+4\\cos t}\\\\ &amp;= \\frac{-\\sin t(24\\cos^2 t + 42\\cos t +9)}{(5+4\\cos t)^{3/2} }\\\\ \\varphi'(t) &amp;= \\frac{\\frac{d}{dt}\\sin \\varphi}{\\cos \\varphi} \\\\ &amp;= \\frac{\\frac{-\\sin t(24\\cos^2 t + 42\\cos t +9)}{(5+4\\cos t)^{3/2} } }{\\frac{-\\sin t - 2\\sin 2t}{\\sqrt{5 + 4\\cos t} } }\\\\ &amp;= \\frac{9+6\\cos t}{5+4\\cos t}\\\\ \\kappa_s &amp;= \\frac{d\\varphi}{ds} = \\frac{9+6\\cos t}{5+4\\cos t} \\frac{1}{\\sqrt{5+4\\cos t} }\\\\ &amp;= \\frac{9+6\\cos t}{(5+4\\cos t)^{3/2} }\\\\ \\frac{d\\kappa_s}{dt} &amp;= \\frac{12\\sin t (2+\\cos t)}{(5+4\\cos t)^{5/2} } \\end{align*}\\] <p>Therefore, \\(\\frac{d\\kappa_s}{dt} = 0\\) only when \\(t=0, \\pi\\) for \\(t\\in [0, 2\\pi)\\) </p>"},{"location":"mat363/gaussian_curvature.html","title":"Gaussian  Curvatures","text":""},{"location":"mat363/gaussian_curvature.html#gaussian-and-mean-curvatures","title":"Gaussian and Mean Curvatures","text":"<p>Let \\(W\\) be the Weingarten maps of some oriented surface \\(\\Sigma\\) at point \\(p\\in \\Sigma\\). The Gaussian curvature \\(K\\) and mean curvature \\(H\\) of \\(\\Sigma\\) at \\(p\\) are defined as </p> \\[K = \\det (W), H = \\frac{1}{2}Tr(W)\\]"},{"location":"mat363/gaussian_curvature.html#theorem-1","title":"Theorem 1","text":"<p>Let \\(\\sigma\\) be a surface patch of \\(\\Sigma\\). Then, \\(W_{p,\\Sigma}\\) w.r.t. basis \\(\\{\\sigma_u, \\sigma_v\\}\\) of \\(T_p\\Sigma\\) is </p> \\[W = \\begin{bmatrix}E&amp;F\\\\F&amp;G\\end{bmatrix}^{-1}\\begin{bmatrix}L&amp;M\\\\M&amp;N\\end{bmatrix} = F_I^{-1}F_{II}\\] <p>proof. Known that \\(W(\\sigma_u) = -\\mathbf N_u, W(\\sigma_v) = -\\mathbf N_v\\). Let the linear matrix with 4 unknowns \\(W = \\begin{bmatrix}a&amp;b\\\\c&amp;d\\end{bmatrix}\\) so that we have equations </p> \\[-\\mathbf N_u = a\\sigma_u + b\\sigma_v, -\\mathbf N_v = c\\sigma_u + d\\sigma_v\\] <p>We also have that </p> \\[\\begin{align*} L &amp;= -\\sigma_{u}\\mathbf N_u = \\sigma_u (a\\sigma_u + b\\sigma_v) = aE + bF\\\\ M &amp;= -\\sigma_u\\mathbf N_v = \\sigma_u (c\\sigma_u + d\\sigma_v) = cE+dF\\\\ &amp;= -\\sigma_v\\mathbf N_u = aF+bG\\\\ N &amp;= -\\sigma_v\\mathbf N_v = \\sigma_v(c\\sigma_u + d\\sigma_v) = cF + dG \\end{align*}\\] <p>Thus, we can solve the 4 unknowns with 4 equations as </p> \\[\\begin{bmatrix}L&amp;M\\\\M&amp;N\\end{bmatrix} = \\begin{bmatrix}E&amp;F\\\\F&amp;G\\end{bmatrix}\\begin{bmatrix}a&amp;b\\\\c&amp;d\\end{bmatrix}\\]"},{"location":"mat363/gaussian_curvature.html#corollary-2","title":"Corollary 2","text":"\\[\\begin{align*} K &amp;= \\det(W) = \\det(F_I)^{-1}\\det(F_{II}) = \\frac{LN-M^2}{EG-F^2}\\\\ H&amp;= \\frac{1}{2}tr(F_I^{-1}F_{II}) \\\\ &amp;= \\frac{1}{2(EG-F^2)}tr(\\begin{bmatrix}G&amp;-F\\\\-F&amp;E\\end{bmatrix}\\begin{bmatrix}L&amp;M\\\\M&amp;N\\end{bmatrix})\\\\ &amp;= \\frac{1}{2(EG-F^2)}tr\\begin{pmatrix}LG-MF&amp;MG-NF\\\\ME-LF&amp;NE-MF\\end{pmatrix}\\\\ &amp;= \\frac{LG-2MF+NE}{2(EG-F^2)} \\end{align*}\\]"},{"location":"mat363/gaussian_curvature.html#example-surface-zfxy","title":"Example: Surface z=f(x,y)","text":"<p>For surface \\(\\Sigma = \\{(x,y,z): z=f(x,y)\\}\\), define \\(\\sigma(u,v) = (u,v,f(u,v))\\). We have that </p> \\[\\begin{align*} \\sigma_u &amp;= (1, 0, f_x), \\sigma_v = (0, 1, f_y)\\\\ \\mathbf N &amp;= \\frac{(-f_x,-f_y,1)}{\\sqrt{1+f_x^2+f_y^2} }\\\\ E &amp;= 1+f_x^2, F = f_xfy, G = 1+f_y^2\\\\ \\sigma_{uu} &amp;= (0, 0, f_{xx}), \\sigma_{uv} = (0, 0, f_{xy}), \\sigma_{vv} = (0, 0, f_{yy})\\\\ L &amp;= \\frac{f_{xx} }{\\sqrt{1+f_x^2+f_y^2} }, M  = \\frac{f_{xy} }{\\sqrt{1+f_x^2+f_y^2} }, N  = \\frac{f_{yy} }{\\sqrt{1+f_x^2+f_y^2} }\\\\ K &amp;= \\frac{LN-M^2}{EG-F^2} \\\\   &amp;= \\frac{f_{xx}f_{yy} - f_{xy}^2}{1+f_x^2+f_y^2}\\frac{1}{(1+f_x^2)(1+f_y^2) - f_x^2f_y^2}\\\\   &amp;= \\frac{f_{xx}f_{yy} - f_{xy}^2}{(1+f_x^2+f_y^2)^2}\\\\ H &amp;= \\frac{1}{2(1+x^2+y^2)}\\frac{(1+f_y^2)f_{xx} - 2f_xf_yf_{xy} + (1+f_x^2)f_{yy} }{\\sqrt{1+f_x^2+f_y^2} }\\\\   &amp;= \\frac{(1+f_y^2)f_{xx} - 2f_xf_yf_{xy} + (1+f_x^2)f_{yy} }{2(1+f_x^2+f_y^2)^{3/2} } \\end{align*}\\]"},{"location":"mat363/gaussian_curvature.html#principal-curvatures","title":"Principal Curvatures","text":"<p>Observe that \\(W\\) is self-adjoint \\(2\\times 2\\) matrix, by eigen decomposition, we have that </p> \\[W(\\mathbf t_1) = \\kappa_1\\mathbf t_1, W(\\mathbf t_2) = \\kappa_2\\mathbf t_2\\] <p>Call the eigenvalues \\(\\kappa_1, \\kappa_2\\) the principal curvatures and eigen vectors \\(\\mathbf t_1,\\mathbf t_2\\) the principal vectors.</p> <p>In addition, using linear algebra, determinant is the product of all eigen values, trace is the sum of all eigen values, </p> \\[K = \\det(W) = \\kappa_1\\kappa_2, H = \\frac{1}{2}Tr(W) = \\frac{\\kappa_1+\\kappa_2}{2}\\] <p>Also, by eigen decomposition, another corollary is that \\(\\mathbf t_1,\\mathbf t_2\\) forms a orthonormal basis of \\(T_p\\Sigma\\).</p> <p>Finding the principal values and principal vectors follow the standard approach for eigen decomposition, i.e. solve </p> \\[\\begin{align*}\\det(F_I^{-1}F_{II} - \\kappa I)  &amp;= \\det(F_I^{-1}(F_{II} - \\kappa F_I))\\\\ &amp;= \\det(F_{II} - \\kappa F_I)\\\\  &amp;= (L-\\kappa E)(N-\\kappa G) - (M-\\kappa F)^2\\\\  &amp;= 0\\end{align*}\\] <p>and each of the corresponding null space on \\(T_p\\Sigma\\). </p>"},{"location":"mat363/gaussian_curvature.html#eulers-theorem","title":"Euler's Theorem","text":"<p>Theorem Let \\(\\Sigma\\) be an oriented surface, \\(\\gamma:\\mathbb R\\rightarrow\\Sigma\\) be a curve on \\(\\Sigma\\). Then, the normal curvature of \\(\\gamma\\) is </p> \\[\\kappa_n = \\kappa_1\\cos^2\\theta + \\kappa_2\\sin^2\\theta, \\theta = \\arccos(\\mathbf t_1\\cdot \\gamma')\\] <p>proof. Note that \\(\\mathbf t_1,\\mathbf t_2\\) is a orthonormal basis, WLOG assume \\(\\mathbf t_1\\cdot\\mathbf t_2 = \\pi/2\\) (otherwise, flip one of them). Therefore, by the turning angle </p> \\[\\gamma' = \\cos\\theta\\mathbf t_1 + \\sin\\theta \\mathbf t_2\\] <p>and the normal curvature, by bilinearity, is </p> \\[\\kappa_n = II(\\gamma', \\gamma') = \\cos^2\\theta II(\\mathbf t_1,\\mathbf t_1) + 2\\sin\\theta\\cos\\theta II(\\mathbf t_1,\\mathbf t_2) +\\sin^2\\theta II(\\mathbf t_2,\\mathbf t_2 )\\] <p>By orthonormal,</p> \\[II(\\mathbf t_1, \\mathbf t_1) = \\mathbf t_1 \\kappa_1 \\mathbf t_1 = \\kappa_1, II(\\mathbf t_1, \\mathbf t_2) = 0, II(\\mathbf t_2, \\mathbf t_2) \\mathbf t_2 \\kappa_2 \\mathbf t_2 =\\kappa_2\\] <p>Corollary WLOG assume \\(\\kappa_1 \\geq \\kappa_2\\), then </p> \\[\\kappa_1 = \\arg\\max_{\\gamma:\\mathbb R\\rightarrow \\Sigma}\\kappa_{n,\\gamma}, \\kappa_2 = \\arg\\min_{\\gamma:\\mathbb R\\rightarrow \\Sigma}\\kappa_{n, \\gamma}\\] <p>proof. By Eulers' Theorem, </p> \\[\\kappa_n = \\kappa_1 - (\\kappa_1 - \\kappa_2) \\sin^2\\theta\\] <p>Since \\(\\kappa_1\\geq \\kappa_2, \\sin^2\\theta \\in [0, 1]\\). The maximum is at \\(\\sin^2\\theta = 0\\implies \\kappa_n = \\kappa_1\\) and minimum is at \\(\\sin^2\\theta = 1\\implies \\kappa_n = \\kappa_2\\)</p>"},{"location":"mat363/gaussian_curvature.html#example-helicoid","title":"Example: Helicoid","text":"<p>\\(\\sigma(u,v) = (v\\cos u, v\\sin u, \\lambda u)\\). </p> \\[\\sigma_u = (-v\\sin u, v\\cos u, \\lambda), \\sigma_v = (\\cos u, \\sin u, 0), \\mathbf N = \\frac{(-\\lambda \\sin u, \\lambda \\cos u, -v)}{\\sqrt{\\lambda^2+v^2} }\\] \\[E = v^2 + \\lambda^2, F = 0, G = 1\\] \\[\\sigma_{uu} = (-v\\cos u, -v\\sin u, 0), \\sigma_{uv} = (-\\sin u, \\cos u, 0), \\sigma_{vv} = 0\\] \\[L = (1-1)(v\\lambda \\cos u\\sin u) + 0 = 0, M = \\frac{\\lambda}{\\sqrt{\\lambda^2+v^2} }, N = 0\\] \\[K = \\frac{1}{\\lambda^2+v^2} \\frac{-\\lambda^2}{\\lambda^2+v^2}= \\frac{-\\lambda^2}{(\\lambda^2+v^2)^2}\\] \\[(-\\kappa (v^2 + \\lambda^2))(-\\kappa) - (\\frac{\\lambda}{\\sqrt{\\lambda^2+v^2} })^2 = \\kappa^2 (v^2 + \\lambda^2) - \\lambda^2 (v^2 + \\lambda^2)^{-1} = 0\\] \\[\\implies \\kappa = \\pm\\lambda (v^2 + \\lambda^2)^{-1}\\]"},{"location":"mat363/gaussian_curvature.html#example-catenoid","title":"Example: Catenoid","text":"<p>\\(\\sigma(u, v) = (\\cosh u\\cos v, \\cosh u \\sin v, u)\\) </p> \\[\\sigma_u = (\\sinh u\\cos v, \\sinh u\\sin v, 1), \\sigma_v = (-\\cosh u\\sin v, \\cosh u\\cos v, 0)\\] \\[E = \\sinh^2 u + 1 = \\cosh^2 u, F = (-1 + 1)(\\cdots) + 0 = 0, G = \\cosh ^2 u\\] \\[\\mathbf N = \\cosh^{-1}  u (-cos v, -\\sin v, \\sinh u)\\] \\[\\begin{align*} \\sigma_{uu} &amp;= (\\cosh u\\cos v, \\cosh u\\sin v, 0)\\\\ \\sigma_{uv} &amp;= (-\\sinh u \\sin v, \\sinh u\\cos v, 0)\\\\ \\sigma_{vv} &amp;= (-\\cosh u \\cos v, -\\cosh u \\sin v, 0) \\end{align*}\\] \\[\\begin{align*} L &amp;= -\\cosh^{-1}u(\\cosh u) = -1\\\\ M &amp;= (-1+1)(\\cdots) + 0 = 0\\\\ N &amp;= -\\cosh^{-1}u(-\\cosh u) = 1 \\end{align*}\\] \\[K = \\frac{-1}{\\cosh^4 u}\\] \\[(-1 -\\kappa \\cosh^2 u)(1 - \\kappa \\cosh ^2 u) = 0\\implies \\kappa = \\pm \\cosh^{-2} u\\]"},{"location":"mat363/gaussian_curvature.html#umbilics","title":"Umbilics","text":"<p>When \\(\\kappa_1 = \\kappa_2=:\\kappa\\), we must have that \\(W = \\kappa I\\), where \\(\\mathbf t\\) can be any tangent vector on \\(T_p\\Sigma\\). We define such point \\(p\\in\\Sigma\\) being umbilics IFF \\(W_{p,\\Sigma} = \\kappa I\\) IFF principal curvatures \\(\\kappa_1 = \\kappa_2\\).</p> <p>Theorem If all points \\(p\\in \\Sigma\\) is an umbilic, then \\(\\Sigma\\) is an open subset of a plane or a sphere. </p> <p>proof. By the assumption, we have that \\(W\\mathbf t = \\kappa\\mathbf t\\) for all \\(p\\) and all tangent vector \\(\\mathbf t(p)\\). Parameterize \\(\\Sigma\\) with \\(\\sigma\\), then </p> \\[W(\\sigma_u)  = -\\mathbf N_u = \\kappa \\sigma_u, W(\\sigma_v) = -\\mathbf N_v = \\kappa\\sigma_v\\] <p>Then, note that \\(\\mathbf N_{uv} = \\mathbf N_{vu}\\) where </p> \\[\\mathbf N_{uv} = -\\frac{\\partial N_u}{\\partial v} = -(\\kappa_v \\sigma_u + \\kappa \\sigma_{uv}) = -\\frac{\\partial N_v}{\\partial u} = -(\\kappa_u \\sigma_v + \\kappa \\sigma_{uv})\\] <p>Then, we have that \\(\\kappa_v \\sigma_u = \\kappa_u \\sigma_v\\). However, \\(\\sigma_u, \\sigma_v\\) is linearly independent, the equation holds IFF \\(\\kappa_u = \\kappa_v = 0\\implies \\kappa = c\\). </p> <p>Suppose \\(\\kappa = 0\\), then \\(\\mathbf N_u = \\mathbf N_v = 0\\implies \\mathbf N\\) is constant, hence \\(\\sigma\\) is an open subset of plane. Suppose \\(\\kappa \\neq 0\\), then \\(\\mathbf N = \\kappa\\sigma + \\mathbf a\\), hence \\(\\sigma\\) is an open subset of sphere. </p>"},{"location":"mat363/gaussian_curvature.html#characterize-the-points-on-the-surface","title":"Characterize the Points on the Surface","text":"<p>Principal values provides local information at \\(p\\) and its neighborhood.  Let \\(p\\in \\Sigma\\)</p> <p>By applying suitable translations, Euler angle rotations, reflections around the standard planes, WLOG, assume that </p> <ul> <li>\\(p = \\mathbf 0\\)</li> <li>\\(T_p\\Sigma = \\Pi_{XY}\\) </li> <li>\\(\\mathbf t_1 =  \\mathbf e_1, \\mathbf t_2 = \\mathbf e_2\\)</li> <li>\\(\\mathbf N = \\mathbf e_3\\)</li> <li>\\(\\sigma_0 = \\sigma(0, 0) = p = \\mathbf 0\\)</li> </ul> <p>Then, the neighborhood \\(\\sigma(u,v)\\) of \\(p = \\sigma_0\\) can be approximated by </p> \\[\\sigma(u,v) = u\\sigma_u + v\\sigma_v + \\frac{1}{2}(u^2 \\sigma_{uu} + 2uv\\sigma_{uv} + v^2\\sigma_{vv}) + rem.\\] <p>The first-orders are on the \\(T_p\\Sigma = \\Pi_{XY}\\) plane, and the second order terms are perpendicular to \\(xy\\) plane, hence </p> \\[\\begin{align*} z &amp;= \\frac{1}{2}(u^2 \\sigma_{uu} + 2uv\\sigma_{uv} + v^2\\sigma_{vv})\\cdot \\mathbf N \\\\   &amp;= \\frac{1}{2}(Lu^2 + 2M uv + Nv^2)\\\\   &amp;= \\frac{1}{2}II(u\\sigma_u +v\\sigma_v, u\\sigma_u +v\\sigma_v) \\end{align*}\\] <p>Change the basis from \\(\\sigma_u, \\sigma_v\\) to \\(\\mathbf e_1,\\mathbf e_2\\), i.e. \\(u\\sigma_u + v\\sigma_v = x\\mathbf e_1 + y\\mathbf e_2\\). </p> \\[\\begin{align*} z &amp;= \\frac{1}{2}II(u\\sigma_u +v\\sigma_v, u\\sigma_u +v\\sigma_v) \\\\ &amp;= \\frac{1}{2}W( x\\mathbf e_1 + y\\mathbf e_2)\\cdot ( x\\mathbf e_1 + y\\mathbf e_2) \\\\ &amp;= \\frac{1}{2}(xW(\\mathbf t_1) + yW(\\mathbf t_2)) \\cdot  ( x\\mathbf e_1 + y\\mathbf e_2)\\\\ &amp;= \\frac{1}{2}(\\kappa_1 x,\\kappa_2 y, 0)\\cdot (x, y, 0)\\\\ &amp;= \\frac{\\kappa_1 x^2 + \\kappa_2x^2}{2} \\end{align*}\\] <p>Therefore, we have 4 cases, the point \\(p\\) is 1. elliptic if \\(\\kappa_1,\\kappa_2\\) are both non-zero and have the same sign  2. hyperbolic if \\(\\kappa_1,\\kappa_2\\) are both non-zero and have different sign 3. parabolic if exactly one of \\(\\kappa_1,\\kappa_2\\) is 0. 4. planar if both of \\(\\kappa_1,\\kappa_2\\) are 0.</p>"},{"location":"mat363/gaussian_curvature.html#line-of-curvature","title":"Line of Curvature","text":"<p>A curve \\(\\gamma: \\mathbb R\\rightarrow \\Sigma\\) is a line of curvature if the tangent vector of \\(\\gamma\\) is a principal vector of \\(\\Sigma\\) at all \\(\\gamma(t)\\).  </p> <p>Claim This definition is equivalent to \\(\\mathbf N' = -\\lambda \\gamma'\\) for some constant \\(\\lambda\\in\\mathbb R\\)</p> <p>proof. By definition of principal vector, \\(\\forall t. W(\\gamma(t)) = \\kappa \\mathbf t(t)\\) for some principal value \\(\\kappa\\). Note that \\(\\mathbf N'= -W\\) and \\(\\mathbf t = \\gamma' / \\|\\gamma'\\|\\), take \\(\\lambda = \\kappa / \\|\\gamma'\\|\\) we have the equality. </p> <p>Claim This definition if equivalent to that for \\(\\gamma(t) = \\sigma(u(t), v(t))\\)</p> \\[(EM-FL)u'^2 + (EN-GL)u'v' + (FN-GM)v'^2 = 0\\] <p>proof. Note that \\(\\gamma' = u'\\sigma_u + v'\\sigma_v\\), we have that \\(F_I^{-1}F_{II} = \\lambda I\\implies F_{II} = \\lambda F_I\\)</p>"},{"location":"mat363/gbt.html","title":"Gauss Bonnet Theorem","text":""},{"location":"mat363/gbt.html#gbt-for-simple-closed-curves","title":"GBT for simple closed curves","text":"<p>Theorem Let \\(\\gamma(s)\\) be a unit-speed simple closed curve on a surface patch \\(\\sigma\\) of length \\(L\\), and assume that \\(\\gamma\\) is positively orientaed. Then, </p> \\[\\int_0^L \\kappa_g ds = 2\\pi - \\int_{int(\\gamma)}K dA_\\sigma\\] <p>where \\(\\kappa_g\\) is the geodesic curvature, \\(K\\) is the Gaussian curvature. </p>"},{"location":"mat363/gbt.html#example","title":"Example","text":"<p>Theorem Suppose that \\(\\sigma\\) has \\(K\\leq 0\\) everywhere. Prove that there are no simple closed geodesics on \\(\\sigma\\). </p> <p>proof. Suppose exists some simple closed geodesic \\(\\gamma\\), so that \\(\\kappa_g = 0\\), thus </p> \\[\\int_{int(\\gamma)} KdA_\\sigma = 2\\pi\\] <p>If \\(K\\leq 0\\), then the integral will be negative, hence not possible. </p>"},{"location":"mat363/gbt.html#gb-for-curvilinear-polygons","title":"GB for Curvilinear polygons","text":"<p>A curvilinear polygon in \\(\\mathbb R^2\\) is a continuous map \\(\\pi:\\mathbb R\\rightarrow\\mathbb R^2\\) s.t. for some real number \\(T\\) and some points \\(0 = t_0 &lt; t_1 &lt; \\cdots &lt; t_n = T\\), \\(\\pi(t) = \\pi(t')\\) IFF \\(t'-t = kT\\), \\(\\pi\\) is piece-wise smooth for each open interval \\((t_i, t_{i+1})\\). The one-sided derivatives exists and are non-zero and not parallel. The points are called vertices, and the intervals are edges. </p> <p>Theorem Let \\(\\gamma\\) be positively oriented unit-speed curvilinear polygon with \\(n\\) edges on a surface \\(\\sigma\\), and \\(a_1,..,a_n\\) ne the interior angles at its vertices. Then, </p> \\[\\int_0^L \\kappa_g ds = \\sum_{i=1}^n a_i - (n-2)\\pi - \\int_{int(\\gamma)} K dA_\\sigma\\]"},{"location":"mat363/gbt.html#corollary","title":"Corollary","text":"<p>If \\(\\gamma\\) is a curvilinear polygon with \\(n\\) edges each of which is an arc of a geodesic, then the internal edges \\(a\\)'s of the polygon satisfy the equation, </p> \\[\\sum_{i=1}^n a_i = (n-2)\\pi - \\int_{int(\\gamma)} K dA_\\sigma\\]"},{"location":"mat363/gbt.html#example-surface-of-revolution","title":"Example: Surface of Revolution","text":"<p>For \\(u_1 &lt; u_2\\) be constant, let \\(\\gamma_1,\\gamma_2\\) be the two parallels at \\(u_1, u_2\\) on \\(\\sigma\\), Let the region \\(U\\) be the surface between the two parallel circles or length \\(L_1, L_2\\). </p> <p>For surface of revolution, the normal is </p> \\[\\mathbf N = (-g'\\cos v, -g'\\sin v, f')\\] <p>Note that both \\(\\gamma_1,\\gamma_2\\) are circles of radius \\(f(u_i)\\), centered at \\((0, 0, g(u_i))\\), consider \\(\\gamma_1\\), </p> \\[\\gamma_1'(v) = f(u_1)(-\\sin v, \\cos v, 0), \\gamma_1'' = f(u_1) (-\\cos v, -\\sin v, 0)\\] <p>Thus, we have the geodesic curvature</p> \\[\\kappa_g = \\frac{\\gamma''\\cdot(\\mathbf N\\times \\gamma')}{\\|\\gamma'\\|^{3/2} } = \\frac{f'(u_1)}{f(u_1)}\\] <p>Then, we have </p> \\[\\int_0^{L1} \\kappa_g ds = \\int_0^{2\\pi f(u_1)} \\frac{f'(u_1)}{f(u_1)} ds = 2\\pi f'(u_1), \\int_0^{L2} \\kappa_g ds = 2\\pi f'(u_2)\\] <p>Then, note that the Gaussian curvature for surface of revolution is \\(K = -f''/f\\), so that </p> \\[\\iint_U K dA_\\sigma = \\int_0^{2\\pi}\\int_{f(u_1)}^{f(u_2)} f''/f dudv = 2\\pi (f'(u_1) - f'(u_2))\\] <p>Thus, we have that </p> \\[\\int_0^{L1} \\kappa_g ds - \\int_0^{L2} \\kappa_g ds = \\iint_U K dA_\\sigma \\] <p>This follows GB for a curvilinear polygon with vertices \\(\\sigma(u_1, v_0), \\sigma(u_2, v_0)\\)</p>"},{"location":"mat363/gbt.html#gb-for-compact-surfaces","title":"GB for Compact Surfaces","text":"<p>A triangulation of a surface \\(\\Sigma\\) is a collection of curvilinear polygons, each of which is contained, together with its interior, in one of the \\(\\sigma_i(U_i)\\) being a region in one of the surface patch in the atlas. Such that,  - each point of \\(\\Sigma\\) is in at least on curvilinear polygon. - each pair of curvilinear polygon are either disjoint, or intersection is a common edge or a common vertex.  - each edge is an edge of exactly two polygons. </p>"},{"location":"mat363/gbt.html#euler-number","title":"Euler Number","text":"<p>For a triangulation of a compact surface with finitely many polygons, the Euler number is </p> \\[\\chi = V-E+F\\] <p>Theorem for any triangulation of \\(\\Sigma\\), </p> \\[\\int_\\Sigma K dA = 2\\pi \\chi\\] <p>Corollary Euler number if independent of choice of triangulation. </p>"},{"location":"mat363/gbt.html#genus","title":"Genus","text":"<p>A genus is obtained for gluing \\(g\\) tori together, where \\(g\\) is the number of holes. When \\(g=0\\), we have a sphere, \\(g=1\\) is a torus, and \\(g=2\\) we get a \"8\" shaped surfaces. </p> <p>Theorem \\(\\chi(T_g) = 2 -2 g\\) where \\(T_g\\) is genus \\(g\\).</p> <p>Corollary \\(\\int_{T_g} KdA = 4\\pi (1-g)\\)</p> <p>Theorem If a compact surface \\(\\Sigma\\) is diffeomorphic to the torus, then \\(\\int_S KdA = 0\\). </p> <p>proof. Take \\(g=1\\) and apply the theorem. Also, since \\(\\Sigma\\) is compact, \\(K &gt; 0\\) for some points \\(p\\in\\Sigma\\). </p> <p>Theorem If \\(\\Sigma\\) is compact with \\(K&gt;0\\) everywhere. Then, \\(S\\) is diffeomorphic to a sphere. </p> <p>proof. \\(\\int_\\Sigma KdA &gt; 0\\implies 1 - g &gt; 0\\), \\(g\\in\\mathbb N\\) hence the only choice of \\(g\\) is 0. </p> <p>Note that the converse is not necessarily true, since we can have \\(K=0\\) for some points. </p>"},{"location":"mat363/gcme.html","title":"Gauss' Remarkable Theorem","text":""},{"location":"mat363/gcme.html#gauss-and-codazzi-mainardi-equations","title":"Gauss and Codazzi-Mainardi Equations","text":""},{"location":"mat363/gcme.html#codazzi-mainardi-equations","title":"Codazzi-Mainardi Equations","text":"<p>Theorem </p> \\[L_v - M_u = L\\Gamma_{12}^1 + M (\\Gamma_{12}^2 - \\Gamma_{11}^1) - N\\Gamma_{11}^2\\] \\[M_v - N_u = L\\Gamma_{22}^1 + M (\\Gamma_{22}^2 - \\Gamma_{12}^1) - N\\Gamma_{12}^2\\] <p>proof. First, consider the first equation, </p> \\[\\sigma_{uuv} = \\sigma_{uvu}\\] <p>Using Gauss equations, </p> \\[\\Gamma_{11}^1\\sigma_u + \\Gamma_{11}^2\\sigma_v + LN)_v = (\\Gamma_{12}^1\\sigma_u + \\Gamma_{12}^2\\sigma_v + MN)_u\\] \\[\\begin{align*} &amp;(\\Gamma_{11}^1)_v \\sigma_u + \\Gamma_{11}^1\\sigma_{uv} + (\\Gamma_{11}^2)_v \\sigma_v + \\Gamma_{11}^2\\sigma_{vv} + L_v\\mathbf N + L\\mathbf N_v\\\\ &amp;= (\\Gamma_{12}^1)_u \\sigma_u + \\Gamma_{12}^1\\sigma_{uu} + (\\Gamma_{12}^2)_u \\sigma_v + \\Gamma_{12}^2\\sigma_{uv} + M_u \\mathbf N + M\\mathbf N_u \\end{align*}\\] <p>Then, take dot product \\(\\mathbf N\\) on both sides of the equation, note \\(\\mathbf N\\) is perpendicular to \\(\\sigma_u,\\sigma_v, \\mathbf N_u, \\mathbf N_v\\) and \\(\\mathbf N\\cdot \\mathbf N = 1\\), </p> \\[\\Gamma_{11}^1\\sigma_{uv}\\cdot \\mathbf N + \\Gamma_{11}^2\\sigma_{vv}\\cdot\\mathbf N + L_v = \\Gamma_{12}^1\\sigma_{uu}\\cdot\\mathbf N + \\Gamma_{12}^1\\sigma_{uv}\\cdot\\mathbf N + M_u\\] \\[\\Gamma_{11}^1 M + \\Gamma_{11}^2N + L_v = \\Gamma_{12}^1L + \\Gamma_{12}^1M + M_u\\] <p>reorder the equations, we have the first equation. </p> \\[L_v - M_u = L\\Gamma_{12}^1 + M (\\Gamma_{12}^2 - \\Gamma_{11}^1) - N\\Gamma_{11}^2\\] <p>Similarly, the second equation can be obtained by \\(\\sigma_{vvu} = \\sigma_{uvv}\\)</p>"},{"location":"mat363/gcme.html#gauss-equations","title":"Gauss Equations","text":"<p>Theorem For \\(K\\) be the Gaussian curvature,</p> \\[\\begin{align*} EK &amp;= (\\Gamma_{11}^2)_v - (\\Gamma_{12}^2)_u + \\Gamma_{11}^1 \\Gamma_{12}^2 + \\Gamma_{11}^2 \\Gamma_{22}^2 - \\Gamma_{12}^1\\Gamma_{11}^2 - (\\Gamma_{12}^2)^2\\\\ FK &amp;= (\\Gamma_{12}^1)_u - (\\Gamma_{11}^1)_v + \\Gamma_{12}^2 \\Gamma_{12}^1 + \\Gamma_{11}^2 \\Gamma_{22}^1\\\\ FK &amp;= (\\Gamma_{12}^2)_v - (\\Gamma_{22}^2)_u + \\Gamma_{12}^1 \\Gamma_{12}^2 + \\Gamma_{22}^1 \\Gamma_{11}^2\\\\ GK &amp;= (\\Gamma_{22}^1)_u - (\\Gamma_{12}^1)_v + \\Gamma_{22}^1 \\Gamma_{11}^1 + \\Gamma_{22}^2 \\Gamma_{12}^1 - (\\Gamma_{12}^2)^2 - \\Gamma_{12}^2\\Gamma_{22}^1 \\end{align*}\\] <p>proof. </p> <p>Reconsider the equation</p> \\[\\begin{align*} &amp;(\\Gamma_{11}^1)_v \\sigma_u + \\Gamma_{11}^1\\sigma_{uv} + (\\Gamma_{11}^2)_v \\sigma_v + \\Gamma_{11}^2\\sigma_{vv} + L_v\\mathbf N + L\\mathbf N_v\\\\ &amp;= (\\Gamma_{12}^1)_u \\sigma_u + \\Gamma_{12}^1\\sigma_{uu} + (\\Gamma_{12}^2)_u \\sigma_v + \\Gamma_{12}^2\\sigma_{uv} + M_u \\mathbf N + M\\mathbf N_u \\end{align*}\\] <p>Now, take dot product \\(\\sigma_u\\), note that \\(\\sigma_u\\cdot\\sigma_u = 1, \\sigma_v\\cdot \\sigma_u = \\mathbf N\\cdot\\sigma_u = 0\\)</p> \\[\\begin{align*} &amp;(\\Gamma_{11}^1)_v + \\Gamma_{11}^1\\sigma_{uv}\\sigma_u + \\Gamma_{11}^2\\sigma_{vv}\\sigma_u + L\\mathbf N_v\\sigma_u\\\\ &amp;= (\\Gamma_{12}^1)_u + \\Gamma_{12}^1\\sigma_{uu}\\sigma_u  + \\Gamma_{12}^2\\sigma_{uv}\\sigma_u + M\\mathbf N_u\\sigma_u \\end{align*}\\] <p>Then, note that Gauss equations represents \\(\\sigma_{uu},\\sigma_{uv}, \\sigma_{vv}\\) in in terms of \\(\\sigma_u, \\sigma_v, \\mathbf N\\), dot product \\(\\sigma_u\\) so that</p> \\[\\sigma_{uu}\\sigma_u = \\Gamma_{11}^1, \\sigma_{uv}\\sigma_u = \\Gamma_{12}^1,  \\sigma_{vv}\\sigma_u = \\Gamma_{22}^1\\] <p>Replacing them back to the equation, </p> \\[(\\Gamma_{11}^1)_v + \\Gamma_{11}^1\\Gamma_{12}^1 + \\Gamma_{11}^2\\Gamma_{22}^1 + L\\mathbf N_v\\sigma_u = (\\Gamma_{12}^1)_u + \\Gamma_{12}^1\\Gamma_{11}^1  + \\Gamma_{12}^2\\Gamma_{12}^1 + M\\mathbf N_u\\sigma_u\\] <p>Finally, consider \\(\\mathbf N_u\\sigma_u, \\mathbf N_v\\sigma_u\\), which are the coefs of Weingarden, i.e. </p> \\[\\mathbf N_u\\sigma_u = \\frac{LG-MF}{EG-F^2}, \\mathbf N_v\\sigma_u = \\frac{MG-NF}{EG-F^2}\\] <p>Therefore, </p> \\[L\\mathbf N_v\\sigma_u - M\\mathbf N_u\\sigma_u = -FK\\] <p>and re-order the equation above, we obtain the second equation in the claim.  </p> <p>Similarly, we can obtain 4 equations from \\(\\sigma_{uuv} = \\sigma_{uvu}, \\sigma_{uvu} = \\sigma_{vvu}\\) and dot product each of \\(\\sigma_u, \\sigma_v\\). </p>"},{"location":"mat363/gcme.html#theorem","title":"Theorem","text":"<p>If two surface patches has the same first and second fundamental forms, then exists a direct isometry between them. </p> <p>Moveover, for \\(V\\subset\\mathbb R^2\\) open, given \\(EFGLMN\\) being 6 smooth functions on \\(V\\) and \\(E&gt;0,G&gt;0,EG-F^2&gt;0\\) and the 6 equations (2 CM, 4 Gauss) hold. Then for any point \\(p\\) on \\(V\\), there is an open set of \\(U\\subset V, p\\in U\\) and a surface patch \\(\\sigma:U\\rightarrow\\mathbb R^3\\) s.t. \\(\\sigma\\) has the first and second fundamental forms defined by \\(EFGLMN\\). </p> <p>In other words, given appropriate functions for first and second fundamental forms, a surface will always exist locally, and being unique up to a direct isometry. </p>"},{"location":"mat363/gcme.html#example-cylinder","title":"Example: Cylinder","text":"<p>Let the \\(E=G=1, F=0, L=-1, M=N=0\\). We verify the conditions that \\(E&gt;0,G&gt;0, EG-F^2 &gt;0\\), all coefs are zero hence \\(\\Gamma\\)'s are all 0 and CM, Gauss are all satisifed. Therefore, a surface patch must exist. </p> <p>Using Gauss equations and plug in the numbers</p> \\[\\sigma_{uu} = \\Gamma_{11}^1\\sigma_u + \\Gamma_{11}^2\\sigma_v + L\\mathbf N = -\\mathbf N, \\sigma_{uv} = \\cdots = 0, \\sigma_{vv} =\\cdots =  0\\] <p>Since \\(\\sigma_v\\)'s derivatives \\(\\sigma_{vu} = \\sigma_{vv} = 0\\), we must have that \\(\\sigma_v = \\mathbf a\\) being a constant. Since \\(\\sigma_{uv} = 0\\), we have that </p> \\[\\sigma(u,v) = \\mathbf b(u) + \\mathbf a v\\] <p>where \\(\\mathbf N = -\\mathbf b''\\) Then, consider \\(\\mathbf N_u,\\mathbf N_v\\) where \\(-\\mathbf N_u = a\\sigma_u + b\\sigma_v, -\\mathbf N_v = c\\sigma_u + d\\sigma_v\\) where </p> \\[\\begin{bmatrix}a&amp;c\\\\b&amp;d\\end{bmatrix} = F_I^{-1}F_{II} = \\begin{bmatrix}1&amp;0\\\\0&amp;1\\end{bmatrix}^{-1} \\begin{bmatrix}-1&amp;0\\\\0&amp;0\\end{bmatrix} = \\begin{bmatrix}-1&amp;0\\\\0&amp;0\\end{bmatrix}\\] <p>so that \\(\\mathbf N_u = \\sigma_u,\\mathbf N_v = 0\\), implying that </p> \\[\\mathbf N_u = \\frac{d}{du}(-\\mathbf b'') = -\\mathbf b''' =\\mathbf b' = \\sigma_u\\] <p>Since \\(\\mathbf b''' + \\mathbf b' = 0\\), we must also have that \\(\\mathbf b'' + \\mathbf b = -\\mathbf N + \\mathbf b\\) being constant vector.  Take \\(\\mathbf b(u) = \\mathbf c \\sin u + \\mathbf d \\cos u\\), and to make \\(\\mathbf b\\) and \\(\\mathbf b''\\) constant, take \\(\\mathbf c = e_1,\\mathbf d = e_2\\). </p> <p>Finally, \\(\\sigma_u\\times \\sigma_v = \\lambda \\mathbf N\\implies \\mathbf b' \\times \\mathbf a = \\lambda \\mathbf b\\implies \\mathbf a = (0, 0, \\lambda)\\) for any \\(\\lambda \\neq 0\\), say \\(\\lambda =1\\). Therefore, we have that \\(\\sigma(u,v) = (\\cos u, \\sin u, v)\\) is a parameterization of the unit cylinder. </p>"},{"location":"mat363/gcme.html#example-sphere","title":"Example: Sphere","text":"<p>Let \\(E = \\cos^2 v, F = 0, G = 1, L =-\\cos^2 v, M = 0, N = -1\\). </p> <p>First, using the Weingarten map we have that </p> \\[\\begin{bmatrix}a&amp;c\\\\b&amp;d\\end{bmatrix} = F_I^{-1}F_{II} = \\begin{bmatrix}\\cos^2 v&amp;0\\\\0&amp;1\\end{bmatrix}^{-1} \\begin{bmatrix}-\\cos^2v&amp;0\\\\0&amp;-1\\end{bmatrix} = \\begin{bmatrix}-1&amp;0\\\\0&amp;-1\\end{bmatrix}\\] <p>so that \\(\\sigma_u = \\mathbf N_u, \\sigma_v = \\mathbf N_v\\) Therefore, \\(\\mathbf N = \\sigma + \\mathbf a\\) where \\(\\mathbf a\\) is constant and \\(\\|\\sigma+\\mathbf a\\| = 1\\). Since the surface patch is equivalent to the Weingarten map, hence the Gauss map, it is an isometry from the Gauss map, which is the unit sphere. </p> <p>The parameterization is given by </p> \\[\\sigma(u, v) = (\\cos v\\cos u, \\cos v\\sin u, \\sin v)\\]"},{"location":"mat363/gcme.html#example-not-a-surface-patch","title":"Example: Not a surface patch","text":"<p>\\(E = 1, F = 0, G = \\cos^2 u, L = \\cos^2 u, M = 0, N = 1\\) In first fundamental form, \\(G\\) is the only non-constant and \\(F\\) is 0. Therefore, the non-zero Christoffel symbol are</p> \\[\\Gamma_{12}^2 = \\frac{-2\\cos u\\sin u}{2\\cos^2 u} = -2\\tan u, \\Gamma_{22}^1 = \\frac{GG_u}{2G} = \\cos u\\sin u\\] <p>Therefore, the RHS of the second equation is </p> \\[\\cos^2 u \\cos u \\sin u + 2\\tan u\\neq 0\\] <p>But the LHS is \\(0\\) since \\(M,N\\) are constants. </p> <p>Therefore, the CM equation is not satisifed and there is no surface patch. </p>"},{"location":"mat363/gcme.html#applying-gauss-equations","title":"Applying Gauss Equations","text":"<p>Theorem Suppose that a surface patch \\(\\sigma(u,v)\\) has first \\(E=G=v^{-2}, v &gt; 0\\), prove that \\(L,N\\) does not depend on \\(u\\). </p> <p>proof. First, the non-zero FFF derivatives are \\(E_v = G_v = -2v^{-3}\\) so that the non-zero Christoffel symbols are </p> \\[\\Gamma_{11}^2 = \\frac{-2v^{-5} }{2v^{-4} } = 1/v, \\Gamma_{12}^1 = -1/v, \\Gamma_{22}^2 = -1/v\\] <p>The CM equations then gives </p> \\[L_v = -Lv^{-1} - Nv^{-1} = -v^{-1}(L+N), N_u = 0\\] <p>\\(N_u=0\\) implies that \\(N\\) does not depend on \\(u\\). </p> <p>Then, consider Gauss equations, plug in the equations for \\(EK\\) or \\(GK\\), we have that</p> \\[v^{-2}K= -v^{-2} - 0 + 0 + 0 - 0- 0 \\implies K = -1\\] <p>where \\(K\\) is the Gaussian curvature </p> \\[K = \\frac{LN-M^2}{EG-F^2} = v^4LN = -1\\implies LN = -v^{-4}\\] <p>does not depend on \\(u\\) as well. </p> <p>Going back to the first CM equation where \\(L_v = -v^{-1}(L+N)\\), we further get that </p> \\[L_v = -v^{-1}(L + L^{-1}v^{-4})\\implies Lv^5 L_v = 1 - L^2 v^4\\] <p>If \\(F=0, M=0\\), then the Christoffel symbols are</p> \\[\\Gamma_{11}^1 = \\frac{E_u}{2E}, \\Gamma_{11}^2 = \\frac{-E_v}{2G}\\] \\[\\Gamma_{12}^1 = \\frac{E_v}{2E}, \\Gamma_{12}^2 = \\frac{G_u}{2G}\\] \\[\\Gamma_{22}^1 = \\frac{-G_u}{2E}, \\Gamma_{22}^2 = \\frac{G_v}{2G}\\] <p>Therefore, the CM equations become </p> \\[L_v = \\frac{1}{2}\\big(\\frac{LE_v}{E} + \\frac{NE_v}{G}\\big) = \\frac{1}{2}E_v(\\frac{L}{E}+\\frac{N}{G})\\] \\[N_u = -\\frac{1}{2}\\big(\\frac{-LG_u}{E} + \\frac{-NG_u}{G}) =\\frac{1}{2}G_u(\\frac{L}{E}+\\frac{N}{G})\\] <p>and then note that the principal curvatures are simply the roots of \\((L-\\kappa E)(N-\\kappa G) = 0\\), since \\(M=F=0\\), thus we have \\(\\kappa_1 = \\frac{L}{E}, \\kappa_2 = \\frac{N}{G}\\) so that </p> \\[\\begin{align*} (\\kappa_1)_v &amp;= \\frac{L_vE - LE_v}{E^2}\\\\  &amp;= \\frac{\\frac{1}{2}E_v(\\kappa_1+\\kappa_2)E - LE_v}{E^2}\\\\  &amp;=\\frac{E}{2}\\frac{E_v(\\kappa_1+\\kappa_2) - \\frac{E}{2}\\frac{2L}{E}E_v}{2E^2}\\\\ &amp;= \\frac{E_v}{2E}(\\kappa_1 + \\kappa_2 - 2\\kappa_1)\\\\ &amp;= \\frac{E_v}{2E}(\\kappa_2 - \\kappa_1)\\\\ (\\kappa_2)_u &amp;= \\frac{N_uG - NG_u}{G^2} \\\\ &amp;= \\frac{G_u}{2G}(\\kappa_1-\\kappa_2) \\end{align*}\\]"},{"location":"mat363/gcme.html#gauss-remarkable-theorem_1","title":"Gauss' Remarkable Theorem","text":"<p>Theorem The Gaussian curvature of a surface is perserved by local isometries. </p> <p>Theorem The Gaussian curvature is given by </p> \\[K = \\frac{1}{(EG-F^2)^2}\\big(\\det\\begin{bmatrix} \\frac{E_{vv} }{2} + F_{uv} - \\frac{G_{uu} }{2} &amp;\\frac{E_u}{2}&amp;F_u-\\frac{E_v}{2}\\\\ F_v-\\frac{G_u}{2} &amp;E&amp;F\\\\ \\frac{G_v}{2}&amp;F&amp;G \\end{bmatrix} - \\det\\begin{bmatrix} 0&amp;\\frac{E_v}{2}&amp;\\frac{G_u}{2}\\\\ \\frac{E_v}{2}&amp;E&amp;F\\\\ \\frac{G_u}{2}&amp;F&amp;G \\end{bmatrix}\\big)\\] <p>Corollary If \\(F=0\\), </p> \\[K = -\\frac{1}{2\\sqrt{EG} } \\big(\\partial_u (\\frac{G_u}{\\sqrt{EG} }) +\\partial_v (\\frac{E_v}{\\sqrt{EG} })\\big)\\] <p>proof. Using the first formula of Gauss equations, and substitute each Chrsitoffel symbol</p> \\[\\begin{align*} EK &amp;= (\\Gamma_{11}^2)_v - (\\Gamma_{12}^2)_u + \\Gamma_{11}^1 \\Gamma_{12}^2 + \\Gamma_{11}^2 \\Gamma_{22}^2 - \\Gamma_{12}^1\\Gamma_{11}^2 - (\\Gamma_{12}^2)^2\\\\ &amp;= (-\\frac{E_v}{2G})_v - (\\frac{G_u}{2G})_u + \\frac{E_u}{2E} \\frac{G_u}{2G} + \\frac{-E_v}{2G}\\frac{G_v}{2G} - \\frac{E_v}{2E}(-\\frac{E_v}{2G}) - (\\frac{G_u}{2G})^2\\\\ &amp;= \\frac{E_vG_v - E_{vv} G}{2G^2} - \\frac{G_{uu} G - G_u^2}{2G^2}+ \\frac{E_uG_u}{4EG} - \\frac{E_vG_v}{2G^2} + \\frac{E_v^2}{4EG} - \\frac{G_u^2}{4G^2}\\\\ \\partial_u (\\frac{G_u}{\\sqrt{EG} }) &amp;= \\frac{G_{uu} }{\\sqrt{EG} } - \\frac{G_u(E_uG + E G_u)}{2(EG)^{3/2} }\\\\ \\partial_v (\\frac{E_v}{\\sqrt{EG} }) &amp;= \\frac{E_{vv} }{\\sqrt{EG} } - \\frac{E_v(E_vG + E G_v)}{2(EG)^{3/2} } \\end{align*}\\] <p>Take \\(2\\sqrt{EG}\\) out of the RHS, and we can obtain the equation. </p> <p>Corollary If \\(E=1,F=0\\), then \\(K = -\\frac{1}{\\sqrt G}\\frac{\\partial^2\\sqrt G}{\\partial u^2}\\)</p>"},{"location":"mat363/gcme.html#example-sphere_1","title":"Example: Sphere","text":"<p>Claim any plane map of any region of a sphere must distort distances. </p> <p>proof. The claim is equivalent to that there is no isometry between open subset of a plane to a sphere.  Known that the Gaussian curvature of plane is \\(0\\) everywhere, while \\(K\\) for sphere is positive constant. Therefore, such isometry cannot exist.</p>"},{"location":"mat363/gcme.html#example","title":"Example","text":"<p>Claim If \\(E=e^\\lambda, F=0,G=e^\\lambda\\), where \\(\\lambda\\) is a smooth function of \\(u,v\\). Then</p> \\[\\Delta \\lambda + 2Ke^\\lambda = 0\\] <p>\\(\\Delta\\) is the Laplacian operator</p> <p>proof. Note that \\(F=0\\), so that we have that </p> \\[\\begin{align*} K &amp;= -\\frac{1}{2e^\\lambda } \\big(\\partial_u (\\frac{G_u}{e^\\lambda }) +\\partial_v (\\frac{E_v}{e^\\lambda })\\big)\\\\ &amp;= -\\frac{1}{2e^\\lambda } \\big(\\partial_u (\\frac{e^\\lambda\\lambda_u}{e^\\lambda }) +\\partial_v (\\frac{e^\\lambda\\lambda_v}{e^\\lambda })\\big)\\\\ &amp;= -\\frac{1}{2e^\\lambda }(\\lambda_{uu} + \\lambda_{vv})\\\\ K &amp;= -\\frac{1}{2e^\\lambda}\\Delta \\lambda \\end{align*}\\]"},{"location":"mat363/geodesics.html","title":"Geodesics","text":""},{"location":"mat363/geodesics.html#geodesics-and-basic-properties","title":"Geodesics and Basic Properties","text":"<p>A curve \\(\\gamma: (a, b)\\rightarrow\\Sigma\\) is a geodesic if \\(\\gamma'' = 0\\) or \\(\\gamma''\\) perpendicular to the tangent plane for all point (i.e., parallel to its unit normal). Equivalently, \\(\\gamma\\) is a geodesic IFF \\(\\gamma'\\) is parallel along \\(\\gamma\\). </p> <p>Theorem any geodesic has constant speed. </p> <p>proof. </p> \\[\\frac{d}{dt}(\\gamma'\\cdot \\gamma') = 2\\gamma''\\cdot \\gamma' = 0\\] <p>since \\(\\gamma'' = 0\\) or \\(\\gamma''\\) perpendicular to the tangent plane, hence \\(\\gamma'\\) as \\(\\gamma'\\) is the tangent vector on the tangent plane. </p> <p>Theorem A unit-speed curve on a surface is a geodesic IFF \\(\\kappa_g = 0\\). </p> <p>proof. \\(\\Rightarrow\\) Assume \\(\\gamma\\) is a geodeisc then \\(\\gamma'' \\parallel \\mathbf N\\), then</p> \\[\\kappa_g = \\gamma''\\cdot (\\mathbf N\\cdot \\gamma') = \\lambda \\mathbf N \\cdot (\\mathbf N \\cdot \\gamma') = 0\\] <p>\\(\\Leftarrow\\), if \\(\\kappa_g = \\gamma''\\cdot (\\mathbf N\\cdot \\gamma') = 0\\), then \\(\\gamma''\\) is perpendicular to \\(\\mathbf N\\times \\gamma'\\), in addition \\(\\gamma', \\mathbf N, \\mathbf N\\times\\gamma'\\) are orthogonal, and \\(\\gamma''\\) is perpendicular to \\(\\gamma'\\), hence we have have that \\(\\gamma''\\parallel \\mathbf N\\)</p> <p>Theorem Any straight line on a surface is a geodesic. </p> <p>proof. Any straight line can be parameterized as \\(\\gamma(t) = \\mathbf at+ \\mathbf b\\) so that \\(\\gamma'' = 0\\). </p> <p>Theorem Any normal section of a surface is a geodesic. </p> <p>proof. The normal section of \\(\\Sigma\\) is the intersection of \\(\\Sigma\\) and plane \\(\\Pi\\), where \\(\\Pi\\) is perpendicular to \\(\\Sigma\\) at each point of intersection. In this case, we can easily show that \\(\\kappa_g = 0\\) </p>"},{"location":"mat363/geodesics.html#example-geodesics-on-hyperboloid","title":"Example: Geodesics on hyperboloid","text":"<p>The hyperboloid is defined as </p> \\[\\Sigma = \\{(x,y,z): x^2 + y^2 - z^2 =1\\}\\] <p>find the geodiscs passing through \\((1, 0, 0)\\)</p> <p>Known that any straight line is a geodesic, let \\(\\gamma(t) = (a,b,c)t + (1, 0, 0)\\) so that \\(\\gamma(0) = (1, 0, 0)\\), and we want \\(\\gamma((a,b))\\in\\Sigma\\), i.e. </p> \\[(at+1)^2 + (bt)^2 = 1 + (ct)^2\\implies a^2 t + 2a + b^2 t = c^2 t\\] <p>holds for any \\(t\\), there are at least two straight lines s.t.  \\(a = 0, b \\neq 0, c = \\pm b\\)</p> <p>Then, consider the normal section of the surface. The two normal planes passing through \\((1, 0, 0)\\) will be the XY-plane and XZ-plane, where the normal section is \\(x^2 + y^2 = 1, x^2 - z^2 = 1\\) respectively. </p>"},{"location":"mat363/geodesics.html#geodesic-equations","title":"Geodesic Equations","text":"<p>Theorem A curve \\(\\gamma: (a,b)\\rightarrow\\Sigma\\) is a geodesic IFF \\(\\gamma = \\sigma(u,v)\\) with the equations</p> \\[\\frac{d}{dt}(Eu'+Fv') = \\frac{1}{2}(E_uu'^2 + 2F_u u'v' + G_uv'^2)\\] \\[\\frac{d}{dt}(Fu'+Gv') = \\frac{1}{2}(E_vu'^2 + 2F_vu'v' + G_v v'^2)\\] <p>proof. Since \\(\\gamma' = u'\\sigma_u + v' \\sigma_v\\) and \\(\\gamma'' \\perp \\sigma_u, \\gamma''\\perp\\sigma_v\\) by the definition of geodesic, we have that </p> \\[\\big[\\frac{d}{dt} (u'\\sigma_u + v'\\sigma_v)\\big] \\cdot \\sigma_u = 0, \\big[\\frac{d}{dt} (u'\\sigma_u + v'\\sigma_v)\\big] \\cdot \\sigma_v = 0\\] <p>By product rule, since \\(d(f\\cdot g) = df \\cdot g + f\\cdot dg\\),</p> \\[\\begin{align*} \\big[\\frac{d}{dt} (u'\\sigma_u + v'\\sigma_v)\\big] \\cdot \\sigma_u &amp;= \\frac{d}{dt}((u'\\sigma_u + v'\\sigma_v)\\cdot \\sigma_u) - (u'\\sigma_u + v'\\sigma_v)\\cdot \\frac{d\\sigma_u}{dt}\\\\ &amp;= \\frac{d}{dt}(Eu'+Fv') - (u'\\sigma_u + v'\\sigma_v)\\cdot (u'\\sigma_{uu} + v'\\sigma_{uv})\\\\ &amp;= \\frac{d}{dt}(Eu'+Fv') - (u'^2\\frac{1}{2}E_u + u'v' F_u+v'\\frac{1}{2}G_u) \\end{align*}\\] <p>Similarly, the equations can be derived for \\(\\sigma_v\\). </p> <p>Theorem A curve \\(\\gamma\\) is a geodesic IFF the equations hold</p> \\[u'' + \\Gamma_{11}^1u'^2 + 2\\Gamma_{12}^1 u'v' + \\Gamma_{22}^1 v'^2 = 0\\] \\[v'' + \\Gamma_{11}^2 u'^2 + 2\\Gamma_{12}^2 u'v' + \\Gamma_{22}^2 v'^2 = 0\\] <p>proof. Since \\(\\gamma\\) is a geodesic IFF \\(\\gamma'\\) is parallel along \\(\\gamma\\), and \\(\\gamma' = u'\\sigma_u + v'\\sigma_v\\), take \\(\\mathbf a = u', \\mathbf b = v'\\) and we have the equations. </p> <p>Theorem For a surface \\(\\Sigma\\), let \\(\\mathbf t\\) be a unit tangent vector of \\(\\Sigma\\) at point \\(p\\in\\Sigma\\). Then, \\(\\exists ! \\gamma:\\mathbb R\\rightarrow \\Sigma\\) be a unit-speed geodesic passing through \\(p\\) and has tangent vector \\(\\mathbf t\\). </p> <p>Corollay Straight lines are the only geodesics on a plane, great circles are the only geodesics on a sphere. proof. For any point and any direction, exists a straight line / great circle, and but uniqueness, they are the only geodesics. </p> <p>Corollay Any local isometry between two surfaces takes the geodesics of one surface to the geodesics of the other. </p> <p>proof. Since \\(\\gamma\\) is a geodesic IFF the equations holds, while the equations only involves first fundamental form, and isometry has the same first fundamental form. </p>"},{"location":"mat363/geodesics.html#example-cylinder","title":"Example: Cylinder","text":"<p>Claim Let \\(p,q\\) be two distinct points on a unit cylinder \\(\\Sigma\\), there are either two or infinitely many geodesics with end points \\(p, q\\). </p> <p>proof. If $p=(x_0, y_0, z_0),q=(x_1, y_1, z_1) $ are on the same parallel \\((z_0 = z_1)\\), then the geodesics are the two arcs of a great circle. If \\(p, q\\) are of the different parallels, WLOG \\(z_1 &gt; z_0\\), then we can have plane of length \\(2\\pi k, k &gt; 0\\) and height \\(z_1 - z_0\\), the plane is locally isometric to the cylinder, and there are infinitely many planes. </p> <p>Alternatively, we can use the geodesics equation. </p> <p>\\(\\Sigma = (\\cos v, \\sin v, u)\\) with first fundamental form \\(E=G=1,F=0\\), the geodesic equations are hence \\(\\frac{d}{dt}u' =u'' = 0, \\frac{d}{dt}v' = v'' = 0\\). Hence \\(u = a+bt, v = c+dt\\) for any constants \\(a,b,c,d\\) are geodesics on the cylinder. </p>"},{"location":"mat363/geodesics.html#example-circular-cones","title":"Example: Circular Cones","text":"<p>For circular cone </p> \\[\\sigma(u,v) = (u\\cos v, u\\sin v, u)\\] <p>we have the local isometry from xy-plane to \\(\\Sigma\\) by </p> \\[(u\\sqrt 2 \\cos \\frac{v}{\\sqrt 2}, u\\sqrt 2\\sin\\frac{v}{\\sqrt 2}, 0)\\] <p>Fix \\(a, b\\) be two constant, straight line on the plane can be written as the level curve \\(ax + by = 1, x = u\\sqrt 2 \\cos \\frac{v}{\\sqrt 2}, y = u\\sqrt 2 \\sin \\frac{v}{\\sqrt 2}\\),  inserting \\(u = (\\sqrt 2 a \\cos \\frac{v}{\\sqrt 2} + \\sqrt 2 b \\sin\\frac{v}{\\sqrt 2})^{-1}\\) so that we have the curve \\(\\gamma(v)\\) on the plane that is a straight line, taking isometry and the curve on the cone will be </p> \\[\\gamma(v) = (\\sqrt 2 a \\cos \\frac{v}{\\sqrt 2} + \\sqrt 2 b \\sin\\frac{v}{\\sqrt 2})^{-1} (\\cos v, \\sin v, 1)\\] <p>In addition, we have the \\(\\gamma_y(u) = (u, 0, u), \\gamma_x (u) = (0, u, u)\\) are straight lines on the cone. </p>"},{"location":"mat363/geodesics.html#example-helicoid","title":"Example: Helicoid","text":"<p>For a helicoid \\(\\sigma(u,v) = (u\\cos v, u\\sin v, v), \\gamma\\) be a curve on the surface</p> <p>Theorem If \\(\\gamma\\) is a geodesic, then \\(v' = \\frac{a}{1+u^2}\\). </p> <p>proof. The first fundamental form of \\(\\sigma\\) is \\(E = 1, F = 0, G = 1+u^2\\), the geodesic equations gives </p> \\[u'' = uv'^2, \\frac{d}{dt}(1+u^2)\\cdot v' = 0\\] <p>Thus, we have that \\((1+u^2)\\cdot v'  = a\\implies v' = \\frac{a}{1+u^2}\\)</p> <p>In addition, by unit speed, we have that </p> \\[u'^2 + (1+u^2)v'^2  = 1\\implies u'^2 = 1 - \\frac{a^2}{1+u^2}\\] <p>When \\(a = 0\\), we have \\(v = c\\), and we have a ruling.  When \\(a = 1\\), we have \\(\\frac{dv}{du} = \\pm\\sqrt{v'^2/ u'^2} = \\pm\\sqrt{\\frac{a / (1+u^2)}{1- a^2 / (1+u^2)} } =\\pm\\frac{a}{\\sqrt{(1-a^2+u^2)(1+u^2)} }\\), \\(v = \\int \\frac{dv}{du} = v_0 \\pm \\sinh^{-1}(u^{-1})\\) </p>"},{"location":"mat363/geodesics.html#geodesics-on-surface-of-revolution","title":"Geodesics on Surface of Revolution","text":"<p>For surface of revolution </p> \\[\\sigma(u,v) = (f(u)\\cos v, f(u)\\sin v, g(u))\\] <p>where \\(f&gt;0, f'+g' = 1\\). First fundamental form is \\(E=1,F=0, g=f^2\\) using geodesics equations, we have </p> \\[u'' = ff'v'^2, \\frac{d}{dt}(f^2 v') = 0\\] <p>and by unit-speed, we have that </p> \\[u'^2 + f^2 v'^2 = 1\\] <p>Theorem Every meridian of surfaces of revolution is a geodesic. proof. meridian implies \\(v = v_0\\), so that \\(v' = 0\\), hence the unit-speed condition gives \\(u' = \\pm 1\\) is constant, then \\(u'' = 0 = ff'0\\) satisfies the first equation. The second equation is obviously satisfied. </p> <p>Theorem A parallel \\(u=u_0\\) is a geodesic IFF \\(f'(u_0) = 0\\).  proof. Let \\(f(u_0) = c\\), since \\(u=u_0, u' = 0, v' = \\pm c^{-1}\\) is a non-zero constant. Therefore, the first equation gives \\(0 = \\pm cf'\\), implying \\(f' = 0\\), the second equation definitely holds as both \\(f\\) and \\(v'\\) are constant. </p>"},{"location":"mat363/geodesics.html#clairauts-theorem","title":"Clairaut's Theorem","text":"<p>Let \\(\\Sigma\\) be a surface of rotation, Let \\(\\gamma: (a,b)\\rightarrow \\Sigma\\) be unit-speed, \\(\\rho: \\Sigma\\rightarrow\\mathbb R\\) be the distance of some point \\(p\\) from the axis of rotation. Let \\(\\psi\\) be the angle between \\(\\gamma'\\) and the meridians of \\(\\Sigma\\).   </p> <p>Claim If \\(\\gamma\\) is a geodesic, then \\(\\rho\\sin\\psi\\) is constant along \\(\\gamma\\).  </p> <p>proof. By our parameterization \\(\\sigma\\), we have that \\(\\rho = f(u)\\). Note that \\(\\sigma_u = (f'\\cos v, f'\\sin v, g')\\) is unit vector tangent to the meridians and \\(\\rho^{-1}\\sigma_v = f^{-1}(-f\\sin v, f\\cos v, 0)\\) is unit vector tangent to the parallels. Also, we have \\(\\sigma_u\\cdot \\sigma_v = 0\\) so that \\(\\sigma_u, \\rho^{-1}\\sigma_v\\) forms a orthonormal basis.  For \\(\\gamma(t) = \\sigma(u(t), v(t))\\) being unit-speed, we have that </p> \\[\\gamma' = \\cos\\psi \\sigma_u + \\rho^{-1}\\sin\\psi\\sigma_v\\] <p>Hence, </p> \\[\\begin{align*} \\sigma_u\\times \\gamma' &amp;= \\cos \\psi \\sigma_u\\times\\sigma_u + \\rho^{-1}\\sin\\psi (\\sigma_u\\times \\sigma_v) \\\\ \\sigma_u\\times (u'\\sigma_u + v'\\sigma_v)&amp;= \\rho^{-1}\\sin\\psi(\\sigma_u\\times \\sigma_v)\\\\ v'(\\sigma_u\\times \\sigma_v) &amp;= \\rho^{-1}\\sin\\psi(\\sigma_u\\times \\sigma_v)\\\\ \\rho v' &amp;= \\sin \\psi\\\\ \\rho^2 v' &amp;= \\rho \\sin\\psi \\end{align*}\\] <p>Note that \\(f = \\rho\\), so that the second geodesic equation gives </p> \\[\\frac{d}{dt}(\\rho^2 v') = 0\\implies, \\rho^2 v' := \\Omega = \\rho\\sin\\psi\\] <p>for some constant \\(\\Omega\\). </p> <p>Claim. Conversely, if \\(\\rho\\sin\\psi\\) is constant along \\(\\gamma\\) and no part of \\(\\gamma\\) is part of some parallel of \\(\\Sigma\\), then \\(\\gamma\\) is a geodesic. </p> <p>proof. We have shown that \\(f^2 v' = \\rho^2 v' = \\rho\\sin\\psi\\), so that if \\(\\rho\\sin\\psi = c\\), the second equation is satisfied. Consider the first equation \\(u'' = \\rho\\rho' v'^2\\), we have that \\(v' = \\frac{\\rho\\sin\\rho}{\\rho^2} = \\frac{\\Omega}{\\rho^2}\\) so that the unit-speed condition gives </p> \\[u'^2 = 1 - \\frac{\\Omega^2}{\\rho^2}\\] <p>differentiating both sides, </p> \\[2u'u'' = \\Omega^2 (-2)\\rho^{-3}\\frac{d\\rho}{du}u'\\implies \\frac{u'}{\\Omega^2}(u'' - \\rho\\frac{d\\rho}{du}v'^2) \\Leftrightarrow u'(u'' - \\rho\\frac{d\\rho}{du}v'^2)= 0\\] <p>As long as \\(u'\\neq 0\\), we have the first equation \\(u'' - \\rho\\frac{d\\rho}{du}v'^2 = u'' - ff'v'^2 = 0\\) satisfied. </p> <p>Corollary Assume that \\(\\Omega &gt; 0\\), then the geodesic is confined to the part of \\(\\Sigma\\) which is at a distance \\(\\geq \\Omega\\) from the axis. </p> <p>proof. Note that we have </p> \\[u'^2 = 1 - \\frac{\\Omega^2}{\\rho^2}\\] <p>Assume that \\(\\Omega &lt; \\rho = f\\implies u'^2 &gt; 0\\), then the geodesic will cross every parallel of \\(\\Sigma\\).  Assume that \\(\\Omega &gt; \\rho\\), then \\(u\\) would be bounded above or below on \\(\\Sigma\\). </p> <p>Corollary For \\(\\Omega = 0\\), the geodesic is the meridian. </p> <p>\\(\\Omega = 0\\implies \\sin\\psi = 0\\implies \\psi = 0\\), it is always parallel with the meridian. </p>"},{"location":"mat363/geodesics.html#example-hyperboloid","title":"Example: Hyperboloid","text":"<p>Suppose that the hyperboloid is obtained by rotating \\(x^2 - z^2 =1, x &gt; 0\\) around the z-axis. The distance is of \\([1,\\infty)\\). </p> <p>For \\(0 &lt; \\Omega &lt; 1\\), the geodesic with angular momentum \\(\\Omega\\) crosses every parallel, and so extends for \\(z\\in (-\\infty, \\infty)\\). For \\(\\Omega &gt; 1\\), since we must have that \\(\\rho^2 = x^2 = 1+z^2\\geq \\Omega^2\\), then the geodesic is confined to either \\(z\\geq \\sqrt{\\Omega^2 - 1}\\) or \\(z \\leq -\\sqrt{\\Omega^2 -1}\\). Consider \\(z\\geq \\sqrt{\\Omega^2 - 1}\\), note that \\(z = \\sqrt{\\Omega^2 - 1}\\) is a circle of radius \\(\\Omega\\), and is a parallel. Therefore, for some point \\(p\\) on the circle, the geodesic \\(C\\) must leave head up (\\(dz &gt; 0\\)) as it leaves \\(p\\). Moreover, \\(C\\) must be symmetric about \\(p\\), since reflection in the plane through \\(p\\) containing the z-axis takes \\(C\\) to another geodesic that also passes through \\(p\\) and is tangent to the circle, and then by uniqueness of geodeisc. Finally, consider that \\(u'^2 = 1 - \\frac{\\Omega^2}{\\rho^2} \\in (0, 1)\\), the geodesic crosses every parallel for \\(z \\geq  \\sqrt{\\Omega^2 - 1}\\). In addition, for \\(z \\leq - \\sqrt{\\Omega^2 - 1}\\), the story is similar.  For \\(\\Omega = 1\\), for \\(p\\) on the unit-circle, we have that \\(\\sin\\psi = 1\\), so that the geodesic \\(C\\) is tangent to \\(\\Gamma\\) at \\(p\\), so that it must coincide with \\(\\Gamma\\). For \\(p\\) below the circle, we have that \\(\\sin\\psi = \\frac{1}{\\rho} &lt; 1\\) so that \\(\\psi &lt; \\pi/2\\) and leaves \\(p\\) in one direction. It will get arbitrarily close to the circle, but never reaching it.</p>"},{"location":"mat363/geodesics.html#example-geodesics-of-spheroid","title":"Example: Geodesics of Spheroid","text":"<p>A spheroid is obtained by rotating a ellipse, say \\(\\frac{x^2}{p^2} + \\frac{z^2}{q^2} = 1, x &gt; 0\\) around z-axis. The distance is of \\([0, p]\\)</p> <p>For \\(0 &lt; \\Omega &lt; p\\), we must have that \\(x^2 = p^2 (1 - \\frac{z^2}{q^2}) \\geq \\Omega^2\\implies z \\in [-q\\sqrt{1-\\frac{\\Omega^2}{p^2} }, q\\sqrt{1-\\frac{\\Omega^2}{p^2} }]\\). Similar to the hyperboloid example, every point on the boundary will bounce off the boundary circle and is symmetric around \\(p\\), and crosses every parallel in the region. For \\(\\Omega = p\\), it is the parallel \\(z=0\\). </p>"},{"location":"mat363/geodesics.html#example-geodesics-of-torus","title":"Example: Geodesics of Torus","text":"<p>Rotating \\((x - a)^2 + z^2 = b^2, b &gt; a &gt; 0\\). The distance is of \\([a-b, a + b]\\). </p> <p>For \\(\\Omega &lt; a-b\\), spirals around the torus. For \\(\\Omega = a - b\\), if \\(p\\) on the circle of radius \\(a-b\\), then the geodesic is the parallel of radius \\(a-b\\). Otherwise, spirals around and approaching the parallel of radius \\(a-b\\) (for the similar reason of hyperboloid). For \\(a-b &lt;\\Omega &lt; a+b\\), we have that the region is the outer annular region with distance \\(\\geq \\Omega\\), and bounces between parallels bounded in this region. For \\(\\Omega = a+b\\), the parallel of radius \\(a+b\\). </p>"},{"location":"mat363/geodesics.html#geodesics-as-shortest-paths","title":"Geodesics as Shortest Paths","text":"<p>Consider some unit-speed curve \\(\\gamma: (a,b)\\rightarrow \\Sigma\\), passing through fixed \\(p,q\\in\\Sigma\\). If \\(\\gamma\\) is a shortest path from \\(p\\) to \\(q\\), then the part of \\(\\gamma\\) contained in any surface patch \\(\\sigma\\) must be the shortest path between any two of its points. Otherwise, we can have a shorter path.  </p> <p>Therefore, consider a family of smooth curves \\(\\gamma^\\tau\\) on some \\(\\sigma\\) passing through \\(p,q\\), for each \\(\\tau \\in (-\\delta, \\delta)\\) s.t.   - \\(\\exists \\epsilon &gt; 0\\) s.t. \\(\\gamma^\\tau\\) is defined for all \\(t\\in(-\\epsilon, \\epsilon)\\) and for all \\(\\tau \\in (-\\delta, \\delta)\\).   - \\(\\gamma^\\tau\\) passing through \\(p, q\\)  - The map \\(M: (-\\delta,\\delta)\\times (-\\epsilon, \\epsilon). M(\\tau, t) = \\gamma^\\tau(t)\\) is smooth.   - \\(\\gamma^0 = \\gamma\\) is the shortest path. </p> <p>And the length is defined by the arc-length \\(L(\\tau) = \\int_a^b \\|\\dot\\gamma^{\\tau}\\|dt\\)</p> <p>Theorem \\(\\gamma\\) is a geodesic IFF \\(\\frac{dL}{dt}\\mid_0 = 0\\).</p> <p>Note that we can only assume \\(\\gamma^0\\) is unit-speed, but not for \\(\\tau \\neq 0\\). </p>"},{"location":"mat363/geodesics.html#implications","title":"Implications","text":"<p>IF \\(\\gamma\\) is a shortest path, then \\(L\\) must have an absolute minimum when \\(\\tau = 0\\), implying that \\(\\frac{dL}{dt}\\mid_0 = 0\\), hence \\(\\gamma\\) is a geodesic. </p> <p>IF \\(\\gamma\\) is a geodeisc on \\(\\sigma\\) through \\(p,q\\), then \\(L(\\tau)\\) has a stationary point when \\(\\tau = 0\\), but this need not to be an absolute minimum, or even a local minimum. Therefore, geodesic does NOT imply shortest path. (Thinking about the two arcs of a great circle around the sphere). </p> <p>A shortest path (minimum) on a surface may not exists, for example, a plane with a hole in the straight line connecting \\(p,q\\).  </p> <p>Theorem If \\(\\Sigma\\) is a closed, connected subset of \\(\\mathbb R^3\\), then shortest path exists. </p>"},{"location":"mat363/intro_curves.html","title":"Introduction to Curves","text":""},{"location":"mat363/intro_curves.html#defining-a-curve","title":"Defining a curve","text":""},{"location":"mat363/intro_curves.html#level-curves","title":"Level Curves","text":"<p>A 2D curve can be described by means of their Cartesian equation \\(f(x, y) = c\\) and the curve is a set of points </p> \\[C = \\{(x,y)\\in\\mathbb R^2 : f(x, y) = c\\}\\] <p>A 3D curve can be described by 2 equations \\(f_1(x, y, z) = c_1, f_2(x,y,z) = c_2\\) s.t. </p> \\[C = \\{(x,y, z)\\in\\mathbb R^3 : f_1(x, y, z) = c_1, f_2(x,y,z) = c_2\\}\\] <p>A level curve is a set of points which the quantity \\(f(x, y)\\) reaches the 'level' \\(c\\). </p>"},{"location":"mat363/intro_curves.html#parameterized-curve","title":"Parameterized Curve","text":"<p>A parameterized curve is a map </p> \\[\\gamma: (a, b)\\rightarrow \\mathbb R^n, -\\infty \\leq a &lt; b\\leq \\infty\\] <p>A parameterization of level curve \\(C\\) is a parametrized curve, whose image is contained in \\(C\\). </p> <p>The parameterization of a curve is not unique, for example, the image of \\((\\cos t, \\sin t)\\) is equal to \\((\\cos 2t, \\sin 2t)\\)</p> <p>Example If \\(\\gamma(t) = (t^2, t^4)\\) is parameterization of \\(y = x^2\\)?</p> <p>No, note that \\(1 = (-1)^2\\), while for any \\(t \\in \\mathbb R\\), \\(t^2 \\geq 0\\) so that we cannot parameterize \\(y=x^2\\). </p> <p>Example Find the parameterization of \\(\\frac{x^2}{4} + \\frac{y^2}{9} = 1\\). Take \\((2\\cos t, 3\\sin t)\\), note that \\(\\frac{(2\\cos t)^2}{4} + \\frac{(3\\sin t)^2}{9} = \\cos t^2 + \\sin t^2 = 1\\)</p> <p>Example Find the Cartesian equations of \\(\\gamma(t) = (e^t, t^2)\\). Let \\(x = e^t\\), then \\(t = \\log x,\\implies y = t^2 = (\\log x)^2\\). </p>"},{"location":"mat363/intro_curves.html#smooth-curve","title":"Smooth Curve","text":"<p>A parameterized curve \\(\\gamma\\) is smooth if \\(\\gamma\\) is infinitely differentiable for all \\(t \\in (a, b)\\). Note that since \\(\\gamma: (a, b)\\rightarrow\\mathbb R^n\\), its derivative is defined as a vector-valued function and each derivative on each component. We will often denote  \\(\\dot\\gamma:=\\frac{d\\gamma}{dt}, \\ddot\\gamma := \\frac{d^2\\gamma}{dt^2}\\). </p>"},{"location":"mat363/intro_curves.html#tangent-vector","title":"Tangent Vector","text":"<p>Let \\(\\gamma(t)\\) be a parameterized curve, then \\(\\dot\\gamma(t)\\) is the tangent vector of \\(\\gamma\\) at point \\(\\gamma(t)\\). </p> <p>Theorem If the tangent vector of a parameterized curve is constant, then the image of the curve is (part of) a straight line. </p> <p>proof. Assume \\(\\dot\\gamma(t) = \\mathbf a\\) for all \\(t\\). Then, integrate \\(\\dot\\gamma\\) component-wise,</p> \\[\\gamma (t) = \\int \\dot\\gamma(t)dt = \\int \\mathbf a dt = t\\mathbf a + \\mathbf b\\] <p>Example Consider the parameterized curve \\(\\gamma(t) = ((1 + 2\\cos t)\\cos t, (1 + 2 \\cos t) \\sin t)\\). Its tangent vector is </p> \\[\\dot\\gamma(t) = (-\\sin t - 2\\sin 2t, \\cos t + 2\\cos 2t)\\] <p>Note that \\(\\gamma\\) has a self-intersection at \\(\\gamma(2\\pi / 3) = \\gamma(4\\pi / 3) = (0, 0)\\), while </p> \\[\\dot\\gamma(2\\pi/3) = (\\sqrt 3/ 2, -3/2)\\neq (-\\sqrt 3/ 2, -3/2) = \\dot\\gamma(4\\pi/3)\\] Source code <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nt = np.arange(-4 * np.pi, 4 * np.pi, 0.001)\nx = (1 + 2 * np.cos(t)) * np.cos(t); y = (1 + 2 * np.cos(t)) * np.sin(t)\ndx1 = 3 ** 0.5 / 2 * t; dy1 = -3 / 2 * t\ndx2 = -3 ** 0.5 / 2 * t; dy2 = -3 / 2 * t\nplt.figure(figsize=(4, 4)); plt.xlim(-1, 4); plt.ylim(-3, 3)\nplt.xticks([]); plt.yticks([])\nplt.plot(x, y); plt.plot(dx1, dy1); plt.plot(dx2, dy2)\nplt.savefig(\"../assets/intro_curves.jpg\")\n</code></pre> <p>\u200b </p>"},{"location":"mat363/intro_curves.html#example-foci-of-ellipse","title":"Example: Foci of ellipse","text":"<p>Consider the ellipse \\(\\frac{x^2}{p^2}+ \\frac{y^2}{q^2} = 1, p &gt; q &gt; 0\\), using the parameterization \\(\\gamma(t) = (p\\cos t, q\\sin t)\\). Define the eccentricity of the ellipse as \\(\\epsilon = \\sqrt{1 - \\frac{q^2}{p^2}}\\) and the foci of the ellipse as \\(\\mathbf f_1 = (- \\epsilon p, 0), \\mathbf f_2 = (\\epsilon p, 0)\\). </p> <p>Claim \\(\\forall a \\in C. d(\\mathbf f_1, a) + d(\\mathbf f_2, a)= \\mathbf c\\) where \\(\\mathbf c\\) does not depend on \\(a\\).   </p> <p>proof. Let \\(a\\in C\\), take some \\(t_0\\) s.t. \\(a = (p\\cos t_0, q\\sin t_0)\\). Consider \\(d(a, \\mathbf f_1)\\)</p> \\[\\begin{align*} d(a, \\mathbf f_1)^2 &amp;= (p\\cos t_0 - \\epsilon p)^2 + (q\\sin t_0)^2 \\\\ &amp;= p^2\\cos^2 t_0 - 2\\epsilon p^2 \\cos t_0 + \\epsilon^2 p^2 + q^2\\sin^2 t_0\\\\ &amp;= (p^2 -q^2)\\cos^2 t_0 - 2\\epsilon p^2 \\cos t_0 + (p^2 - q^2) + q^2\\cos^2 t_0 + q^2\\sin^2 t_0\\\\ &amp;= (p^2 -q^2)\\cos^2 t_0 - 2\\epsilon p^2 \\cos t_0 + p^2\\\\ &amp;= p^2(\\epsilon^2 \\cos^2 t_0 - 2\\epsilon\\cos t_0 + 1)\\\\ &amp;= p^2(\\epsilon\\cos t_0 - 1)^2 \\end{align*}\\] <p>Then, taking the square root, notice that \\(\\epsilon &lt; 1. -1 &lt; \\cos t_0 &lt; 1\\) so that \\(1 - \\epsilon\\cos t_0\\) &gt; 0. \\(d(a, \\mathbf f_1) = p(1 - \\epsilon \\cos t_0)\\). Similarly, \\(d(a, \\mathbf f_2) = p(1 + \\epsilon \\cos t_0)\\). Therefore,</p> \\[d(\\mathbf f_1, a) + d(\\mathbf f_2, a) = 2p\\] <p>Claim \\(\\forall a \\in C\\). The product of the distances from \\(\\mathbf f_1\\) and \\(\\mathbf f_2\\) to the tangent line at \\(a\\) does not depend on \\(a\\). </p> <p>proof. For some \\(t\\), the tangent line is </p> \\[\\gamma'(t) = (-p\\sin t, q\\cos t)\\] <p>Hence the normal is \\(\\mathbf n(t) = (q\\cos t, p\\sin t)\\) so that \\(\\gamma'(t)\\cdot \\mathbf n(t) =\\mathbf 0\\).  Note that the distance between some point \\(\\mathbf f_1\\) and the tangent line is \\((\\mathbf f_1 - a)\\cdot \\frac{\\mathbf n}{\\|\\mathbf n\\|}\\). Therefore, the product is </p> \\[\\begin{align*} d(\\mathbf f_1, l(t))&amp;=\\quad (\\mathbf f_1 - a)\\cdot \\frac{\\mathbf n}{\\|\\mathbf n\\|}\\\\ &amp;= \\frac{(\\epsilon p - p\\cos t)q\\cos t + q\\sin t p \\sin t}{(q^2\\cos^t + p^2 \\sin^2 t)^{1/2}}\\\\ &amp;= \\frac{\\epsilon pq\\cos t - pq\\cos^2 t + pq\\sin^2 t}{(q^2\\cos^2 t + p^2 \\sin^2 t)^{1/2}}\\\\ &amp;= \\frac{pq(\\epsilon \\cos t - 1)}{(q^2\\cos^2 t + p^2 \\sin^2 t)^{1/2}}\\\\ d(\\mathbf f_2, l(t))&amp;=\\frac{pq(-\\epsilon \\cos t - 1)}{(q^2\\cos^2 t + p^2 \\sin^2 t)^{1/2}}\\\\ d(\\mathbf f_1, l(t))d(\\mathbf f_2, l(t))&amp;= \\frac{p^2q^2(1 + \\epsilon \\cos t)(1 - \\epsilon \\cos t)}{q^2\\cos^2 t + p^2 \\sin^2 t}\\\\ &amp;= q^2\\frac{p^2-(p^2 - q^2)\\cos^2 t)}{p^2\\sin^2 t + p^2\\cos^2 t + (q^2 - p^2)\\cos^2 t}\\\\ &amp;= q^2 \\end{align*}\\]"},{"location":"mat363/intro_curves.html#example-vivianis-curve","title":"Example (Viviani's Curve)","text":"<p>Show that </p> \\[\\gamma(t) = (\\cos^2 t - 1/2, \\sin t\\cos t, \\sin t)\\] <p>is a parameterization of the curve of intersection of the cylinder of radius \\(1/2\\) and axis the z-axis with the sphere of radius \\(1\\) and center \\((-1/2, 0, 0)\\). </p> <p>The surface of the circle can be represented as </p> \\[C_1 = \\{(x, y, z) : (x + 1/2)^2 + y^2 + z^2 = 1\\}\\] <p>The surface of the cylinder is </p> \\[C_2 = \\{(x, y, z): x^2 + y^2 = 1/4\\}\\] <p>Let \\(z = \\sin t\\), we have the equations</p> \\[\\begin{cases}x^2 + x + 1/4 + y^2 + \\sin^2 t = 1\\\\x^2 + y^2 = 1/4\\end{cases}\\] <p>solves to have \\(x + 1/2 = 1 - \\sin^2 t\\Rightarrow x = \\cos^2 t - 1/2\\), then \\(y = \\sqrt{1/4 - (\\cos^2 t - 1/2)^2} = \\sqrt{\\cos^2 t(1-\\cos^2 t)} = \\sin t\\cos t\\).</p>"},{"location":"mat363/intro_surface.html","title":"Defining Surfaces","text":""},{"location":"mat363/intro_surface.html#topology-refresher","title":"Topology Refresher","text":"<p>A subset \\(U\\) of \\(\\mathbb R^n\\) is open if \\(\\forall a\\in U. \\exists \\epsilon &gt; 0. \\forall u \\in \\mathbb R^n. \\|u-a\\|&lt;\\epsilon\\implies u\\in U\\). </p> <p>A map \\(f: (X\\subset \\mathbb R^m)\\rightarrow (Y\\subset\\mathbb R^n)\\) if continuous at \\(a\\in X\\) if </p> \\[\\forall \\epsilon &gt; 0.\\exists \\delta &gt; 0. \\forall u\\in X. \\|u-a\\| &lt; \\delta \\implies \\|f(u)-f(a)\\| &lt; \\epsilon\\] <p>Equivalently, \\(f\\) is continuous IFF for all open set \\(V\\subset\\mathbb R^n\\), exists open set \\(U\\subset\\mathbb R^m\\)  s.t. \\(U\\cap X = \\{x\\in X: f(x)\\in V\\}\\).</p> <p>\\(f\\) is a homeomorphism is it is continuous, bijective, and its inverse is also continuous, and \\(X,Y\\) are homeomorphic. </p>"},{"location":"mat363/intro_surface.html#defining-surfaces_1","title":"Defining Surfaces","text":"<p>Suppose that we are trying to parameterize the surface of a unit sphere \\(\\{x\\in\\mathbb R^3: \\|x\\| = 1\\}\\) with the xy-plane. Intuitively, if we pick the south pole on the sphere, map it to the origin, and then wrap the plane on the surface. Then, we will have one circle on the plane being map to the north pole (Stereographic Projection). However, we will lose regularity in this case as the derivative will be all 0. </p> <p>This is a very simple case, but from this we can see that a plane is not enough to parameterize all kinds of surfaces. Instead, we will locally parameterize surfaces by subsets of \\(\\mathbb R^2\\). i.e. for some surface \\(\\Sigma\\), for some open subset \\(V\\subset \\Sigma\\), we are interested in some homeomorphism \\(f\\) to map \\(U\\subset\\mathbb R^2\\) to \\(V\\). </p> <p>A subset \\(\\Sigma\\subset\\mathbb R^3\\) if a surface if \\(\\forall p\\in \\Sigma\\), exists open subset \\(U\\subset \\mathbb R^2\\) and open \\(V\\in\\mathbb R^3\\) s.t. \\(p\\in V\\) and \\(V\\cap \\Sigma\\) is homeomorphic to \\(U\\).  A open subset of surface \\(\\Sigma\\) is defined as \\(\\Sigma\\cap V\\) where \\(V\\subset\\mathbb R^3\\) and open. A surface patch of the open subset is a homeomorphism \\(\\sigma: U\\rightarrow \\Sigma\\cap V\\). A atlas is a finite collection of surface patches whose images \\(\\cup_{i=1}^N(\\Sigma\\cap V_i) = \\Sigma\\)</p>"},{"location":"mat363/intro_surface.html#regular-surfaces","title":"Regular Surfaces","text":"<p>A surface is regular if all of its surface patches in atlas is regular. A surface patch is regular if \\(\\sigma\\) is smooth and regular. i.e. For  \\(\\sigma(u, v) = (x(u,v), y(u,v), z(u,v))^T\\), its Jacobian </p> \\[D\\sigma = \\begin{bmatrix}\\partial_u x&amp;\\partial_vx\\\\\\partial_uy&amp;\\partial_vy\\\\\\partial_u z&amp;\\partial_vz\\end{bmatrix}\\] <p>has linearly independent columns (equivalently, rank = 2, \\(D\\sigma\\) is injective, exists 2 rows where the formed \\(2\\times 2\\) matrix is invertible). </p> <p>If a surface path \\(\\sigma\\) is regular. Then \\(\\{\\partial_u\\sigma, \\partial_v\\sigma\\}\\) are linearly independent, hence can span a plane. For some fixed \\((u_0, v_0)\\), we have that </p> \\[D\\sigma(u_0, v_0)\\begin{bmatrix}a\\\\b\\end{bmatrix} = a\\frac{\\partial\\sigma}{\\partial u}(u_0) + b\\frac{\\partial\\sigma}{\\partial v}(v_0)\\] <p>is the tangent plane at \\((u_0, v_0)\\). Otherwise, we cannot span such plane, hence not regular. </p>"},{"location":"mat363/intro_surface.html#example-image-of-2d-functions","title":"Example: Image of 2D functions","text":"<p>A plane can be defined as \\(f(u,v) = \\mathbf a+ u  \\mathbf b+ v  \\mathbf c\\) where \\(\\mathbf b, \\mathbf c\\) are linearly independent. Then, note that if we define \\(\\sigma := f\\) to be the surface patch from \\(U=\\mathbb R^2\\) to \\(V=\\Sigma\\), note that \\(\\sigma = f\\) is obviously bijective. In addition, \\(Df = [\\mathbf b,\\mathbf c]\\) is linearly independent. Therefore, plane is a regular surface. </p> <p>Moreover, if we consider image of some differentiable function \\(f: U\\subset\\mathbb R^2\\rightarrow V\\subset\\mathbb R\\). Define \\(\\sigma(u,v) = (u, v, f(u, v))\\), given that \\(f\\) is differentiable, \\(\\sigma\\) is obviously continuous and bijective, and </p> \\[D\\sigma = \\begin{bmatrix}1&amp;0\\\\0&amp;1\\\\\\partial_uf&amp;\\partial_vf\\end{bmatrix}\\] <p>is obviously linearly independent. </p> <p>Corollary. If \\(\\Sigma\\) is a surface, then \\(\\forall p\\in \\Sigma, \\exists V\\subset \\Sigma\\) s.t. \\(p\\in V, V\\) is open and \\(V\\) is the graph of anyone of \\(z = \\psi(x,y), y = \\varphi(x,z), x = \\tau(x,y)\\) where the mapping is smooth. Then, \\(\\Sigma\\) is regular. </p>"},{"location":"mat363/intro_surface.html#example-sphere","title":"Example: Sphere","text":"<p>Consider the unit sphere \\(\\{(x,y,z) : x^2 + y^2 + z^2 = 1\\}\\). </p> <p>First, try \\(z = \\pm\\sqrt{1 - x^2 - y^2}\\), and map from 2 circles to the upper and lower hemisphere. </p> \\[\\sigma^z_1(u, v) = (u, v, \\sqrt{1-u^2-y^2}), \\sigma^z_2 = (u, v) = (u, v, -\\sqrt{1-u^2-y^2})\\] <p>However, since that square root is defined on \\(\\mathbb R^{\\geq 0}\\) and we need that the domain of \\(\\sigma\\) be open, we have that </p> \\[U_1 = U_2 = \\{(u, v): u^2 + v^2 &lt; 1\\}\\] <p>Then, note that the circle \\(\\{(x, y, 0): x^2 + y^2 = 1\\}\\) is not covered.  Similarly, we can take </p> \\[\\sigma^x_1(u, v) = ( \\sqrt{1-u^2-y^2}, u, v), \\sigma^x_2 = (u, v) = ( -\\sqrt{1-u^2-y^2}, u, v)\\] \\[\\sigma^y_1(u, v) = (u,  \\sqrt{1-u^2-y^2}, v), \\sigma^y_2 = (u, v) = (u,  -\\sqrt{1-u^2-y^2}, v)\\] <p>Note that each pair alone is unable to cover the sphere. However, the six patches together can cover the sphere surface. </p> <p>However, if we consider the example at the very beginning, although one patch is not enough, it looks like that the plane should be able to cover the sphere with one circular open subset uncovered. If we have another patch to cover that subset, then the sphere can be covered with two patches. </p> <p>Consider some point \\(\\mathbf a = (x, y, z)\\) on the surface, and let \\(\\mathbf b = (x, y, 0)\\) be its projection on the xy-plane. Then, let </p> \\[\\theta = \\arccos(\\mathbf a \\cdot \\mathbf b), \\psi = \\arccos(\\mathbf b\\cdot \\mathbf e_1)\\] <p>Therefore, we can have that </p> \\[\\begin{pmatrix}x\\\\y\\\\z\\end{pmatrix} = \\begin{pmatrix} \\cos\\theta\\cos\\psi\\\\ \\cos\\theta\\sin\\psi\\\\ \\sin\\theta \\end{pmatrix}, \\theta\\in[-\\pi/2, \\pi/2], \\psi\\in[0, 2\\pi]\\] <p>Then, since \\(U\\) must be open, we can only take </p> \\[U_1 = \\{(\\theta, \\psi): \\theta\\in(-\\pi/2, \\pi/2), \\psi\\in(0, 2\\pi)\\}\\] <p>In this case, consider the uncovered parts: </p> \\[\\psi = 0 = 2\\pi\\implies \\begin{pmatrix} \\cos\\theta\\cos 0\\\\ \\cos\\theta\\sin 0\\\\ \\sin\\theta \\end{pmatrix} = \\begin{pmatrix} \\cos\\theta\\\\ 0\\\\ \\sin\\theta \\end{pmatrix}\\] \\[\\theta = \\pm\\pi/2 \\implies \\begin{pmatrix} 0\\\\ 0\\\\ \\pm\\sin\\psi \\end{pmatrix}\\] <p>Therefore, the uncovered part is the upper semicircle \\(\\{(x, 0, z): x \\geq 0\\}\\) on the xz-plane. However, we can easily obtain another patch, where the uncovered part is the lower semicircle \\(\\{(x, y, 0): x\\leq 0\\}\\) by rotating \\(\\sigma\\) by \\(\\pi\\) along z-axis, and \\(\\pi/2\\) along x-axis. </p> \\[\\sigma_2 = R_x(\\pi/2)R_z(\\pi)\\sigma_1 = \\begin{pmatrix} \\cos\\theta\\cos \\psi\\\\ \\cos\\theta\\sin \\psi\\\\ \\sin\\theta \\end{pmatrix} = R_x(\\pi/2)\\begin{pmatrix} -\\cos\\theta\\cos \\psi\\\\ -\\cos\\theta\\sin \\psi\\\\ \\sin\\theta \\end{pmatrix} = \\begin{pmatrix} -\\cos\\theta\\cos \\psi\\\\ -\\sin\\theta\\\\ -\\cos\\theta\\sin \\psi\\\\ \\end{pmatrix}\\]"},{"location":"mat363/intro_surface.html#example-circular-cone-not-a-surface","title":"Example: Circular Cone (Not a Surface)","text":"<p>Note that each surface patch \\(\\sigma\\) maps from an open set of a plane to an open subset, and the map is invertible and continuous. Therefore, we can apply continuous mapping theorem. For any point \\(p\\in \\Sigma\\) on the surface and some surface patch \\(\\sigma\\), the neighborhood of \\(p\\) (or an open disk containing \\(p\\)) should be mapped to the neighborhood of its pre-image (an open disk containing \\(\\sigma^{-1}(p)\\)). </p> <p>Consider the circular cone </p> \\[S = \\{(x,y,z)\\in\\mathbb R^3: x^2 + y^2 = z^2\\}\\] <p>Let \\(\\sigma: U\\rightarrow V \\cap S\\) be a surface patch containing \\(p = (0, 0, 0)\\in S\\), \\(a = \\sigma^{-1}(p)\\in U\\). Take \\(p_+,p_-\\) in \\(p\\)'s neighborhood and \\(p_+\\) in the upper cone \\((z&gt;0)\\) and \\(p_-\\) in the lower cone \\((z&lt;0)\\), and \\(a_+, a_- \\in U\\) be the pre-image, respectively. </p> <p>Then, consider a continuous path \\(\\gamma:[0, 1]\\rightarrow V\\) from \\(p_- = \\gamma(0)\\) to \\(p_+ = \\gamma(1)\\), we claim that \\(\\gamma\\) must pass through \\(p\\). </p> <p>Let \\(\\pi: \\mathbb R^3\\rightarrow\\mathbb R, \\pi(x,y,z) =z\\), then \\(\\pi\\circ\\gamma: [0, 1]\\rightarrow\\mathbb R\\). Then we have \\([\\pi\\circ\\gamma](0) &lt; 0, [\\pi\\circ\\gamma](1) &gt; 0\\). By IVT, there exists some \\([\\pi\\circ\\gamma](t) = 0\\), and since \\(\\gamma([0, 1]) \\subseteq S\\), the only possible point with \\(z=0\\) is \\(\\gamma(t) = (0,0,0)\\).</p> <p>Therefore, by continuous mapping theorem, any path from \\(a_-\\) to \\(a_+\\) must pass through \\(a\\), which violates the assumption that \\(U\\) is an open set. </p> <p>Another similar example is two planes intersecting together. </p> <p>The takeaway here is that if we remove the point, the set has at least 2 connected components, then the set is not a surface. </p>"},{"location":"mat363/iso_ineq.html","title":"Isoperimetric Inequality","text":""},{"location":"mat363/iso_ineq.html#area-of-a-simple-closed-curve","title":"Area of a Simple Closed Curve","text":"<p>Let \\(\\gamma: (0,L) \\rightarrow \\mathbb R^2, \\gamma(s) = (x(s), y(s))\\) be a simple closed unit-speed curve, \\(\\text{int}(\\gamma)\\) be the bounded interior. Then the area of \\(\\gamma\\) is defined as </p> \\[A(\\gamma) = \\iint_{\\text{int}(\\gamma)}dxdy \\] <p>By Green's Theorem, we can instead integrating through the boundary, i.e. </p> \\[\\iint_S(\\partial_x g - \\partial f_y)dxdy = \\int_{\\partial S} (f(x, y)dx + g(x, y)dy)\\] <p>Applied on the area integral, by taking \\(f(x,y)= \\frac{-y}{2}, g(x,y) = \\frac{x}{2}\\)</p> \\[\\begin{align*} A(\\gamma) &amp;= \\iint_{\\text{int}(\\gamma)}dxdy \\\\ &amp;= \\int_{\\gamma} (\\frac{x}{2}dy - \\frac{y}{2}dx)\\\\ &amp;= \\int_{\\gamma} (\\frac{x}{2}\\frac{dy}{ds}ds - \\frac{y}{2}dy\\frac{dx}{ds}ds)\\\\ &amp;= \\frac{1}{2}\\int_0^L x'(s)y(s) - x(s)y'(s) ds \\end{align*}\\] <p>Also, if we take \\(f(x, y) = -y, g(x,y)=0\\) or \\(f(x,y) = 0, g(x,y)=x\\), then </p> \\[A = \\int_0^L x(s)y'(s) ds = -\\int_0^Lx'(s)y(s) ds\\]"},{"location":"mat363/iso_ineq.html#geometric-interpratations","title":"Geometric Interpratations","text":"<p>Lemma Consider a triangle \\(A = (0, 0), B = (x_1,y_1), C = (x_2, y_2)\\), WLOG assume \\(x_1&gt;x_2, y_1&lt;y_2\\). Then, its area is </p> \\[A = \\frac{1}{2}(x_1y_2 - x_2y_1)\\] <p>proof. </p> <p>\u200b </p> \\[\\begin{align*} A &amp;= R - T_1 - T_2 -T_3 \\\\ &amp;= R - \\frac{1}{2}(R_1+R_2+R_3)\\\\ &amp;= R - \\frac{1}{2}(Rec - R_4) \\\\ &amp;= \\frac{1}{2}R - R_4 \\\\ &amp;= \\frac{1}{2}(x_1y_2 - x_2y_1) \\end{align*}\\] <p>Claim For a polygon of \\(n \\geq 3\\) points \\(P_k = (x_k, y_k), k = 1,2,...,n\\), its area is </p> \\[A_n = \\frac{1}{2}\\sum_{i=1}^{n} x_i y_{i+1} - x_{i+1}y_i, P_{n+1} = P_0\\] <p>proof.  Let \\(n = 3\\), translating the polygon by \\(-(x_1, y_1)\\) and use the equation above. </p> <p>Let \\(n &gt; 3\\), then note that the polygon can be divided into a polygon \\(P_1,...,P_{n-1}\\) and a triangle \\(P_{n-1}, P_{n}, P_1\\). Therefore, </p> \\[\\begin{align*} A_n &amp;= A_{n-1} + T_n\\\\ &amp;= \\frac{1}{2}(\\sum_{i=1}^{n-2} (x_i y_{i+1} - x_{i+1}y_i) + x_{n-1}y_{1} - x_1y_{n-1}) \\\\ &amp;\\:+ \\frac{1}{2}(x_{n-1}y_{n} - x_{n-1}y_{n} + x_ny_1 - x_1y_n + x_1y_{n-1} - x_{n-1}y_1)\\\\ &amp;= \\frac{1}{2}\\sum_{i=1}^{n} (x_i y_{i+1} - x_{i+1}y_i) \\end{align*}\\] <p>Therefore, for a curve \\(\\gamma\\), take each \\(P_{k} = \\gamma(s_k), P_{k+1} = \\gamma(s_k + \\Delta s) = (x_k + dx, y_k + dy)\\), </p> \\[\\begin{align*} \\tilde A &amp;= \\frac{1}{2}\\sum_{i=1}^{n} (x_i (y_{i} + dy) - (x_i + dx)y_i)\\\\ &amp;= \\frac{1}{2}\\sum_{i=1}^{n} x_idy - dxy_i\\\\ A &amp;= \\frac{1}{2}\\int_\\gamma xdy-ydx \\end{align*}\\]"},{"location":"mat363/iso_ineq.html#isoperimetric-inequality-e-schmidt","title":"Isoperimetric Inequality (E. Schmidt)","text":"<p>Claim For a simple closed curve \\(\\gamma(s) = (x(s), y(s))\\) with length \\(L\\) and area \\(A\\). Then \\(A\\leq \\frac{L^2}{4\\pi}\\). Moreover, \\(A = \\frac{L^2}{4\\pi}\\) IFF \\(\\gamma\\) is a circle. </p> <p>proof. Since \\(\\gamma\\) is a simple closed curve, Take \\(x_0 = \\min(x(s)), x_1 = \\max(x(s)), w= x_1-x_0\\), take \\(R = \\frac{w}{2}\\).  Let \\(C\\) be a circle of radius \\(R\\) centered at \\((0, 0)\\). WLOG, translate \\(\\gamma\\) by \\((-\\frac{x_0+x_1}{2}, 0)\\) so that \\(x_0=-R, x_1=R\\). Parameterized \\(C\\) by projecting \\(\\gamma\\) to the circle. i.e.  \\(c(s) = \\text{proj}(x(s), y(s)) = (x(s), \\hat y(s))\\) where \\(x(s)^2 + \\hat y(s)^2 = R^2\\). Note that \\(c\\) is not regular. </p> <p>For each \\(s\\), look at its projection to the outer unit normal \\(-\\mathbf n_s(s) = (y'(s), -x'(s))\\) (rotating tangent clockwise by \\(\\pi/2\\)).  Take \\(\\text{proj}_c\\) to be the projection from \\(c(s)\\) to \\(-\\mathbf n_s(s)\\), </p> \\[\\begin{align*} &amp;\\text{proj}_c(s)  (x(s), \\hat y(s)) \\cdot (y'(s), -x'(s))\\\\= &amp;\\|c(s)\\| \\|-\\mathbf n_s(s)\\|\\cos(\\alpha(s))\\\\= &amp;R\\cos(\\alpha(s)) \\leq R \\end{align*}\\] <p>where \\(a\\) is the angle between \\((x,\\hat y), (y',-x')\\) Note that \\(\\text{proj}_c(s) = R\\iff \\alpha(s) = 0\\) IFF the outer normal is directed exactly the same as the vector on the circle.  Also, we have the inequality </p> \\[xy'-x'\\hat y = (x, \\hat y) \\cdot (y', -x') \\leq R\\] <p>and integrating along the curve</p> \\[\\begin{align*} \\int_0^L (xy'-x'\\hat y) ds &amp;\\leq RL\\\\ \\int_0^L xy' ds+ (- \\int_0^L x'\\hat y ds)  &amp;\\leq RL\\\\ A + \\pi R^2 &amp;\\leq RL &amp;\\text{Green's Theorem}\\\\ A &amp;\\leq LR - \\pi R^2 = \\frac{L^2}{4\\pi} - \\frac{\\pi}{4}(\\frac{L}{\\pi} - 2R)^2\\\\ A &amp;\\leq \\frac{L^2}{4\\pi} \\end{align*}\\] <p>Then, consider the conditions for the equality. First, we must have \\(L = 2\\pi R\\) from the computations above, and this must be try for each direction. Also, we must have that for all \\(s, \\alpha = 0 \\iff (x,\\hat y) \\perp (x',y')\\). </p>"},{"location":"mat363/iso_ineq.html#isoperimetric-inequality-steiner","title":"Isoperimetric Inequality (Steiner)","text":"<p>Fact 1 \\(A\\leq \\pi L^2\\), i.e. the curve can be contained within a circle of radius \\(L\\)</p> <p>Fact 2 \\(\\gamma\\) is convex. Suppose that the line segment \\(\\gamma(s_0), \\gamma(s_1)\\) is not contained in \\(\\text{int}(\\gamma)\\), then flip \\(\\gamma([s_0, s_1])\\) around the line segment and we can get a larger area with the same arc length. </p> <p>Fact 3 If \\(\\gamma\\) is convex, then \\(\\kappa_s(s)\\) does not change its sign for all \\(s\\). </p> <p>Fact 4 For any point \\(P\\) on a circle \\(C\\), and a diameter \\(AB\\), for the triangle formed by \\(P, A, B\\), the angle at \\(P, \\theta_P = \\pi/2\\). </p> <p>Claim  If for any simple closed plane curve of length \\(L\\), exists some curve \\(\\gamma\\) that attains the maximum, then \\(\\gamma\\) is a circle of \\(R = \\frac{L}{2\\pi}\\). </p> <p>Note that the assumption (existence of maximum won't not proved here)</p> <p>proof. Take some convex \\(\\gamma\\) be the curve that attains maximum area, take \\(a,b\\) s.t. each \\(\\gamma([a, b)) = \\gamma([b, a)) = L/2\\). Then, \\(\\text{int}(\\gamma)\\) is divided into two parts, call then \\(D_1, D_2\\) with area \\(A_1, A_2\\). Then, we must have that \\(A_1 = A_2\\), otherwise we can find a curve with larger area by flipping the larger half. Therefore, we have that for all \\(P_1, P_2\\) that divides the arc into 2 equal pieces, the divided areas must be equal. </p> <p>Then, we want to show that \\(D_1\\) and \\(D_2\\) are both semicircles with diameter \\(P_1P_2\\), by Fact 4, equivalently we want to show that \\(\\theta_p = \\pi/2\\) for any \\(P\\) on the curve.  </p> <p>Suppose \\(D_1\\) is not a semicircle, then take some \\(P\\) s.t. \\(\\theta_P\\neq \\pi/2\\). </p> <p>Then, consider the domain enclosed by the line segment and curve between \\(PP_1\\) and the line segment and curve between \\(PP_2\\). We can  rotate the two domains around \\(P\\) so that \\(\\theta_P = \\pi/2\\). Note that the area of the triangle formed by \\(PP_1P_2\\) has its maximum when \\(\\theta_P=\\pi/2\\), the area of other 2 domains does not change, the arc length does not change. Therefore, by contradiction, we proved that \\(\\theta_P = \\pi/2\\) for all \\(P\\). </p>"},{"location":"mat363/iso_ineq.html#example-isoperimetric-inequality-on-ellipse","title":"Example: Isoperimetric Inequality on Ellipse","text":"<p>Claim. Let the ellipse be defined as \\(\\frac{x^2}{p^2} + \\frac{y^2}{q^2} =1\\). Then, we have that </p> \\[\\int_0^{2\\pi} \\sqrt{p^2\\sin^2 t + q^2\\cos^2t} dt \\geq 2\\pi \\sqrt{pq}\\] <p>with equality IFF \\(p=q\\). </p> <p>proof. Let \\(\\gamma(t) = (p\\cos t, q\\sin t)\\) be the parameterization of the ellipse. so that \\(\\gamma'(t) = (-p\\sin t, q\\cos t)\\). and the arc-length </p> \\[L = \\int_0^{2\\pi} \\|\\gamma'(t)\\| dt = \\int_0^{2\\pi} \\sqrt{p^2\\sin^2 t + q^2\\cos^2t} dt\\] <p>and the area of the ellipse is </p> \\[A = \\frac{1}{2}\\int_0^{2\\pi} p\\cos t q\\cos t + p\\sin t q\\sin t = \\frac{1}{2}pq\\int_0^{2\\pi} dt = \\pi pq\\] <p>By Isoperimetric Inequality, </p> \\[\\begin{align*} L &amp;\\geq \\sqrt{4\\pi A}\\\\ \\int_0^{2\\pi} \\sqrt{p^2\\sin^2 t + q^2\\cos^2t} dt &amp;\\geq \\sqrt{4\\pi^2 pq}\\\\ \\int_0^{2\\pi} \\sqrt{p^2\\sin^2 t + q^2\\cos^2t} dt &amp;\\geq 2\\pi\\sqrt{pq} \\end{align*}\\] <p>\\(p=q\\) IFF the ellipse is a circle IFF Isoperimetric equality holds. </p>"},{"location":"mat363/isometry.html","title":"Isometric, Conformal, Equiareal","text":""},{"location":"mat363/isometry.html#isometry","title":"Isometry","text":"<p>Let \\(f:\\Sigma_1\\rightarrow\\Sigma_2\\) be smooth. \\(f\\) is a local isometry if any curve \\(\\gamma_1\\) in \\(\\Sigma_1\\) has the same length as \\(\\gamma_2 = f\\circ \\gamma_1\\). Then, \\(\\Sigma_1, \\Sigma_2\\) are locally isometric. </p> <p>If \\(f\\) is a local isometry and a diffeomorphism, then \\(f\\) is a isometry. </p> <p>Also, for notation, define </p> \\[f^*\\langle v,w\\rangle_p := \\langle D_pf(v),D_pf(w)\\rangle_{f(p)}\\] <p>Theorem \\(f\\) is a local isometry IFF the first fundamental form for \\(p\\) on \\(\\Sigma_1\\) is the same as \\(f(p)\\) on \\(\\Sigma_2\\). </p> <p>proof. Let \\(\\gamma_1: \\mathbb R\\rightarrow \\Sigma_1, \\gamma_2 = f\\circ \\gamma_1: \\mathbb R\\rightarrow\\Sigma_2\\). \\(\\Rightarrow\\): Assume \\(f\\) preserves the length of the curve. Then, for any \\(t_0&lt;t_1\\), we have that \\(\\int_{t_0}^{t_1}\\|\\gamma_1'\\|dt = \\int_{t_0}^{t_1}\\|\\gamma_2'\\|dt\\). Implying that \\(\\|\\gamma'_1\\| = \\|\\gamma_2'\\|\\) By chain rule, </p> \\[\\gamma'_2 = (f\\circ \\gamma_1)' = D_pf \\gamma_1'\\] <p>Note that \\(D_pf \\gamma_1'\\) lies on the tangent plane \\(T_{f(p)}\\Sigma_2\\) since \\(D_pf: T_p\\Sigma_1\\rightarrow T_{f(p)}\\Sigma_2\\). Therefore, we have that </p> \\[\\|\\gamma'\\|_1^2 = \\langle \\gamma_1',\\gamma_1'\\rangle_{T_p\\Sigma_1} = \\langle  D_pf \\gamma_1',  D_pf \\gamma_1'\\rangle_{T_f(p)\\Sigma_2}\\] <p>\\(\\Leftarrow\\): Assume \\(\\langle \\gamma_1',\\gamma_1'\\rangle_{T_p\\Sigma_1} = \\langle  D_pf \\gamma_1',  D_pf \\gamma_1'\\rangle_{T_f(p)\\Sigma_2}\\), then  \\(\\|\\gamma'_1\\| = \\langle \\gamma_1',\\gamma_1'\\rangle_{T_p\\Sigma_1}, \\|\\gamma'_2\\| = \\langle  D_pf \\gamma_1',  D_pf \\gamma_1'\\rangle_{T_f(p)\\Sigma_2}\\) and it is proven. </p> <p>Corollary \\(f\\) is an local isometry if \\(\\sigma_1: \\mathbb R^2 \\rightarrow V\\cap\\Sigma_1\\) has the same fundamental form as \\(f\\circ \\sigma_1\\).</p>"},{"location":"mat363/isometry.html#example-cylinder","title":"Example: Cylinder","text":"<p>Claim. Cylinder is locally isometric to a plane. </p> <p>proof. Consider the unit cylinder \\(\\sigma(u,v) = (\\cos u, \\sin u, v)\\). Then, </p> \\[\\sigma_u = (-\\sin u, \\cos u, 0), \\sigma_v = (0, 0, 1)\\] \\[E = \\sin^2 u + \\cos^2 u = 1, F = 0, G = 1\\] <p>The first fundamental form is \\(u'^2 + v'^2\\), is the same as the plane. However, this is not an isometry, since \\(\\sigma(0, v) = \\sigma(2\\pi, v)\\) is not injective. </p>"},{"location":"mat363/isometry.html#example-tangent-developables","title":"Example: Tangent Developables.","text":"<p>A tangent developable can be parameterized as </p> \\[\\sigma(u,v) = \\gamma(u) + v(\\gamma'(u))\\] <p>In other words, the union of all the tangent lines to a curve. </p> <p>Claim Tangent developables are isometric to a plane. Assume \\(\\gamma\\) is unit-speed, </p> \\[\\sigma_u = \\gamma'(u) + v\\gamma''(u), \\sigma_v = \\gamma'(u)\\] \\[\\begin{align*} N &amp;= \\sigma_u\\times \\sigma_v\\\\   &amp;= (\\gamma' + v\\gamma'')\\times (\\gamma')\\\\   &amp;= (\\mathbf t \\times \\mathbf t) + v\\kappa\\mathbf n\\times \\mathbf t\\\\   &amp;= 0 + v\\kappa(-\\mathbf b)\\\\   &amp;= -v\\kappa\\mathbf b \\end{align*}\\] <p>Therefore, tangent developables are regular IFF \\(v\\neq 0, \\kappa&gt;0\\)</p> \\[\\begin{align*} E &amp;= \\|\\gamma'\\|^2 + v\\gamma'\\cdot\\gamma'' + v^2\\|\\gamma''\\|^2 = 1 + v^2\\kappa^2\\\\ F &amp;= (\\gamma' + v\\gamma'')\\cdot \\gamma' = 1\\\\ G &amp;= \\gamma'\\cdot\\gamma' = 1 \\end{align*}\\] <p>Note that there exists a unit-speed plane curve with signed curvature being \\(\\kappa\\). And note that it's a tangent developable in a plane, hence has the same first fundamental form. </p>"},{"location":"mat363/isometry.html#example-cone","title":"Example: Cone","text":"<p>Consider the standard circular cone </p> \\[\\sigma: (0, \\infty)\\times (0, 2\\pi) \\rightarrow \\mathbb R^3. \\sigma(u,v) = (u\\cos v, u\\sin v, u)\\] <p>Find its isometry from the xy-plane. </p> <p>First, its 1st fundamental form is </p> \\[\\sigma_u = (\\cos v, \\sin v, 1), \\sigma_v = (-u\\sin v, u\\cos v, 0)\\] \\[E = 2, F = 0, G = u^2\\] <p>Then, to map from xy-plane, we want some \\(x(u, v)\\) and \\(y(u,v)\\) s.t. \\(\\tilde\\sigma(u, v) = (x(u, v), y(u, v), 0)\\) and the same 1st fundamental form</p> \\[x_u^2 + y_u^2 = 2, x_v^2 + y_v^2 = u^2, x_ux_v + y_uy_v = 0\\] \\[x = \\sqrt{2} u\\cos(v/\\sqrt 2), y = \\sqrt{2} u\\sin(v/\\sqrt 2)\\]"},{"location":"mat363/isometry.html#example-catenoid-to-helicoid","title":"Example: Catenoid to Helicoid","text":"<p>A catenoid is parameterized as \\(\\sigma^c(u, v) = (\\cosh u\\cos v, \\cosh u\\sin v, u)\\) A helicoid is parameterized as \\(\\sigma^t(u, v) = (u\\cos v, \\sin v, v)\\)</p> <p>Claim the map from \\(\\sigma^c(u,v) \\implies \\sigma^t(\\sinh u, v)\\) is an isometry.</p> <p>proof. (some computations are skipped)</p> \\[E^c = \\sinh^2 u + 1 = \\cosh^2 u, E^t = \\cosh^2 u\\] \\[F^c = (-1 + 1)\\sinh u\\cos h u \\sin v\\cos v = 0 = F^c\\] \\[G^c = \\cosh^2 u(\\cos^2 v + \\sin^2 v) = \\cosh^2, G^t = \\sinh^2 u + 1 = \\cosh^2 u\\] <p>Define an isometric deformation of the catenoid into a helicoid. First define </p> \\[\\sigma^{-t}(u,v) = (-\\sin h u\\sin v, \\sinh u\\cos v, -v)\\] <p>which reflect \\(\\sigma^{t}\\) in the xy-plane and translating it by \\(\\pi/2\\) parallel to the z-axis. Then, define </p> \\[\\sigma^{ct} = \\cos(t) \\sigma^c(u,v) + \\sin(t)\\sigma^{-t}(u,v)\\] <p>t Claim This isometric deformation is a local isometry, regardless of the choice of \\(t\\). </p> <p>proof. First, note that the first fundamental forms of \\(\\sigma^c,\\sigma^t, \\sigma^{-t}\\) are all the same since they are all isometric (rotation and translation are isometric transformations).  Then, note that \\(\\cos^2 t+ \\sin^2 t = 1\\) and \\(t\\) is not related to \\(u,v\\), hence are scalars in partial derivatives. Therefore, as \\(\\sigma^{ct}\\) is a linear transformation of \\(\\sigma^c, \\sigma^{-t}\\), it is also isometric. </p>"},{"location":"mat363/isometry.html#conformal-mappings-of-surfaces","title":"Conformal Mappings of Surfaces","text":"<p>For \\(f: \\Sigma_1\\rightarrow\\Sigma_2\\) being a local diffeomorphism, \\(f\\) is a conformal map if for any two curves \\(\\gamma_1, \\gamma_2\\) on \\(\\Sigma_1\\), The angle at intersection \\(p\\) is equal to the angle between \\(f(\\gamma_1(\\mathbb R)), f(\\gamma_2(\\mathbb R))\\) at point \\(f(p)\\). </p> <p>In short, \\(f\\) is conformal IFF it preserves angles. </p> <p>Theorem \\(f\\) is conformal IFF exists \\(\\lambda: \\Sigma_1\\rightarrow \\mathbb R\\) s.t. </p> \\[\\forall p \\in \\Sigma_1. f^*\\langle v,w\\rangle_p = \\lambda(p) \\langle v,w\\rangle_p\\] <p>proof. \\(\\Rightarrow\\) Assume that the angle is preserved </p> \\[\\frac{\\langle \\gamma_1', \\gamma_2'\\rangle}{\\langle  \\gamma_1', \\gamma_1'\\rangle^{1/2}\\langle  \\gamma_2', \\gamma_2'\\rangle^{1/2}} = \\frac{f^*\\langle \\gamma_1', \\gamma_2'\\rangle}{f^*\\langle  \\gamma_1', \\gamma_1'\\rangle^{1/2}f^*\\langle  \\gamma_2', \\gamma_2'\\rangle^{1/2}}\\] <p>Since \\(\\gamma_1,\\gamma_2\\) are choose arbitrarily, this is equalent to any vecto on \\(T_p\\Sigma_1\\). Take \\(\\{v_1, v_2\\}\\) be the orthonormal basis of \\(T_p\\Sigma_1\\). Let \\(v = v_1, w = \\cos\\theta v_1 + \\sin\\theta v_2\\) so that \\(v\\cdot w = \\cos \\theta (v_1\\cdot v_1) + \\sin\\theta (v_1\\cdot v_2) = \\cos\\theta\\)</p> <p>Note that \\(f^*\\langle \\cdot, \\cdot\\rangle\\) is also dot product restricted to \\(T_{f(p)}\\Sigma_2\\). Therefore, </p> \\[\\begin{align*} f^*\\langle v, v\\rangle &amp;= f^*\\langle v_1, v_1\\rangle\\\\ f^*\\langle v, w\\rangle &amp;= cos\\theta f^*\\langle v_1, v_1\\rangle + \\sin\\theta  f^*\\langle v_1, v_2\\rangle\\\\ f^*\\langle w, w\\rangle &amp;= cos^2\\theta f^*\\langle v_1, v_1\\rangle  + 2\\sin\\theta\\cos\\theta f^*\\langle v_1, v_2\\rangle + \\sin^2\\theta f^*\\langle v_2, v_2\\rangle\\\\ \\end{align*}\\] <p>Write \\(\\lambda = f^*\\langle v_1, v_1\\rangle, \\mu = f^*\\langle v_1, v_2\\rangle, \\nu = f^*\\langle v_2, v_2\\rangle\\) and by our assumption, we have </p> \\[\\cos\\theta = \\frac{\\lambda \\cos\\theta + \\mu\\sin\\theta}{\\lambda^{1/2}(\\lambda\\cos^2\\theta + 2\\mu\\sin\\theta\\cos\\theta + \\nu\\sin^2\\theta)^{1/2}}\\] <p>We can solve it for \\(\\theta = \\pi/2\\), implying </p> \\[\\lambda = \\lambda\\cos^2\\theta + \\nu\\sin^2\\theta\\] <p>Therefore, \\(\\lambda = \\nu\\implies \\lambda = f^*\\)</p> <p>\\(\\Leftarrow\\), Assume that \\(\\forall p \\in \\Sigma_1. f^*\\langle v,w\\rangle_p = \\lambda(p) \\langle v,w\\rangle_p\\), then \\(\\lambda\\)'s cancles out in the angle equation. </p> \\[\\frac{\\lambda(p)\\langle \\gamma_1', \\gamma_2'\\rangle}{  \\lambda(p)^{1/2}\\langle\\gamma_1', \\gamma_1'\\rangle^{1/2}\\lambda(p)^{1/2}\\langle  \\gamma_2', \\gamma_2'\\rangle^{1/2}} = \\frac{\\langle \\gamma_1', \\gamma_2'\\rangle}{\\langle  \\gamma_1', \\gamma_1'\\rangle^{1/2}\\langle  \\gamma_2', \\gamma_2'\\rangle^{1/2}}\\] <p>Corollary \\(f\\) is conformal IFF for any patch \\(\\sigma_1\\) of \\(\\Sigma_1\\) and \\(\\sigma_2 = f\\circ \\sigma_1\\) of \\(\\Sigma_2\\), their first fundamental forms are proportional. </p>"},{"location":"mat363/isometry.html#example-stereographic-projection","title":"Example: Stereographic Projection","text":"<p>Consider a standard sphere \\(S = \\{x^2 + y^2 + z^2 = 1\\}\\) and XY-plane. Define \\(\\mathbf n = (0, 0, 1)\\), i.e. the north pole of the sphere. The sterepographic projection maps \\(\\mathbf q \\in S\\) to \\(\\mathbf p \\in \\Pi_{XY}\\) s.t. \\(n, p, q\\) lie on the same straight line, i.e. </p> \\[\\mathbf q - \\mathbf n = c(\\mathbf p-\\mathbf n), \\|\\mathbf p\\|^2 = 1\\] <p>Now we have 3 equations and 3 unknowns if \\(\\mathbf q\\) known, or 4 if \\(\\mathbf p\\) known</p> \\[\\begin{align*} x - 0 = c(u - 0)&amp;\\implies x &amp;= cu\\\\ y - 0 = c(v - 0)&amp;\\implies y &amp;= cv\\\\ z - 1 = c(0 - 1)&amp;\\implies 1 - z &amp;= c\\\\ &amp;x^2 + y^2 + z^2 &amp;= 1 \\end{align*}\\] <p>Therefore, the sterepographic projection \\(\\Pi: S^2-\\{\\mathbf n\\} \\rightarrow \\Pi_{XY}\\) is defined as</p> \\[\\Pi(x, y, z) = (\\frac{x}{1-z}, \\frac{y}{1-z}, 0)\\] <p>and a parameterization of \\(S^2-\\{\\mathbf n\\}\\) is</p> \\[\\sigma_1(u, v) = \\frac{1}{u^2+v^2+1}(2u, 2v, u^2+v^2-1)\\] <p>Claim sterepographic projection \\(\\Pi\\) is conformal.  Let \\(\\sigma_2 (u, v) = (u, v, 0)\\) be a parameterization of XY-plane, and we have that \\(\\Pi\\circ\\sigma_1 = \\sigma_2\\), we need to show that first fundamental form of \\(\\sigma_1\\) is proportional to that of \\(\\sigma_2\\).</p> \\[\\begin{align*} \\sigma_{1u} &amp;= \\frac{-2u}{(u^2+v^2+1)^2}(2u, 2v, u^2+v^2-1) + \\frac{1}{u^2+v^2+1}(2, 0, 2u)\\\\ &amp;= \\frac{1}{(u^2+v^2+1)^2}(2(v^2-u^2+1), -4uv, 4u)\\\\ \\sigma_{1v} &amp;= \\frac{1}{(u^2+v^2+1)^2}(-4uv, 2(u^2-v^2+1), 4u)\\\\ E_1 &amp;= \\frac{1}{(u^2+v^2+1)^4}(4(v^2-u^2+1)^2 + 16u^2v^2 + 16u^2) = \\frac{4}{(u^2+v^2+1)^2}\\\\ G_1 &amp;=  \\frac{4}{(u^2+v^2+1)^2}\\\\ F_1 &amp;=  \\frac{1}{(u^2+v^2+1)^4}(-(8uv(v^2-u^2 + 1+u^2-v^2+1) + 16uv) = 0 \\end{align*}\\] <p>Note that \\(E_2= G_2 = 1, F_2 = 0\\), so that take </p> \\[\\lambda =  \\frac{4}{(u^2+v^2+1)^2}\\] <p>and we have the conclusion.</p> Source code <pre><code>import plotly.graph_objects as go\nimport numpy as np\n\nu, v = np.mgrid[-2:2:20j, -2:2:20j]\nS = go.Surface(\n    x=u - u**3 / 3 + u * v * v,\n    y = v - v**3/3 + v*u*u,\n    z=u*u-v*v\n)\nfig = go.Figure(data=[S])\n\nfig.update_traces(showscale=False)\nfig.update_layout(margin=dict(l=0, r=0, b=0, t=0))\nwith open(\"../assets/isometry.json\", \"w\") as f:\n    f.write(fig.to_json())\n</code></pre>"},{"location":"mat363/isometry.html#example-ennepers-surface","title":"Example: Enneper's Surface","text":"<p>Parameterize the surface as </p> \\[\\sigma(u, v) = (u - \\frac{u^3}{3} + uv^2, v - \\frac{v^3}{3} + vu^2, u^2-v^2)\\] <p>Claim The Enneper's Surface is conformally parameterized.  The first fundamental form is</p> \\[\\sigma_u = (1-u^2+v^2, 2uv, 2u), \\sigma_v = (2uv, 1-v^2+u^2, -2v)\\] \\[E = (1-u^2+v^2)^2 + 4u^2v^2+4u^2 = 1+u^2+v^2\\] \\[G = E = 1+u^2+v^2\\] \\[F = 4uv-4uv = 0\\] <p>Define \\(\\lambda(u,v) = 1+u^2+v^2\\), this surface is conformal to the plane. </p>"},{"location":"mat363/isometry.html#example-mercator-parameterization","title":"Example: Mercator Parameterization","text":"<p>Known that the first fundamental form for the sphere is \\(du^2 + \\cos^2 udv'^2\\), find a smooth function \\(\\phi\\) s.t. the reparameterization \\(\\tilde \\sigma (u,v) = \\sigma(\\phi(u), v)\\) is conformal. </p> <p>Note that the Jacobian of the transformation is </p> \\[J = \\begin{bmatrix}\\partial_{\\tilde u} u &amp;\\partial_{\\tilde v} u\\\\\\partial_{\\tilde u} v&amp;\\partial_{\\tilde v} v\\end{bmatrix} = \\begin{bmatrix}\\phi' &amp;0\\\\0&amp;1\\end{bmatrix}\\] <p>so that the first fundamental form of \\(\\tilde \\sigma\\) is</p> \\[\\tilde E = (\\phi'^2 E) = \\phi'^2, \\tilde F = (\\phi' F) = 0, \\tilde G = G = \\cos^2 (\\phi(u)) \\] <p>Therefore, \\(\\tilde \\sigma\\) is conformal IFF \\(\\phi'^2 = \\cos(\\phi)\\)</p> <p>We can verify this on Mercator Parameterization, taking \\(\\cos\\phi = \\text{sech} u\\) and we can verify that this is proven. </p>"},{"location":"mat363/isometry.html#equiareal-map","title":"Equiareal Map","text":"<p>For \\(f: \\Sigma_1\\rightarrow \\Sigma_2\\) by a local diffeomorphism, \\(f\\) is equiareal if for any open subset \\(W\\subset \\Sigma_1\\) has the same area as \\(f(W)\\subset \\Sigma_2\\). </p> <p>Theorem \\(f: \\Sigma_1\\rightarrow \\Sigma_2\\) is equiareal IFF </p> \\[E_1G_1-F_1^2 = E_2G_2- F_2^2\\] <p>proof. By the integral of area (see First Fundamental Form), and this comes directly from the equation. </p>"},{"location":"mat363/isometry.html#archimedes-theorem","title":"Archimedes' Theorem","text":"<p>Let \\(S^2 = \\{(x,y,z): x^2+y^2+z^2 = 1\\}\\) be the unit sphere and \\(C = \\{(x,y,z): x^2+y^2 = 1\\}\\) be the unit cylinder. For each point \\(p = (x,y,z)\\in S^2 -\\{(0,0,1), (0, 0, -1)\\}\\), \\(p\\) can be mapped to \\(q =(X,Y,z)= f(p)\\in C\\) via the ray connecting \\((0, 0, z), p, q\\).</p> \\[f: (x,y,z)\\rightarrow (X = \\lambda(x,y,z)x, Y = \\lambda(x,y,z)y, z)\\] <p>Note that \\(X^2 + Y^2 = 1\\implies \\lambda(x^2 + y^2) = 1\\implies \\lambda = \\frac{1}{\\sqrt{x^2+y^2}}\\), so that we can obtain </p> \\[f(x,y,z) = (\\frac{x}{\\sqrt{x^2+y^2}}, \\frac{y}{\\sqrt{x^2+y^2}}, z)\\] <p>Claim \\(f\\) is equiareal. </p> <p>proof. Parameterize \\(S^2\\) using \\(\\sigma_1(u,v) = (\\cos u \\cos v, \\cos u \\sin v, \\sin u)\\), so that </p> \\[\\sigma_2: \\mathbb R^2\\rightarrow C. \\sigma_2 = f\\circ\\sigma_1 = (\\cos v, \\sin v, \\sin u)\\] <p>For \\(\\sigma_1\\), this is the sphere, known that \\(E_1 = 1, F_1 = 0, G_1 = \\cos^2 u\\) For \\(\\sigma_2\\), \\(E_2 = \\|(0, 0, \\cos u)\\|^2 = \\cos^2 u, G_2 = \\|(-\\sin v, \\cos v, 0)\\|^2 = 1, F_2 = 0\\).  Therefore, \\(E_1G_1-F_1^2 = E_2G_2 - F_2^2\\). Hence it is equiareal. </p> <p>Lemma \\(A\\times B \\times C = (A\\cdot C)B - (A\\cdot B)C\\)</p> <p>proof. Note that \\(S = A\\times (B\\times C)\\) will be perpendicular to \\(A\\) and \\(B\\times C\\), hence should reside on \\(\\text{span}\\{B,C\\}\\). Let \\(S = mB + nC\\), Note that \\(S\\cdot A = 0\\), </p> \\[mB\\cdot A + nC\\cdot A = 0\\] <p>This equation must be valid for any \\(A,B,C\\), so that let \\(m =  (C\\cdot A), n = - (B\\cdot A)\\) and we obtain the equation in the claim. </p> <p>Theorem For \\(\\sigma(u,v)\\) be a surface patch, and $ N$ be the unit normal. </p> \\[N \\times \\sigma_u = \\frac{E\\sigma_v - F\\sigma_u}{\\sqrt{EG - F^2}}, N \\times \\sigma_v = \\frac{F\\sigma_v - G\\sigma_v}{\\sqrt{EG - F^2}}\\] <p>proof. First, note that \\(N = \\frac{\\sigma_u\\times\\sigma_v}{\\|\\sigma_u \\times \\sigma_v\\|} = \\frac{\\sigma_u\\times\\sigma_v}{\\sqrt{EG-F^2}}\\). Then, apply the lemma on the equations. </p>"},{"location":"mat363/minimal_surface.html","title":"Minimal Surfaces","text":""},{"location":"mat363/minimal_surface.html#minimal-surfaces_1","title":"Minimal Surfaces","text":"<p>Similar to the shortest paths, we can study a family of surface patches \\(\\sigma^{\\tau}:U\\rightarrow\\mathbb R^3\\) and let \\(\\sigma = \\sigma^0\\). And the family must be smooth, a.k.a the map </p> \\[M: \\{(u,v,\\tau): (u,v) \\in U, \\tau\\in(-\\delta, \\delta)\\}\\rightarrow \\mathbb R^3, M(u,v,\\tau):=\\sigma^{\\tau}(u,v)\\] <p>is smooth. Then define the surface variation of the family as \\(\\varphi: U\\rightarrow\\mathbb R^4 := \\dot\\sigma^{\\tau}\\mid_{\\tau = 0}\\)</p> <p>For some simplse closed curve \\(\\pi: (a,b)\\rightarrow U\\) where the curve and its interior is in \\(U\\), then \\(\\gamma^\\tau := \\sigma^\\tau\\circ \\pi\\) is also a closed curve in \\(\\sigma^\\tau\\), we can then define the area function of the enclosed region as </p> \\[A(\\tau) = \\int_{int(\\pi)}dA_{\\sigma^\\tau}\\] <p>Then, if a family of surfaces has a fixed boundary curve \\(\\gamma\\), which means \\(\\forall \\tau, \\gamma^\\tau = \\gamma\\), then we have that \\(\\varphi(u,v) = \\vec 0\\) for \\((u,v)\\) on the curve \\(\\pi\\). </p> <p>Theorem If the surface variation \\(\\varphi^tau\\) vanishes along the boundary curve \\(\\pi\\), then </p> \\[A'(0) = -2\\int_{int(\\pi)} H(EG-F^2)^{1/2} (\\varphi\\cdot\\mathbf N) dudv\\] <p>Then, similar to how we define shortest path. Intuitively, a surface \\(\\Sigma\\) is a minimal surface if \\(\\Sigma\\) has the leastarea among all surfaces with the same boundary curve. Which means its surface patches \\(\\sigma = \\sigma^0, A'(0) = 0\\). Therefore, this formula is possible only if \\(H=0\\) for all points of \\(U\\).  </p> <p>Therefore, a minimal surface is a surface whose mean curvature is zero everywhere. </p>"},{"location":"mat363/minimal_surface.html#gaussian-curvature-of-minimal-surfaces","title":"Gaussian Curvature of minimal surfaces","text":"<p>Theorem Gaussian curvature of a minimal surface is \\(\\leq 0\\) everywhere, and \\(=0\\) IFF the surface is a open subset of a plane. </p> <p>proof. By minimal surface, we have that \\(H = \\kappa_1 + \\kappa_2 = 0\\) for principal curvatures \\(\\kappa_1,\\kappa_2\\), thus \\(K=\\kappa_1\\kappa_2 = -\\kappa_1^2 \\geq 0\\). If \\(K = 0\\), then at least one of \\(\\kappa_1,\\kappa_2 = 0\\), by minimal surface, both of them must be 0. </p>"},{"location":"mat363/minimal_surface.html#parallel-surfaces","title":"Parallel Surfaces","text":"<p>For an oriented surface \\(\\Sigma\\) and some \\(\\lambda \\in \\mathbb R\\). The parallel surface \\(\\Sigma^{\\lambda}\\) is </p> \\[\\Sigma^\\lambda = \\{\\mathbf p + \\lambda \\mathbf N_p:p\\in\\Sigma\\}\\] <p>a.k.a. translate the surface \\(\\Sigma\\) along the unit normal for a distance of \\(\\lambda\\). </p> <p>Theorem \\(\\sigma^\\lambda_u\\times \\sigma^\\lambda_v = (1-\\lambda\\kappa_1)(1-\\lambda\\kappa_2) (\\sigma_u\\times\\sigma_v)\\)</p> <p>proof. \\(\\sigma^\\lambda = \\sigma + \\lambda \\mathbf N\\), if we write derivatives of the normal in the basis of \\(\\sigma_u,\\sigma_v\\), </p> \\[\\sigma^\\lambda_u = \\sigma_u + \\lambda \\mathbf N_u =  \\sigma_u + \\lambda(-a\\sigma_u - b\\sigma_v) = (1-\\lambda a)\\sigma_u - \\lambda b \\sigma_v\\] \\[\\sigma_v^\\lambda = (1-\\lambda d)\\sigma_v  - \\lambda c\\sigma_u\\] <p>Therefore, the cross product is </p> \\[\\begin{align*} \\sigma^\\lambda_u \\times \\sigma^\\lambda_v &amp;= ((1-\\lambda a)\\sigma_u - \\lambda b \\sigma_v) \\times ((1-\\lambda d)\\sigma_v  - \\lambda c\\sigma_u)\\\\ &amp;= (1-\\lambda a - \\lambda d + \\lambda^2 (ad-bc))(\\sigma_u\\times \\sigma_v)\\\\ &amp;= (1-\\lambda tr(W) + \\lambda^2 \\det(W))(\\sigma_u\\times \\sigma_v)\\\\ &amp;= (1 - \\lambda(\\kappa_1 + \\kappa_2) + \\lambda^2\\kappa_1\\kappa_2)(\\sigma_u\\times \\sigma_v)\\\\ &amp;= (1-\\lambda\\kappa_1)(1-\\lambda\\kappa_2) (\\sigma_u\\times\\sigma_v) \\end{align*}\\] <p>Theorem Let \\(\\sigma\\) be a minimal surface patch, let \\(U\\) be some region s.t. the bounded area of the region on \\(\\sigma\\) is finite, a.k.a. \\(A_\\sigma(U) &lt; \\infty\\). Let \\(\\lambda \\neq 0\\) and assume that the principal curvatures \\(\\kappa\\) of \\(|\\lambda \\kappa| &lt; 1\\) everywhere, so that the parallel surface \\(\\sigma^\\lambda\\) is a regular surface patch. Then,  \\(A_{\\sigma^\\lambda}(U) \\leq A_{\\sigma}(U)\\) and the equality holds IFF \\(\\sigma(U)\\) is an open subset of a plane. </p> \\[\\begin{align*} A_{\\sigma^\\lambda}(U) &amp;= \\iint_U \\|\\sigma_u^\\lambda \\times \\sigma_v^\\lambda\\|dudv\\\\ &amp;= \\iint_U |1-\\lambda\\kappa_1||1-\\lambda\\kappa_2| \\|\\sigma_u\\times\\sigma_v\\|dudv\\\\ &amp;= \\iint_U |1-\\lambda\\kappa_1||1 + \\lambda\\kappa_1| \\|\\sigma_u\\times\\sigma_v\\|dudv &amp;\\kappa_1 = -\\kappa_2\\\\ &amp;= \\iint_U |1-\\lambda^2 \\kappa_1^2| \\|\\sigma_u\\times\\sigma_v\\|dudv\\\\ &amp;\\leq \\iint_U \\|\\sigma_u\\times\\sigma_v\\|dudv &amp;|1-\\lambda^2 \\kappa_1^2| &lt; 1\\\\ &amp;= A_{\\sigma}(U) \\end{align*}\\] <p>The equality IFF \\(|1-\\lambda^2 \\kappa_1^2| = 1\\) for all possible \\(\\lambda\\), implying \\(\\kappa_1 = -\\kappa_2 = 0\\) IFF \\(\\sigma\\) is an open subset of plane.  </p>"},{"location":"mat363/minimal_surface.html#compact-surface","title":"Compact Surface","text":"<p>A set of \\(\\mathbb R^3\\) is compact iff closed and bounded, and it attains its maximum and minimum for any continuous function \\(f:\\mathbb R^3\\rightarrow\\mathbb R\\). Since a surface is a subset of \\(\\mathbb R^3\\), this also applies to surfaces. </p> <p>Theorem If \\(\\Sigma\\) is compact, then exists \\(p\\in Sigma\\) where \\(K(p) &gt; 0\\), \\(K\\) is the Gaussian curvature. </p> <p>proof. Let \\(f(x) = \\|x\\|^2\\), \\(f\\) is continuous, take \\(p\\in\\Sigma\\) s.t. \\(f(p)\\) attains the maximum on \\(\\Sigma\\). Since \\(f(p)\\) attains its maximum, it implies that the surface must be contained in the sphere center at 0 of radius \\(\\|p\\|\\). Thus, the Gaussian curvature at \\(p\\) is at least the Gaussian curvature of the sphere \\(K(p) \\geq \\|p\\|^{-2} &gt; 0\\).</p> <p>Corollary There is no compact minimal surface. </p> <p>proof. Minimal surface means \\(K\\leq 0\\) for all points on the surface, contradicting with the fact that \\(K &gt; 0\\) for some point on the compact surface. </p>"},{"location":"mat363/minimal_surface.html#isometry-of-minimal-surfaces","title":"Isometry of Minimal Surfaces","text":"<p>Theorem Applying an isometry or dilation on a minimal surface gives another minimal surface.  proof. The first fundamental form does not change after the isometry, and the second fundamental form differs by a scale (\\(\\times a\\) for dilation, \\(a=\\pm 1\\) for isometry). hence \\(H = a0 = 0\\). </p> <p>Theorem Applying a local isometry does not necessarily gives another minimal surface. proof. Consider from a plane to a cylinder. </p>"},{"location":"mat363/minimal_surface.html#examples-of-minimal-surfaces","title":"Examples of Minimal Surfaces","text":""},{"location":"mat363/minimal_surface.html#catenoid-and-surfaces-of-revolution","title":"Catenoid and Surfaces of Revolution","text":"<p>For catenoid \\(\\sigma(u,v) = (\\cosh u \\cos v, \\cosh u\\sin u, u)\\), we can compute its first and second fundamental form</p> \\[E=G=\\cosh^2 u, F=0, L=-1, M=0,N=1\\] <p>thus, its mean curvature is </p> \\[H = \\frac{LG-2MF+NE}{2(EG-F^2)} = 0\\] <p>Catenoid is a minimal surface.</p> <p>Note that the theorem only applies to closed region bounded by a curve, for example</p> <p>For \\(a &gt; 0, b = \\cosh a\\). The surface bounded by \\(|z| &lt; a\\) of two circles of radius \\(b\\) has the area</p> \\[A = \\int_0^{2\\pi}\\int_{-a}^a \\sqrt{EG-F^2} dudv = \\int_0^{2\\pi}\\int_{-a}^a \\cosh^2 u dudv = 2\\pi(a+\\sinh a\\cosh a)\\] <p>However, this two circles also bounds the area of two circles of radius \\(b\\), where the total area is \\(2\\pi b = 2\\pi \\cosh a\\).  </p> <p>Theorem Any minimal surface of revolution is an open subset of a plane or a catenoid. </p>"},{"location":"mat363/minimal_surface.html#helicoid-and-ruled-surfaces","title":"Helicoid and Ruled Surfaces","text":"<p>A helicoid is obtained by rotating a straight line at a constant speed, and move along an axis perpendicular to the line at constant speed. </p> <p>By isometry, we define a helicoid of a line on the x-axis at a constant angular speed \\(\\omega\\), along the z-axis at constant speed \\(a\\). Thus the parameterization is </p> \\[\\sigma(u,v) = (u\\cos(\\omega v), u\\sin(\\omega v), av)\\] <p>The first and second fundamental form is </p> \\[E= 1, F=0, G=(u^2 + a^2), L = N = 0, M = \\frac{a^2}{\\sqrt{a^2 + u^2} }\\] <p>Thus, the mean curvature is \\(H = 0\\) since \\(L = F = N = 0\\)</p> <p>Theorem Any minimal ruled surface is an open subset of a plane or a helicoid. </p>"},{"location":"mat363/minimal_surface.html#catalans-surface","title":"Catalan's Surface","text":"<p>The Catalan's surface can be parameterized as </p> \\[\\sigma(u,v) = (u-\\sin u \\cosh v, 1 - \\cos u\\cosh v, -4 \\sin\\frac{u}{2}\\sinh\\frac{v}{2})\\] <p>The first fundamental form is </p> \\[(\\cosh v + 1)(\\cosh v - \\cos u)(du^2 + dv^2)\\] <p>Since \\(E=G\\), to make \\(H = 0\\), we need \\(L=-N\\), a.k.a \\(\\sigma_{uu} = -\\sigma_{vv}\\), which can be verified. </p> <p>Then, take \\(u = 0, \\gamma(v) = (0, 1 + \\cosh v, 0)\\) is a straight line, it is a geodesic. </p> <p>Take \\(u = \\pi, \\gamma(v) = (\\pi - \\cosh v, 1, -4 \\sinh(v/2))\\) is the parabola \\(z^2 = 8(y-2)\\), using the geodesic euqation, we can verify this is a geodesic. </p> <p>Take \\(v = 0, \\gamma(u) = (u-\\sin u, 1-cos u, 0)\\) is a cycloid, using the geodesic euqation, we can verify this is a geodesic.</p> Source code <pre><code>import plotly.graph_objects as go\nfrom numpy import sin, cos, sinh, cosh, pi, mgrid\n\nu, v = mgrid[-2 * pi: 2 * pi:20j, -0.5*pi:0.5*pi:10j]\n\nfig = go.Figure(data=[go.Surface(\n    x=u - sin(u) * cosh(v), \n    y=1 - cos(u) * cosh(v), \n    z=-4*sin(u/2) * sinh(v/2)\n    )])\nfig.update_traces(showscale=False)\nfig.update_layout(margin=dict(l=0, r=0, b=0, t=0))\n\nwith open(\"../assets/catalan.json\", \"w\") as f:\n    f.write(fig.to_json())\n</code></pre>"},{"location":"mat363/parallel_transport.html","title":"Parallel Transport","text":""},{"location":"mat363/parallel_transport.html#christoffel-symbols-gauss-equations","title":"Christoffel Symbols (Gauss Equations)","text":"<p>Consider some surface \\(\\Sigma\\) with (locally) parameterization \\(\\sigma\\). Note that \\(\\sigma_u, \\sigma_v, \\mathbf N\\) spans \\(\\mathbb R^3\\), hence we can write the second order derivatives as</p> \\[\\begin{align*} \\sigma_{uu} &amp;= a_1\\sigma_u + a_2\\sigma_v + a_3\\mathbf N\\\\ \\sigma_{uv} &amp;= b_1\\sigma_u + b_2\\sigma_v + b_3\\mathbf N\\\\ \\sigma_{vv} &amp;= c_1\\sigma_u + c_2\\sigma_v + c_3\\mathbf N\\\\ \\end{align*}\\] <p>Now, consider the second fundamental form</p> \\[L = \\sigma_{uu}\\cdot N = a_1\\sigma_u\\cdot N + a_2\\sigma_v \\cdot N + a_3\\mathbf N\\cdot \\mathbf N = 0 + 0 + a_3  = a_3\\] <p>we have that </p> \\[a_3 = L, b_3 = M, c_3 = N\\] <p>Then, we take dot product with \\(\\sigma_u, \\sigma_v\\)</p> \\[\\begin{align*} \\sigma_{uu}\\cdot \\sigma_u &amp;= a_1 (\\sigma_u\\cdot\\sigma_u) + a_2(\\sigma_v\\cdot \\sigma_u) + a_3 (\\mathbf N\\cdot\\sigma_u) = Ea_1 + Fa_2\\\\ \\sigma_{uu}\\cdot \\sigma_v &amp;= F a_1 + Ga_2\\\\ \\sigma_{uv}\\cdot\\sigma_u &amp;= Eb_1 + Fb_2\\\\ \\sigma_{uv}\\cdot\\sigma_v &amp;= Fb_1 + Gb_2\\\\ \\sigma_{vv}\\cdot\\sigma_u &amp;= Ec_1+ Gc_2\\\\ \\sigma_{vv}\\cdot\\sigma_v &amp;= Fc_1+Gc_2\\\\ \\end{align*}\\] <p>On the other hand, we have that </p> \\[\\begin{align*} &amp;E_u = (\\sigma_u\\cdot\\sigma_u)_u = 2 \\sigma_{uu}\\cdot\\sigma_{u} &amp;E_v = 2\\sigma_{uv} \\cdot\\sigma_{u}\\\\ &amp;F_u = \\sigma_{uu}\\cdot \\sigma_{uv} &amp;F_v = \\sigma_{vv}\\cdot\\sigma_{uv}\\\\ &amp;G_u = 2\\sigma_{uv}\\cdot \\sigma_{v} &amp;G_v = 2\\sigma_{vv}\\cdot \\sigma_{v} \\end{align*}\\] <p>Combine the equations, we have 6 equations w.r.t. to the 6 unknowns</p> \\[\\begin{align*} E a_1 + F a_2 &amp;= \\frac{1}{2}E_u&amp;F a_1 + G a_2 = F_u - \\frac{1}{2}E_v\\\\ E b_1 + F b_2 &amp;= \\frac{1}{2}E_v&amp;F b_1 + G b_2 = \\frac{1}{2}G_u\\\\ E c_1 + F c_2 &amp;= F_v - \\frac{1}{2}G_u&amp;F c_1 + G c_2 = \\frac{1}{2}G_v \\end{align*}\\] <p>The solution is that </p> \\[\\begin{align*} \\Gamma_{11}^1 = \\frac{GE_u - 2FF_u+FE_v}{2(EG-F^2)}&amp;&amp;\\Gamma_{11}^2 = \\frac{2EF_u-EEv-FEu}{2(EG-F^2)}\\\\ \\Gamma_{12}^1 = \\frac{GE_v - FG_u}{2(EG-F^2)}&amp;&amp;\\Gamma_{12}^2 = \\frac{EG_u-FE_v}{2(EG-F^2)}\\\\ \\Gamma_{22}^1 = \\frac{2GF_v-GG_u-FG_v}{2(EG-F^2)}&amp;&amp;\\Gamma_{22}^2 = \\frac{EG_v-2FF_v+FG_u}{2(EG-F^2)} \\end{align*}\\] <p>so that the equations are called Gauss Equations </p> \\[\\begin{align*} \\sigma_{uu} &amp;= \\Gamma_{11}^1\\sigma_u + \\Gamma_{11}^2\\sigma_v + L\\mathbf N\\\\ \\sigma_{uv} &amp;= \\Gamma_{12}^1\\sigma_u + \\Gamma_{12}^2\\sigma_v + M\\mathbf N\\\\ \\sigma_{vv} &amp;= \\Gamma_{22}^1\\sigma_u + \\Gamma_{22}^2\\sigma_v + N\\mathbf N\\\\ \\end{align*}\\] <p>and the \\(\\Gamma\\)'s are Christoffel symbols. </p>"},{"location":"mat363/parallel_transport.html#covariant-derivative","title":"Covariant Derivative","text":"<p>Consider a particle's trajectory on some surface \\(\\Sigma\\) as \\(\\gamma:(a, b) \\rightarrow\\Sigma\\) and its velocity is the tangent vector \\(\\mathbf v = \\gamma'\\) and acceleration \\(\\mathbf a = \\mathbf v' = \\gamma''\\). Suppose that \\(\\Sigma\\) is a plane, then \\(\\gamma\\) can be considered as a 2D curve, and both \\(\\mathbf v\\) and \\(\\mathbf a\\) resides on the plane. </p> <p>However, \\(\\Sigma\\) is not always a plane. For each \\(t, \\mathbf v(t) \\in T_{\\gamma(t)}\\Sigma\\) being a vector field, which is (most likely) constantly changing. Since each tangent plane is spanned by \\(\\sigma_u(t), \\sigma_v(t)\\), we can write </p> \\[\\mathbf v(t) = a(t)\\sigma_u(t) + b(t)\\sigma_v(t)\\] <p>Then, acceleration \\(\\mathbf a= \\mathbf v'\\) is a 3D vector, hence can be spanned by \\(\\{\\sigma_u(t), \\sigma_v(t), \\mathbf N(t)\\}\\). Then, consider the particle's perspective, since it moves along the surface, from its perspective, it treats its movement as on a plane, and only perceive the change on this plane (think of car's velocity, while earth is actually a sphere). Therefore, we can interested in the perceived rate of change of \\(\\mathbf v\\) as the tangential component of \\(\\mathbf v'\\), a.k.a. the orthogonal projection of \\(\\mathbf v'\\) onto \\(T_{\\gamma(t)}\\Sigma\\), as </p> \\[\\nabla_\\gamma \\mathbf v = \\mathbf v' - (\\mathbf v' \\cdot \\mathbf N)\\mathbf N\\] <p>Note that orthogonal projection is unchanged of the sign of \\(\\mathbf N\\). </p> <p>Therefore, we define \\(\\nabla_\\gamma \\mathbf v: (a,b)\\rightarrow T_{\\gamma(t)}\\Sigma\\) as the covariant derivative of \\(\\mathbf v\\) along \\(\\gamma\\). If \\(\\nabla_\\gamma\\mathbf v = 0\\) for all \\(t\\), then \\(\\mathbf v\\) is parallel along \\(\\gamma\\). </p>"},{"location":"mat363/parallel_transport.html#theorem-1","title":"Theorem 1","text":"<p>\\(\\mathbf v\\) is parallel along \\(\\gamma\\) IFF \\(\\mathbf v'\\) is perpendicular to the tangent plane of \\(\\Sigma\\) for all \\(t\\). </p> <p>proof. This is obvious, since \\(\\nabla_\\gamma\\mathbf v\\) is the projection of \\(\\mathbf v'\\) onto \\(T_p\\Sigma\\), if \\(\\mathbf v'\\) is perpendicular to \\(T_p\\Sigma\\), then it is parallel to \\(\\mathbf N\\), and its projection onto \\(T_p\\Sigma\\) is always 0. </p>"},{"location":"mat363/parallel_transport.html#theorem-2","title":"Theorem 2","text":"<p>\\(\\mathbf v\\) is parallel along \\(\\gamma\\) IFF </p> \\[a' + (\\Gamma_{11}^1 u' +\\Gamma_{12}^1v')a + (\\Gamma_{12}^1u' + \\Gamma_{22}^1v')b = 0\\] \\[b' + (\\Gamma_{11}^2 u' + \\Gamma_{12}^2v')a + (\\Gamma_{12}^2u' + \\Gamma_{22}^2v')b = 0\\] <p>For \\(\\mathbf v(t) = a(t)\\sigma_u (t) + b(t)\\sigma_v(t), a,b\\) are smooth scalar functions. </p> <p>proof. We have that \\(\\mathbf v'\\) </p> \\[\\begin{align*} \\mathbf v' &amp;= a' \\sigma_u + a(\\sigma_{uu}u'+\\sigma_{uv}v') + b'\\sigma_v + b(\\sigma_{uv}u' + \\sigma_{vv}v')\\\\ &amp;= a'\\sigma_u + b'\\sigma_v + au'\\sigma_{uu} + (av'+bu')\\sigma_{uv} + bv'\\sigma_{vv} \\end{align*}\\] <p>where each of \\(\\sigma_{uu}, \\sigma_{uv}, \\sigma_{vv}\\) can be written into Gauss equations, and only involves \\(\\sigma_u, \\sigma_v, \\mathbf N\\). Then, since \\(\\mathbf v\\) is parallel along \\(\\gamma\\), we must have that \\(\\mathbf v' = \\lambda N\\) and all coefficient of \\(\\sigma_{u}, \\sigma_{v}\\) must be \\(0\\), a.k.a the equations in the statement must be zero. </p>"},{"location":"mat363/parallel_transport.html#theorem-3","title":"Theorem 3","text":"<p>Let \\(\\gamma: (a,b)\\rightarrow \\Sigma\\) and some \\(\\mathbf v_0\\) be a tangent vector of \\(\\Sigma\\) at \\(\\gamma(t_0)\\), then \\(\\exists !\\mathbf v\\) s.t. \\(\\mathbf v\\) is parallel along \\(\\gamma\\) and \\(\\mathbf v(t_0) = \\mathbf v_0\\). </p> <p>proof. Note that the statement is equivalent to say that there exists unique scalar function \\(a,b\\) s.t. </p> \\[a(t_0)\\sigma_u(t_0) + b(t_0) \\sigma_v(t_0) = \\mathbf v_0\\] \\[a' + (\\Gamma_{11}^1 u' +\\Gamma_{12}^1v')a + (\\Gamma_{12}^1u' + \\Gamma_{22}^1v')b = 0\\] \\[b' + (\\Gamma_{11}^2 u' + \\Gamma_{12}^2v')a + (\\Gamma_{12}^2u' + \\Gamma_{22}^2v')b = 0\\] <p>Then, this is a ODE</p> \\[a' = f(a(t), b(t), t) = -((\\Gamma_{11}^1 u' +\\Gamma_{12}^1v')a + (\\Gamma_{12}^1u' + \\Gamma_{22}^1v')b)\\] \\[b' = g(a(t), b(t), t) = -((\\Gamma_{11}^2 u' + \\Gamma_{12}^2v')a + (\\Gamma_{12}^2u' + \\Gamma_{22}^2v')b)\\] <p>given the initial conditions, it is proven to have a unique solution. </p>"},{"location":"mat363/parallel_transport.html#example-circle-on-sphere","title":"Example: circle on sphere","text":"<p>Let \\(\\gamma\\) be a circle of latitute \\(\\theta_0 \\in (-\\pi/2, \\pi)\\) on the standard unit sphere. \\(\\gamma(t) = \\sigma(\\theta_0, t)\\), and the first fundamental form is \\(E=1, F=0, G=\\cos^2\\theta\\). The Christoffel symbosls are then </p> \\[\\Gamma_{12}^1 = -\\tan\\theta, \\Gamma_{22}^1 =\\sin\\theta\\cos\\theta\\] <p>others are all 0. </p> <p>Therefore, we have that </p> \\[a' = -(\\cos\\theta_0\\sin\\theta_0)b, b' = \\tan\\theta_0 a\\] <p>solves to be </p> \\[a = A\\cos(t\\sin\\theta_0) + B\\sin(t\\sin\\theta_0)\\] \\[b = A\\frac{\\sin(t\\sin\\theta_0)}{\\cos\\theta_0}  - B\\frac{\\cos(t\\sin\\theta_0)}{\\sin\\theta_0}\\] <p>Consider the case when \\(t = 0\\), then \\(a = 0, b = 1\\), which gives \\(A = 0, B = -\\sin\\theta_0\\) so that </p> \\[\\mathbf v(t) = -\\sin\\theta_0 \\sin(t\\sin\\theta_0)\\sigma_\\theta + \\cos(t\\sin\\theta_0)\\sigma_\\phi\\] <p>Thus the tangent vector of \\(\\gamma\\) is parallel along \\(\\gamma\\) IFF \\(\\gamma\\) is a great circle. </p>"},{"location":"mat363/parallel_transport.html#example-triangle-on-the-sphere","title":"Example: Triangle on the Sphere","text":"<p>Suppose that a triangle \\(T\\) on the unit sphere whose sides are arcs of great circles has vertices \\(p, q, r\\). Let \\(v_0\\) be a non-zero tangent vector to the arc \\(pq\\) through \\(p, q\\) at \\(p\\). Show that, if we parallel transport \\(v_0\\) along \\(pq\\), then \\(qr\\) and then \\(rp\\), the result is to rotate \\(v_0\\) through an angle \\(2\\pi - A(T)\\). </p> <p>proof. Since the sides of the triangle are arcs of great circle. The parallel transporting of the tirangle sides is always parallel along the arc. Suppose that the internal angles are \\(a,b,c\\), then at each point of \\(p, q, r\\), the angle is \\(\\pi -a,\\pi-b, \\pi-c\\). Therefore, the total angle made is </p> \\[\\pi-a+\\pi-b+\\pi -c = 2\\pi - (a+b+c - \\pi) = 2\\pi - A(T)\\]"},{"location":"mat363/plane_curve.html","title":"Plane Curve","text":""},{"location":"mat363/plane_curve.html#signed-curvature","title":"Signed curvature","text":"<p>Suppose \\(\\gamma: \\mathbb R\\rightarrow \\mathbb R^2\\) is a unit-speed curve, let \\(\\mathbf t = \\gamma'\\) be the tangent vector of \\(\\gamma\\), and \\(\\mathbf t\\) is also a unit vector. Therefore, there are two unit vectors \\(\\mathbf n_s\\) and \\(-\\mathbf n_s\\) that is perpendicular to \\(\\mathbf t\\). By left-handed rules, we conventionally define \\(\\mathbf n_s\\) be the signed unit normal of \\(\\gamma\\), by rotating \\(\\mathbf t\\) c.c.w by \\(\\pi/2\\), in other word</p> \\[\\mathbf n_s = \\begin{bmatrix}0&amp;-1\\\\1&amp;0\\end{bmatrix}\\mathbf t\\] <p>Then, note that \\(\\mathbf t' = \\gamma''\\) is perpendicular to \\(\\mathbf t = \\gamma'\\). Thus there exists \\(\\kappa_s\\) s.t. \\(\\gamma'' = \\kappa_s \\mathbf n_s\\), define \\(\\kappa_s\\) be the signed curvature of \\(\\gamma\\), since we have that the curvature \\(\\kappa = \\|\\gamma''\\| = \\|\\kappa_s \\mathbf n_s\\| = |\\kappa_s|\\). </p> <p>Using this definition, if the curve is to the left of the tangent line, then \\(\\kappa_s &gt; 0\\), if to the right of the tangent line, then \\(\\kappa_s &lt; 0\\).</p> <p>For a regular plane curve \\(\\gamma(t)\\) (not necessarily unit-speed), we can define unit tangent vector \\(\\mathbf t\\), signed unit normal \\(\\mathbf n_s\\) and signed curvature \\(\\kappa_s\\)  at each point \\(\\gamma(t)\\) via unit-speed parameterizations \\(\\tilde \\gamma(s)\\), where \\(s(t)\\) is the arc-length. Thus, we have that </p> \\[\\frac{d\\mathbf t}{dt} = \\frac{d\\mathbf t}{ds}\\frac{ds}{dt} = \\kappa_s \\mathbf n_s \\|\\gamma'\\|\\]"},{"location":"mat363/plane_curve.html#example-signed-curvature-on-a-circle","title":"Example: signed curvature on a circle","text":"<p>Let \\(\\gamma(t) = (\\cos t, \\sin t), = \\gamma'(t) = (-\\sin t, \\cos t)\\). </p> Source code <pre><code>import matplotlib.pyplot as plt \nimport numpy as np\n\nt = np.arange(0, np.pi, 0.01)\nt0_idx = len(t) // 3\n\narc1 = np.array((np.cos(t), np.sin(t)))\ntangent1 = np.array((-np.sin(t), np.cos(t)))\nd2gamma1 = np.array((-np.cos(t), -np.sin(t)))\narc2 = np.array((np.cos(t), -np.sin(t)))\ntangent2 = np.array((-np.sin(t), -np.cos(t)))\nd2gamma2 = np.array((-np.cos(t), np.sin(t)))\n\ndef plot_curve(t0, gamma, dgamma, d2gamma, ax):\n    t0 = t[t0_idx] # 30 degree\n    p = np.array((gamma[0, t0_idx], gamma[1, t0_idx]))\n    tangent = dgamma[:, t0_idx]\n    normal = np.array(((0, -1), (1, 0))) @ tangent\n    kappa_s = (d2gamma[:, t0_idx] / normal)[0]\n    ax.plot(gamma[0], gamma[1])\n    ax.arrow(p[0], p[1], tangent[0] / 30, tangent[1]/30,  head_width=.05, color=\"b\")\n    ax.arrow(p[0], p[1], tangent[0], tangent[1], shape='full', head_width=.05, color=\"r\")\n    ax.arrow(p[0], p[1],  normal[0], normal[1], shape='full', head_width=.05, color=\"g\")\n    ax.set_title(rf\"$\\kappa_s = {kappa_s}$\");\n\nplt.figure(figsize=(8, 4))\nax = plt.subplot(121); ax.set_aspect('equal'); ax.axis(\"off\")\nplot_curve(t, arc1, tangent1, d2gamma1, ax)\nax = plt.subplot(122); ax.set_aspect('equal'); ax.axis(\"off\")\nplot_curve(t, arc2, tangent2, d2gamma2, ax)\n\nplt.savefig(\"../assets/plane_curve.jpg\")\n</code></pre> <p>\u200b </p>"},{"location":"mat363/plane_curve.html#claim-1","title":"Claim 1","text":"<p>If \\(\\gamma\\) is a unit-speed plane curve, then \\(\\mathbf n'_s = -\\kappa_s \\mathbf t\\).</p> <p>proof. Note that </p> \\[\\begin{align*} \\gamma'' &amp;= \\kappa_s \\mathbf n_s\\\\ \\gamma''&amp;= \\kappa_s\\begin{bmatrix}0&amp;-1\\\\1&amp;0\\end{bmatrix}\\mathbf t\\\\ \\begin{bmatrix}0&amp;1\\\\-1&amp;0\\end{bmatrix}\\mathbf \\gamma''&amp;=\\kappa_s \\mathbf t\\\\ -\\mathbf n_s' &amp;= \\kappa_s \\mathbf t \\end{align*}\\]"},{"location":"mat363/plane_curve.html#turning-angle","title":"Turning angle","text":"<p>Geometrically, the signed curvature can be interpreted as the rate at which the tangent vector rotates. If \\(\\gamma\\) is a unit-speed curve, the direction of the tangent vector \\(\\gamma'(s)\\) is measured by the angle \\(\\varphi(s)\\) s.t. </p> \\[\\gamma'(s) = (\\cos\\varphi(s), \\sin\\varphi(s))\\] <p>Note that \\(\\cos, \\sin\\) are periodic functions for \\(2\\pi\\), hence \\(\\varphi\\) is not unique. But we can have some guarantee so that \\(\\varphi\\) is always smooth. </p> <p>The turning angle of \\(\\gamma\\) determined by \\(\\varphi(s_0) = \\varphi_0\\) is a smooth function \\(\\varphi:(a,b)\\rightarrow\\mathbb R\\) s.t. </p> \\[\\forall s\\in (a, b). \\gamma'(s) = (\\cos\\varphi(s), \\sin\\varphi(s)), \\varphi(s_0) = \\varphi_0\\]"},{"location":"mat363/plane_curve.html#existence-and-uniqueness","title":"Existence and Uniqueness","text":"<p>Theorem Let \\(\\gamma:(a, b) \\rightarrow \\mathbb R^2\\) be a unit-speed plane curve, fix \\(s_0\\in(a, b), \\varphi_0\\in\\mathbb R\\) s.t. </p> \\[\\gamma'(s_0) = (\\cos\\varphi_0, \\sin\\varphi_0)\\] <p>Then, \\(!\\exists \\varphi: (a, b)\\rightarrow \\mathbb R\\) s.t. \\(\\varphi\\) is smooth and \\(\\varphi(s_0) = \\varphi_0\\) and \\(\\forall s\\in(a, b). \\gamma'(s) = (\\cos\\varphi(s), \\sin\\varphi(s))\\)</p> <p>proof (informal). Let  \\(\\gamma'(s) = (f(s), g(s))\\). Geometrically speaking, since \\(\\gamma\\) is unit-speed, \\(f^2 + g^2 = 1\\) so that the image of \\(\\gamma'(s)\\) will be an arc of the unit-circle centered at \\(0\\). Then, fix some \\(s_0\\), we can have the angle \\(\\varphi_0\\) defined on the circle. Then, by changing \\(s\\), \\(\\gamma'(s)\\) is moving smoothly on the unit circle, hence its corresponding angle will also move smoothly. </p> <p>proof (existence). Let \\(\\gamma'(s) = (f(s), g(s))\\). Since \\(\\gamma\\) is unit-speed, \\(f^2 + g^2 = 1\\).  Define </p> \\[\\varphi(s) = \\varphi_0 + \\int_{s_0}^{s} f(t)g'(t) - f'(t)g(t)dt\\] <p>Since \\(f,g\\) are both smooth, \\(\\varphi\\) will also be smooth. </p> <p>Therefore, we only need to show that \\(f(s) = \\cos\\varphi(s), g(s) = \\sin\\varphi(s)\\) for all \\(s\\). First, we note that we have \\(\\varphi' = fg' - gf', f^2 + g^2 = 1\\) and \\(\\frac{d}{dt}(f^2 + g^2) = 2ff'+2gg' = \\frac{d}{dt}1 = 0\\)</p> <p>Define </p> \\[F = f\\cos\\varphi + g\\sin\\varphi, G = f\\sin\\varphi - g\\cos\\varphi\\] <p>Then </p> \\[F' = (f'+g\\varphi')\\cos\\varphi + (g' - f\\varphi')\\sin\\varphi\\] <p>Note that we can then have </p> \\[\\begin{align*} f'+g\\varphi'   &amp;= f' + g(fg' - gf') &amp;\\text{given } \\varphi' = fg' - gf'\\\\ &amp;= f' - f'g^2 + fgg' \\\\ &amp;= f'f^2 + fgg' &amp;\\text{given } f'^2 + g^2 = 1\\\\ &amp;= f(ff'+gg') \\\\ &amp;= 0  &amp;\\text{given } f'f + g'g = 0\\\\ g' - f\\varphi' &amp;= g' - f(fg' - gf') = 0 \\end{align*}\\] <p>therefore, \\(F' = 0 + 0 = 0\\implies F\\) is constant, where \\(F(s_0) = f(s_0)\\cos\\varphi_0 + g(s_0)\\sin\\varphi_0 = f^2(s_0) + g^2(s_0) = 1\\) similarly, \\(G' = 0\\) and we can evaluate \\(G(s_0) = \\cos\\varphi_0\\sin\\varphi_0 - \\sin\\varphi_0\\cos\\varphi_0 = 0\\) Therefore, we have that </p> \\[f\\cos\\varphi + g\\sin\\varphi = 1, f\\sin\\varphi - g\\cos\\varphi = 0\\] <p>solves to be \\(f = \\cos\\varphi, g = \\sin\\varphi\\). </p> <p>proof (uniqueness). Let \\(\\psi\\) by another smooth function satisfying all conditions. Then \\(\\cos(\\psi(s)) = \\cos(\\varphi(s)), \\sin(\\psi(s)) = \\sin(\\varphi(s))\\), implying that \\(\\psi(s) - \\varphi(s) = 2\\pi n(s)\\) where \\(n(s): \\mathbb R\\rightarrow\\mathbb N\\). However, \\(n\\) need to be smooth since \\(\\varphi, \\psi\\) are both smooth, hence \\(n\\) must be a constant integer. However, since we define \\(\\varphi, \\psi\\) from the same \\(\\varphi_0\\), we must have \\(n =0\\) and hence \\(\\varphi = \\psi\\). </p>"},{"location":"mat363/plane_curve.html#turning-angle-and-signed-curvature","title":"Turning Angle and Signed Curvature","text":"<p>Geometrically, the signed curvature is the rate at which the tangent vector of the curve rotates. </p> <p>Theorem Let \\(\\gamma(s)\\) be a unit-speed plane curve, \\(\\varphi(s)\\) be a turning angle, then \\(\\kappa_s = \\frac{d\\varphi}{ds}\\). </p> <p>proof. Note that \\(\\kappa_s\\) is defined via \\(\\gamma'' = \\kappa_s \\mathbf n_s\\). Then note, </p> \\[\\begin{align*} \\gamma'' &amp;= \\mathbf t' = \\frac{d}{ds}(\\cos\\varphi(s), \\sin\\varphi(s)) = \\varphi'(s)(-\\sin\\varphi(s), \\cos\\varphi(s))\\\\ \\mathbf n_s &amp;= \\begin{bmatrix}0&amp;-1\\\\1&amp;0\\end{bmatrix}\\mathbf t = \\begin{bmatrix}0&amp;-1\\\\1&amp;0\\end{bmatrix}\\begin{bmatrix}\\cos\\varphi(s)\\\\\\sin\\varphi(s)\\end{bmatrix} = (-\\sin\\varphi(s), \\cos\\varphi(s)) \\end{align*}\\] \\[\\begin{align*} \\varphi'(s)(-\\sin\\varphi(s), \\cos\\varphi(s)) &amp;=  \\kappa_s (-\\sin\\varphi(s), \\cos\\varphi(s))\\\\  \\kappa_s  &amp;= \\varphi'(s) \\end{align*}\\]"},{"location":"mat363/plane_curve.html#example-1","title":"Example 1","text":"<p>Find the signed curvature for \\(\\gamma(t) = (t,\\cosh t)\\).  First, to find \\(\\varphi\\), </p> \\[\\begin{align*} \\gamma'(t) &amp;= (1, \\sinh t)\\\\ s(t) &amp;= \\int_0^t \\|\\gamma'(t)\\| = \\int_0^t\\sqrt{1+\\sinh^2 x}dx = \\sinh t \\end{align*}\\] <p>Define \\(\\varphi\\) be the angle between \\(\\gamma'\\) and x-axis, then </p> \\[\\begin{align*} \\tan \\varphi = \\frac{\\sinh t}{1} &amp;= s(t)\\\\ \\frac{d}{ds}\\tan \\varphi &amp;= \\frac{d}{ds} s\\\\ \\sec^2(\\varphi) \\varphi' &amp;= 1\\\\ \\varphi' &amp;= \\sec^2 \\varphi\\\\ \\kappa_s &amp;= \\varphi' = (1+\\tan^2 \\varphi) = (1+s^2) \\end{align*}\\]"},{"location":"mat363/plane_curve.html#theorem-1","title":"Theorem 1","text":"<p>the total signed curvature of a closed plane curve is \\(2\\pi k\\) for some integer \\(k\\). </p> <p>proof. Let \\(\\gamma\\) be unit-speed plane curve, since \\(\\gamma\\) closed, take \\(l\\) be the arc-length so that \\(\\gamma(s+l) = \\gamma(s)\\) for any \\(s\\). The total signed curvature is defined as</p> \\[\\int_0^l \\kappa_s(s)ds =  \\int_0^l \\varphi'(s)ds =\\varphi(l) - \\varphi(0)\\] <p>Then, because \\(\\gamma\\) is closed, hence periodic, we have that \\(\\gamma'(l) = \\gamma'(0)\\), which have </p> \\[(\\cos\\varphi(l), \\sin\\varphi(l)) = (\\cos\\varphi(0), \\sin\\varphi(0))\\] <p>Therefore, \\(\\varphi(l) - \\varphi(0) = 2\\pi k\\)</p>"},{"location":"mat363/plane_curve.html#plane-curve-via-signed-curvature","title":"Plane Curve via signed curvature","text":"<p>Geometrically, the signed curvature determines where the curve rotates. Intuitively, if we have an initial direction and starting point, and we know the signed curvature for all time, then we can recover the curve by turning at each time. </p> <p>Theorem Let \\(k:(a,b)\\rightarrow\\mathbb R\\) be any smooth function. Then, \\(\\exists \\gamma:(a,b)\\rightarrow\\mathbb R^2\\) s.t. \\(\\gamma\\) is unit-speed and its signed curvature is \\(k\\). Furthermore, if another curve \\(\\tilde \\gamma\\) also has its signed curvature as \\(k\\), then \\(\\exists M\\) be a direct isometry (the image only involves a translation and rotation) s.t. \\(\\tilde\\gamma(s) = M(\\gamma(s)), \\forall s\\in(a,b)\\)</p> <p>proof. Let \\(s_0\\in (a,b)\\) and take </p> \\[\\varphi(s) = \\int_{s_0}^s k(u)du\\] \\[\\gamma(s) = (\\int_{s_0}^s \\cos\\varphi(t)dt, \\int_{s_0}^s \\sin\\varphi(t)dt)\\] <p>Note that \\(\\gamma'(s) = (\\cos\\varphi(s), \\sin\\varphi(s))\\) is unit-speed and </p> \\[\\kappa_s = \\frac{d\\varphi}{ds} = k(s)\\] <p>Then, assume \\(\\tilde\\gamma\\) has a smooth turning angle \\(\\tilde\\varphi\\) and \\(k(s) = \\frac{d\\tilde\\varphi}{ds}\\), then we have that </p> \\[\\tilde\\varphi(s) = \\int_{s_0}^s k(u)du + \\tilde\\varphi(s_0) = \\varphi(s) + \\tilde\\varphi(s_0)\\] <p>We can then substitute into \\(\\tilde\\gamma\\) and verify the isometry properties. </p>"},{"location":"mat363/plane_curve.html#example-k0","title":"Example: k=0","text":"<p>Claim For any regular plane curve \\(\\gamma\\), if \\(\\kappa_s = 0\\), then \\(\\gamma\\) is a an arc of a straight line. </p> <p>proof. Since \\(\\kappa_s = 0\\), then \\(\\kappa = 0\\). By definition of curvature, \\(\\kappa = \\|\\gamma''\\| = 0\\implies \\gamma''(t) = 0\\). Therefore \\(\\gamma'(t) = c\\) for some constant \\(c\\), and \\(\\gamma(t) = ct+b\\) for some constant b. </p>"},{"location":"mat363/plane_curve.html#example-k-c-0","title":"Example: k = c &gt; 0","text":"<p>Claim For any regular plane curve \\(\\gamma\\), if \\(\\kappa\\) is a positive constant, then \\(\\gamma\\) is a circle, or an arc of a circle. </p> <p>proof. Note that \\(\\kappa_s = \\pm\\kappa = \\pm c\\). However, since \\(\\kappa_s\\) is smooth (\\(\\gamma'' = \\kappa_s\\mathbf n_s, \\mathbf n_s, \\gamma''\\) are both smooth), by IVT, \\(\\kappa_s\\) must take only one value.  Then, all we need is to find a circle s.t. its signed curvature is \\(c\\), and then all curves will be isometry of such circle.</p> <p>When \\(\\kappa_s = c\\), \\(\\frac{d\\varphi}{ds} = \\kappa_s = c\\) so that we take \\(\\varphi(s) = cs\\). Then we have </p> \\[\\mathbf t(s) = \\gamma'(s) = (\\cos(cs), \\sin(cs))\\] <p>Then, we can take </p> \\[\\gamma(s) = (\\frac{1}{c}\\sin(cs), -\\frac{1}{c}\\cos(cs))\\] <p>is a circle. </p> <p>When \\(\\kappa_s = -c\\), we can have similar results, only with a flip of the sign. </p>"},{"location":"mat363/reparam.html","title":"Reparameterization and unit-length curve","text":""},{"location":"mat363/reparam.html#reparameterization","title":"Reparameterization","text":"<p>A curve can have different parameterizations (even infinite ways). </p> <p>For parameterized curves \\(\\gamma: (a, b)\\rightarrow\\mathbb R^n\\) and \\(\\tilde\\gamma: (\\tilde a, \\tilde b)\\rightarrow\\mathbb R^n\\). IF \\(\\exists \\phi: (a, b)\\rightarrow (\\tilde a, \\tilde b)\\) and its inverse \\(\\phi^{-1}\\) (since bijective) and \\(\\phi, \\phi^{-1}\\) are both smooth and </p> \\[\\forall s\\in (\\tilde a, \\tilde b). \\tilde\\gamma(s) = \\gamma(\\phi(s))\\] \\[\\forall t\\in (a, b). \\gamma(t) = \\tilde\\gamma(\\phi^{-1}(t)) = \\tilde\\gamma(s)\\] <p>THEN \\(\\tilde\\gamma\\) and \\(\\gamma\\) are reparameterizations of each other. </p> <p>Example \\(\\gamma(t) = (\\cos t, \\sin t), \\tilde \\gamma(s)= (\\sin s, \\cos s)\\) are both reparameterizations of each other, parameterizing the same circle \\(x^2 + y^2 = 1\\). Take \\(\\phi(s) = \\pi/2 - s, \\phi^{-1}(t) = \\pi/2 - t\\), \\(\\gamma(t) = \\gamma(\\phi(s)) = \\tilde\\gamma(\\phi^{-1}(t))\\). </p>"},{"location":"mat363/reparam.html#claim-1","title":"Claim 1","text":"<p>If \\(\\gamma_1\\) is a reparameterization of \\(\\gamma_0\\), \\(\\gamma_2\\) is a reparameterization of \\(\\gamma_1\\), then \\(\\gamma_2\\) is a reparameterization of \\(\\gamma_0\\). </p> <p>proof. Take map \\(s_0\\) s.t. \\(\\gamma_1(s_0(t)) = \\gamma_0(t)\\), take map \\(s_1\\) s.t. \\(\\gamma_2(s_1(t)) = \\gamma_1(t)\\). Then, let \\(s_2 = s_1\\circ s_0\\) so that \\(\\gamma_2(s_2(t)) = \\gamma_2(s_1(s_0(t))) = \\gamma_1(s_0(t)) = \\gamma_0(t)\\).  Note that \\(s^{-1}_2 = s_0^{-1}\\circ s_1^{-1}\\) as a composition of invertible functions, is also invertible; and composition of smooth functions is also smooth. </p>"},{"location":"mat363/reparam.html#example-cissoid-of-diocles","title":"Example: Cissoid of Diocles","text":"<p>The cissoid of Diocles (see below) is the curve </p> \\[\\{(r, \\theta)\\in \\mathbb R \\times (-\\frac{\\pi}{2},\\frac{pi}{2}) : r = \\sin\\theta\\tan\\theta\\}\\] <p>Claim the curve can be parameterized by </p> \\[\\gamma: (-1, 1)\\rightarrow\\mathbb R^2, \\gamma(t) = (t^2, \\frac{t^3}{\\sqrt{1-t^2} })\\] <p>proof. Convert the polar coordinates to Cartesian coordinates</p> \\[\\begin{align*} (x, y) &amp;= (r\\cos\\theta, r \\sin\\theta) \\\\ &amp;= (\\sin^2\\theta, \\sin^2\\theta\\tan\\theta) \\\\ &amp;= (\\sin^2\\theta, \\sin^2 \\theta \\frac{\\sin \\theta}{\\cos\\theta})\\\\ &amp;= (\\sin^2\\theta, \\frac{\\sin^3 \\theta}{\\sqrt{1-\\sin^2 \\theta} })\\\\ \\end{align*}\\] <p>Therefore, let \\(t = \\sin\\theta\\), note that this function is bijective on \\((-1, 1)\\rightarrow (-\\frac{\\pi}{2},\\frac{pi}{2})\\) and both \\(\\sin\\theta\\) and \\(arc\\sin\\theta\\) are smooth on the specified domain. </p>"},{"location":"mat363/reparam.html#example-ordinary-cusp","title":"Example: Ordinary cusp","text":"<p>A point \\(\\mathbf p\\) of \\(\\gamma\\), corresponding to a parameter value \\(t\\), is an ordinary cusp if the curve is singular at the point and \\(\\gamma''(t)\\) and \\(\\gamma'''(t)\\) are linearly indepedent and non-zero. </p> <p>Claim If \\(\\gamma\\) has an ordinary cusp at a point \\(\\mathbf p\\), so does any reparam of it. </p> <p>proof. Let \\(\\gamma\\) be a parameterized curve and with reparameterization \\(\\tilde\\gamma\\) and map \\(s\\) s.t. \\(\\tilde\\gamma(s(t)) = \\gamma(t)\\). Assume that for some \\(t_0\\) s.t. \\(\\gamma'(t_0) = 0\\), \\(\\gamma''(t_0), \\gamma'''(t_0)\\) are linearly independent and non-zero. </p> \\[\\frac{d\\tilde\\gamma}{dt}= \\frac{d\\tilde\\gamma}{ds}\\frac{ds}{dt} = \\frac{d\\gamma}{dt} = 0\\] <p>Since \\(s\\) is smooth, \\(ds/dt \\neq 0\\implies \\frac{d\\tilde\\gamma}{ds} = 0\\)</p> \\[\\begin{align*} \\frac{d^2\\tilde\\gamma}{dt^2} &amp;= \\frac{d}{dt}(\\frac{d\\tilde\\gamma}{ds}\\frac{ds}{dt})\\\\ &amp;= \\frac{d^2\\tilde\\gamma}{ds^2}\\frac{ds}{dt}\\frac{ds}{dt} +\\frac{d\\tilde\\gamma}{ds}\\frac{d^2s}{dt^2}\\\\ &amp;= \\frac{d^2\\tilde\\gamma}{ds^2}(\\frac{ds}{dt})^2&amp;\\frac{d\\tilde\\gamma}{ds}=0\\\\ \\frac{d^3\\tilde\\gamma}{dt^3} &amp;= \\frac{d}{dt}(\\frac{d^2\\tilde\\gamma}{ds^2}(\\frac{ds}{dt})^2+\\frac{d\\tilde\\gamma}{ds}\\frac{d^2s}{dt^2})\\\\ &amp;= \\frac{d^3\\tilde\\gamma}{ds^3}(\\frac{ds}{dt})^3 + 2\\frac{d^2\\tilde\\gamma}{ds^2}\\frac{ds}{dt}\\frac{d^2s}{dt^2} + \\frac{d^2\\tilde\\gamma}{ds^2}\\frac{ds}{dt}\\frac{d^2s}{dt^2} + \\frac{d\\tilde\\gamma}{ds}\\frac{d^3s}{dt^3}\\\\ &amp;= \\frac{d^3\\tilde\\gamma}{ds^3}(\\frac{ds}{dt})^3 + 3\\frac{d^2\\tilde\\gamma}{ds^2}\\frac{ds}{dt}\\frac{d^2s}{dt^2}&amp;\\frac{d\\tilde\\gamma}{ds}=0 \\end{align*}\\] <p>Obviously, the two vectors aren't multiple of each other. </p> Source code <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\n\nt = np.arange(-0.999, 1., 0.001)\nplt.plot(t*t, t*t*t / (1 - t*t)**0.5)\nplt.savefig(\"../assets/reparam.jpg\")\n</code></pre> <p>\u200b </p>"},{"location":"mat363/reparam.html#regular-point-and-regular-curves","title":"Regular point and regular curves","text":"<p>A point \\(\\gamma(t)\\) is a regular point if \\(\\gamma'(t)\\neq 0\\), otherwise \\(\\gamma(t)\\) is a singular point. A parameterized curve \\(\\gamma\\) is a regular curve if \\(\\forall t \\in (a, b). \\gamma'(t)\\neq 0\\). Note that for the same level curve, it might be parameterized into both regular curve and non-regular curve. </p>"},{"location":"mat363/reparam.html#example-of-regular-curves","title":"Example of Regular Curves","text":"<p>Claim the circle with parameterization \\(\\gamma: \\mathbb R\\rightarrow \\mathbb R^2, \\gamma(t) = (\\cos^2 t, \\sin^2 t)\\) is not regular</p> \\[\\gamma'(t) = (-2\\cos t\\sin t, 2\\sin t\\cos t) = (-\\sin(2t), \\sin(2t))\\] \\[\\gamma'(\\frac{k}{2}\\pi) = (0, 0), \\forall k\\in \\mathbb Z\\] <p>Claim \\(\\gamma(t) = (t, \\cosh t)\\) is regular</p> \\[\\gamma'(t) = (1, \\sinh t)\\neq 0\\]"},{"location":"mat363/reparam.html#properties-of-regular-curves","title":"Properties of regular curves","text":"<p>Claim Any reparameterization of a regular curve is regular. </p> <p>proof. Let \\(\\gamma(t)\\) be a regular curve and \\(\\tilde\\gamma(s)\\) be reparameterization with \\(t = \\phi(s) = \\phi(\\phi^{-1}(t))\\). Differentiating the equation will give</p> \\[1 = \\frac{d\\phi}{ds}\\frac{d\\phi^{-1} }{dt}\\] <p>Hence \\(\\frac{d\\phi}{ds}\\neq 1\\). So that </p> \\[\\tilde\\gamma'(s) = \\tilde\\gamma'(\\phi(t))\\phi'(t) \\neq 0, \\forall s\\] <p>Claim If \\(\\gamma(t)\\) is regular, then its arc-length function \\(s\\) is smooth. </p> <p>proof. Let \\(\\gamma(t) = (x(t), y(t))\\), note that we already have </p> \\[s'(t) = \\|\\gamma'(t)\\| = \\sqrt{x'(t)^2 + y'(t)^2}\\] <p>Known that square root is smooth on \\((0, \\infty)\\), and we have that \\(\\gamma'(t) \\neq 0\\) so that \\(x'(t)^2 + y'(t)^2 &gt; 0\\). Therefore, \\(s'\\) is also smooth. </p>"},{"location":"mat363/reparam.html#unit-speed-reparameterization","title":"Unit-speed Reparameterization","text":"<p>Claim A parametrized curve has a unit-speed reparametrization if and only if it is regular.</p> <p>proof.  \\(\\Rightarrow\\)  Let \\(\\hat\\gamma\\) be a unit-speed reparametrization of \\(\\gamma\\). Since \\(\\hat\\gamma\\) is unit-speed, i.e. \\(\\forall t. \\|\\hat\\gamma'(t)\\| = 1\\). It is regular, then \\(\\gamma\\) is a reparametrization of \\(\\hat\\gamma\\), hence also regular.  </p> <p>\\(\\Leftarrow\\)  Let \\(\\gamma\\) be a regular curve, then it has a smooth arc-length function \\(s\\). By inverse function theorem, \\(s\\) is injective with an open interval image \\(s: (a,b)\\rightarrow (c, d)\\). Then, we take some \\(\\tilde \\gamma\\) s.t. \\(\\gamma(t) = \\tilde \\gamma(s(t))\\). Differentiate both sides and take the arc-length, we have that</p> \\[\\gamma'(t) = \\tilde\\gamma(s(t))s'(t)\\] <p>Take arc-length of both sides, </p> \\[\\| \\tilde\\gamma(s(t))s'(t)\\| =  \\|\\tilde\\gamma(s(t))\\||s'(t)| = \\| \\tilde\\gamma(s(t))s'(t)\\|s'(t) = \\|\\gamma'(t)\\| = s'(t)\\] <p>so that we can conclude that \\(\\|\\tilde\\gamma(s(t))\\|= 1\\).</p> <p>Corollary A parametrized curve \\(\\gamma\\) has a unit-speed reparameterization IFF the reparametrization map \\(u\\) follows that \\(u(t) = \\pm s + c\\), where \\(c\\) is a constant. </p> <p>proof. note that </p> \\[\\begin{align*}  \\|\\gamma'(t)\\| &amp;= \\|\\tilde\\gamma(u(t))u'(t)\\|\\\\  \\|\\tilde\\gamma'(s(t))\\||s'(t)| &amp;= \\|\\tilde\\gamma'(u(t))\\||u'(t)|\\\\  |s'(t)| &amp;= |u'(t)| \\end{align*}\\] <p>Thus, it implies that \\(u(t) = \\pm s + c\\)</p>"},{"location":"mat363/sff.html","title":"Surface Curvatures","text":""},{"location":"mat363/sff.html#second-fundamental-form","title":"Second Fundamental Form","text":"<p>To define the curvature of a surface, the first attempt is to imitate the curvature of a curve. In other words, how quick the surface moves away from the plane parallel to the tangent plane and intersecting \\(p\\). </p> <p>For a surface \\(\\Sigma\\) parameterized by \\(\\sigma\\), Consider a point \\(p = \\sigma(u,v)\\) and the tangent plane at \\(p\\) with unit normal \\(\\mathbf N\\). The intuition above leads to </p> \\[(\\sigma(u+\\Delta u, v+\\Delta v) - \\sigma(u,v))\\cdot \\mathbf N\\] <p>Using Taylor expansion, \\((\\sigma(u+\\Delta u, v+\\Delta v) - \\sigma(u,v))\\) is linearly approximated (first two orders) as </p> \\[(\\sigma_u u' + \\sigma_v v') + \\frac{1}{2}(\\sigma_{uu}u'^2 + 2\\sigma_{uv}u'v' + \\sigma_{vv}v'^2)+ rem.\\] <p>Then, note that \\(N \\parallel (\\sigma_u\\times\\sigma_v)\\) and \\(rem.\\) approches 0, so that the equation becomes</p> \\[\\frac{1}{2}(\\sigma_{uu}\\cdot \\mathbf N u'^2 + 2\\sigma_{uv} \\cdot \\mathbf N  u'v' + \\sigma_{vv}\\cdot \\mathbf N  v'^2)\\] <p>Let \\(L = \\sigma_{uu}\\cdot \\mathbf N , M = \\sigma_{uv}\\cdot \\mathbf N, N = \\sigma_{vv}\\cdot \\mathbf N\\) so that we can describe curvature by </p> \\[Lu'^2 + 2M u'v' + N v'^2\\] <p>which is called the second fundamental form of the surface patch \\(\\sigma\\).  </p>"},{"location":"mat363/sff.html#example-surface-of-revolution","title":"Example: Surface of Revolution","text":"\\[\\sigma(u,v) = (f\\cos v, f \\sin v, g)\\] <p>with assumption that \\(f&gt;0\\) and \\(\\gamma(u) = (f(u), 0, g(u))\\) is unit-speed. </p> <p>Known that \\(\\sigma_u = (f'\\cos v, f'\\sin v, g'), \\sigma_v = (-f\\sin v, f\\cos v, 0)\\). </p> \\[\\begin{align*} \\mathbf N &amp;= \\sigma_u\\times \\sigma_v = (-fg'cos v, -f g'\\sin v, ff')\\\\ \\|\\mathbf N\\| &amp;= \\sqrt{f^2g'^2(\\cos^2 v + \\sin^v) + f^2f'^2} = f\\sqrt{f'^2+g'^2} = f\\\\ \\hat{\\mathbf N} &amp;= (-g'\\cos v, -g'\\sin v, f')\\\\ L &amp;= \\sigma_{uu}\\cdot \\hat{\\mathbf N} \\\\ &amp;= (f''\\cos v, f''\\sin v, g'')\\cdot (-g'\\cos v, -g'\\sin v, f')\\\\ &amp;= f'g'' - f''g'\\\\ M &amp;= \\sigma_{uv}\\cdot \\hat{\\mathbf N} \\\\ &amp;= (-f'\\sin v, f'\\cos v, 0)\\cdot (-g'\\cos v, -g'\\sin v, f')\\\\ &amp;= 0\\\\ N &amp;= \\sigma_{vv}\\cdot \\hat{\\mathbf N} \\\\ &amp;= (-f\\cos v, -f\\sin v, 0)\\cdot (-g'\\cos v, -g'\\sin v, f')\\\\ &amp;= fg' \\end{align*}\\] <p>The second fundamental form is </p> \\[(f'g''-f''g')u'^2 + fg' v'^2\\] <p>For sphere using lat-long coordinate parameterization, inserting \\(f = \\cos u, g = \\sin u\\) so that the second fundamental form is </p> \\[u'^2 + \\cos^2(u)v'^2\\] <p>which is the same as the first fundamental form. </p>"},{"location":"mat363/sff.html#example-elliptic-paraboloid","title":"Example: elliptic paraboloid","text":"\\[\\sigma(u, v) = (u,v,u^2+v^2)\\] \\[\\begin{align*} \\mathbf N &amp;= \\frac{\\sigma_u\\times \\sigma_v}{\\|\\sigma_u\\times \\sigma_v\\|}\\\\ &amp;= \\frac{(1, 0, 2u)\\times (0, 1, 2v)}{\\|(1, 0, 2u)\\times (0, 1, 2v)\\|}\\\\ &amp;= \\frac{1}{\\sqrt{4u^2+4v^2+1} }(-2u, -2v, 1)\\\\ L &amp;= \\sigma_{uu} \\cdot\\mathbf N \\\\ &amp;= \\frac{1}{\\sqrt{4u^2+4v^2+1} }(0, 0, 2)\\\\ N &amp;= \\sigma_{vv} \\cdot\\mathbf N \\\\ &amp;= \\frac{1}{\\sqrt{4u^2+4v^2+1} }(0, 0, 2)\\\\ M &amp;= \\sigma_{uv}\\cdot {\\mathbf N }  = 0 \\end{align*}\\]"},{"location":"mat363/sff.html#reparameterization","title":"Reparameterization","text":"<p>Claim If \\(\\tilde \\sigma\\) is a reparameterization of a surface patch \\(\\sigma\\) with map \\((u,v) = \\Phi(\\tilde u, \\tilde v)\\), then </p> \\[\\begin{bmatrix}\\tilde L &amp;\\tilde M\\\\\\tilde M&amp;\\tilde N\\end{bmatrix} = \\pm J(\\Phi)^T \\begin{bmatrix}L&amp;M\\\\M&amp;N\\end{bmatrix}J(\\Phi)\\] <p>First, by reparameterization we have that \\(\\tilde{\\mathbf  N} = \\pm \\mathbf N\\), then given that</p> \\[\\tilde\\sigma(\\tilde u, \\tilde v) = \\sigma(\\Phi(\\tilde u, \\tilde v))\\] <p>Differentiate both sides we have that </p> \\[\\frac{d\\tilde \\sigma}{d\\tilde u} = \\frac{d\\sigma}{d u}\\frac{d u}{d\\tilde u} + \\frac{d\\sigma}{d v}\\frac{d v}{d\\tilde u}, \\frac{d\\tilde \\sigma}{d\\tilde v} = \\frac{d\\sigma}{d u}\\frac{d u}{d\\tilde v} + \\frac{d\\sigma}{d v}\\frac{d v}{d\\tilde v}\\] <p>Differentiating twice, we have that </p> \\[\\begin{align*}\\frac{d^2\\tilde \\sigma}{d \\tilde u^2} &amp;= \\frac{d}{d\\tilde u}(\\frac{d\\sigma}{d u}\\frac{d u}{d\\tilde u} + \\frac{d\\sigma}{d v}\\frac{d v}{d\\tilde u})\\\\ &amp;= \\frac{d\\sigma}{d u}\\frac{d^2 u}{d\\tilde u^2} + \\frac{d}{d\\tilde u}(\\frac{d\\sigma}{d u})\\frac{d u}{d\\tilde u} + \\frac{d\\sigma}{d v}\\frac{d^2 v}{d\\tilde u^2} + \\frac{d}{d\\tilde u}(\\frac{d\\sigma}{d v})\\frac{d v}{d\\tilde u}\\\\ \\frac{d}{d\\tilde u}(\\frac{d\\sigma}{d u})&amp;= \\frac{d^2\\sigma}{d u^2}\\frac{d u}{d \\tilde u} + \\frac{d^2\\sigma}{d udv}\\frac{d v}{d \\tilde u}\\\\ \\frac{d}{d\\tilde u}(\\frac{d\\sigma}{d v})&amp;= \\frac{d^2\\sigma}{d v^2}\\frac{d v}{d \\tilde u} + \\frac{d^2\\sigma}{d udv}\\frac{d v}{d \\tilde u} \\end{align*}\\] <p>Inserting all differentials back with simplified notations</p> \\[\\tilde{\\sigma}_{\\tilde u\\tilde u} = \\sigma_u \\frac{d^2u}{d\\tilde u^2} + \\sigma_v \\frac{d^2u}{d\\tilde u^2} + \\sigma_{uu}(\\frac{d u}{d\\tilde u})^2 + 2\\sigma_{uv}\\frac{d u}{d\\tilde u}\\frac{d v}{d\\tilde u} + \\sigma_{vv}(\\frac{d v}{d\\tilde u})^2\\] <p>Note that \\(\\tilde{\\mathbf  N} = \\pm \\mathbf N, \\sigma_u\\cdot \\mathbf N = \\sigma_v\\cdot \\mathbf N = 0\\), we have that </p> \\[\\tilde L = \\tilde{\\sigma}_{\\tilde u\\tilde u} \\cdot \\tilde{\\mathbf  N} = L(\\frac{d u}{d\\tilde u})^2 + 2M\\frac{d u}{d\\tilde u}\\frac{d v}{d\\tilde u} +N(\\frac{d v}{d\\tilde u})^2\\] <p>Similarly, we can obtain \\(\\tilde M,\\tilde N\\)</p>"},{"location":"mat363/sff.html#example-plane","title":"Example: Plane","text":"<p>Claim \\(\\sigma\\) parameterizes an open subset of a plane IFF the second fundamental form is zero everywhere. </p> <p>proof. \\(\\Rightarrow\\) Note that if one parameterization of the plane has second fundamental form being 0, then all of its reparameterizations have 0 second fundamental form by the reparameterization theorem (\\(\\pm J^T \\mathbf 0 J = \\mathbf 0\\) for any \\(J\\)). We simply take \\(\\sigma(u,v) = u\\mathbf a + v\\mathbf b + \\mathbf c\\), so that \\(\\sigma_u = \\mathbf a, \\sigma_v = \\mathbf b, \\sigma_{uu} = \\sigma_{uv} = \\sigma_{vv} = 0\\), hence its second fundemental form is \\(0\\). </p> <p>\\(\\Leftarrow\\) Assume that \\(\\sigma_{uu}\\cdot \\mathbf N = \\sigma_{uv}\\cdot \\mathbf N = \\sigma_{vv}\\cdot \\mathbf N = 0\\).  Note that by product rule</p> \\[d_u (\\sigma_u \\cdot\\mathbf N) = \\sigma_u \\cdot \\mathbf N_u + \\sigma_{uu}\\cdot\\mathbf N\\] <p>while we also have that \\(\\sigma_u \\cdot\\mathbf N = 0\\) since they are perpendicular, hence  </p> \\[\\sigma_u \\cdot \\mathbf N_u = -\\sigma_{uu}\\cdot\\mathbf N = 0\\] <p>Similarly, we can obtain that </p> \\[\\sigma_u \\cdot \\mathbf N_u = \\sigma_u \\cdot \\mathbf N_v= \\sigma_v \\cdot \\mathbf N_u= \\sigma_v \\cdot \\mathbf N_v = 0\\] <p>Also, since \\(\\mathbf N\\) is a unit vector, its partial derivatives </p> \\[\\mathbf N \\cdot \\mathbf N_u = \\mathbf N \\cdot \\mathbf N_v = 0\\] <p>However, \\(N\\) is perpendicular to \\(\\sigma_u, \\sigma_v\\) so that we must have that \\(\\mathbf N_u = \\mathbf N_v = 0\\), hence \\(\\mathbf N(u,v)\\) is a constant.</p>"},{"location":"mat363/sff.html#gauss-and-weingarten-maps","title":"Gauss and Weingarten Maps","text":"<p>Another approach for defining curvature is the speed of change in the unit normal \\(\\mathbf N\\). Note that the values of \\(\\mathbf N\\) on a surface belongs to a unit sphere \\(S^2\\). Therefore, we define the Gauss map </p> \\[G: \\Sigma\\rightarrow S^2, G(p) = \\mathbf N(p)\\] <p>The rate of change of normal thus becomes </p> \\[D_pG : T_p \\Sigma\\rightarrow T_{N(p)}S^2\\] <p>Note that the tangent plane \\(T_{N(p)}S^2\\) is the tangent plane perpendicular to \\(N(p)\\), which is the same plane as \\(T_p\\Sigma\\). </p> <p>Define the Weingarten map </p> \\[W_p = -D_pG\\] <p>and define the blinear form of second fundamental form of \\(\\Sigma\\) at \\(p\\) by </p> \\[\\langle\\langle v, w\\rangle\\rangle = \\langle W_p(v), w\\rangle\\] <p>We will then show how Weingarten map is related to the second fundamental form. </p> <p>Lemma \\(\\mathbf N_u\\cdot \\sigma_u = -L, \\mathbf N_u\\cdot \\sigma_v = \\mathbf N_v\\cdot \\sigma_u = -M, \\mathbf N_v\\cdot \\sigma_v = -N\\). </p> <p>proof. Note that \\(\\mathbf N = \\frac{\\sigma_u\\times \\sigma_v}{\\|\\sigma_u\\times \\sigma_v\\|}\\) is perpendicular to both \\(\\sigma_u, \\sigma_v\\), we have that </p> \\[N\\cdot \\sigma_u = N\\cdot\\sigma_v = 0\\] <p>Differentiating \\(N\\cdot \\sigma_u\\), we have that </p> \\[\\begin{align*} \\frac{\\partial}{\\partial u}(\\mathbf N\\cdot\\sigma_u) &amp;= \\mathbf N_u\\cdot \\sigma_u +  \\mathbf N\\cdot\\sigma_{uu} = 0\\\\ \\mathbf N_u\\cdot \\sigma_u &amp;= - \\mathbf N\\cdot\\sigma_{uu} = -L \\end{align*}\\] <p>Similarly, differentiate wrt. \\(u, v\\) on each of \\(N\\cdot \\sigma_u, N\\cdot\\sigma_v\\) will get the claim. </p> <p>Theorem The inner product is  a symmetric bilinear form </p> \\[\\langle\\langle v, w\\rangle\\rangle_{p} = \\begin{bmatrix}c\\\\d\\end{bmatrix}\\begin{bmatrix}L&amp;M\\\\M&amp;N\\end{bmatrix}\\begin{bmatrix}a\\\\b\\end{bmatrix} = Lv_u w_u + M(v_u + w_v + v_v + w_v) + Nv_vw_v\\] <p>For \\(v = a \\sigma_u + b\\sigma_v, w = c \\sigma_u + d\\sigma_v\\). </p> <p>Note that both sides of the equation is bilinear form on \\(T_p\\Sigma\\), we only need to prove on the set \\(\\{\\sigma_u, \\sigma_v\\}\\). Also, note that \\(W_p = -D_pG = D_p \\mathbf N\\), so that </p> \\[\\frac{\\partial W_p}{\\partial_u} = -\\frac{\\partial G}{\\partial u} = -\\frac{\\partial\\mathbf N}{\\partial u}, \\frac{\\partial W_p}{\\partial_v} = -\\frac{\\partial \\mathbf N}{\\partial_u}\\] <p>Therefore, using the lemma above</p> \\[\\langle\\langle \\sigma_u, \\sigma_u\\rangle\\rangle = \\langle W_p(v), w\\rangle = \\langle -\\mathbf N_u, \\sigma_u\\rangle = \\mathbf N \\cdot \\sigma_{uu}\\] <p>And we can obtain the similar results for </p> \\[\\langle\\langle \\sigma_u, \\sigma_v\\rangle\\rangle = \\langle\\langle \\sigma_v, \\sigma_u\\rangle\\rangle = M, \\langle\\langle \\sigma_v, \\sigma_v\\rangle\\rangle = N\\] <p>Corollary Weingarten map is self adjoint. \\(\\langle W_p(v), w\\rangle =  \\langle v, W_p(w)\\rangle\\)</p>"},{"location":"mat363/sff.html#normal-and-geodesic-curvatures","title":"Normal and Geodesic Curvatures","text":"<p>The third attempt to define curvature is to define via the curvature of curves on the surface. </p> <p>Let \\(\\gamma: \\mathbb R\\rightarrow\\Sigma\\) be a unit-speed curve on \\(\\Sigma\\), parameterized by \\(\\sigma\\). Let \\(\\mathbf t = \\gamma', \\mathbf n = \\frac{\\gamma''}{\\|\\gamma''\\| }, \\mathbf b = \\mathbf t\\times \\mathbf n\\) be the unit tangent, unit normal, and binormal, which forms the orthonormal basis. Also, let \\(\\mathbf N\\) be the unit normal, hence \\(\\{\\mathbf N, \\mathbf t, \\mathbf N\\times \\mathbf t\\}\\) forms another orthonormal basis. Then, note that \\(\\gamma\\) is unit-speed, hence \\(\\gamma''\\perp \\mathbf t\\), thus, \\(\\gamma''\\) must resides on the plane spanned by \\(\\mathbf N, \\mathbf N\\times \\mathbf t\\), in other words</p> \\[\\gamma'' = \\kappa_n \\mathbf N + \\kappa_g(\\mathbf N\\times\\mathbf t)\\] <p>Define \\(\\kappa_n\\) be the normal curvature and \\(\\kappa_g\\) be the geodesic curvature of \\(\\gamma\\). </p> <p>Since \\(\\{\\mathbf N, \\mathbf t, \\mathbf N\\times \\mathbf t\\}\\) forms a orthonormal basis, by Pythagorean theorem or trig. identities, we can easily obtain</p> \\[\\kappa^2 = \\kappa_n^2 + \\kappa_g^2\\] \\[\\kappa_n = \\gamma''\\cdot \\mathbf N = \\kappa\\cos\\psi, \\kappa_g = \\gamma'' \\cdot (\\mathbf N\\times \\gamma') = \\pm\\kappa\\sin\\psi\\] <p>for \\(\\psi = \\arccos(\\mathbf N\\cdot\\mathbf n)\\)</p> <p>Theorem \\(\\kappa_n\\) is invariant of reparameterization of unit-speed \\(\\gamma\\), while \\(\\kappa_g\\) changes up to the sign. </p> <p>proof. Let \\(\\gamma, \\tilde\\gamma\\) be two unit-speed parameterizations of the same curve, then the map can only be \\(\\tilde t = \\pm t + c\\). </p> <p>Theorem If \\(\\gamma:\\mathbb R\\rightarrow\\Sigma\\) is unit-speed on an oriented surface \\(\\Sigma\\), then </p> \\[\\kappa_n = II(\\gamma',\\gamma') = Lu'^2 + 2Mu'v' + Nv'^2\\] <p>proof. Let \\(\\gamma(t) = \\sigma(u(t), v(t))\\). Note that \\(\\mathbf t = \\gamma'\\) is a tangent vector to \\(\\Sigma\\), </p> \\[\\begin{align*} \\mathbf N\\cdot \\gamma' &amp;= 0\\\\ \\frac{d}{dt}(N\\cdot \\gamma') = \\mathbf N\\cdot \\gamma'' + \\mathbf N'\\cdot \\gamma' &amp;= 0\\\\ \\mathbf N\\cdot \\gamma'' &amp;= -\\mathbf N'\\cdot \\gamma' \\end{align*}\\] <p>Therefore, we have that </p> \\[\\kappa_n = \\mathbf N\\cdot \\gamma'' = -\\mathbf N'\\cdot \\gamma' = W(\\gamma')\\cdot\\gamma' = II(\\gamma',\\gamma')\\] <p>Theorem If \\(\\gamma\\) is regular (not necessarily unit-speed), then </p> \\[\\kappa_n = \\frac{II(\\gamma',\\gamma')}{I(\\gamma',\\gamma')}. \\kappa_g = \\frac{\\gamma''\\cdot (\\mathbf N\\times \\gamma')}{I(\\gamma',\\gamma')^{3/2} }\\] <p>proof. Let \\(\\hat\\gamma\\) be the unit-speed parameterization of \\(\\gamma, \\gamma(t) = \\hat\\gamma(s(t))\\) where \\(s\\) is the arc-length of \\(\\gamma\\), and \\(s'\\) is the speed Therefore, we have that </p> \\[\\frac{d\\gamma}{dt} = \\frac{d\\hat\\gamma}{ds}\\frac{ds}{dt}\\implies \\hat\\gamma' = \\frac{\\gamma'}{s'}\\] \\[\\frac{d^2\\gamma}{dt^2} = \\frac{d^2\\hat\\gamma}{ds^2}(\\frac{ds}{dt})^2 + \\frac{d\\hat\\gamma}{ds}\\frac{d^2s}{dt^2}\\implies \\hat\\gamma'' = s'^{-2}(\\gamma'' - \\hat\\gamma's'')\\] <p>Therefore, </p> \\[\\begin{align*} \\kappa_n &amp;= II(\\hat\\gamma',\\hat\\gamma') \\\\ &amp;= II(\\frac{d\\gamma}{dt}(\\frac{ds}{dt})^{-1}, \\frac{d\\gamma}{dt}(\\frac{ds}{dt})^{-1}) \\\\ &amp;= \\frac{II(\\gamma', \\gamma')}{s'(t)^2} \\\\ &amp;= \\frac{II(\\gamma',\\gamma')}{I(\\gamma',\\gamma')}\\\\ \\kappa_g &amp;= \\hat\\gamma''\\cdot (\\mathbf N\\times \\hat\\gamma') \\\\ &amp;= s'^{-2}(\\gamma'' - \\hat\\gamma's')\\cdot (\\mathbf N \\times s'^{-1}\\gamma')\\\\ &amp;= s'^{-3}\\gamma''\\cdot (\\mathbf N \\times \\hat\\gamma')\\\\ &amp;= I(\\gamma',\\gamma')^{-3/2}\\gamma''\\cdot (\\mathbf N \\times \\hat\\gamma') \\end{align*}\\]"},{"location":"mat363/sff.html#example-sphere","title":"Example: Sphere","text":"<p>Claim For any curve \\(\\gamma: \\mathbb R\\rightarrow S^2_R\\) on the sphere of radius \\(R\\), \\(\\kappa_n = \\pm R^{-1}\\). </p> <p>proof. WLOG assume that the sphere is centered at origin and \\(\\gamma\\) is unit-speed. Then, the unit normal at point \\(p = \\gamma(t) \\in S^2\\) is \\(\\pm\\frac{\\gamma(t)}{R}\\).  Also, we have that \\(\\gamma\\cdot \\gamma = R^2\\), differentiate both side gives that </p> \\[2\\gamma'\\cdot \\gamma = 0\\] <p>Differentiate twice gives that </p> \\[\\gamma''\\cdot \\gamma + \\gamma'\\cdot\\gamma' = 0\\implies \\gamma''\\cdot \\gamma' = -1\\] <p>Therefore, the normal curvature</p> \\[\\kappa_n = \\mathbf N \\cdot \\gamma'' = \\pm\\frac{\\gamma}{R}\\cdot \\gamma'' = \\pm R^{-1}\\] <p>Claim For any circle \\(c\\) on the sphere of radius \\(R\\), its geodesic curvature is </p> \\[\\kappa_g = \\pm r^{-1}\\sin(\\frac{r}{R})\\] <p>proof. Note that \\(\\kappa_g\\) is invariant of rotations, WLOG assume that \\(c(\\theta) = (r\\cos\\theta, r\\sin\\theta, \\sqrt{R^2-r^2})\\) where \\(0 &lt; r\\leq R\\), then we have the unit normal for curve \\(c\\) being</p> \\[\\mathbf n(\\theta) = \\frac{c''}{\\|c''\\|} = \\frac{(-r\\cos\\theta, -r\\sin\\theta, 0)}{r} = (-\\cos\\theta, -\\sin\\theta, 0)\\] <p>The unit normal for sphere at \\(c(\\theta)\\) is simply \\(\\pm (R^{-1} c(\\theta)) = \\pm(\\frac{r}{R}\\cos\\theta, \\frac{r}{R}\\sin\\theta, \\sqrt{1-\\frac{r^2}{R^2} })\\). Known that the curvature for a circle of radius \\(r\\) is \\(r^{-1}\\), we have that</p> \\[\\kappa_g = \\pm \\kappa \\sin (\\mathbf n\\cdot\\mathbf N) = \\pm r^{-1}\\sin(\\frac{r}{R})\\]"},{"location":"mat363/sff.html#asymptotic-curves","title":"Asymptotic curves","text":"<p>A curve \\(\\gamma:\\mathbb R\\rightarrow\\Sigma\\) is called asymptotic if \\(\\kappa_n = 0\\).  </p> <p>For example, any straight line on a surface is asymptotic, since \\(\\kappa_n = \\mathbf N\\cdot \\gamma'' = \\mathbf N \\cdot 0 = 0\\).  </p> <p>Theorem \\(\\gamma\\) with positive curvature is aymptotic IFF its binormal \\(\\mathbf b\\) is parallel to the unit normal of \\(\\Sigma\\) for all points of \\(\\gamma\\). </p> <p>proof. Since \\(\\mathbf b = \\mathbf t\\times \\mathbf n\\), \\(\\mathbf b \\parallel \\mathbf N\\) so that \\(\\mathbf N \\perp \\mathbf n\\) implying that \\(\\kappa_n = \\kappa (\\mathbf N \\cdot \\mathbf n) = \\kappa 0 = 0\\)</p>"},{"location":"mat363/space_curves.html","title":"Space Curves","text":""},{"location":"mat363/space_curves.html#torsion","title":"Torsion","text":"<p>Let \\(\\gamma\\) be a unit-speed curve in \\(\\mathbb R^3\\), \\(\\mathbf t = \\gamma'\\) be its unit tangent vector.  </p> <p>If \\(\\kappa\\) is non-zero, then we define the principal normal of \\(\\gamma\\) at the point \\(\\gamma(s)\\) to be </p> \\[\\mathbf n(s) = \\frac{1}{\\kappa(s)} \\mathbf t'(s)\\] <p>Note that the principal normal is a unit vector, or normalized \\(\\gamma''\\). </p> <p>Then, define the binormal vector   of \\(\\gamma\\) at the point \\(\\gamma(s)\\) to be \\(\\mathbf b = \\mathbf t\\times \\mathbf n\\), we then obtain an orthonormal, right-handed basis \\(\\{\\mathbf t, \\mathbf n, \\mathbf b\\}\\), i.e.,</p> \\[\\mathbf b = \\mathbf t\\times \\mathbf n, \\mathbf n = \\mathbf b\\times \\mathbf t, \\mathbf t = \\mathbf n \\times \\mathbf b\\] <p>Then, if we differentiate \\(\\mathbf b\\), we get </p> \\[\\begin{align*} \\mathbf b' &amp;= (\\mathbf t\\times \\mathbf n)' \\\\ &amp;=\\mathbf t' \\times \\mathbf n + \\mathbf t \\times \\mathbf n'\\\\ &amp;= \\mathbf t \\times \\mathbf n' &amp;\\mathbf t' = \\kappa \\mathbf n \\text{, parallel to } \\mathbf n \\end{align*}\\] <p>So that \\(\\mathbf b'\\perp \\mathbf t\\).  In addition, note that \\(\\mathbf b'\\perp \\mathbf b\\) since \\(\\mathbf b\\) is unit-length. Hence it must follows that </p> \\[\\mathbf b' = -\\tau \\mathbf n\\] <p>We define such \\(\\tau\\) as torsion.</p> <p>Note that the torsion is only defined if the curvature is non-zero. </p>"},{"location":"mat363/space_curves.html#formulas-for-torsion","title":"Formulas for Torsion","text":"<p>Theorem Let \\(\\gamma: (a,b)\\rightarrow\\mathbb R^3\\) be a regular curve with nowhere-vanishing curvature. Then its torsion is given by </p> \\[\\tau = \\frac{(\\gamma' \\times \\gamma'')\\cdot \\gamma'''}{\\|\\gamma'\\times \\gamma''\\|^2}\\] <p>proof. We first sketch the proof by assuming \\(\\gamma\\) is unit-speed so that</p> \\[\\begin{align*} -\\tau \\mathbf n\\cdot \\mathbf n &amp;= -\\mathbf n\\cdot \\mathbf b' \\\\ \\tau &amp;= \\mathbf n \\cdot (\\mathbf t \\times \\mathbf n')\\\\ \\tau &amp;= \\frac{1}{\\kappa}\\gamma''\\cdot \\big(\\gamma' \\times (\\frac{1}{\\kappa}\\gamma''' - \\frac{\\kappa'}{\\kappa^2}\\gamma'')\\big)\\\\ &amp;= \\kappa^{-2}\\gamma'''\\cdot (\\gamma'\\times \\gamma'')\\\\ &amp;= \\frac{(\\gamma' \\times \\gamma'')\\cdot \\gamma'''}{\\|\\gamma'\\times \\gamma''\\|^2} \\end{align*}\\] <p>In general, we can use \\(s\\) to be the arc-length along \\(\\gamma\\) so that \\(\\frac{d\\gamma}{dt} = \\frac{d\\gamma}{ds}\\frac{ds}{dt}\\) and so on, and replaces each differentials. </p>"},{"location":"mat363/space_curves.html#torsion-for-plane-curves","title":"Torsion for plane curves","text":"<p>Theorem For some \\(\\gamma\\) be a regular curve in \\(\\mathbb R^3\\) with nowhere vanishing curvature, \\(\\tau(s) = 0\\) for all \\(s\\) IFF the image of \\(\\gamma\\) is contained in some plane.</p> <p>proof. Assume \\(\\gamma((a,b))\\) is contained in some plane \\(\\{v: \\mathbf v\\cdot \\mathbf N = d\\}\\) so that we have </p> \\[\\begin{align*} \\gamma\\cdot N &amp;= d\\\\ \\frac{d}{ds}(\\gamma\\cdot N) &amp;= 0\\\\ \\mathbf t \\cdot \\mathbf N &amp;= 0\\\\ \\mathbf t' \\cdot \\mathbf N &amp;= 0\\\\ \\kappa \\mathbf n\\cdot \\mathbf N &amp;= 0 \\end{align*}\\] <p>Therefore, both \\(\\mathbf t \\perp\\mathbf N\\) and \\(\\mathbf n \\perp\\mathbf N\\). Hence \\(\\mathbf b = \\mathbf t\\times \\mathbf n\\) is parallel to \\(\\mathbf N\\). Since \\(\\mathbf N, \\mathbf b\\) are both unit vectors and \\(\\mathbf b(s)\\) is smooth, we have \\(\\mathbf b = \\mathbf N\\) or \\(\\mathbf b = -\\mathbf N\\). So that \\(\\mathbf b' = 0\\implies \\tau = 0\\).</p> <p>Assume \\(\\tau = 0\\) everywhere, so that \\(\\mathbf b' = 0\\) and \\(\\mathbf b\\) is constant. Then consider </p> \\[\\frac{d}{ds}(\\gamma\\cdot \\mathbf n) = \\gamma' \\cdot \\mathbf b = \\mathbf t\\times \\mathbf b = 0\\] <p>Hence \\(\\gamma\\cdot \\mathbf n = d\\) for some constant \\(d\\).  </p>"},{"location":"mat363/space_curves.html#frenet-serret-equations","title":"Frenet-Serret Equations","text":"<p>Theorem Let \\(\\gamma\\) be a unit speed curve in \\(\\mathbb R^3\\) with nowhere vanishing curvature. Then </p> \\[\\begin{bmatrix}\\mathbf t' \\\\\\mathbf n' \\\\\\mathbf b' \\end{bmatrix} =  \\begin{bmatrix}0&amp;\\kappa&amp;0\\\\-\\kappa&amp;0&amp;\\tau\\\\0&amp;-\\tau&amp;0\\end{bmatrix} \\begin{bmatrix}\\mathbf t \\\\\\mathbf n \\\\\\mathbf b \\end{bmatrix} =  \\begin{bmatrix}\\kappa\\mathbf n \\\\-\\kappa \\mathbf t + \\tau \\mathbf b \\\\-\\tau\\mathbf n \\end{bmatrix}\\] <p>proof. We have already obtained \\(\\mathbf t', \\mathbf n'\\), for \\(\\mathbf n'\\) we have </p> \\[\\mathbf n' = \\mathbf b'\\times \\mathbf t + \\mathbf b\\times \\mathbf t' = -\\tau\\mathbf n\\times \\mathbf t + \\kappa\\mathbf b \\times \\mathbf n = \\tau \\mathbf b - \\kappa\\mathbf t\\]"},{"location":"mat363/space_curves.html#example-k-c-t-0","title":"Example: k = c, t = 0","text":"<p>Claim For \\(\\gamma\\) be a unit-speed curve in 3D, if \\(\\kappa\\) is a constant and \\(\\tau = 0\\) for all \\(s\\), then, \\(\\gamma\\) parameterizes a circle or an arc of a circle.</p> <p>proof. Since \\(\\tau = 0\\), let \\(\\Pi\\) be the plane that \\(\\gamma\\) lies, and \\(\\Pi\\) has its plane normal be \\(\\mathbf b\\). By FS equation, \\(\\mathbf n' = -\\kappa\\mathbf t + \\tau\\mathbf b = -\\kappa \\mathbf t\\). Then, </p> \\[\\frac{d}{ds}(\\gamma + \\frac{1}{\\kappa}\\mathbf n) = \\mathbf t + \\frac{1}{\\kappa}\\mathbf n' = \\mathbf t- \\mathbf t = 0\\] <p>So that \\(\\gamma + \\kappa^{-1}\\mathbf n\\) is a constant, and </p> \\[\\|\\gamma - (\\gamma + \\frac{1}{k}\\mathbf n)\\|  = \\|-\\frac{1}{k}\\mathbf n\\| = \\kappa^{-1}\\] <p>So that \\(\\gamma\\) lies on the sphere \\(S\\) with center \\(\\gamma + \\frac{1}{k}\\mathbf n\\) and radius \\(\\kappa^{-1}\\) and the intersection of \\(\\Pi\\).</p>"},{"location":"mat363/space_curves.html#space-curve-via-k-and-t","title":"Space Curve via k and t","text":"<p>Lemma Let \\(A\\) be a skew-symmetric \\(3\\times 3\\) matrix (\\(a_{ij} = -a_{ji}\\)). Let \\(v_1,v_2,v_3\\) be smooth functions of a parameter \\(s\\) satisfying the differential equations \\(v_i' = \\sum_{j=1}^3 a_{ij}v_j\\) and suppose that for some \\(s_0\\), \\(v_1(s_0), v_2(s_0), v_3(s_0)\\) are orthonormal. Show \\(\\forall s. v_1(s), v_2(s), v_3(s)\\) are orthonormal. </p> <p>proof. We aim to show that \\(v_i\\cdot v_j = 1\\) for all \\(i\\neq j\\). Note that </p> \\[\\begin{align*} \\frac{d}{ds}(v_i\\cdot v_j) &amp;= v_i'\\cdot v_j + v_i\\cdot v_j' \\\\ &amp;= \\sum_{k=1}^3 a_{ik}v_k\\cdot v_j + \\sum_{k=1}^3 a_{jk}v_k \\cdot v_i \\\\ &amp;= \\sum_{k=1}^3 a_{ik}v_k\\cdot v_j - \\sum_{k=1}^3 a_{kj}v_k \\cdot v_i \\end{align*}\\] <p>Have a unique solution given that the condition satisfies for \\(s=s_0\\). </p> <p>Theorem For \\(k:\\mathbb R\\rightarrow \\mathbb R^{&gt;0}\\) and \\(t:\\mathbb R\\rightarrow \\mathbb R\\) be two smooth function, there is a unit-speed curve in \\(\\mathbb R^3\\) whose curvature is \\(k\\) and torsion is \\(t\\).</p> <p>proof. Following from FS equations, we want to solve for functions \\(\\mathbf T, \\mathbf N, \\mathbf B\\) for some point \\(s_0\\) s.t. \\(\\mathbf T(s_0) = e_1, \\mathbf N(s_0) = e_2, \\mathbf B(s_0) = e_3\\). Note that the FS matrix is skew-symmetric so that by lemma we have a unique solution for \\(\\mathbf T,\\mathbf N, \\mathbf B\\) s.t. they are orthonormal for all \\(s\\). Then, let </p> \\[\\gamma(s) = \\int_{s_0}^s \\mathbf T(u)du\\] <p>and we can verify all of \\(\\mathbf T, \\mathbf N, \\mathbf B, \\gamma\\) are unit-length. </p> <p>Theorem And all curves with the same curvature and torsion are isomorphic to each other. </p>"},{"location":"mat363/surface_examples.html","title":"Examples of Surfaces","text":""},{"location":"mat363/surface_examples.html#level-surfaces","title":"Level Surfaces","text":"<p>A surface can be given as a level surface as </p> \\[\\{(x,y,z): f(x,y,z) = 0\\] <p>where \\(f\\) is smooth.</p> <p>Claim If \\(\\Sigma\\) is a level surface with smooth \\(f\\) and \\(\\forall p \\in \\Sigma. \\nabla f\\neq \\vec 0\\). Then \\(\\Sigma\\) is a regular surface. </p> <p>proof. Near each point \\(\\mathbf p=(x_0, y_0, z_0)\\in \\Sigma, f(\\mathbf p) = 0\\), we want to show that \\(\\Sigma\\) is a graph of coordinate function of the other two coordinates.  WLOG assuming that \\(\\sigma\\) is the map from XZ-plane. </p> \\[\\sigma(x, z) := (x, \\psi(x, z), z)\\] <p>where \\(\\psi(x_0, z_0) = y_0\\). </p> <p>By implicit funciton theorem, consider \\(f\\) near \\(\\mathbf p\\). </p> \\[\\begin{align*} f(\\mathbf a) &amp;= f(\\mathbf p) + Df\\vert_{\\mathbf p} (\\mathbf a-\\mathbf p) + rem.\\\\ &amp;= 0 + \\begin{bmatrix} \\frac{\\partial f}{\\partial x}(x_0, y_0, z_0)\\\\ \\frac{\\partial f}{\\partial y}(x_0, y_0, z_0)\\\\ \\frac{\\partial f}{\\partial z}(x_0, y_0, z_0)\\end{bmatrix}\\begin{bmatrix}x-x_0\\\\y-y_0\\\\z-z_0\\end{bmatrix} + rem. \\end{align*}\\] <p>Note that \\(Df\\vert_{\\mathbf p}\\neq 0\\) so that the linear expansion is invertible. Therefore, we can apply implicit function theorem and confirm that the non-linear \\(f\\) is also invertible. </p> <p>In this case, \\(Df|_{\\mathbf p}\\) is non-zero and is the normal vector of \\(\\Sigma\\). In this case, we also have that \\(\\Sigma\\) is orientable, since \\(N(p) = Df|_{\\mathbf p}\\neq 0\\) and \\(f\\) is smooth. </p>"},{"location":"mat363/surface_examples.html#examples-of-level-surface","title":"Examples of level surface","text":"<p>For level surface </p> \\[\\Sigma = \\{(x,y,z): x^2 + y^2 + z^4 = 1\\}\\] <p>we have that \\(Df = (2x, 2y, 4z^3)\\) vanishes only when at \\(p=(0, 0, 0)\\), while \\(p\\not\\in\\Sigma\\), hence it is smooth. </p> <p>Consider \\(a&gt;b&gt;0\\) and </p> \\[\\Sigma = \\{(x,y,z): (x^2+y^2+z^2 + a^2 - b^2)^2 = (4a^2(x^2+y^2))\\}\\] <p>Let \\(f(x,y,z) = (x^2+y^2+z^2 + a^2 - b^2)^2 - (4a^2(x^2+y^2))\\), then </p> \\[Df = \\begin{pmatrix} 4x(x^2+y^2+z^2 - a^2 - b^2)\\\\ 4y(x^2+y^2+z^2 - a^2 - b^2)\\\\ 4z(x^2+y^2+z^2 + a^2 - b^2) \\end{pmatrix}\\] <p>The only possible point where \\(Df=0\\) is \\(p(0,0,0)\\not\\in\\Sigma\\), hence \\(\\Sigma\\) is smooth.  Furthere more, consider </p> \\[\\begin{pmatrix} x\\\\y\\\\z \\end{pmatrix} = \\begin{pmatrix} (a+b\\cos\\theta)\\cos\\varphi\\\\ (a+b\\cos\\theta)\\sin\\varphi\\\\ b\\sin\\theta \\end{pmatrix}\\] <p>Note that </p> \\[x^2+y^2 + z^2 = a^2+2ab\\cos\\theta + b^2\\cos^2\\theta + b^2\\sin^2\\theta = a^2 + 2ab\\cos\\theta + b^2\\] \\[\\begin{align*} LHS &amp;= (a^2 + 2ab\\cos\\theta + b^2 +a^2 - b^2)^2\\\\ &amp;= (2a)^2(a + b\\cos\\theta)^2\\\\ &amp;= 4a^2(x^2 + y^2)\\\\ &amp;= RHS \\end{align*}\\]"},{"location":"mat363/surface_examples.html#lagranges-method-of-undetermined-multipliers","title":"Lagrange's Method of Undetermined Multipliers","text":"<p>Let \\(\\Sigma = \\{(x,y,z): f(x,y,z) = 0\\}\\) be smooth. </p> <p>Claim 1 \\(\\nabla f\\) is perpendicular to the tangent plane at every \\(p\\in \\Sigma\\).</p> <p>proof. Since \\(\\Sigma\\) is smooth. Let \\(p\\in\\Sigma\\), let \\(\\sigma: U\\rightarrow V\\) be some patch containing \\(p\\). Then, we have that </p> \\[\\frac{df}{du} = \\frac{df}{dx}\\frac{dx}{du}+\\frac{df}{dy}\\frac{dy}{du}+\\frac{df}{dz}\\frac{dz}{du} = \\nabla f\\cdot \\sigma_u\\] \\[\\frac{df}{dv} = \\frac{df}{dx}\\frac{dx}{dv}+\\frac{df}{dy}\\frac{dy}{dv}+\\frac{df}{dz}\\frac{dz}{dv} = \\nabla f\\cdot \\sigma_v\\] <p>However, \\(\\Sigma\\) is defined on a level surface so that \\(\\frac{df}{du} = \\frac{df}{dv} = 0\\). while \\(\\nabla f\\neq 0, D\\sigma \\neq 0\\), hence \\(\\nabla f\\perp \\sigma_v\\). </p> <p>Claim 2. \\(\\Sigma\\) is orientable. </p> <p>proof. Note that \\(\\nabla f\\) is perpendicular to the tangent plane at all \\(p \\in S\\). Which means it is parallel to \\(\\hat N\\). Therefore, we can take \\(\\hat N = \\frac{\\nabla f}{\\|\\nabla f\\|}\\), since \\(f\\) is smooth and nowhere vanishing, \\(\\hat N\\) is well-defined and \\(\\Sigma\\) is therefore orientable. </p> <p>Claim 3. For some \\(F:\\mathbb R^3\\rightarrow \\mathbb R\\) is smooth, and \\(F\\), with the restriction to \\(\\Sigma\\), has a local extremum at \\(p\\), then \\(\\nabla F = \\lambda \\nabla f\\). </p> <p>proof. Let \\(\\gamma\\) be arbitrary smooth curve on \\(\\Sigma\\) passing through \\(p\\), since \\(F\\) has a local extremum on \\(\\Sigma\\), it is also a extremum on such path \\(\\gamma\\), therefore we have that </p> \\[\\frac{dF}{dt} = \\nabla f\\cdot \\frac{d\\gamma}{dt} = 0\\] <p>Therefore, \\(\\nabla F\\perp \\gamma'\\) is perpendicular to any curve on \\(\\Sigma\\) at \\(p\\), implying that \\(\\nabla F\\) is perpendicular to \\(T_p\\Sigma\\). Therefore \\(\\nabla F\\) parallel to \\(\\nabla f\\implies \\nabla f = \\lambda \\nabla F\\)</p>"},{"location":"mat363/surface_examples.html#surface-of-revolution","title":"Surface of  Revolution","text":"<p>A surface of revolution is the surface obtained by rotating a plane curve around a straight line in the plane. </p> <p>Consider the example where the curve resides in the XZ plane, defined as </p> \\[\\gamma: (a, b)\\rightarrow \\mathbb R^3. \\gamma(t)= (f(t), 0, g(t)), f(t) &gt; 0\\] <p>and the surface is obtained by rotating \\(\\gamma\\) about z-axis with rotation parameter \\(\\theta\\), the surface patch is then</p> \\[\\sigma(t, \\theta) = (f(t)\\cos\\theta, f(t)\\sin\\theta, g(t))\\] <p>Where \\(\\sigma_1: U_1\\rightarrow V_1\\cap \\Sigma\\) with \\(\\theta\\in (0, 2\\pi),  \\sigma_2: U_2\\rightarrow V_2\\cap \\Sigma\\) with \\(\\theta\\in (-\\pi, \\pi)\\)</p> <p>For example, the sphere without the north and south pole is a surface of revolution with \\(\\gamma(t) = (\\cos t, 0, \\sin t), t\\in (-\\pi/2, \\pi/2)\\). Cylinder with \\(\\gamma(t) = (R, 0, t), t\\in\\mathbb R\\)</p> <p>Note that </p> \\[\\begin{align*} \\partial_t\\sigma &amp;= (f'(t)\\cos\\theta, f'(t)\\sin\\theta, g'(t))\\\\ \\partial_\\theta\\sigma &amp;= (-f(t)\\sin\\theta, f(t)\\cos\\theta, 0)\\\\ N = \\sigma_t\\times \\sigma_\\theta &amp;= (f(t)g'(t)\\cos\\theta, -f(t)g'(t)\\sin\\theta, f(t)f'(t))\\\\ \\|N\\| &amp;= \\sqrt{f^2(t)(g'(t))^2 + f^2(t)(f'(t))^2}\\\\ &amp;= f(t)\\sqrt{(f'(t)^2) + (g'(t)^2) }\\\\ &amp;= f(t)\\|\\gamma'(t)\\| \\end{align*}\\] <p>Frequently, \\(\\gamma\\) is parameterized by the arc-length \\(s=t\\) so that \\((f'(t)^2) + (g'(t)^2) = 1\\), if so </p> \\[\\|N(t,\\theta)\\| = f(t)\\]"},{"location":"mat363/surface_examples.html#ruled-surfaces","title":"Ruled Surfaces","text":"<p>A ruled surface is a union of straight lines called rullings of the surface along some curve \\(\\gamma\\), where the curve is often unit-speed. </p> <p>Let \\(\\gamma:(a,b)\\rightarrow \\mathbb R^3\\) parameterizes the curve, and \\(d:(a,b)\\rightarrow \\mathbb R^3\\) parameterize the direction of the line. Conveniently, assume \\(\\gamma, d\\) are both unit-speed. Then, the surface is parameterized by </p> \\[\\sigma(u, v) = \\gamma(u) + vd(u)\\] <p>where \\(u\\in (a, b), v\\in\\mathbb R\\)</p> \\[\\begin{align*} \\sigma_u &amp;= \\gamma'(u) + vd'(u)\\\\ \\sigma_v &amp;= d(u) \\end{align*}\\] <p>Therefore, we need that \\(d\\neq 0, \\gamma'(u) + vd'(u)\\) are \\(d(u)\\) are linearly independent to satisfy the regularity conditions.  One way to ensure that is to require \\(\\gamma'\\) and \\(d\\) being linearly independent and \\(v\\) sufficiently small, say \\(v\\in (-\\epsilon, \\epsilon)\\).  </p> <p>Oppositely, one can consider surfaces where \\(d\\) is parallel to \\(\\gamma'\\) and \\(d'\\neq 0\\) (since \\(d\\) is unit-speed, this means \\(d\\perp d'\\)). Then, we can replace \\(d\\) with \\(\\gamma'\\) so that </p> \\[\\sigma_u = \\gamma' + vd', \\sigma_v = \\gamma'\\] <p>is linearly independent IFF \\(\\gamma'\\) is not parallel to \\(d'\\) IFF \\(d\\) is not parallel to \\(d'\\) and \\(v\\in\\mathbb R - \\{0\\}\\)</p>"},{"location":"mat363/surface_examples.html#examples-of-ruled-surface","title":"Examples of Ruled Surface","text":"<p>Mobius band is a ruled surface with </p> \\[\\gamma(u) = (\\cos u, \\sin u, 0), d(v) = (\\sin u, 0, \\cos u)\\] <p>A generalized cylinder is when the rulings is constant, \\(d(u) = \\mathbf c\\), so that the rulings are always parallel. Then, \\(\\sigma_u = \\gamma'(u), \\sigma_v = \\mathbf c\\) so that \\(\\sigma\\) is regular IFF \\(\\gamma\\) is never tangent to \\(\\mathbf c\\). </p> <p>A generalized cone with vertex \\(\\mathbf p\\) is when all rulings pass through some fixed point \\(\\mathbf p\\). In this case, \\(d(u) = \\gamma(u)-\\mathbf p\\) so that </p> \\[\\sigma_u = \\gamma'(u) + v\\gamma'(u), \\sigma_v = \\gamma(u)-\\mathbf p\\] <p>Therefore, \\(\\sigma\\) is regular IFF \\(v\\neq -1\\), i.e. \\(\\mathbf p\\) is not in the surface. </p>"},{"location":"mat363/surface_normals.html","title":"Surface Normals and Orientability","text":""},{"location":"mat363/surface_normals.html#normal-vector","title":"Normal Vector","text":"<p>Another finding is that \\(\\partial_u\\sigma(u_0, v_0)\\times \\partial_v\\sigma(u_0, v_0)\\) is perpendicular to \\(T_p\\Sigma\\) since the cross product is perpendicular to the spanning vectors of the plane.  Therefore, define the unit normal vector of \\(\\Sigma\\) at \\(p\\) as </p> \\[\\hat N(p) = \\frac{\\partial_u\\sigma(u_0, v_0)\\times \\partial_v\\sigma(u_0, v_0)}{\\|\\partial_u\\sigma(u_0, v_0)\\times \\partial_v\\sigma(u_0, v_0)\\|}\\] <p>Note that the tangent plane is independent of the choice of the path. Therefore, there are only 2 possible normal vectors. Say \\(\\hat N(p) = \\frac{\\partial_u\\sigma(u_0, v_0)\\times \\partial_v\\sigma(u_0, v_0)}{\\|\\partial_u\\sigma(u_0, v_0)\\times \\partial_v\\sigma(u_0, v_0)\\|}\\), then another possible reparameterization of the surface patch will result in \\(\\tilde N(p) = \\frac{\\partial_v\\sigma(u_0, v_0)\\times \\partial_u\\sigma(u_0, v_0)}{\\|\\partial_v\\sigma(u_0, v_0)\\times \\partial_u\\sigma(u_0, v_0)\\|}=-\\hat N(p)\\)</p>"},{"location":"mat363/surface_normals.html#reparameterization-of-surface-patches","title":"Reparameterization of Surface Patches","text":"<p>Note that a corollary of the regularity condition: Consider the intersection of the 2 surface patch of a regular surface \\(\\sigma_1: U_1\\rightarrow V_1, \\sigma_2: U_2\\rightarrow V_2. V_1\\cap V_2\\neq\\emptyset\\). Then, let \\(f:U_1\\rightarrow U_2\\) be the map from one plane to another. Then, \\(f\\) is invertible, and \\(f\\) and \\(f^{-1}\\) are both differentiable. </p> <p>Claim If \\(\\sigma: U\\rightarrow V\\) is a regular coordinate patch of a surface and \\(f: \\tilde U\\rightarrow U\\) is a reparameterization. Then \\(\\sigma\\circ f: \\tilde U\\rightarrow V\\) is also a regular coordinate patch. </p> <p>Another interesting finding is that the normal vector of the reparameterized tangent plane. </p> \\[\\begin{align*} \\tilde N &amp;= (\\frac{\\partial \\sigma}{\\partial u} \\frac{\\partial u}{\\partial \\tilde u} + \\frac{\\partial \\sigma}{\\partial v} \\frac{\\partial v}{\\partial \\tilde u})\\times (\\frac{\\partial \\sigma}{\\partial u} \\frac{\\partial u}{\\partial \\tilde v} + \\frac{\\partial \\sigma}{\\partial v} \\frac{\\partial v}{\\partial \\tilde v})\\\\ &amp;= \\frac{\\partial \\sigma}{\\partial u} \\times \\frac{\\partial \\sigma}{\\partial v} (\\frac{\\partial u}{\\partial \\tilde u}  \\frac{\\partial v}{\\partial \\tilde v} - \\frac{\\partial v}{\\partial \\tilde u}  \\frac{\\partial u}{\\partial \\tilde v})\\\\ &amp;= \\det(Df)(\\frac{\\partial \\sigma}{\\partial u} \\times \\frac{\\partial \\sigma}{\\partial v}) \\end{align*}\\] <p>Therefore, the unit normal will be </p> \\[\\hat{\\tilde N} = \\frac{\\tilde N}{\\|\\tilde N\\|} = \\frac{\\det(Df)}{|\\det(Df)|}\\hat N = \\text{sign}(\\det(Df))\\hat N\\]"},{"location":"mat363/surface_normals.html#orientable-surface","title":"Orientable Surface","text":"<p>From the above findings, we can define an surface's orientability through its atlas A surface \\(\\Sigma\\) is orientable if exists an atlas with the property that, if \\(f\\) is the transition map between any two surface patches, then \\(\\det(Df) &gt; 0\\). In other words, for any two overlapping surface patches \\(\\sigma_1: U_1\\rightarrow V_1, \\sigma_2: U_2\\rightarrow V_2. V_1\\cap V_2\\neq \\emptyset\\) the transition map \\(f:= \\sigma_1^{-1}\\circ \\sigma_2:W_2\\rightarrow W_1\\) where \\(W_i = \\sigma_i^{-1}(V_1\\cap V_2)\\subseteq U_i\\) has a positive Jacobian determinant. </p> <p>Equivalently, there exists a unit normal function \\(\\hat N_\\Sigma: \\Sigma\\rightarrow C = \\{(x,y,z): x^2+y^2+z^2=1\\}\\) where \\(\\hat N_\\Sigma\\) is smooth and continuous. In other words, the change is unit normal vector along any path on the surface must be continuous.</p>"},{"location":"mat363/surface_normals.html#example-mobius-band-non-orientable","title":"Example: Mobius Band (Non-orientable)","text":"<p>The Mobius band is the surface obtained by rotating a straight line segment \\(l\\) around its midpoint \\(p\\) at the same time as \\(p\\) moves around a circle \\(C\\).</p> Source code <pre><code>import plotly.graph_objects as go\nimport numpy as np\n\nt = np.arange(0, 2*np.pi, 0.05 * np.pi)\np = np.array((np.cos(t), np.sin(t), np.zeros_like(t)))\norient = np.array((np.sin(t/2), np.zeros_like(t), np.cos(t/2))) / 2.\ntop = p + orient\nbot = p - orient\nsurface = np.empty((3, top.shape[1] * 2))\nsurface[:, ::2] = top\nsurface[:, 1::2] = bot\n\nfig = go.Figure(\n    data=[\n        go.Scatter3d(x=top[0], y=top[1], z=top[2], mode=\"lines\", name=\"'top'\"),\n        go.Scatter3d(x=bot[0], y=bot[1], z=bot[2], mode=\"lines\", name=\"'bottom'\"),\n        go.Scatter3d(x=surface[0, ::3], y=surface[1, ::3], z=surface[2, ::3], mode=\"lines\", name=\"normal\")\n    ],\n    )\nfig.update_layout(margin=dict(l=0, r=0, b=0, t=0), height=480)\nwith open(\"../assets/mobius.json\", \"w\") as f:\n    f.write(fig.to_json())\n</code></pre> <p>As shown in the example, let \\(C\\) be the unit circle, and \\(l\\) be the line segment connecting \\((0,0, -1/2), (0,0, 1/2)\\). </p> <p>The Mobius band can be parameterized by 2 surface patches </p> \\[\\sigma(t, \\theta) = (\\cos\\theta(1-t\\sin\\frac{\\theta}{2}), \\sin\\theta(1-t\\sin\\frac{\\theta}{2}), t\\cos\\frac{\\theta}{2})\\] <p>where \\(U_1 = (-1/2, 1/2) \\times (0,2\\pi), U_2 = (-1/2, 1/2)\\times (-\\pi, \\pi)\\). </p> <p>Consider the unit normal </p> \\[\\begin{align*} \\partial_t\\sigma|{(0,\\theta_0)} &amp;= (-\\sin\\frac{\\theta_0}{2}\\cos\\theta_0, \\sin\\theta_0\\sin\\frac{\\theta_0}{2}, \\cos\\frac{\\theta_0}{2})\\\\ \\partial_\\theta\\sigma|_{(0, \\theta_0)} &amp;= (-\\sin\\theta_0, \\cos\\theta_0, 0)\\\\ \\partial_t\\sigma\\times \\partial_\\theta\\sigma &amp;= (-\\cos\\frac{\\theta}{2}\\cos\\theta, -\\cos\\frac{\\theta}{2}\\sin\\theta, -\\sin\\frac{\\theta}{2}\\cos^2\\theta -\\sin\\frac{\\theta}{2}\\sin^2\\theta)\\\\ N&amp;=  (-\\cos\\frac{\\theta}{2}\\cos\\theta, -\\cos\\frac{\\theta}{2}\\sin\\theta, -\\sin\\frac{\\theta}{2}) \\end{align*}\\] <p>Note that the unit normal vector \\(\\hat N = N\\) since \\(\\|N\\| = 1\\) Then, note that </p> \\[\\lim_{\\theta\\rightarrow0+} N(0,\\theta) =(-1, 0, 0) \\neq (1, 0, 0) = \\lim_{\\theta\\rightarrow 2\\pi-}N(0,  \\theta)\\] <p>contradict with the orientability assumption. </p> <p>Furthermore, a surface is non-orientable if it contains a sub surface diffeomorphic to the Mobius band. </p>"},{"location":"mat363/surface_tangent.html","title":"Surface Tangents and Derivatives","text":""},{"location":"mat363/surface_tangent.html#tangent-plane","title":"Tangent Plane","text":"<p>The tangent vector to a surface \\(\\Sigma\\) at point \\(p\\) are tangent vector to all smooth curves on \\(\\Sigma\\) through \\(p\\) at \\(p\\). </p> <p>The tangent plane to a surface \\(\\Sigma\\) at point \\(p\\) is a 2-dim linear subspace of \\(T_p\\Sigma\\subset\\mathbb R^3\\), a.k.a. a plane in \\(\\mathbb R^3\\) pass through the origin. its points are precisely all tangent vectors to \\(\\Sigma\\) at \\(p\\). </p>"},{"location":"mat363/surface_tangent.html#spanning-tangent-plane","title":"Spanning Tangent Plane","text":"<p>Claim Let \\(\\sigma:U\\rightarrow V\\cap \\Sigma\\) be a surface path containing \\(p\\). If \\(U\\) is spanned by vectors \\(u, v\\), then \\(T_p\\Sigma\\) is spanned by \\(d_u\\sigma, d_v\\sigma\\). </p> <p>proof. Consider a surface patch \\(\\sigma: U\\rightarrow V\\cap \\Sigma\\) containing \\(p\\). Let \\(\\gamma\\) be a smooth curve in \\(S\\) with some \\(\\gamma(t_0) = p\\). Then, consider \\(\\mathbf a:[t_0-\\epsilon, t_0+\\epsilon]\\rightarrow U\\) s.t. \\(\\gamma(t) =\\sigma(\\mathbf a(t)))\\), a.k.a. \\(\\mathbf a\\) is a path on \\(U\\) that is mapped to \\(\\gamma\\). Then, we must have that \\(\\mathbf a\\) is a smooth curve on \\(U\\). </p> <p>Let \\(a(t) = (u(t), v(t))\\), then by chain rule, we have </p> \\[\\frac{d\\gamma}{dt} = \\frac{d\\sigma}{d\\mathbf a}\\frac{d\\mathbf a}{dt} = \\frac{d\\sigma}{du}\\frac{du}{dt} + \\frac{d\\sigma}{dv}\\frac{dv}{dt}\\] <p>Therefore, \\(\\frac{d\\gamma}{dt}\\) is spanned by \\(d_u\\sigma,d_v\\sigma\\). </p> <p>Conversely, define \\(\\tilde \\gamma(t) = \\sigma(u(t_0)+\\lambda t, v(t_0) + \\mu t)\\) for some \\(\\lambda, \\mu\\). Then, \\(\\tilde \\gamma\\) is a smooth curve in \\(\\Sigma\\) and \\(\\gamma(0) = p\\) we have that </p> \\[\\frac{d\\tilde \\gamma}{dt} = \\frac{d\\sigma}{du}\\lambda + \\frac{d\\sigma}{dv}\\mu\\] <p>Therefore, every vector in \\(\\text{span}\\{d_u\\sigma, d_v\\sigma\\}\\) is the tangent vector at \\(p\\) of \\(\\Sigma\\). </p> <p>By regularity assumption of \\(\\sigma\\), we have that \\(\\text{span}\\{d_u\\sigma, d_v\\sigma\\}\\) are 2-dim, hence spans a plane, in which we call it the tangent plane. </p>"},{"location":"mat363/surface_tangent.html#independence-of-surface-patch-choices","title":"Independence of surface patch choices","text":"<p>Claim For some surface path \\(\\sigma(u,v)\\) at \\(p\\), the spanned subset space of \\(d_u\\sigma, d_v\\sigma\\) independent of the choice of \\(\\sigma\\). </p> <p>proof. Suppose that \\(\\tilde \\sigma: \\tilde U\\rightarrow V\\cap \\Sigma\\) is a reparameterization of the surface patch. Then, note that both \\(\\sigma, \\tilde\\sigma\\) are bijective and smooth. Therefore, exists a bijective smooth map \\(\\phi: U\\rightarrow\\tilde U\\) s.t. \\((\\tilde u, \\tilde v) = \\phi(u,v)\\). Then, we have </p> \\[\\begin{align*} \\sigma(u,v) &amp;= \\tilde\\sigma(\\tilde u(u, v), \\tilde v(u, v))\\\\ \\frac{d\\sigma}{du} &amp;= \\frac{d\\tilde \\sigma}{d\\tilde u}\\frac{d\\tilde u}{du} + \\frac{d\\tilde \\sigma}{d\\tilde v}\\frac{d\\tilde v}{du}\\\\ \\frac{d\\sigma}{dv} &amp;= \\frac{d\\tilde \\sigma}{d\\tilde u}\\frac{d\\tilde u}{dv} + \\frac{d\\tilde \\sigma}{d\\tilde v}\\frac{d\\tilde v}{dv} \\end{align*}\\] <p>Therefore, \\(d_u\\sigma\\) and \\(d_v\\sigma\\) are both linear combinations of \\(d_{\\tilde u}\\tilde \\sigma, d_{\\tilde v}\\tilde \\sigma\\). </p>"},{"location":"mat363/surface_tangent.html#surface-derivatives","title":"Surface Derivatives","text":""},{"location":"mat363/surface_tangent.html#implicit-function-theorem","title":"Implicit Function Theorem","text":"<p>By inverse function theorem, for non-linear function \\(F\\) IF  \\(F(\\mathbf x_0) = \\mathbf y_0 ,DF(\\mathbf x)\\) is invertible, and \\(y_0 + DF(x_0)(x-x_0)\\) is invertible THEN, the inverse exists, \\(F^{-1}(\\mathbf y_0)= \\mathbf x_0, \\mathbf y = \\mathbf x_0 + DF^{-1}|_{\\mathbf y_0}(\\mathbf y-\\mathbf y_0)\\)</p>"},{"location":"mat363/surface_tangent.html#regularity-conditions","title":"Regularity Conditions","text":"<p>Claim for any point \\(p\\) on the regular surface \\(\\Sigma\\), there exists an open set \\(V\\subset \\Sigma\\) s.t. \\(p\\in V\\) and \\(V\\) is the graph of a smooth function \\(z=\\phi(x,y), y=\\varphi(x,z), x=\\psi(y,z)\\).</p> <p>In other words, for any points on a surface. Locally, the surface patch can be mapped from a standard plane. </p> <p>proof. Let \\(\\sigma^{-1}(p) = (u_0, v_0)\\). Consider the surface patch mapping \\(\\sigma\\), by regularity we have that </p> \\[D\\sigma = \\begin{bmatrix}\\partial_u x&amp;\\partial_vx\\\\\\partial_uy&amp;\\partial_vy\\\\\\partial_u z&amp;\\partial_vz\\end{bmatrix}\\] <p>such that each of the \\(2\\times 2\\) submatrix are invertible. </p> <p>WLOG, assume that</p> \\[  \\begin{bmatrix}  \\frac{\\partial x}{\\partial u} (u_0, v_0)&amp;\\frac{\\partial v}{\\partial u} (u_0, v_0)\\\\  \\frac{\\partial z}{\\partial u} (u_0, v_0)&amp;\\frac{\\partial z}{\\partial u} (u_0, v_0)  \\end{bmatrix} \\] <p>is invertible, then consider the orthogonal projection \\(\\Pi_{XZ}\\) of \\(V\\) to XZ plane. Consider the composition \\(\\Pi_{XZ} \\circ \\sigma: U\\rightarrow V\\rightarrow W:= (u, v)\\rightarrow (x, 0, z)\\) where \\(W\\) is an open subset of XZ plane. </p> <p>Therefore, the claim is equivalent to that \\(\\Pi_{XZ}\\circ\\sigma\\) is locally invertible, a.k.a. </p> \\[\\exists f: W \\rightarrow U. \\forall p\\in V. f\\circ(\\Pi_{XZ}\\circ\\sigma)(u, v) = (u, v)\\] <p>Consider the Taylor expansion of \\(\\Pi_{XZ}\\circ \\sigma\\) near \\((u_0, v_0)\\)</p> \\[\\begin{align*} \\Pi_{XZ}\\circ \\sigma(u,v) &amp;= \\Pi_{XZ}\\circ \\sigma (u_0, v_0) + D[\\Pi_{XZ}\\circ \\sigma]\\vert_{(u_0, v_0)}\\begin{bmatrix}u-u_0\\\\v-v_0\\end{bmatrix} + rem.\\\\ &amp;= \\Pi_{XZ}\\circ \\sigma (u_0, v_0) + \\Pi_{XZ}(D\\sigma\\vert_{(u_0, v_0)}) \\begin{bmatrix}u-u_0\\\\v-v_0\\end{bmatrix}+ rem.\\\\ &amp;= \\Pi_{XZ}\\circ \\sigma (u_0, v_0) + \\begin{bmatrix}\\frac{\\partial x}{\\partial u} (u_0, v_0)&amp;\\frac{\\partial v}{\\partial u} (u_0, v_0)\\\\\\frac{\\partial z}{\\partial u} (u_0, v_0)&amp;\\frac{\\partial z}{\\partial u} (u_0, v_0)\\end{bmatrix}\\begin{bmatrix}u-u_0\\\\v-v_0\\end{bmatrix} + rem. \\end{align*}\\] <p>Therefore, we are approximating the composition by the invertible linear map. Then, by implicit funciton theorem, there exists a unique solution \\(y = \\psi(x, z)\\) near \\((x_0, z_0)\\) where \\(\\psi\\) is differentiable. </p> <p>Corollary Regularity condition satisfies IFF the tangent plane at \\(p\\) exists, and indeed, is a plane. </p>"},{"location":"mat363/surface_tangent.html#smooth-maps","title":"Smooth Maps","text":"<p>Consider \\(\\Sigma_1, \\Sigma_2\\) to be smooth surfaces, and let \\(f: \\Sigma_1\\rightarrow \\Sigma_2\\) be a map.   </p> <p>For some point \\(p_1\\in \\Sigma_1\\), take \\(p_2 = f(p_1) \\in \\Sigma_2\\). Let \\(\\sigma_1: U_1\\rightarrow V_1\\cap \\Sigma_1, p_1\\in V_1\\) and \\(\\sigma_2: U_2\\rightarrow V_2\\cap \\Sigma_2, p_2\\in V_2\\). Consider the composition function \\(g:=\\sigma_2^{-1}\\circ f\\circ \\sigma_1: U_1\\subset \\mathbb R^2\\rightarrow U_2\\subset \\mathbb R^2\\), i.e. </p> \\[g(u,v) = \\sigma_2^{-1}\\circ f\\circ \\sigma_1 (u,v) = (a(u,v), \\beta(u,v))\\] <p>Then, we define \\(f\\) is differentiable (smooth) at \\(p = \\sigma(u,v)\\) if \\(g\\) is differentiable as \\(Dg = \\begin{bmatrix}\\partial_u a&amp;\\partial_va \\\\\\partial_u \\beta&amp;\\partial_v\\beta\\end{bmatrix}\\), and \\(f\\) is a differentiable (smooth) map if \\(\\forall p\\) is differentiable. </p> <p>If we linearize \\(f\\) near a point \\(p_1\\), then we have that </p> \\[Df : T_{p_1}\\Sigma_1\\rightarrow T_{p_2}\\Sigma_2\\] <p>maps from a a tangent plane \\(T_{p_1}\\Sigma_1 = \\text{span}\\{\\partial_u\\sigma_1, \\partial_v\\sigma_1\\}\\) to \\(T_{p_2}\\Sigma_2 = \\text{span}\\{\\partial_u\\sigma_2, \\partial_v\\sigma_2\\}\\)</p>"},{"location":"notes/llama.html","title":"LLM Basics","text":"<p>The note will focus on transformer-based large language models, their inference pass, and some implementation basics. The note mainly use LLaMA series Dubey et al. [2024]<sup>1</sup><sup>2</sup> as a model example, and <code>hunggingface/transformers</code> as a reference for LLM framework implementation. </p> <p>Additional resources:  - <code>huggingface/nlp-course</code>  - GPT from scratch by Jay Mody</p>"},{"location":"notes/llama.html#pipeline-components","title":"Pipeline Components","text":"<p>As a high-level view, a LLM inference pipeline consists of </p> <ul> <li>tokenizer: input natural language text stream, output tokens. </li> <li>embedding: maps the tokens into a numerical format so that the model can consume. </li> <li>model: the actual LLM model, which takes the embedding and outputs according to the model tasks. <ul> <li>A typical transformer model has a <code>encoder</code> (plus <code>positional encoding</code>) to encode the input into feature vectors, a <code>decoder</code> that takes the features and other inputs to generate outputs.  </li> <li>Primarily if we consider the text generation task, we only need a <code>decoder</code>. </li> </ul> </li> <li>postprocessing: Take the outputs from the model (often logits of embeddings), and format them back to text. </li> </ul>"},{"location":"notes/llama.html#tokenizers","title":"Tokenizers","text":"<p>In general, tokenizer split the characters into sequences of tokens. In also handles irregular or illegal input, do normalization (striping whitespace, remove accent chars, lowercasing, etc.).  </p> <p>A simple view is to consider the token as one word, but not always true. Typically, the tokenizer only uses CPU, but not always true for some modern models. </p> <pre><code>from tokenizer import Tokenizer\ntokenizer = Tokenizer.from_file(\"tokenizer.json\")\noutput = tokenizer.encode(\"Hello, y'all! How are you \ud83d\ude01 ?\")\nprint(output.tokens)\n# [\"Hello\", \",\", \"y\", \"'\", \"all\", \"!\", \"How\", \"are\", \"you\", \"[UNK]\", \"?\"]\nprint(output.ids)\n# [27253, 16, 93, 11, 5097, 5, 7961, 5112, 6218, 0, 35]\n</code></pre> <p>Note that the tokenizer is often associated with the specific model. </p> OpenAI tokenizer"},{"location":"notes/llama.html#embedding","title":"Embedding","text":"<p>The tokenizer generate a sequence of tokens (or token IDs), but the model cannot directly consume them as input. Using an <code>embedding</code>, which serve as a fixed size dictionary of all tokens. </p> <p>In general, we will also add a positional encoding to add an extra understanding of where the input is within the sentence. </p> <pre><code># embedding is a lookup table of size (n_vocab, dim)\nembeddings = Embedding(\n    num_embedding=n_vocab, # number of vocab\n    embedding_dim=embedding_dim # a configurable dim for embedding\n)\n# pos_encoder is a Encoding of size (max_seq_len, dim)\npos_encoder = PositionalEncoding(\n    seq_len=max_seq_len, # configurable maximum sequence length\n    dim=embedding_dim\n)\n# token_ids: Array[Int], shape (seq_len, )\n# x: Array[Float], shape (seq_len, dim)\nx = embeddings[token_ids] + pos_encoder[range(len(token_ids))]\n</code></pre>"},{"location":"notes/llama.html#decoder-only-transformers","title":"Decoder-only Transformers","text":"<p>We focus on the auto-regressive models, or the decoder-only transformer models.   </p> <p></p> <p>The embedding is passed through many Transformer blocks, and finally outputs the logits of the vocabulary to generate the next most likely word.</p>"},{"location":"notes/llama.html#attention-module","title":"Attention Module","text":"<p>Consider an intuitive example: we have a set of word keys \\(k_1, k_2, ..., k_n\\) and each word \\(k_i\\) is associated with some feature vector \\(v_i\\), now we have a new word \\(q\\) to query, and we'd like to compute the value vector \\(v_q = \\sum_{i=1}^n a_i v_i\\) with some attention score \\(a_i\\), and such \\(a_i\\) represents how similar/relevant is the query to the \\(i\\)th word. </p> <p>Therefore, let \\(K\\: (n_k\\times d_k)\\) be the stacked matrix of keys, \\(V\\: (n_k\\times d_v)\\) be the stacked matrix of value vectors, and \\(Q: (n_q\\times d_v)\\) be the stack matrix of queries. We can derive the attention  to be </p> \\[A = \\text{softmax}(\\frac{QK^T}{\\sqrt{d_k}}) V\\] <p>(See an intuitive explanation from Jay Mody, and detailed derivation in Attention is all You Need [Vaswani 2017]<sup>3</sup>)</p>"},{"location":"notes/llama.html#self-attention","title":"Self-attention","text":"<p>An interesting discovery is that \\(k\\) and \\(v\\) can come from the same source, and we gets self attention, i.e. input sequence attend to itself. which means <code>attention(q=x, k=x, v=x)</code>, which is just the similarity of all the words \\(A = \\text{softmax}(XX^T/\\sqrt{d_k}) X\\) to each other in the sentence, and no trainable parameters to embed the global context. </p> <p>Therefore, we can introduce projections for the input \\(Q = W_Q X, K = W_K X, V=W_VX\\) and bring it back to original dimension by \\(Y = W_{proj} A\\), all the weight matrices are now trainable. In practice, we can stack \\(W_Q, W_K, W_V\\) into one matrix to combine the multiplication for a better parallelism. </p>"},{"location":"notes/llama.html#multi-head","title":"Multi-head","text":"<p>To have a truly \"large\" language model, we want the projections to have more parameters. However, \\(QK^T\\) part of the attention takes </p> \\[\\text{FLOP} = n_Q\\times d_K \\times d_K \\times n_K\\] <p>Multi-head is introduced to reduce computation, in which we split the \\(d_K, d_V\\) into \\(h\\) \"heads\", i.e. smaller, separated features vectors. We compute attention on each and stack them back, so that the computation is </p> \\[h(n_q\\times \\frac{d_K}{h} \\times\\frac{d_K}{h} \\times n_K) = \\frac{1}{h}\\text{FLOP}\\]"},{"location":"notes/llama.html#causal-masking","title":"Causal Masking","text":"<p>For a text generation model, all words should only see words before it. Otherwise, it will be biased towards the known answer. </p> <p>One natural way is to mask out the relevance in the context. Which means \\(0\\) for all the keys after the current key. However, we need to pass a <code>softmax</code> and have \\(0\\) in the output. We can do this by adding a negative-infinity matrix \\(M\\) to \\(QK^T\\). </p>"},{"location":"notes/llama.html#kv-caching-for-casual-inference","title":"KV Caching for Casual Inference","text":"<p>For text generation tasks with a transformer model, the inference is done as </p> <pre><code>prompt_tokens = tokenizer.encode(input_text)\n\nfor _ in range(n_next_tokens):\n    new_token = model(prompt_tokens)\n    prompt_tokens.append(new_token)\n\noutput_tokens = tokenizer.decode(prompt_tokens)\n</code></pre> <p>Because we always want to generate new tokens based on all previous context. However, in each iteration we only have 1 new token, we are always recomputing the prompt tokens. Considering the computation in <code>attention</code> module, the overhead is exponential to the <code>max_sequence_len</code>. </p> <p>However, casual inference masks out tokens after the current token. For each queried token, its attention \\(A, Q, K, V\\) are only relevant to previous tokens. Therefore, for each iteration - We only need to query the newly generated input. - We reuse all previous \\(K,V\\), concat with the new \\(k, v\\) w.r.t input \\(x\\). </p> <p>The inference becomes</p> <pre><code>prompt_tokens = tokenizer.encode(input_text)\n\n# context encoding phase\n# K_cache, V_cache shape (len(prompt_tokens), ...)\nnew_token, K_cache, V_cache = model(\n    prompt_tokens, K_cache=[], V_cache=[])\npropmpt_tokens.append(new_token)\n\n# token generation phase\nfor _ in range(n_next_tokens):\n    # new_K_cache, new_V_cache, shape (1, ...)\n    new_token, new_K_cache, new_V_cache = model(\n        new_token, K_cache=K_cache, V_cache=V_cache)\n    K_cache.append(new_K_cache)\n    V_cache.append(new_V_cache)\n    prompt_tokens.append(new_token)\n\noutput_tokens = tokenizer.decode(prompt_tokens)\n</code></pre>"},{"location":"notes/llama.html#putting-all-together","title":"Putting all Together","text":"<pre><code># forward part of Attention module from llama3\n# https://github.com/meta-llama/llama3/blob/main/llama/model.py\ndef forward(\n    self,\n    x: torch.Tensor,\n    start_pos: int,\n    freqs_cis: torch.Tensor,\n    mask: Optional[torch.Tensor],\n):\n    bsz, seqlen, _ = x.shape\n\n    # projections\n    xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)\n\n    xq = xq.view(bsz, seqlen, self.n_local_heads, self.head_dim)\n    xk = xk.view(bsz, seqlen, self.n_local_kv_heads, self.head_dim)\n    xv = xv.view(bsz, seqlen, self.n_local_kv_heads, self.head_dim)\n\n    # llama3's rotary positional encoding\n    # refer to the original paper for detail\n    xq, xk = apply_rotary_emb(xq, xk, freqs_cis=freqs_cis)\n\n    # KV caching\n    self.cache_k = self.cache_k.to(xq)\n    self.cache_v = self.cache_v.to(xq)\n\n    self.cache_k[:bsz, start_pos : start_pos + seqlen] = xk\n    self.cache_v[:bsz, start_pos : start_pos + seqlen] = xv\n\n    keys = self.cache_k[:bsz, : start_pos + seqlen]\n    values = self.cache_v[:bsz, : start_pos + seqlen]\n\n    # Multi-head\n\n    # repeat k/v heads if n_kv_heads &lt; n_heads\n    keys = repeat_kv(\n        keys, self.n_rep\n    )  # (bs, cache_len + seqlen, n_local_heads, head_dim)\n    values = repeat_kv(\n        values, self.n_rep\n    )  # (bs, cache_len + seqlen, n_local_heads, head_dim)\n\n    xq = xq.transpose(1, 2)  # (bs, n_local_heads, seqlen, head_dim)\n    keys = keys.transpose(1, 2)  \n    # (bs, n_local_heads, cache_len + seqlen, head_dim)\n    values = values.transpose(1, 2)  \n    # (bs, n_local_heads, cache_len + seqlen, head_dim)\n\n    # self-attention\n    scores = torch.matmul(xq, keys.transpose(2, 3)) / math.sqrt(self.head_dim)\n    if mask is not None:\n        scores = scores + mask  # (bs, n_local_heads, seqlen, cache_len + seqlen)\n    scores = F.softmax(scores.float(), dim=-1).type_as(xq)\n    output = torch.matmul(scores, values)  # (bs, n_local_heads, seqlen, head_dim)\n    output = output.transpose(1, 2).contiguous().view(bsz, seqlen, -1)\n\n    # projection back\n    return self.wo(output)\n</code></pre>"},{"location":"notes/llama.html#feed-forward-network","title":"Feed forward Network","text":"<p>Simple fully connected linear layers to expand and contract the embedding dimension, so that we have more trainable weights for the context. For LLaMA3 the design is a bit different to have more efficiency. </p> <pre><code># https://github.com/meta-llama/llama3/blob/main/llama/model.py\nclass FeedForward(nn.Module):\n    def __init__(\n        self,\n        dim: int,\n        hidden_dim: int,\n        multiple_of: int,\n        ffn_dim_multiplier: Optional[float],\n    ):\n        super().__init__()\n        hidden_dim = int(2 * hidden_dim / 3)\n        # custom dim factor multiplier\n        if ffn_dim_multiplier is not None:\n            hidden_dim = int(ffn_dim_multiplier * hidden_dim)\n        hidden_dim = multiple_of * ((hidden_dim + multiple_of - 1) // multiple_of)\n\n        self.w1 = ColumnParallelLinear(\n            dim, hidden_dim, bias=False, gather_output=False, init_method=lambda x: x\n        )\n        self.w2 = RowParallelLinear(\n            hidden_dim, dim, bias=False, input_is_parallel=True, init_method=lambda x: x\n        )\n        self.w3 = ColumnParallelLinear(\n            dim, hidden_dim, bias=False, gather_output=False, init_method=lambda x: x\n        )\n\n    def forward(self, x):\n        return self.w2(F.silu(self.w1(x)) * self.w3(x))\n</code></pre> <ol> <li> <p>Touvron, H., Martin, L., Stone, K., et al. 2023. Llama 2: Open foundation and fine-tuned chat models.\u00a0\u21a9</p> </li> <li> <p>Dubey, A., Jauhri, A., Pandey, A., et al. 2024. The llama 3 herd of models.\u00a0\u21a9</p> </li> <li> <p>Vaswani, A. 2017. Attention is all you need. Advances in Neural Information Processing Systems.\u00a0\u21a9</p> </li> </ol>"},{"location":"notes/trainium.html","title":"AWS Trainium and NKI kernel","text":"<p>Based on Stanford CS149 Assignment,  NKI kernel Docs</p>"},{"location":"notes/trainium.html#trainium-hardware-architecture","title":"Trainium Hardware Architecture","text":"<ul> <li>HBM: High bandwidth memory, device memory. Host to device should be managed by ML framework or external of NKI kernel. </li> <li>SBUF: State buffer. Software-managed on-chip SRAM. In NKI programming, on-chip SRAM is not a hardware managerd \"cache\", HBM to SBUF needs explicit <code>load</code> and <code>store</code>. </li> <li>PSUM: Partial Sum Buffer, a small, dedicated memory designed for storing matrix multiplication results. </li> <li>Tensor Engine: for matmuls, or other operators that can be executed as matmuls. The engine has 128x128 systolic processing elements, which streams input data from SBUF and write output to PSUM. </li> <li>Vector Engine: for vector operations that depends on multiple elements from input tensors (vector reduction, element-wise binary operations). VectorE consists of 128 parallel vector lanes. </li> <li>Scaler Engine: for element-wise operations, where every element in the output tensor only depends on one element of the input tensor. Usually used for hardware-accelerated activation functions. ScalarE consists of 128 parallel vector lanes. </li> <li>GpSimd Engine: for general SIMD operations. Basically a 8-core CPU, with 512-bit vector machine. </li> </ul>"},{"location":"notes/trainium.html#systolic-arrays","title":"Systolic Arrays","text":"<p>Trainium chips are operated in systolic arrays, hence all data movements and computations are tiled. From the hardware perspective, the on-chip memories, SBUF and PSUM, are arranged as 2D memory arrays. The first dimension is the partition dimension <code>P</code> with <code>nki.tile_size.pmax = 128</code> memory partitions that can be read and written in parallel by compute engines. The second dimension is the free dimension <code>F</code> where elements are read and written sequentially.</p> <p></p>"},{"location":"notes/trainium.html#psum-constraints","title":"<code>PSUM</code> Constraints","text":"<p>Remind that <code>PSUM</code> is the small, dedicated on-chip memory for matmul reduction operations. Currently, its size is <code>2MiB</code>. The hardware design poses a constraint for tiles in PSUM, <code>nki.tile_size.psum_fmax == 512</code>, which comes from <code>2MiB / 128 / 32B</code>. </p> <p>For matrix multiplicaion of size \\((M, K) \\times (K, N)\\). The contraction dimension \\(K\\)  consider how matrix is mapped onto tensorE and PSUM. We define that input tiles F dimension size must not exceed <code>M_per_tile = nki.tile_size.gemm_stationary_fmax == 128</code> on the left-hand side (LHS), or <code>N_per_tile = nki.tile_size.gemm_moving_fmax == 512</code> on the right hand size (RHS). More explanation in Tensor Engine: Matrix Multiplication.</p>"},{"location":"notes/trainium.html#nki-neuron-kernel-interface","title":"NKI (Neuron Kernel Interface)","text":"<p>NKI kernels are written in Python. Essentially, NKI programming and some optimization considerations are</p> <ol> <li> <p>Loading data from HBM to on-chip SBUF</p> <ul> <li>How to minimize the data movement.</li> <li>How to do data tiling so that we can fully use the 128 lane.</li> </ul> </li> <li> <p>Computations on the compute engines</p> <ul> <li>How to overlap computations on different cores. </li> </ul> </li> <li> <p>Storing outputs from SBUF back to HBM</p> </li> </ol> <p>Each NKI kernel is a python function decorated with <code>@nki.jit</code>, and the arguments should be tensors already reside in HBM. NKI kernels can be directly inserted into ML frameworks (PyTorch, Tensorflow). </p> <pre><code>@nki.jit\ndef vector_add(a_vec: Tensor, b_vec: Tensor) -&gt; Tensor:\n    # Allocate space for the output vector in HBM\n    out = nl.ndarray(shape=a_vec.shape, dtype=a_vec.dtype, buffer=nl.hbm)\n\n    # Load the input vectors from HBM into variables stored in SBUF \n    a = nl.load(a_vec)\n    b = nl.load(b_vec)\n\n    # Add the input vectors\n    res = nl.add(a, b)\n\n    # Store the result into HBM\n    nl.store(out, value=res)\n\n    return out\n</code></pre> <p>NKI provides a <code>nki.baremetal</code> decorator function to directly run kernels from numpy arrays. </p> <pre><code>vec_size = nki.tile_size.pmax # 128\na = np.random.rand(vec_size, dtype=np.float32)\nb = np.random.rand(vec_size, dtype=np.float32)\nout = nki.baremetal(vector_add)(a, b)\n</code></pre>"},{"location":"notes/trainium.html#data-tiling","title":"Data Tiling","text":"<p>The on-chip memories, SBUF and PSUM, store data that is arranged as 2D memory arrays. The first dimension of the 2D array is called the \"partition dimension\" P. The second dimension is referred to as the \"free dimension\" F (more details later). For vector add, we have <code>P = vec_size, F = 1</code></p>"},{"location":"notes/trainium.html#partition-dimension","title":"Partition Dimension","text":"<p>Note that if we run the above code with <code>vec_size &gt; 128</code>, we get </p> <pre><code>&gt; a = nl.load(a_vec)\n\nValue Error: number of partition in src[12800, 1] of 'load' \nexceed architecture limitation of 128.\n</code></pre> <p>NeuronCores loads 128 elements in parallel along the P-dim in each cycle, which means the max size of P dimension for each <code>nl.load</code> is 128. Therefore, we need to manually tile of data into 128 chunks. </p> <pre><code>@nki.jit\ndef vector_add_tiled(a_vec, b_vec):\n\n    CHUNK_SIZE = nki.tile_size.pmax  # 128\n\n    out = nl.ndarray(shape=a_vec.shape, dtype=a_vec.dtype, buffer=nl.hbm)\n    M = a_vec.shape[0]\n\n    # nl.affine_range assumes there are no loop-carried dependencies\n    # and allow more aggressive optimizations for the compiler pipelining\n    for m in nl.affine_range((M // ROW_CHUNK)):\n\n        # Allocate row-chunk sized tiles for the input vectors\n        a_tile = nl.ndarray((CHUNK_SIZE, 1), dtype=a_vec.dtype, buffer=nl.sbuf)\n        b_tile = nl.ndarray((CHUNK_SIZE, 1), dtype=b_vec.dtype, buffer=nl.sbuf)\n\n        # Load a chunk of rows\n        a_tile[...] = nl.load(a_vec[m * CHUNK_SIZE : (m + 1) * CHUNK_SIZE])\n        b_tile[...] = nl.load(b_vec[m * CHUNK_SIZE : (m + 1) * CHUNK_SIZE])\n\n        res = nl.add(a_tile, b_tile)\n        nl.store(out[m * CHUNK_SIZE : (m + 1) * CHUNK_SIZE], value=res)\n\n    return out\n</code></pre>"},{"location":"notes/trainium.html#free-dimension","title":"Free Dimension","text":"<p>The compiler is responsible the <code>store</code> and <code>load</code> are converted into direct memory access (DMA) instructions. Similar to how CUDA hides the data loading to threads, NeuronCore has 16 DMA engines to move multiple lanes of data in parallel / in pipeline. DMA are parallelized over the free dimension. In addition, the computation engines support pipelining over the free dimension. </p> <pre><code>@nki.jit\ndef vector_add_stream(a_vec, b_vec):\n\n    # The maximum size of our Partition Dimension\n    PARTITION_DIM = nki.tile_size.pmax # 128\n\n    # Free dim is a tunable parameter, and it depends on\n    # compiler optimizations/hardware specifications\n    FREE_DIM = 200\n\n    # The total size of each tile\n    TILE_M = PARTITION_DIM * FREE_DIM\n\n    # Get the total number of vector rows\n    M = a_vec.shape[0]\n\n\n    # Reshape the the input vectors\n    a_vec_re = a_vec.reshape((M // TILE_M, PARTITION_DIM, FREE_DIM))\n    b_vec_re = b_vec.reshape((M // TILE_M, PARTITION_DIM, FREE_DIM))\n\n    # Allocate space for the reshaped output vector in HBM\n    out = nl.ndarray(shape=a_vec_re.shape, dtype=a_vec_re.dtype, buffer=nl.hbm)\n\n    # Loop over the total number of tiles\n    for m in nl.affine_range((M // TILE_M)):\n\n        # Allocate space for a reshaped tile\n        a_tile = nl.ndarray((PARTITION_DIM, FREE_DIM), dtype=a_vec.dtype, buffer=nl.sbuf)\n        b_tile = nl.ndarray((PARTITION_DIM, FREE_DIM), dtype=a_vec.dtype, buffer=nl.sbuf)\n\n        # Load the input tiles\n        a_tile = nl.load(a_vec_re[m])\n        b_tile = nl.load(b_vec_re[m])\n\n        # Add the tiles together\n        res = nl.add(a_tile, b_tile)\n\n        # Store the result tile into HBM\n        nl.store(out[m], value=res)\n\n    # Reshape the output vector into its original shape\n    out = out.reshape((M,))\n\n    return out\n</code></pre>"},{"location":"notes/trainium.html#data-movement-and-computation","title":"Data Movement and Computation","text":"<p>F-dim is a tunable parameter, each DMA transfer has an overhead. However, F-dim is not always a \"bigger means better\" thing. Choosing a smaller F-dim may allow a better pipelining. In this case, since <code>add</code> requires small computation cycles, smaller free-dim means more but quicker data movement, and allow for more overlapping between the engines. In practice, we need to profile and decide the dimension size to harness better performance. </p>"},{"location":"notes/trainium.html#tensor-engine-matrix-multiplication","title":"Tensor Engine: Matrix Multiplication","text":"<p>Consider the matmul, note that the hardware constrains that PSUM can only hold \\(128\\times 512\\) elements. Thus, <code>TILE_M = nki.tile_size.gemm_stationary_fmax == 128</code>, <code>TILE_N = nki.tile_size.gemm_moving_fmax == 512</code>. In addition, the contraction dimension \\(K\\) has to be loaded in parallel and do element-wise multiplication, thus <code>TILE_K = nki.tile_size.pmax = 128</code>.</p> <p>Similar to any parallel programming architecture, data locality (reducing data loading) is a key optimization. Check Matrix Multiplications notes. Reordering loop (<code>nkm</code> instead <code>mnk</code> to avoid reloading lhs elements) and blocked matmul apply to our case. </p> <p>For trainium, <code>SBUF</code> is <code>24MiB</code>. Assuming that we know the dtype and the rough shape of matrices (so that we can better choose number of tiles per block), we can compute the number of blocks s.t. data movement is minimized. </p> nki blocked matmul <pre><code>import neuronxcc.nki as nki\nimport neuronxcc.nki.language as nl\nimport neuronxcc.nki.isa as nisa\n\n@nki.jit\ndef nki_matmul_fully_optimized_(\n    lhsT,\n    rhs,\n):\n\n  TILES_IN_BLOCK_M=16\n  TILES_IN_BLOCK_N=2\n  TILES_IN_BLOCK_K=8\n\n  K, M = lhsT.shape\n  K_, N = rhs.shape\n  assert K == K_, \"lhsT and rhs must have the same contraction dimension\"\n  result = nl.ndarray((M, N), dtype=lhsT.dtype, buffer=nl.shared_hbm)\n\n  TILE_M = nl.tile_size.gemm_stationary_fmax  # 128\n  TILE_K = nl.tile_size.pmax  # 128\n  TILE_N = nl.tile_size.gemm_moving_fmax  # 512\n\n  BLOCK_M = TILE_M * TILES_IN_BLOCK_M\n  BLOCK_N = TILE_N * TILES_IN_BLOCK_N\n  BLOCK_K = TILE_K * TILES_IN_BLOCK_K\n\n  # the size has to be multiple of block size\n  # nl indexing cannot handle out of bound indexing\n  assert M % BLOCK_M == 0\n  assert N % BLOCK_N == 0\n  assert K % BLOCK_K == 0\n\n  NUM_BLOCK_M = M // BLOCK_M\n  NUM_BLOCK_N = N // BLOCK_N\n  NUM_BLOCK_K = K // BLOCK_K\n\n  # Blocking N dimension (the RHS free dimension)\n  for n in nl.affine_range(NUM_BLOCK_N):\n    result_tiles = nl.zeros((NUM_BLOCK_M, TILES_IN_BLOCK_M, TILES_IN_BLOCK_N,\n                             nl.par_dim(TILE_M), TILE_N),\n                            dtype=lhsT.dtype,\n                            buffer=nl.sbuf)\n\n    # Blocking K dimension (the contraction dimension)\n    # Use `sequential_range` because we do not want the compiler to change this loop by, \n    # for example, vectorizing it\n    for k in nl.sequential_range(NUM_BLOCK_K):\n      # Loading tiles from rhs\n      # setting the load tile to `TILE_K x BLOCK_SIZE_N` to optimize DMA performance\n      i_rhs = nl.mgrid[0:TILE_K, 0:BLOCK_N]\n      rhs_tiles = nl.ndarray((TILES_IN_BLOCK_K, nl.par_dim(TILE_K), BLOCK_N),\n                             dtype=rhs.dtype,\n                             buffer=nl.sbuf)\n\n      for bk_r in nl.affine_range(TILES_IN_BLOCK_K):\n        rhs_tiles[bk_r, i_rhs.p, i_rhs.x] = nl.load(\n            rhs[(TILES_IN_BLOCK_K * k + bk_r) * TILE_K + i_rhs.p,\n                BLOCK_N * n + i_rhs.x])\n\n      # Blocking M dimension (the LHS free dimension)\n      for m in nl.affine_range(NUM_BLOCK_M):\n        # Loading tiles from lhsT\n        i_lhsT = nl.mgrid[0:TILE_K, 0:BLOCK_M]\n        lhsT_tiles = nl.ndarray((TILES_IN_BLOCK_K, nl.par_dim(TILE_K), BLOCK_M),\n                                dtype=lhsT.dtype,\n                                buffer=nl.sbuf)\n        for bk_l in nl.affine_range(TILES_IN_BLOCK_K):\n          lhsT_tiles[bk_l, i_lhsT.p, i_lhsT.x] = nl.load(\n              lhsT[(TILES_IN_BLOCK_K * k + bk_l) * TILE_K + i_lhsT.p,\n                   BLOCK_M * m + i_lhsT.x])\n\n        # Do matmul with all tiles in the blocks\n        i_lhsT_mm = nl.mgrid[0:TILE_K, 0:TILE_M]\n        i_rhs_mm = nl.mgrid[0:TILE_K, 0:TILE_N]\n        i_res_mm = nl.mgrid[0:TILE_M, 0:TILE_N]\n        for bn in nl.affine_range(TILES_IN_BLOCK_N):\n          for bm in nl.affine_range(TILES_IN_BLOCK_M):\n            res_tile = nl.zeros((TILE_M, TILE_N), dtype=nl.float32, buffer=nl.psum)\n\n            for bk in nl.affine_range(TILES_IN_BLOCK_K):\n              res_tile[...] += nisa.nc_matmul(\n                  lhsT_tiles[bk, i_lhsT_mm.p, bm * TILE_M + i_lhsT_mm.x],\n                  rhs_tiles[bk, i_rhs_mm.p, bn * TILE_N + i_rhs_mm.x])\n\n            # Accumulate on corresponding SBUF tile\n            result_tiles[m, bm, bn, i_res_mm.p,\n                         i_res_mm.x] += res_tile[i_res_mm.p, i_res_mm.x]\n\n    # Copying the result from SBUF to HBM\n    for m in nl.affine_range(NUM_BLOCK_M):\n      for bm in nl.affine_range(TILES_IN_BLOCK_M):\n        i_res = nl.mgrid[0:TILE_K, 0:TILE_N]\n        i_res_packed = nl.mgrid[0:TILE_K, 0:BLOCK_N]\n        result_packed = nl.ndarray((TILE_K, BLOCK_N),\n                                   dtype=result_tiles.dtype,\n                                   buffer=nl.sbuf)\n\n        # coalesce result tiles for better DMA performance\n        for bn in nl.affine_range(TILES_IN_BLOCK_N):\n          result_packed[i_res.p,\n                        bn * TILE_N + i_res.x] = nl.copy(result_tiles[m, bm, bn,\n                                                                      i_res.p,\n                                                                      i_res.x])\n        nl.store(result[(TILES_IN_BLOCK_M * m + bm) * TILE_K + i_res_packed.p,\n                        BLOCK_N * n + i_res_packed.x],\n                 value=result_packed[i_res_packed.p, i_res_packed.x])\n\n  return result\n</code></pre>"},{"location":"sta303/Summary.html","title":"Summary Sheet","text":""},{"location":"sta303/Summary.html#pooled-two-sample-t-tests","title":"pooled two sample t-tests","text":"<ul> <li>assume equal population variances</li> <li>\\(s^2_p = \\frac{(n_x-1)s_x^2 + (n_y-1)s_y^2}{n_x+n_y-2}\\)</li> <li>\\(t = \\frac{(\\bar{x} - \\bar{y} -D_0)}{\\sqrt{s_p^2(n_x^{-1}+n_y^{-1})} }\\sim t_{(n_x-1)(n_y-1)}\\)</li> </ul>"},{"location":"sta303/Summary.html#slm-dummy-variable","title":"SLM Dummy variable","text":"<p>\\(Y_i=\\beta_0+\\beta_1X_i + \\epsilon_i\\) assumptions</p> <ul> <li>linear model is appropriate</li> <li>\\(\\epsilon_i \\sim N(0,\\sigma^2)\\)</li> </ul> <p>Hypothesis test: \\(H_0:\\beta_1 = 0, H_a:\\beta_1\\neq 0\\) \\(t=b_1/se(b_1)\\sim t_{N-2}\\), \\(N\\) is the total number of observations</p>"},{"location":"sta303/Summary.html#one-way-anova-glm","title":"One Way ANOVA  &amp; GLM","text":"<p>\\(Y_i=\\vec{X_i}\\vec{\\beta}+\\vec{\\epsilon}\\)</p> <ul> <li>assumptions: same as dummy variable, jointly normally distributed errors</li> <li>\\(F=MSReg/MSE = \\frac{(SSR/G-1)}{SSE/(N-G)}\\sim F_{G-1,N-G}\\)</li> </ul>"},{"location":"sta303/Summary.html#multiple-comparisons","title":"Multiple Comparisons","text":"<p>Bonferrroni's Method: </p> <ul> <li>\\(P(\\cup A_i)\\leq \\sum P(A_i)\\)</li> <li>\\(k= {G \\choose 2}\\)</li> <li>level at \\(a/k\\)</li> </ul> <p>Tukey's Method: less conservative than Bonferroni's method</p>"},{"location":"sta303/Summary.html#two-way-anova","title":"Two Way ANOVA","text":"<p>Overall vs. Partiral F-tests $H_0: $ a subset of \\(\\beta\\)'s are 0. $H_a: $ some of the \\(\\beta\\) in the subset are not 0. Let FULL model be with all explanatory variables, REDUCED be without the coefficients in testing.  </p> \\[F=\\frac{(RSS_r - RSS_f)/\\# \\beta \\text{ tested} }{MSE_f}\\sim F_{\\# \\beta \\text{ tested}, d.f. RES_f}\\] <p>Describing \"interactions\"</p>"},{"location":"sta303/Summary.html#glm-vs-transformation","title":"GLM vs. Transformation","text":"<p>Transform Y so it has an approximate normal  distribution with constant variance, </p> <p>GLM: distribution of Y not restricted to Normal, model parameters describe \\(g(E(Y))\\) rather than \\(E(g(Y))\\) GLMs provide a unified theory of modeling that encompasses the most important models for continuous and discrete variables. </p>"},{"location":"sta303/Summary.html#glm-tests","title":"GLM tests","text":"<p>Wald \\(H_0:\\beta_j = 0, H_a: \\beta_j\\neq 0\\) \\(z=\\hat\\beta_j / se(\\hat\\beta_j)\\sim N(0,1)\\). CI: \\(\\hat\\beta_j \\pm z_{a/2} se(\\hat\\beta_j)\\)</p> <p>LRT \\(H_0:\\) some \\(beta\\) are 0, $H_a: $ at least one tested \\(\\beta\\) is not 0. </p> \\[G^2 = (-2\\log \\mathcal L_R) - (-2\\log \\mathcal L_F) = -2\\log (\\mathcal L_R / \\mathcal L_F)\\sim \\chi^2_k\\] <p>\\(k= \\# \\beta\\) tested</p> <p>Global LRT LRT comparing to the NULL model (null deviance)</p>"},{"location":"sta303/Summary.html#aic-bic","title":"AIC, BIC","text":"<ul> <li>combines log-likelihood with a penalty </li> <li>\\(AIC = -2\\log\\mathcal L + 2(p+1)\\)</li> <li>\\(BIC =  -2\\log\\mathcal L + \\log N(p+1)\\)</li> <li>\\(p\\) number of explanatory variables, \\(N\\) sample size</li> <li>Smaller is better</li> <li>Better = \\(diff(AIC) &gt; 10\\)</li> <li>Same = \\(diff(AIC) &lt; 2\\)</li> </ul>"},{"location":"sta303/Summary.html#slr-vs-binary-lr","title":"SLR vs. Binary LR","text":"<ul> <li> <p>both use MLE</p> </li> <li> <p>Binary LR has fewer assumptions </p> </li> <li>no outelires</li> <li>no residual plots</li> <li>non constant variance </li> </ul>"},{"location":"sta303/Summary.html#binary-logistic-regression","title":"Binary Logistic Regression","text":"<p>underlying distribution for each independent observation: \\(Bernoulli(\\pi_i)\\)</p> <p>We cannot estimate \\(\\pi_i\\) for individual \\(i\\). </p> <ul> <li>Let \\(\\pi = P(success)\\), </li> <li>ODDS: \\(\\pi/(1-\\pi)\\)</li> <li>LOG ODDS: \\(\\log(\\pi/(1-\\pi))\\)</li> <li>ODDS RATIO is the ratio of two ODDS</li> </ul> <p>\\(E(Y\\mid X)=\\pi, var(Y\\mid X) = \\pi(1-\\pi)\\)</p> <p>The model</p> \\[\\log(\\pi/(1-\\pi)) = X\\beta\\] \\[\\log(\\frac{\\pi_i}{1-\\pi_i}) = X_i\\beta \\quad\\text{(no error term)}\\] <p>MLE:\\(P(Y_i=y_i)=\\pi_i^{y_i}(1-\\pi_i)^{1-y_i}\\)</p> \\[\\mathcal{L} = \\prod_1^n\\pi_i^{y_i}(1-\\pi_i)^{1-y_i}\\] <p>where \\(\\pi_i = \\frac{\\exp(X_i\\beta)}{1+\\exp(X_i\\beta)}=e^{\\mu}/(1+e^\\mu)\\) and </p> \\[1-\\mu_i = 1-\\frac{e^\\mu}{1+e^\\mu} = (1+e^\\mu)^{-1}\\] \\[\\log\\mathcal{L} =  \\sum_1^ny_i(X_i\\beta) - y_i\\log(1+\\exp(X_i\\beta))-(1-y_i)\\log(1+X_i\\beta))\\] <p>Let \\((a,b)\\) be CI, CI for Odds ratio is \\(e^a, e^b\\), while we cannot compute CI for \\(\\pi\\) since \\(\\pi\\) is not normally distributed</p> <p>Assumptions:</p> <ul> <li>underlying model for Y is Bernoulli</li> <li>independent observations</li> <li>Correct form of model (linear relationship, included all relevant variables and excluded irrelevant)</li> <li>enough large sample size</li> </ul>"},{"location":"sta303/Summary.html#binomial-logistic-regression","title":"Binomial Logistic Regression","text":"<p>Let \\(Y\\) be the count of the number of \"success\" </p> <p>\\(P(Y=y)={m\\choose y}\\pi^y (1-\\pi)^{m-y}\\)</p> <p>\\(E(Y)=m\\pi, var(Y)=m\\pi(1-\\pi)\\)</p> <p>Then the proportion of successes  \\(E(Y/m)=\\pi, var(Y/m)=\\pi(1-\\pi)/m\\)</p> <p>Assume for each group of observation, it is independent. </p> <p>We can estimate \\(\\pi_i\\) is this case</p> <p>MLE:  </p> \\[P(Y_i=y_i) = {m_i\\choose y_i}\\pi^{y_i}(1-\\pi_i)^{m_i-y_i}\\] \\[\\mathcal L = \\prod_1^n {m_i\\choose y_i}\\pi^{y_i}(1-\\pi_i)^{m_i-y_i}\\] <p>where \\(\\pi_i = \\frac{e^\\mu}{1+e^\\mu}\\)</p> \\[\\log\\mathcal L = \\sum y_i\\log(\\pi_i)+(m_i-y_i)\\log(1-\\pi_i) + \\log{m_i\\choose y_i}\\] <p>Deviance \\(=-2\\log(\\mathcal L_M/\\mathcal L_S) = -2(\\log \\mathcal L_M - \\log \\mathcal L_S)\\). </p> <p>Saturated model has log likelihood ratio 0. </p>"},{"location":"sta303/Summary.html#logistic-regression-problems","title":"Logistic Regression Problems","text":"<ul> <li>Extrapolation: model outside of range of observed data may not be appropriate</li> <li>Multicollinearity</li> <li>unstable fitted equation</li> <li>coefficient significance and signs</li> <li>large standard error of coefficients</li> <li>MLR may not converge</li> <li>Influential points</li> </ul> <p>Specific to logistic </p> <ul> <li>Complete separation<ul> <li>one of a linear combination of explanatory variables perfectly predict \\(Y\\), then MLE cannot be computed</li> </ul> </li> <li>Quasi-complete separation<ul> <li>almost perfectly predict Y</li> <li>Solution simplify model, or try other options</li> </ul> </li> </ul> <p>Extra-binomial variation</p> <ul> <li>when Bernoulli observations are not independent</li> <li>use quasibinomial </li> <li>model for variance: \\(var(Y_i)=\\phi m_i \\pi_i(1-\\pi_i)\\)</li> <li>$\\hat\\phi = $ sum of squared Pearson residuals / d.f. </li> </ul>"},{"location":"sta303/Summary.html#gof","title":"GOF","text":"<p>To check model adequacy using LRT</p> <p>$H_0: $ fitted model fits data as well as Saturated model. $H_a: $ saturated model is better, the fitted model is inadequate </p> <p>\\(G^2 = -2\\log(\\mathcal L_F /\\mathcal L_S)\\sim \\chi^2_{n-(p+1)}\\)</p>"},{"location":"sta303/Summary.html#log-linear-model","title":"Log linear Model","text":"<ul> <li>Why not linear</li> <li>outcome is counts and small numbers</li> <li>Won't have a normal distribution conditional on age</li> <li>Why no logistic</li> <li>Not a binary outcome</li> <li>Not a binomial outcome since not a fixed number of trials</li> </ul> <p>\\(P(Y=y)=\\mu^y e^{-\\mu} / y!, E(Y)=var(Y)=\\mu\\)</p> \\[\\mathcal L = \\prod_1^n \\mu_i^{y_i} e^{-\\mu_i} / y_i!\\] \\[\\log\\mathcal L = \\sum_1^n y_i \\log (\\mu_i) -\\mu_i - \\log(y_i!)\\]"},{"location":"sta303/Summary.html#two-factor-independence","title":"Two Factor Independence","text":"<p>Binomial Sampling For \\(2\\times 2\\) table \\(H_0: \\mu_a = \\mu_b, H_a: \\mu_a\\neq \\mu_b\\)</p> \\[z=\\frac{\\hat\\mu_a - \\hat\\mu_b}{se(\\hat\\mu_a - \\hat\\mu_b)}\\sim N(0,1)\\] <p>Assumption: </p> <ul> <li>each trial is a Bernoulli</li> <li>the number of groups are fixed</li> <li>The underlying distribution is \\(y_a\\sim binomial(n_a, \\pi_a), y_b\\sim binomial(n_b, \\pi_b)\\)</li> </ul>"},{"location":"sta303/Summary.html#contingency-table","title":"Contingency Table","text":"<p>test statistics \\(\\chi^2 = \\sum_j\\sum_i (y_{ij} - \\hat\\mu_{ij})^2 / \\hat\\mu_{ij}\\sim \\chi^2_{(I-1)(J-1)}\\) where \\(\\hat\\mu_{ij} = \\pi_{i.}\\pi_{.j}/n\\)</p> <p>Contingency table model: \\(Y_{ij}\\) be the r.v. representing the number of observations in the cell \\(y_{ij}\\) be the observed cell counts</p> <p>The underlying distribution of \\(Y=(Y_{11},...,Y_{nn})\\sim Multinomial\\)</p> \\[P(Y=y)=\\frac{n!}{y_{11}!...y_{nn}!} \\prod_{i,j}\\pi_{ij}^{y_{ij} }\\] <p>Using MLE subjecting to \\(\\sum_{ij}\\pi_{ij} = 1\\), we get \\(\\hat\\mu_{ij} = y_{ij} / n\\)</p> <p>With null hypothesis of independence we can get \\(\\hat\\mu_{ij} = \\hat\\mu_{i.}\\hat\\mu_{.j}\\) Then we can use LRT where the full model contains the interaction terms</p> \\[\\log\\mathcal L_F = \\sum_{ij}y_{ij}\\log(y_{ij}/n)\\] \\[\\log\\mathcal L_R = \\sum_{ij}y_{ij}\\log(y_{i.}y_{.j}/nn)\\] \\[d.f. = (IJ-1)-(I+J-2)\\] <p>lose 1 for constraint \\(\\sum_{ij}\\pi_{ij} = 1\\), lose 2 for constraints \\(\\sum_i \\pi_{i.}=1,\\sum_j\\pi_{.j}=1\\)</p>"},{"location":"sta303/Summary.html#fishers-exact-test","title":"Fisher's Exact Test","text":"<ul> <li>randomization test</li> <li>appropriate for small sample size</li> <li>assumes the row and column totals are fixed</li> <li>p-value is calculated from hypergeometrix distribution</li> </ul> \\[P=\\frac{ {a+b\\choose a}{c+d\\choose c} }{ {n\\choose a+c} }\\]"},{"location":"sta303/Summary.html#poisson-regression","title":"Poisson Regression","text":"<ul> <li>counts aren't fixed</li> <li>treat IJ count as realizations of a Possion random variable</li> </ul> <p>Compare the interactions term</p> <p>Three-way interactions</p> <ul> <li>complete independence: does not have any interaction terms</li> <li>block independence: joint probability of two factors (say A,B) is independent of the third (C). Then include the interaction term between \\(AB\\)</li> <li>partial independence: \\(P(AB\\mid C)=P(A\\mid C)P(B\\mid C)\\), AB are conditionally independent on \\(C\\). Include interactions between \\(AC,BC\\)</li> <li>Uniform association: include all two-way interactions</li> <li>Saturated model: include three-way interactions</li> </ul>"},{"location":"sta303/binary_logistic_regression.html","title":"Binary logistic regression","text":""},{"location":"sta303/binary_logistic_regression.html#case-study-donner-party","title":"Case Study: Donner Party","text":"<pre><code>library(Sleuth2)\ndonner = case2001\nAge = donner$Age\nSex = donner$Sex\nStatus = donner$Status\n</code></pre> <pre><code># contingency table\nxtabs(~Status+Sex, data=donner)\n</code></pre> <pre><code>          Sex\nStatus     Male Female\n  Died       20      5\n  Survived   10     10\n</code></pre> <pre><code>summary(Age)\n</code></pre> <pre><code>   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   15.0    24.0    28.0    31.8    40.0    65.0\n</code></pre> <p>Response \\(Y_i=\\{0,1\\}\\) survived or died  Predictor: \\(X_i\\) age/sex</p>"},{"location":"sta303/binary_logistic_regression.html#model","title":"Model","text":"<p>\\(Y_i\\mid X_i=\\mathbb{I}\\)(response is in category of interest)\\(\\sim Bernoulli(\\pi_i)\\) \\(E(Y_i\\mid X_i)=\\pi_i, var(Y_i\\mid X_i)=\\pi_1(1-\\pi_i)\\)</p>"},{"location":"sta303/binary_logistic_regression.html#generalized-linear-models","title":"Generalized Linear Models","text":"<p>Having response \\(Y\\) can a set of explanatory variables \\(X_1,...,X_p\\)</p> <p>Want \\(E(Y)\\) as a linear function in the parameters, ie. \\(g(E(Y))=X\\beta\\)</p> <p>Key idea: find the link function \\(g\\) such that the model is true</p> <p>Some link functions</p> link function usual distribution of \\(Y\\mid X\\) identity \\(g(\\mu)=\\mu\\) normal log \\(g(\\mu)=\\log\\mu,\\mu&gt;0\\) Poisson(count data) logistic \\(g(\\mu)=\\log(\\mu/(1-\\mu)),0&lt;\\mu&lt;1\\) Bernoulli(binary),Binomial"},{"location":"sta303/binary_logistic_regression.html#glm-vs-transformation","title":"GLM vs. Transformation","text":"<ul> <li>For transformation, transforming \\(Y\\) so it has an approximate normal distribution with constant variance</li> <li>For GLM, distribution of \\(Y\\) not restricted to Normal</li> <li>Model parameters describe \\(g(E(Y))\\) rather than \\(E(g(Y))\\)</li> <li>GLM provide a unified theory of modeling that encompasses the most important models for continuous and discrete variables</li> </ul>"},{"location":"sta303/binary_logistic_regression.html#binary-logistic-regression","title":"Binary Logistic Regression","text":"<ul> <li>Let \\(\\pi=P(\"success\"),0&lt;\\pi&lt;1\\)</li> <li>The odds in favor of success is \\(\\frac{\\pi}{1-\\pi}\\)</li> <li>The log odds is \\(\\log(\\frac{\\pi}{1-\\pi})\\)</li> </ul>"},{"location":"sta303/binary_logistic_regression.html#model_1","title":"Model","text":"<p>\\(\\log(\\frac{\\pi}{1-\\pi}) = \\beta_0 + \\beta_1X_1+...+\\beta_pX_p\\)</p> <p>Let the linear predictor \\(\\eta=\\beta_0 + \\beta_1X_1+...+\\beta_pX_p\\), then Logistic function is \\(\\pi(\\eta)=\\frac{e^{\\eta}}{1+e^\\eta}\\)</p> <pre><code>eq &lt;- function(x){exp(x)/(1+exp(x))}\nplot(eq(-10:10), type='l')\n</code></pre> <p>\u200b </p> <ul> <li>log-odds \\(\\log(\\frac{\\pi}{1-\\pi})\\in(-\\infty, \\infty)\\), is increasing function.</li> <li>Predicts the natural log of the odds for a subject bing in one category or another</li> <li>Regression coefficients can be used to estimate odds ratio for each of the independent variables</li> <li>Tell which predictors can be used to determine if a subject was in a category of interest</li> </ul>"},{"location":"sta303/binary_logistic_regression.html#mle","title":"MLE","text":"<ul> <li>Date: \\(Y_i=\\mathbb{I}(\\text{if response is in category of interest})\\)</li> <li>Model: \\(P(Y_i=y_1)=\\pi_i^{y_{i}}(1-\\pi_i)^{1-y_{i}}\\)</li> <li>Assumption: The observations are independent.</li> <li> <p>Joint density: </p> \\[P(Y_1=y_1,...,Y_n=y_n)=\\prod_1^n \\pi_i^{y_{i}}(1-\\pi_i)^{1-y_{i}}\\] \\[\\pi_i = \\frac{\\exp(\\beta_0+\\beta_1X_{i1}+...+\\beta_pX_{ip})}{1+\\exp(\\beta_0+\\beta_1X_{i1}+...+\\beta_pX_{ip})}\\] </li> </ul> <p>Likelihood function: plug in observed data and think of the joint density as a function of \\(\\beta\\)'s</p> \\[\\mathcal{L}(\\beta_0,...,\\beta_p)=\\prod_{1^n}\\pi_i\\vec\\beta^{y_i}(1-\\pi_i{\\vec\\beta})^{1-y_i}\\] <p>There is no explicit expression for the max likelihood estimators. Two iterative numerical solution methods are Newton-Raphson algorithm and Fisher scoring or IWLS (In R, glm())</p> <pre><code># additive\nfita&lt;-glm(Status~Age+Sex, family=binomial, data=donner)\nsummary(fita)\n</code></pre> <pre><code>Call:\nglm(formula = Status ~ Age + Sex, family = binomial, data = donner)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-1.7445  -1.0441  -0.3029   0.8877   2.0472\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)  \n(Intercept)  1.63312    1.11018   1.471   0.1413  \nAge         -0.07820    0.03728  -2.097   0.0359 *\nSexFemale    1.59729    0.75547   2.114   0.0345 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 61.827  on 44  degrees of freedom\nResidual deviance: 51.256  on 42  degrees of freedom\nAIC: 57.256\n\nNumber of Fisher Scoring iterations: 4\n</code></pre> <pre><code>anova(fita)\n</code></pre> A anova: 3 \u00d7 4 DfDevianceResid. DfResid. Dev &lt;int&gt;&lt;dbl&gt;&lt;int&gt;&lt;dbl&gt; NULLNA      NA4461.82654 Age 15.5358204356.29072 Sex 15.0344374251.25628 <pre><code># modelling \"Died\"\nStatus2 = relevel(Status, ref=\"Survived\")\nfitad &lt;- glm(Status2 ~ Age + Sex, family=binomial, data=donner)\nsummary(fitad)\n</code></pre> <pre><code>Call:\nglm(formula = Status2 ~ Age + Sex, family = binomial, data = donner)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-2.0472  -0.8877   0.3029   1.0441   1.7445\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)  \n(Intercept) -1.63312    1.11018  -1.471   0.1413  \nAge          0.07820    0.03728   2.097   0.0359 *\nSexFemale   -1.59729    0.75547  -2.114   0.0345 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 61.827  on 44  degrees of freedom\nResidual deviance: 51.256  on 42  degrees of freedom\nAIC: 57.256\n\nNumber of Fisher Scoring iterations: 4\n</code></pre> <pre><code># Reference group as \"Male\"\nSex2 = relevel(Sex, ref=\"Male\")\nfitadf &lt;- glm(Status2~Age + Sex2, family=binomial, data=donner)\nsummary(fitadf)\n</code></pre> <pre><code>Call:\nglm(formula = Status2 ~ Age + Sex2, family = binomial, data = donner)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-2.0472  -0.8877   0.3029   1.0441   1.7445\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)  \n(Intercept) -1.63312    1.11018  -1.471   0.1413  \nAge          0.07820    0.03728   2.097   0.0359 *\nSex2Female  -1.59729    0.75547  -2.114   0.0345 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 61.827  on 44  degrees of freedom\nResidual deviance: 51.256  on 42  degrees of freedom\nAIC: 57.256\n\nNumber of Fisher Scoring iterations: 4\n</code></pre> <pre><code># Additive model for survived\nfitasf &lt;- glm(Status~Age + Sex2, family=binomial, data=donner)\nsummary(fitasf)\n</code></pre> <pre><code>Call:\nglm(formula = Status ~ Age + Sex2, family = binomial, data = donner)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-1.7445  -1.0441  -0.3029   0.8877   2.0472\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)  \n(Intercept)  1.63312    1.11018   1.471   0.1413  \nAge         -0.07820    0.03728  -2.097   0.0359 *\nSex2Female   1.59729    0.75547   2.114   0.0345 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 61.827  on 44  degrees of freedom\nResidual deviance: 51.256  on 42  degrees of freedom\nAIC: 57.256\n\nNumber of Fisher Scoring iterations: 4\n</code></pre> <pre><code># the order of independent variable does not metter\nfitsaf &lt;- glm(Status~ Sex2+Age, family=binomial, data=donner)\nsummary(fitsaf)\n</code></pre> <pre><code>Call:\nglm(formula = Status ~ Sex2 + Age, family = binomial, data = donner)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-1.7445  -1.0441  -0.3029   0.8877   2.0472\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)  \n(Intercept)  1.63312    1.11018   1.471   0.1413  \nSex2Female   1.59729    0.75547   2.114   0.0345 *\nAge         -0.07820    0.03728  -2.097   0.0359 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 61.827  on 44  degrees of freedom\nResidual deviance: 51.256  on 42  degrees of freedom\nAIC: 57.256\n\nNumber of Fisher Scoring iterations: 4\n</code></pre> <pre><code>anova(fitasf)\n</code></pre> A anova: 3 \u00d7 4 DfDevianceResid. DfResid. Dev &lt;int&gt;&lt;dbl&gt;&lt;int&gt;&lt;dbl&gt; NULLNA      NA4461.82654 Age 15.5358204356.29072 Sex2 15.0344374251.25628 <pre><code>anova(fitsaf)\n</code></pre> A anova: 3 \u00d7 4 DfDevianceResid. DfResid. Dev &lt;int&gt;&lt;dbl&gt;&lt;int&gt;&lt;dbl&gt; NULLNA      NA4461.82654 Sex2 14.5402674357.28628 Age 16.0299914251.25628 <pre><code># Higher order model with 3 higher order/interaction terms\nfitfull &lt;- glm(Status~Age + Sex2 + Age:Sex2+I(Age^2)+I(Age^2):Sex2, family=binomial, data=donner)\nsummary(fitfull)\n</code></pre> <pre><code>Call:\nglm(formula = Status ~ Age + Sex2 + Age:Sex2 + I(Age^2) + I(Age^2):Sex2, \n    family = binomial, data = donner)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-2.3396  -0.9757  -0.3438   0.5269   1.5901\n\nCoefficients:\n                     Estimate Std. Error z value Pr(&gt;|z|)\n(Intercept)         -3.318484   3.940184  -0.842    0.400\nAge                  0.183031   0.226632   0.808    0.419\nSex2Female           0.265286  10.455222   0.025    0.980\nI(Age^2)            -0.002803   0.002985  -0.939    0.348\nAge:Sex2Female       0.299877   0.696050   0.431    0.667\nSex2Female:I(Age^2) -0.007356   0.010689  -0.688    0.491\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 61.827  on 44  degrees of freedom\nResidual deviance: 45.361  on 39  degrees of freedom\nAIC: 57.361\n\nNumber of Fisher Scoring iterations: 5\n</code></pre> <pre><code># interaction model\nfitas &lt;- glm(Status~Age*Sex2, family=binomial,data=donner)\nsummary(fitas)\n</code></pre> <pre><code>Call:\nglm(formula = Status ~ Age * Sex2, family = binomial, data = donner)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-2.2279  -0.9388  -0.5550   0.7794   1.6998\n\nCoefficients:\n               Estimate Std. Error z value Pr(&gt;|z|)  \n(Intercept)     0.31834    1.13103   0.281   0.7784  \nAge            -0.03248    0.03527  -0.921   0.3571  \nSex2Female      6.92805    3.39887   2.038   0.0415 *\nAge:Sex2Female -0.16160    0.09426  -1.714   0.0865 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 61.827  on 44  degrees of freedom\nResidual deviance: 47.346  on 41  degrees of freedom\nAIC: 55.346\n\nNumber of Fisher Scoring iterations: 5\n</code></pre> <pre><code>anova(fitasf, test=\"Chisq\")\n</code></pre> A anova: 3 \u00d7 5 DfDevianceResid. DfResid. DevPr(&gt;Chi) &lt;int&gt;&lt;dbl&gt;&lt;int&gt;&lt;dbl&gt;&lt;dbl&gt; NULLNA      NA4461.82654        NA Age 15.5358204356.290720.01863103 Sex2 15.0344374251.256280.02484816 <pre><code>vcov(fitasf, digits=3)\n</code></pre> A matrix: 3 \u00d7 3 of type dbl (Intercept)AgeSex2Female (Intercept) 1.23250837-0.038472741 0.06007099 Age-0.03847274 0.001390134-0.00823197 Sex2Female 0.06007099-0.008231970 0.57073339 <pre><code>cbind(bhat=coef(fitasf), confint.default(fitasf)) # CI for beta's\n</code></pre> A matrix: 3 \u00d7 3 of type dbl bhat2.5 %97.5 % (Intercept) 1.63312031-0.5428002 3.809040837 Age-0.07820407-0.1512803-0.005127799 Sex2Female 1.59729350 0.1166015 3.077985503 <pre><code>exp(coef(fitasf)) # odd ratios\n</code></pre> <pre><code>(Intercept) 5.11982524825608\nAge         0.924775685984312\nSex2Female  4.93964517580916\n</code></pre> <pre><code>exp(cbind(OR=coef(fitasf), confint.default(fitasf))) # Ci for odd ratios\n</code></pre> A matrix: 3 \u00d7 3 of type dbl OR2.5 %97.5 % (Intercept)5.11982520.581118745.1071530 Age0.92477570.8596067 0.9948853 Sex2Female4.93964521.123671621.7146143"},{"location":"sta303/binary_logistic_regression.html#wald-chi-sq-procedures","title":"Wald Chi-sq Procedures","text":"<p>logistic regression inference on a single \\(\\beta\\) </p> <ul> <li>Hypotheses: \\(H_0:\\beta_j=0,H_a:\\beta_j\\neq 0\\)</li> <li>Test statistic: \\(z=\\hat\\beta_j/se(\\hat\\beta_j)\\), \\(\\hat\\beta_j\\) is MLE for \\(\\beta_j\\)</li> <li>By standard large-sample results, MLE's are normally distributed. Thus, for large \\(n\\), under \\(H_0\\), z is an observation from an approx. \\(N(0,1)\\). Note that \\(Z^2\\sim \\chi_1\\), we can use chisq for p-value</li> <li>95% CI: \\(\\hat\\beta_j \\pm 1.96se(\\hat\\beta_j)\\)</li> </ul> <p>Since the logistic function is monotonic, we can expand CI by doing monotonic transformation (e.x. multiplication and exp)</p> <pre><code># Wald test\nlibrary(aod)\nwald.test(Sigma=vcov(fitasf), b=coef(fitasf), Terms=2:3)\n</code></pre> <pre><code>Wald test:\n----------\nChi-squared test:\nX2 = 6.9, df = 2, P(&gt; X2) = 0.032\n</code></pre> <pre><code>wald.test(Sigma=vcov(fitas), b=coef(fitas), Terms=4)\n</code></pre> <pre><code>Wald test:\n----------\nChi-squared test:\nX2 = 2.9, df = 1, P(&gt; X2) = 0.086\n</code></pre>"},{"location":"sta303/binary_logistic_regression.html#model-assumptions","title":"Model Assumptions","text":"<ul> <li>Underlying probability model for response is Bernoulli</li> <li>Observations are independent</li> <li>The form of the model is correct<ul> <li>Linear relationship between logits and explanatory variables</li> <li>All relevant variables are included; irrelevant ones excluded</li> </ul> </li> <li>Sample size is large enough for valid inference</li> </ul> <p>B Logistic R vs. Linear R - both utilize MLR's for the \\(\\beta\\)'s - Less assumption to check for than in linear regression      - No need to check for outliers      - No residual plots      - Variance is not constant</p> <p>Need to test - (designing aspect) Independence of observations  - (inference) Form of the model: test higher-order terms such as non-linear in X, interaction, and higher order interaction</p>"},{"location":"sta303/binary_logistic_regression.html#wald-cs-lrt-test","title":"Wald cs. LRT test","text":"<ul> <li>Walk test on single \\(\\beta = 0\\), LRT find \\(k\\geq 1\\) \\(\\beta\\)'s. </li> <li>LRT compares nested models</li> <li>LRT allow small to moderate sample sizes \\(\\beta\\) near boundary of parameter space</li> </ul>"},{"location":"sta303/binary_logistic_regression.html#comparing-models-likelihood-ratio-test","title":"Comparing models: Likelihood Ratio Test","text":"<ul> <li>Idea: compare likelihood of data under Full model, \\(\\mathcal{L}_F\\) to likelihood under REDUCED model \\(\\mathcal{L}_R\\) of same data. the likelihood ratio \\(\\mathcal{L}_R/\\mathcal{L}F\\) where \\(\\mathcal{L}_R\\leq \\mathcal{L}_F\\)</li> <li>Hypothesis: \\(H_0:\\beta_1=...\\beta_k=0\\) (Reduced model is appropriate, fits data as swll as Full model)\\(H_a:\\) at least one of betas is not 0 (full model is better)</li> <li>Test statistic: Deviance (residual)</li> </ul> \\[G^2 = -2\\log\\mathcal{L}_R - (-2\\log\\mathcal{L}_F)=-2\\log(\\mathcal{L}_R/\\mathcal{L}_F)\\sim \\chi^2_k\\] <p>For large \\(n\\), under \\(H_0\\), \\(G^2\\) is an observation from a chi-square distribution with \\(k\\) d.f.</p>"},{"location":"sta303/binary_logistic_regression.html#exercise","title":"Exercise","text":"<p>Models </p> \\[\\begin{align*} log(\\frac{\\pi_i}{1-\\pi_i}) &amp;= \\\\&amp;(1) a_0+a_1Age_i+a_2I_{Female, i}\\\\  &amp;(2) \\beta_0 +\\beta_1Age_i+\\beta_2 I_{female_i,i} + \\beta_3Age_i*I_{female,i}+\\beta_4Age_i^2 + \\beta_5Age_i^2*I_{Female, i} \\\\ &amp;(3) \\gamma_0 + \\gamma_1Age_i + \\gamma_2I_{Female, i} + \\gamma_3Age_i *I_{Female, i}\\end{align*}\\] <pre><code># additive model (model 1)\nfitadd &lt;- glm(Status~Age + Sex, family=binomial, donner)\nsummary(fitadd)\n</code></pre> <pre><code>Call:\nglm(formula = Status ~ Age + Sex, family = binomial, data = donner)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-1.7445  -1.0441  -0.3029   0.8877   2.0472\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)  \n(Intercept)  1.63312    1.11018   1.471   0.1413  \nAge         -0.07820    0.03728  -2.097   0.0359 *\nSexFemale    1.59729    0.75547   2.114   0.0345 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 61.827  on 44  degrees of freedom\nResidual deviance: 51.256  on 42  degrees of freedom\nAIC: 57.256\n\nNumber of Fisher Scoring iterations: 4\n</code></pre> <pre><code># full model (model 2)\nfitfull &lt;- glm(Status~Age + Sex + Age: Sex + I(Age^2) + I(Age^2):Sex, \n               family=binomial, donner)\nsummary(fitfull)\n</code></pre> <pre><code>Call:\nglm(formula = Status ~ Age + Sex + Age:Sex + I(Age^2) + I(Age^2):Sex, \n    family = binomial, data = donner)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-2.3396  -0.9757  -0.3438   0.5269   1.5901\n\nCoefficients:\n                    Estimate Std. Error z value Pr(&gt;|z|)\n(Intercept)        -3.318484   3.940184  -0.842    0.400\nAge                 0.183031   0.226632   0.808    0.419\nSexFemale           0.265286  10.455222   0.025    0.980\nI(Age^2)           -0.002803   0.002985  -0.939    0.348\nAge:SexFemale       0.299877   0.696050   0.431    0.667\nSexFemale:I(Age^2) -0.007356   0.010689  -0.688    0.491\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 61.827  on 44  degrees of freedom\nResidual deviance: 45.361  on 39  degrees of freedom\nAIC: 57.361\n\nNumber of Fisher Scoring iterations: 5\n</code></pre> <pre><code># interative model (model 3)\nfitint &lt;- glm(Status~Age*Sex, family=binomial, donner)\nsummary(fitint)\n</code></pre> <pre><code>Call:\nglm(formula = Status ~ Age * Sex, family = binomial, data = donner)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-2.2279  -0.9388  -0.5550   0.7794   1.6998\n\nCoefficients:\n              Estimate Std. Error z value Pr(&gt;|z|)  \n(Intercept)    0.31834    1.13103   0.281   0.7784  \nAge           -0.03248    0.03527  -0.921   0.3571  \nSexFemale      6.92805    3.39887   2.038   0.0415 *\nAge:SexFemale -0.16160    0.09426  -1.714   0.0865 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 61.827  on 44  degrees of freedom\nResidual deviance: 47.346  on 41  degrees of freedom\nAIC: 55.346\n\nNumber of Fisher Scoring iterations: 5\n</code></pre> <pre><code># p-value for model2 vs. model1\n1 - pchisq(5.895, 3)\n</code></pre> <pre><code>0.11683196297711\n</code></pre> <p>Q1</p> <p>Using R output determine whether a model with the 3 higher order polynomial terms and interaction term is an improvement over the additive model</p> <p>Higher order interaction vs. additive Hypotheses: \\(H_0:\\beta_3=\\beta_4=\\beta_5 = 0\\)</p> <p>Test statistic \\(G^2 = 51.256 - 45.361 = 5.895\\sim \\chi^2_3\\)</p> <p>p-value: 0.12</p> <p>Conclusion: There is some evidence that the additive model is better than the higher order model</p> <p>Q2</p> <p>determine whether the effect of Age on the odds of survival differ with Sex</p> <p>Hypothesis: \\(H_0: \\gamma_3 = 0\\)</p> <p>Test statistic (LRT): \\(51.256 - 47.346 = 3.91\\sim \\chi^2_1\\)</p> <p>p-value(LRT): 0.048</p> <p>Test statistic (Wald): \\(-0.16160/0.09426 = -1.71\\sim N(0,1)\\)</p> <p>p-value(Wald): 0.0865</p> <p>(Shown in R-output)</p> <p>Conclusion: There is inconclusive evidence that the interactive model is better than the additive model</p> <pre><code># p-value for Q2 LRT\n1 - pchisq(51.256-47.346, 1)\n</code></pre> <pre><code>0.0479995973077516\n</code></pre> <p>Q3</p> <p>determine whether or not the additive model fits better then the Null model</p> <p>Hypothesis: \\(H_0: a_i = 0, i\\neq 0\\)</p> <p>Test statistic: \\(G^2 = 61.827 - 51.256=10.571\\sim \\chi_2^2\\)</p> <p>p-value: 0.005</p> <p>Conclusion: There is strong evidence that age and sex are relevant factors for estimating the odds of survival</p> <pre><code># p-value for Q3\n1 - pchisq(61.827-51.256, 2)\n</code></pre> <pre><code>0.00506449930534014\n</code></pre>"},{"location":"sta303/binary_logistic_regression.html#aic-bic","title":"AIC &amp; BIC","text":"<p>Akaike's Information Criterion  </p> \\[AIC=-2log\\mathcal{L}+2(p+1)\\] <p>Schwarz's (Bayesian Information) Criterion  </p> \\[-2log\\mathcal{L}+(p+1)\\log N\\] <p>\\(p\\) number of variables, \\(N\\) sample size</p> <p>Smaller means better model fit BIC applies stronger penalty for model complexity than AIC</p> <p>AIC Rule of Thumb: one model fits better if \\(diff(AIC) &gt; 10\\), equivalent if \\(diff(AIC)&lt;2\\)</p> <p>For the example, we can choose the simplest additive model</p>"},{"location":"sta303/binary_logistic_regression.html#estimated-probability-of-survival","title":"Estimated probability of survival","text":"<pre><code>phats &lt;- predict.glm(fitasf, type=\"response\")\nphats[1:5]\n</code></pre> <pre><code>1   0.458700966259221\n2   0.525540478323231\n3   0.183166067768785\n4   0.328935876664949\n5   0.364335972178779\n</code></pre> <pre><code>library(ggplot2)\nggplot(donner, aes(x=Age, y=Status, color=Sex))+geom_point()\n</code></pre> <p>\u200b </p> <pre><code>ggplot(donner, aes(x=Age, y=phats)) + ylab(\"Estimated Probablities\")+geom_line(aes(color=Sex), size=1)\n</code></pre> <p>\u200b </p>"},{"location":"sta303/binomial_logistic_regression.html","title":"Binomial Logistic Regression","text":""},{"location":"sta303/binomial_logistic_regression.html#y-binomialm","title":"Y ~ Binomial(m, \u03c0)","text":"<p>\\(P(Y=y)={m\\choose y} \\pi^y (1-\\pi)^{m-y}\\) v.s. Bernoulli =&gt; \\(\\pi\\) remains the same in binomial, while in Bernoulli case it is independent for each \\(X_i\\)</p> <p>\\(E(Y)=m\\pi\\), \\(var(Y)=m\\pi(1-\\pi)\\)</p> <p>Consider modelling \\(Y.m\\) be the proportion of success out of \\(m\\) independent Bernoulli trials  \\(E(Y/m)=\\pi, var(Y/m)=\\pi(1-\\pi)/m\\)</p> <pre><code>library(Sleuth3)\nlibrary(ggplot2)\nkrunnit = case2101\n\nExtinct = krunnit$Extinct # number of \"success\"\nAtRisk = krunnit$AtRisk\nArea = krunnit$Area\n\npis = Extinct/AtRisk\nNExtinct = AtRisk - Extinct # number of \"failure\"\nlogitpi = log(pis/(1-(pis)))\nlogarea = log(Area)\ncase2101\n</code></pre> A data.frame: 18 \u00d7 4 IslandAreaAtRiskExtinct &lt;fct&gt;&lt;dbl&gt;&lt;int&gt;&lt;int&gt; Ulkokrunni     185.8075 5 Maakrunni      105.8067 3 Ristikari       30.706610 Isonkivenletto   8.5051 6 Hietakraasukka   4.8028 3 Kraasukka        4.5020 4 Lansiletto       4.3043 8 Pihlajakari      3.6031 3 Tyni             2.6028 5 Tasasenletto     1.7032 6 Raiska           1.2030 8 Pohjanletto      0.7020 2 Toro             0.7031 9 Luusiletto       0.6016 5 Vatunginletto    0.4015 7 Vatunginnokka    0.3033 8 Tiirakari        0.204013 Ristikarenletto  0.07 6 3 <p>Let \\(\\pi_i\\) be the probability of extinction for each island, assume that this is the same for each species for bird on a particular island</p> <p>Assume species survival is independent. Then \\(Y_i\\sim Binomial(m_i,\\pi_i)\\)</p> <p>Unlike binary logistic model for Bernoulli distribution, we can estimate \\(\\pi_i\\) from the data. </p> <p>Observed response proportion \\(\\bar{\\pi_i} = y_i/m_i\\)</p> <p>Observed or empirical logits: (S-\"saturated)</p> \\[\\log(\\frac{\\bar{\\pi}_{S,i}}{1 - \\bar{\\pi}_{S,i}}) = \\log(\\frac{y_i}{m_i-y_i})\\] <pre><code>ggplot(krunnit, aes(x=Area, y=logitpi)) + geom_point()\n</code></pre> <p>\u200b </p> <pre><code>ggplot(krunnit, aes(x=log(Area), y=logitpi)) + geom_point()\n</code></pre> <p>\u200b </p> <p>Then the proposed model is based on plot</p> \\[\\log(\\frac{\\pi_i}{1-\\pi_i})=\\beta_0 + \\beta_1 \\log(Area_i)\\] <pre><code># cbind(Extinct, NExtinct ~ log(Area) is the way to represent it in R\nfitbl &lt;- glm(cbind(Extinct, NExtinct)~log(Area), family=binomial, data=krunnit)\nsummary(fitbl)\n</code></pre> <pre><code>Call:\nglm(formula = cbind(Extinct, NExtinct) ~ log(Area), family = binomial, \n    data = krunnit)\n\nDeviance Residuals: \n     Min        1Q    Median        3Q       Max  \n-1.71726  -0.67722   0.09726   0.48365   1.49545\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -1.19620    0.11845 -10.099  &lt; 2e-16 ***\nlog(Area)   -0.29710    0.05485  -5.416 6.08e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 45.338  on 17  degrees of freedom\nResidual deviance: 12.062  on 16  degrees of freedom\nAIC: 75.394\n\nNumber of Fisher Scoring iterations: 4\n</code></pre> <pre><code>anova(fitbl, test=\"Chisq\")\n</code></pre> A anova: 2 \u00d7 5 DfDevianceResid. DfResid. DevPr(&gt;Chi) &lt;int&gt;&lt;dbl&gt;&lt;int&gt;&lt;dbl&gt;&lt;dbl&gt; NULLNA      NA1745.33802          NA log(Area) 133.276511612.061517.994246e-09 <pre><code>vcov(fitbl) # variance co-variance matrix\n</code></pre> A matrix: 2 \u00d7 2 of type dbl (Intercept)log(Area) (Intercept) 0.014029452-0.002602237 log(Area)-0.002602237 0.003008830 <pre><code># 95% CI for beta\nCL = cbind(bhat=coef(fitbl), confint.default(fitbl))\nCL\n</code></pre> A matrix: 2 \u00d7 3 of type dbl bhat2.5 %97.5 % (Intercept)-1.1961955-1.4283454-0.9640456 log(Area)-0.2971037-0.4046132-0.1895942"},{"location":"sta303/binomial_logistic_regression.html#model-summary","title":"Model summary","text":"<p># observations: 18 # coefficients: 2 fitted model \\(logit(\\hat\\pi) = -1.196-0.297\\log(Area)\\)</p> <p>Wald procedures still work the same</p> <p>\\(H_0:\\beta_1=0\\) \\(z=\\frac{\\hat\\beta_1}{se(\\hat\\beta_1)}\\sim N(0,1)\\) CI: \\(\\hat\\beta_1\\pm t_{\\alpha,1}se(\\hat\\beta_1)\\)</p>"},{"location":"sta303/binomial_logistic_regression.html#interpretation-of-beta1","title":"Interpretation of beta1","text":"<p>Model: </p> \\[logit(\\pi) = \\beta_0 + \\beta_1\\log(x)\\Rightarrow\\\\ \\\\frac{pi}{1-\\pi} = e^{\\beta_0}e^{\\beta_1\\log(x)} = e^{\\beta_0}x^{\\beta_1}\\] <p>CHanging \\(x\\) by a factor of \\(h\\) will change the odds by a multiplicative factor of \\(h^{\\beta_1}\\)</p> <p>Example: Halving island area changes odds by a factor of \\(0.5^{-0.2971} = 1.23\\)</p> <p>Therefore, the odds of extinction on a smaller island are 123% of the odds of extinction on an island double its size. </p> <p>Halving of area is associated with an increase in the odds of extinction by an estimated 23%. An approximate 95% confidence interval for the percentage change in odds is 14%-32%</p> <pre><code># CI of beta's for halfing area\n0.5^(CL)\n</code></pre> A matrix: 2 \u00d7 3 of type dbl bhat2.5 %97.5 % (Intercept)2.2913462.6913791.950773 log(Area)1.2286751.3237341.140443 <pre><code>phats = predict.glm(fitbl, type=\"response\")\noptions(digits=4)\nrbind(Extinct, NExtinct, pis, phats)\n</code></pre> A matrix: 4 \u00d7 18 of type dbl 123456789101112131415161718 Extinct 5.00000 3.0000010.00000 6.0000 3.0000 4.000 8.0000 3.00000 5.0000 6.0000 8.0000 2.0000 9.0000 5.00007.0000 8.000013.00003.0000 NExtinct70.0000064.0000056.0000045.000025.000016.00035.000028.0000023.000026.000022.000018.000022.000011.00008.000025.000027.00003.0000 pis 0.06667 0.04478 0.15152 0.1176 0.1071 0.200 0.1860 0.09677 0.1786 0.1875 0.2667 0.1000 0.2903 0.31250.4667 0.2424 0.32500.5000 phats 0.06017 0.07036 0.09854 0.1380 0.1595 0.162 0.1639 0.17125 0.1854 0.2052 0.2226 0.2516 0.2516 0.26030.2842 0.3019 0.32780.3998 <p>Question: estimate the probability of extinction for a species on the Ulkokrunni island (Area \\(= 185.5 km^2\\))</p> <p>\\(logit(\\hat\\pi_{M,1}=)-1.196 - 0.297\\log(185.5) = -2.75\\)</p> <p>\\(\\hat\\pi_{M,1} = 0.06\\)</p> <pre><code>logit_result = -1.196 - 0.297 * log(185.5)\npi_m1 = exp(logit_result) / (1+ exp(logit_result))\nprint(logit_result)\nprint(pi_m1)\n</code></pre> <pre><code>[1] -2.747\n[1] 0.06024\n</code></pre>"},{"location":"sta303/binomial_logistic_regression.html#diagnostics","title":"Diagnostics","text":"<p>Model assumptions</p> <ul> <li>Underlying probability model is Binomial  variance non constant, is a function of the mean</li> <li>Observation independent</li> <li>The form of the model is correct  </li> <li>Liner relationship between logits and explanatory variables</li> <li>All relevant variables are included, irrelevant ones excluded</li> <li>Sample size is large enough for valid inferences</li> <li>outliers</li> </ul>"},{"location":"sta303/binomial_logistic_regression.html#saturated-model","title":"Saturated Model","text":"<p>Model that fits exactly with the data Most general model possible for the data</p> <p>Consider one explanatory variable, X with \\(n\\) unique levels for the outcome, \\(Y\\sim Binomial(m,\\pi)\\)</p> <p>Saturated Model as many parameter coefficients as \\(n\\)</p> <p>Fitted Model  nested within a FULL model, has (p+1) parameters</p> <p>Null ModelL intercept only</p>"},{"location":"sta303/binomial_logistic_regression.html#checking-model-adequacy-form-of-the-model-deviance-goodness-of-fit-test","title":"Checking model adequacy: Form of the model - Deviance Goodness of Fit Test","text":"<p>Form of Hypothesis: $H_0: $ reduced model, $H_a: $ Full model</p> <p>Deviance GOF test compares the fitted model \\(M\\) to the saturated model \\(S\\)</p> <p>Test Statistic: </p> \\[Deviance = -2\\log(\\mathcal{L}_R/\\mathcal{L}_F)\\sim \\chi^2_{n-p-1}\\] <p>This is an asymptotic approximation, so it works better if each \\(m_i&gt;5\\)</p> <pre><code>summary(fitbl)\n</code></pre> <pre><code>Call:\nglm(formula = cbind(Extinct, NExtinct) ~ log(Area), family = binomial, \n    data = krunnit)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-1.7173  -0.6772   0.0973   0.4837   1.4954\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  -1.1962     0.1184  -10.10  &lt; 2e-16 ***\nlog(Area)    -0.2971     0.0549   -5.42  6.1e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 45.338  on 17  degrees of freedom\nResidual deviance: 12.062  on 16  degrees of freedom\nAIC: 75.39\n\nNumber of Fisher Scoring iterations: 4\n</code></pre> <p>Example Deviance GOF with R-output \\(H_0:\\) Fitted model \\(logit(\\pi)~\\log(Area)\\)  $H_a: $ Saturated model</p> <p>Test statistic: \\(Deviance = 12.062\\) (Residual deviance)</p> <p>Distribution: \\(Deviance\\sim \\chi^2_{18-2}\\)</p> <p>p-value: \\(P(\\chi^2_{16}\\leq 12.062) = 0.74\\)</p> <p>Conclusion: the data are consistent with \\(H_0\\); the simpler model with linear function of log(Area) is adequate (fit as well as the saturated model)</p> <pre><code># calculate p-value for example\n1 - pchisq(12.062, 16)\n</code></pre> <p>0.739700862578766</p> <p>Small deviance leads to larger p-value and vice versa.</p> <p>Large p-value means:  - fitted model is adequate  - Test is not powerful enough to detect inadequacies</p> <p>Small p-value means:  - fitted model is not adequate, consider a more complex model with more explanatory variables or higher order terms and so on OR  - response distribution is not adequately modeled by the Binomial distribution, OR  - There are severe outliers</p>"},{"location":"sta303/binomial_logistic_regression.html#outliers","title":"Outliers","text":"<p>Response (raw) residuals</p> \\[\\hat\\pi_{S,i}-\\hat\\pi_{M,i}=y_i/M_i-\\hat\\pi_{M,i}\\] <p>Standardized residuals - Pearson Residuals: uses estimate of s.d. of \\(Y\\)</p> \\[P_{res,i}=\\frac{y_i-m_i\\hat\\pi_{M,i}}{\\sqrt{m_i\\hat\\pi_{M,i}(1-\\hat\\pi_{M,i})}}\\] <ul> <li>Deviance Residuals: defined so that the sum of the square of the residuals is the deviance</li> </ul> \\[D_{res,i}=sign(y_i-m_i\\hat\\pi_{&lt;i})\\sqrt{2(y_i\\log(\\frac{y_i}{m_i\\hat\\pi_{M,i}})+(m_i-y_i)\\log(\\frac{m_i-y_i}{m_i-m_i\\pi\\hat\\pi_{M,i}}))}\\] <pre><code># response residuals\nrres = residuals(fitbl, type=\"response\")\n# pearson residuals\npres = residuals(fitbl, type=\"pearson\")\n# Deviance residuals\ndres = residuals(fitbl, type=\"deviance\")\ncbind(pis, phats, rres, pres, dres)\n</code></pre> pisphatsrrespresdres 10.066670.06017 0.006493 0.23646 0.23266 20.044780.07036-0.025585-0.81883-0.87369 30.151520.09854 0.052975 1.44400 1.34958 40.117650.13800-0.020351-0.42139-0.43071 50.107140.15946-0.052319-0.75619-0.79584 60.200000.16205 0.037951 0.46058 0.44746 70.186050.16389 0.022155 0.39247 0.38577 80.096770.17125-0.074480-1.10075-1.18097 90.178570.18542-0.006844-0.09318-0.09363 100.187500.20524-0.017742-0.24850-0.25127 110.266670.22264 0.044030 0.57969 0.56727 120.100000.25158-0.151576-1.56220-1.71726 130.290320.25158 0.038747 0.49717 0.48934 140.312500.26030 0.052203 0.47588 0.46659 150.466670.28415 0.182515 1.56733 1.49545 160.242420.30185-0.059429-0.74367-0.75939 170.325000.32783-0.002828-0.03810-0.03813 180.500000.39984 0.100157 0.50082 0.49570 <pre><code>par(mfrow=c(1,3))\nplot(log(Area), rres)\nplot(log(Area), pres)\nplot(log(Area), dres)\n</code></pre> <p>\u200b </p> <p>Pearson vs Deviance Residuals</p> <ul> <li>Both asymptotically to \\(N(0,1)\\)</li> <li>Possible outlier if \\(|res|&gt;2\\)</li> <li>Definite outlier if \\(|res|&gt;3\\)</li> <li>Under small \\(n\\) \\(D_{res}\\) closer to \\(N(0,1)\\) then \\(P_{res}\\)</li> <li>When \\(\\hat\\pi\\) close to extreme (0, 1), \\(P_{res}\\) are unstable</li> </ul>"},{"location":"sta303/binomial_logistic_regression.html#problems-and-solutions","title":"Problems and Solutions","text":""},{"location":"sta303/binomial_logistic_regression.html#issues-related-to-general-linear-models","title":"Issues Related to General Linear Models","text":"<p>extrapolation  </p> <ul> <li>Don't make inferences/predictions outside range of observed data</li> <li>Model may no longer be appropriate</li> </ul> <p>Multicollinearity Consequences includes</p> <ul> <li>unstable fitted equation</li> <li>Coefficient that should be statistically significant while not</li> <li>Coefficient may have wrong sign</li> <li>Sometimes large \\(se(\\beta)\\)</li> <li>Sometimes numerical procedure to find MLEs does not converge</li> </ul> <p>Influential Points</p> <p>Model: overfit issue, should build model on training data (cross validation)</p>"},{"location":"sta303/binomial_logistic_regression.html#problems-specific-to-logistic-regression","title":"Problems specific to Logistic Regression","text":"<p>Extra-binomial variation (over dispersion)  - variance of \\(Y_i\\) greater than \\(m_i\\pi_i(1-\\pi_i)\\)  - Does not bias \\(\\hat\\beta\\)'s but se of \\(\\hat\\beta\\)'s will be too small </p> <p>Solution: add one more parameter to the model, \\(\\phi\\)-dispersion parameter. Then \\(var(Y_i)=\\phi m_i\\pi_i(1-\\pi_i)\\)</p> <p>Complete separation</p> <ul> <li>One or a linear combination of explanatory variables perfectly predict whether \\(Y=1\\lor 0\\)</li> <li>In Binary response, when \\(y_i=1,\\hat y_i=1\\), then \\(\\sum_{i=1}^n\\{y_i\\log(\\hat y_i + (1-y_i)\\log(1-\\hat y_i))=0\\}\\)</li> <li>MLE cannot be computed</li> </ul> <p>Quasi-complete separation:</p> <ul> <li>explanatory variables predict \\(Y=1\\lor 0\\) almost perfectly</li> <li>MLE are numerically unstable</li> </ul> <p>Solution: simplify the model. Try other methods like penalized maximum likelihood, exact logistic regression, Bayesian methods</p>"},{"location":"sta303/binomial_logistic_regression.html#conclusions-about-logistic-regression","title":"Conclusions about Logistic Regression","text":"<p>False Logistic regression describes population proportion of probability as a linear function of explanatory variables. </p> <p>Non-linear \\(\\hat\\pi = e^{\\hat\\mu}/(1+e^{\\hat\\mu})\\)</p> <p>Hence Logistic regression is a nonlinear regression model</p>"},{"location":"sta303/binomial_logistic_regression.html#extra-binomial-variation","title":"Extra-binomial variation","text":"<p>Suppose \\(X_1,...,X_m\\) are not independent but identically distributed \\(\\sim Bernoulli(\\pi)\\). Further suppose \\(\\forall X_i,X_j. cov(X_1,X_j)=\\rho, \\rho&gt;0\\).</p> <p>Let \\(Y_1=\\sum_i^{m_1}X_i\\)</p> <p>Then </p> \\[var(Y_i)= \\sum_i^{m_1}var(X_i) + \\sum_{i\\neq j} cov(X_i,X_j)\\\\ = m_1\\pi(1-\\pi)+\\sum_{i\\neq j} \\rho\\sqrt{var(X_i)var(X_j)}\\\\  =m_1\\pi(1-\\pi) + m_1(m_1-1)\\rho\\pi(1-\\pi)\\\\ &gt;m_1\\pi(1-\\pi)\\] <p>Therefore, the model for variance:</p> \\[var(Y_i)=\\psi m_i\\pi_i(1-\\pi_i)\\] <p>estimate of \\(\\hat\\psi\\) by scaled Pearson chi-square statistic</p> \\[\\hat\\psi = \\sum_i^n \\frac{P^2_{res,i}}{n-(p+1)}=\\frac{\\text{sum of squared Pearson residuals}}{d.f.}\\] <p>\\(\\hat\\psi&gt;&gt;1\\) indicates evidence of over-dispersion</p> <p>\\(\\psi\\) does not effect \\(E(Y_i)\\), hence using over-dispersion does not change \\(\\hat\\beta\\)</p> <p>The standard errors under this assumption is \\(se_{\\psi}(\\hat\\beta)=\\sqrt{\\hat\\psi}se(\\hat\\beta)\\)</p> <p>The following two ways are equivalent</p> <pre><code># manually conpute the psi hat\npsihat = sum(residuals(fitbl, type=\"pearson\")^2/fitbl$df.residual)\npsihat\n</code></pre> <pre><code>0.73257293737338\n</code></pre> <pre><code># apply the dispersion\nsummary(fitbl, dispersion=psihat)\n</code></pre> <pre><code>Call:\nglm(formula = cbind(Extinct, NExtinct) ~ log(Area), family = binomial, \n    data = krunnit)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-1.7173  -0.6772   0.0973   0.4837   1.4954\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  -1.1962     0.1014  -11.80  &lt; 2e-16 ***\nlog(Area)    -0.2971     0.0469   -6.33  2.5e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 0.7326)\n\n    Null deviance: 45.338  on 17  degrees of freedom\nResidual deviance: 12.062  on 16  degrees of freedom\nAIC: 75.39\n\nNumber of Fisher Scoring iterations: 4\n</code></pre> <pre><code># use quasi binomial family\nfitbl2 &lt;- glm(cbind(Extinct, NExtinct)~log(Area), family=quasibinomial, data=krunnit)\nfitbl2\n</code></pre> <pre><code>Call:  glm(formula = cbind(Extinct, NExtinct) ~ log(Area), family = quasibinomial, \n    data = krunnit)\n\nCoefficients:\n(Intercept)    log(Area)  \n     -1.196       -0.297\n\nDegrees of Freedom: 17 Total (i.e. Null);  16 Residual\nNull Deviance:      45.3 \nResidual Deviance: 12.1     AIC: NA\n</code></pre>"},{"location":"sta303/binomial_logistic_regression.html#using-logistic-regression-for-classification","title":"Using Logistic Regression for Classification","text":"<p>Want: \\(y^*\\mid (x_1^*, ...,x_p^*)= \\mathbb{I}\\)</p> <p>Do: calculate \\(\\hat\\pi_M^*\\) be the estimated probability that \\(y^*=1\\) based on the fitted model given \\(X_i=x_i^*\\), then predict that \\(y^* = \\mathbb{I}(\\hat\\pi_M^* \\text{is large enough})\\)</p> <p>Need: a good cutoff \\(\\hat\\pi_M^*\\) </p>"},{"location":"sta303/binomial_logistic_regression.html#classification","title":"Classification","text":"<p>Try different cutoffs and see which gives fewest incorrect classifications</p> <ul> <li>Useful if proportion of 1's and 0's in date reflect their relative proportions in the population</li> <li>Likely to overestimate. To overcome,cross-validation (training group vs. validation group)</li> </ul>"},{"location":"sta303/binomial_logistic_regression.html#confusion-matrix","title":"Confusion Matrix","text":"<p>https://en.wikipedia.org/wiki/Confusion_matrix</p> <p>Choose a cutoff probability based on one of the 5 criteria for success of classification that is most important to you </p> <p>Examples </p> <ul> <li>High sensitivity makes good screening test</li> <li>High specificity makes a good confirmatory test</li> <li>A screening test followed by a confirmatory test is good (but expensive) diagnostic procedure</li> </ul>"},{"location":"sta303/case_spock.html","title":"One Way ANOVA","text":"<pre><code># import data\nlibrary(Sleuth2)\npercent = case0502$Percent\njudge = case0502$Judge\n</code></pre> <pre><code># plot the data\nplot(judge, percent)\n</code></pre> <p>\u200b </p> <pre><code>summary(percent)\n</code></pre> <pre><code>   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   6.40   19.95   27.50   26.58   32.38   48.90\n</code></pre> <pre><code>boxplot(percent, horizontal = T)\n</code></pre> <p>\u200b </p> <p>This looks to be normal, based on Mean vs. Median, and the IQR</p> <pre><code>t.test(percent, mu = 50)\n</code></pre> <pre><code>    One Sample t-test\n\ndata:  percent\nt = -17.303, df = 45, p-value &lt; 2.2e-16\nalternative hypothesis: true mean is not equal to 50\n95 percent confidence interval:\n 23.85675 29.30847\nsample estimates:\nmean of x \n 26.58261\n</code></pre>"},{"location":"sta303/case_spock.html#one-sample-two-sided-test","title":"One sample two sided test","text":"<p>hypothesis: \\(H_0:\\mu=50\\)  test statistic: \\(\\frac{\\bar{X}-\\mu_0}{S/\\sqrt{n}}\\sim T_{n-1}\\)  The result is significant, we can reject the hypothesis. </p> <pre><code>par(mfrow = c(1,2))\nhist(percent)\nqqnorm(percent)\nqqline(percent)\n</code></pre> <p>\u200b </p>"},{"location":"sta303/case_spock.html#normality-check","title":"Normality Check","text":"<pre><code>shapiro.test(percent)\n</code></pre> <pre><code>    Shapiro-Wilk normality test\n\ndata:  percent\nW = 0.98763, p-value = 0.9013\n</code></pre> <p>\\(H_0\\): data is normal  Test statistics: 0.98763 Probability: 0.9013 is larger  We have evidence that data is normal.</p> <p>Consider two-sided t-test</p>"},{"location":"sta303/case_spock.html#two-sample-t-test","title":"Two Sample t-test","text":"<pre><code>groupS &lt;- percent[judge == \"Spock's\"]\ngroupS\n</code></pre> <pre><code> 6.40000009536743\n 8.69999980926514\n 13.3000001907349\n 13.6000003814697\n 15\n 15.1999998092651\n 17.7000007629395\n 18.6000003814697\n 23.1000003814697\n</code></pre> <pre><code>groupNS &lt;- percent[judge != \"Spock's\"]\ngroupNS\n</code></pre> <pre><code> 16.7999992370605\n 30.7999992370605\n 33.5999984741211\n 40.5\n 48.9000015258789\n 27\n 28.8999996185303\n 32\n 32.7000007629395\n 35.5\n 45.5999984741211\n 21\n 23.3999996185303\n 27.5\n 27.5\n 30.5\n 31.8999996185303\n 32.5\n 33.7999992370605\n 33.7999992370605\n 24.2999992370605\n 29.7000007629395\n 17.7000007629395\n 19.7000007629395\n 21.5\n 27.8999996185303\n 34.7999992370605\n 40.2000007629395\n 16.5\n 20.7000007629395\n 23.5\n 26.3999996185303\n 26.7000007629395\n 29.5\n 29.7999992370605\n 31.8999996185303\n 36.2000007629395\n</code></pre> <pre><code>boxplot(groupS, groupNS, xlab=\"JUDGE\", names = c(\"Spock\", \"Other\"))\n</code></pre> <p>\u200b </p> <p>Purpose to compare two population means  \\(H_0\\): \\(\\mu_x-\\mu_y = D_0 (\\text{ commonly }D_0=0)\\) Assumptions  - two samples are iid from approximately Normal populations - Two samples are independent of each other</p> <p>Test statistic \\(t = \\frac{(\\bar{x}-\\bar{y})-D_0}{se(\\bar{x}-\\bar{y})}\\)  $$ \\begin{align} var(\\bar{x}-\\bar{y}) &amp;= var(\\bar{x}) + var(-\\bar{y}) = \\sigma_x^2/n_x + (-1)^2\\sigma_y^2/n_y \\ se(\\bar{x}-\\bar{y}) &amp;= \\sqrt{\\sigma_x^2/n_x + \\sigma_y^2/x_y}  \\end{align}$$</p>"},{"location":"sta303/case_spock.html#check-equal-variance-assumption","title":"Check equal variance assumption","text":"<pre><code>var(groupS)\nvar(groupNS)\nmax(var(groupS), var(groupNS)) / min(var(groupS), var(groupNS)) # Rule of Thumb\nmax(sd(groupS), sd(groupNS)) / min(sd(groupS), sd(groupNS))\n</code></pre> <pre><code> 25.3894461176131\n 55.2163209681473\n 2.17477453869475\n 1.47471167985296\n</code></pre> <p>Rule of thumb test  \\(H_0: \\sigma_x^2 = \\sigma_y^2\\)  Test statistic: $S_{max}^2 / S^2_{min} = $ larger sample variance / smaller sample variance.  Reject \\(H_0\\) is test-statistic $ &gt; 4$</p>"},{"location":"sta303/case_spock.html#variance-ratio-f-test","title":"Variance Ratio F-test","text":"<pre><code>var.test(groupS, groupNS)\n</code></pre> <pre><code>    F test to compare two variances\n\ndata:  groupS and groupNS\nF = 0.45982, num df = 8, denom df = 36, p-value = 0.2482\nalternative hypothesis: true ratio of variances is not equal to 1\n95 percent confidence interval:\n 0.1789822 1.7739665\nsample estimates:\nratio of variances \n         0.4598178\n</code></pre> <p>Assumptions  - Random samples \\(X_1,X_2\\) with size \\(n_1,n_2\\) is drawn from \\(N(\\mu_1,\\sigma_1^2), N(\\mu_2, \\sigma_2^2)\\)  - \\(X_1,X_2\\) are independent.  - Samples size are large (better when samples size are equal)</p> <p>Test statistic \\(F = S_1^2/S_2^2 \\sim F_{n_1-1,n_2-1}\\) </p> <p>\\(p = 0.07668 &gt; 0.05\\), we don't reject the null hypothesis, evidence of equal variance</p>"},{"location":"sta303/case_spock.html#two-sample-t-test-satterwaite-approximation","title":"Two-sample t-test (Satterwaite approximation)","text":"<pre><code>t.test(groupS, groupNS, var.equal = F)\n</code></pre> <pre><code>    Welch Two Sample t-test\n\ndata:  groupS and groupNS\nt = -7.1597, df = 17.608, p-value = 1.303e-06\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -19.23999 -10.49935\nsample estimates:\nmean of x mean of y \n 14.62222  29.49189\n</code></pre> <p>Used when population variance can't be assume to be equal  Test statistic \\(t = \\frac{(\\bar{x}-\\bar{y}-D_0)}{\\sqrt{s^2_x/n_x + s_y^2/n_y}}\\sim t_v\\), \\(v = \\frac{(s^2_x/n_x + s_y^2/n_y)^2}{(s_x^2/n_x)^2/(n_x-1) + (s_y^2/n_y)^2/(n_y-1)}\\). \\(v\\) is calculated by Satterhwaite approximation, round down to the nearest integer</p>"},{"location":"sta303/case_spock.html#pooled-two-sample-t-test","title":"Pooled two-sample t-test","text":"<pre><code>t.test(groupS, groupNS, var.equal = T)\n</code></pre> <pre><code>    Two Sample t-test\n\ndata:  groupS and groupNS\nt = -5.6697, df = 44, p-value = 1.03e-06\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -20.155294  -9.584045\nsample estimates:\nmean of x mean of y \n 14.62222  29.49189\n</code></pre> <p>Assumption population variance are equal  Estimate pooled variance \\(s_p^2 = \\frac{(n_x-1)^2 s_x^2 + (n_y-1)^2 s_y^2}{n_x+n_y-2}\\) Test statistic \\(t = \\frac{(\\bar{x}-\\bar{y})-D_0}{\\sqrt{s_p^2(n_x^{-1}+n_y^{-1})}}\\sim t_{n_x+n_y-2}\\)</p> <p>Based on the tests, we can reject the hypothesis that two samples have the same means Conclusion Evidence that the percentage of women differs in the two groups</p> <p>Paired t-test Requirement \\(n_x = n_y\\), independent samples</p>"},{"location":"sta303/case_spock.html#pooled-t-test-left-tailed","title":"Pooled t-test (Left tailed)","text":"<p>\\(H_0: \\mu_x - \\mu_y = 0, H_a: \\mu_x &lt; \\mu_y\\)</p> <pre><code>t.test(groupS, groupNS, alternative=\"less\", var.equal= T)\n</code></pre> <pre><code>    Two Sample t-test\n\ndata:  groupS and groupNS\nt = -5.6697, df = 44, p-value = 5.148e-07\nalternative hypothesis: true difference in means is less than 0\n95 percent confidence interval:\n    -Inf -10.463\nsample estimates:\nmean of x mean of y \n 14.62222  29.49189\n</code></pre>"},{"location":"sta303/case_spock.html#dummy-variable-slr","title":"Dummy Variable (SLR)","text":"<pre><code>X &lt;- c(rep(1, length(groupS)), rep(0, length(groupNS)))\nmodel &lt;- lm(percent~X)\nsummary(model)\n</code></pre> <pre><code>Call:\nlm(formula = percent ~ X)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-12.9919  -4.6669   0.2581   3.7854  19.4081\n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   29.492      1.160   25.42  &lt; 2e-16 ***\nX            -14.870      2.623   -5.67 1.03e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7.056 on 44 degrees of freedom\nMultiple R-squared:  0.4222,    Adjusted R-squared:  0.409 \nF-statistic: 32.15 on 1 and 44 DF,  p-value: 1.03e-06\n</code></pre> <p>Model \\(Y_i=\\beta_0+\\beta_1X_i+\\epsilon_i\\), where \\(X_i=\\mathbb{I}(\\text{ith observation is from group A})\\). Assumptions  - The linear model is appropriate  - Gauss-Markov assumptions (\\(E(\\epsilon_i)=0, var(\\epsilon_i)=\\sigma^2\\): Uncorrelated errors)  - \\(\\epsilon_i\\sim N(0, \\sigma^2)\\)</p> <p>\\(H_0:\\beta_1 = 0\\) Test statistic \\(t = \\frac{b_1}{se(b_1)}\\sim t_{N-2}\\), $N = n_A + n_{A^c} $</p>"},{"location":"sta303/case_spock.html#regression-diagnostics","title":"Regression diagnostics","text":"<pre><code>yhats = fitted(model)\nerrors = residuals(model)\npar(mfrow=c(3,2))\n\nhist(errors, xlab=\"Residuals\", breaks = 5)\n\nplot(errors)\nabline(0, 0)\n\nplot(model)\n</code></pre> <p>\u200b </p> <p>Check Assumptions - Normality: looks like a little bit right skewed (but they might just be outliers) - Constant variance: yes - \\(E(\\epsilon) = 0\\): yes</p> <pre><code>anova(model)\n</code></pre> A anova: 2 \u00d7 5 DfSum SqMean SqF valuePr(&gt;F) &lt;int&gt;&lt;dbl&gt;&lt;dbl&gt;&lt;dbl&gt;&lt;dbl&gt; X 11600.6231600.6229032.145381.029666e-06 Residuals442190.903  49.79325      NA          NA"},{"location":"sta303/case_spock.html#anova-for-linear-regression","title":"ANOVA for linear regression","text":"<p>\\(H_0:\\beta_1 = 0\\) \\(F=MSR/MSE\\sim F_{d.f.variables,\\: d.f.errors}\\)</p>"},{"location":"sta303/contingency.html","title":"Factor independence and Contingency Table","text":""},{"location":"sta303/contingency.html#case-study-framingham-heart-study","title":"Case Study: Framingham Heart Study","text":"<p>Data considered:</p> <ul> <li>n = 1329 men</li> <li>X = Cholesterol measurement in 1948 (mg/dl)</li> <li>Y = after 10 years, did they developed CVD (present/absernt)</li> </ul>"},{"location":"sta303/contingency.html#binomial-sampling","title":"Binomial Sampling","text":"<p>Let \\(\\pi_H=p(present\\mid high),\\pi_L = P(present\\mid low)\\).</p> <p>Hypothese: \\(H_0: \\pi_H = \\pi_L, H_a: \\pi_H \\neq \\pi_L\\)</p> <p>Assumptions: </p> <ul> <li>depending on level of cholesterol, each person is a Bernoulli trial with chance of developing CVD as \\(n_H = 284, n_L = 1043\\). </li> <li>Then for fixed \\(n_H, n_L\\), the count of the number of people who develop CVD \\(y_H\\sim Binomial(n_H=286,\\pi_H)\\) \\(y_L\\sim Binomial(n_L=1043,\\pi_L)\\)</li> <li>Then estimate of \\(\\pi_H-\\pi_L\\) is \\(\\hat\\pi_H-\\hat\\pi_L\\) where \\(\\hat\\pi_H = y_H/n_H, \\hat\\pi_L = y_L/n_L\\) are the sample proportions.</li> <li>\\(var(\\hat\\pi_H-\\hat\\pi_L) = var(\\hat\\pi_H) + var(\\hat\\pi_L)\\\\=n_H\\pi_H(1-\\pi_H)/n^2_H + n_L\\pi_L(1-\\pi_L)/n^2_L\\\\= \\pi_H(1-\\pi_H)/n_H+\\pi_L(1-\\pi_L)/n_L\\)</li> <li>\\(se(\\hat\\pi_H-\\hat\\pi_L) = \\sqrt{\\hat\\pi_c(1-\\hat\\pi_c)(n_H^{-1}+n_L^{-1})}\\) where \\(\\hat\\pi_c=\\frac{y_L+y_H}{n+L + n_H}\\) is the combined sample proportion</li> <li>By CLT, the test statistic \\(\\sim N(0,1)\\)</li> </ul> <p>Test statistic: \\(\\frac{\\hat\\pi_H-\\hat\\pi_L}{se(\\hat\\pi_H-\\hat\\pi_L)}=5.575\\)</p> <p>p-value \\(2P(Z\\geq 5.575)&lt;0.05\\)</p> <p>Conclusion: We have strong evidence that the probability of developing CVD is not the same for High and Low cholesterol groups.</p> <pre><code>cvd&lt;-matrix(c(41,245,51,992), nrow=2,byrow=TRUE)\ndimnames(cvd)&lt;-list(c(\"High\",\"Low\"), c(\"Present\",\"Absent\"))\nnames(dimnames(cvd))&lt;-c(\"Cholesterol\",\"Cardio Vascular Disease\")\nprint(cvd)\n</code></pre> <pre><code>           Cardio Vascular Disease\nCholesterol Present Absent\n       High      41    245\n       Low       51    992\n</code></pre> <pre><code># estimate for pi\npi_h = 41/(41+245)\npi_l = 51/(51+992)\nprint(pi_h)\nprint(pi_l)\n</code></pre> <pre><code>[1] 0.1433566\n[1] 0.04889741\n</code></pre> <pre><code># sample size \nn_h = 41 + 245\nn_l = 51 + 992\nconf.level = 0.95\ncrit.val = qnorm(1-(1-conf.level)/2)\ncrit.val\n</code></pre> <pre><code>1.95996398454005\n</code></pre> <pre><code># standard error\nse.hat = sqrt(pi_h * (1 - pi_h)/n_h + pi_l * (1 - pi_l)/n_l)\nse.hat\n</code></pre> <pre><code>0.0217710596635901\n</code></pre> <pre><code># 95% CI\nc((pi_h-pi_l)-crit.val*se.hat, (pi_h-pi_l)+crit.val*se.hat)\n</code></pre> <pre><code>0.0517887391972153\n0.137129724889034\n</code></pre> <pre><code># easier way for bonimial sampling\nprop.test(cvd, correct=FALSE)\n</code></pre> <pre><code>    2-sample test for equality of proportions without continuity\n    correction\n\ndata:  cvd\nX-squared = 31.082, df = 1, p-value = 2.474e-08\nalternative hypothesis: two.sided\n95 percent confidence interval:\n 0.05178874 0.13712972\nsample estimates:\n    prop 1     prop 2 \n0.14335664 0.04889741\n</code></pre> <pre><code># or chisq test\nchisq.test(cvd, correct=F)\n</code></pre> <pre><code>    Pearson's Chi-squared test\n\ndata:  cvd\nX-squared = 31.082, df = 1, p-value = 2.474e-08\n</code></pre> <pre><code># Don't use this, provide different result from the manual way\nprop.test(cvd)\n</code></pre> <pre><code>    2-sample test for equality of proportions with continuity correction\n\ndata:  cvd\nX-squared = 29.633, df = 1, p-value = 5.221e-08\nalternative hypothesis: two.sided\n95 percent confidence interval:\n 0.0495611 0.1393574\nsample estimates:\n    prop 1     prop 2 \n0.14335664 0.04889741\n</code></pre> <p>The CI does not include 0. </p>"},{"location":"sta303/contingency.html#contingency-table","title":"Contingency Table","text":"<p>Have a row factor with \\(I\\) levels and a column factor with \\(J\\) levels</p> <p>Then, define \\(P(C=i,R=j)=\\pi_{ij}, P(C=i)=\\pi_{i\\cdot}, P(R=j)=\\pi_{\\cdot j}\\)</p> <p>Hypothesis: \\(H_0: \\pi_{ij} = \\pi_{i\\cdot}\\pi_{\\cdot j}, H_a: \\pi_{ij} \\neq\\pi_{i\\cdot}\\pi_{\\cdot j}\\) null: there is no relationship between the two factors</p> <p>For each cell, estimated expected cell count \\(\\hat\\mu_{ij} = n\\hat\\pi_i\\hat\\pi_j = y_{i\\cdot}y_{\\cdot j}/n\\)</p> <p>Test statistic: \\(X^2 = \\sum_{j=1}^J\\sum_{i=1}^I \\frac{(y_{ij}-\\hat\\mu_{ij})^2}{\\hat\\mu_{ij}}\\sim \\chi^2_{(I-1)(J-1)}\\)</p> <p>If \\(var(y)=E(y)=\\mu\\Rightarrow y\\sim Poisson(\\mu)\\)</p> <p>For this case, test statistic: \\(31.08\\sim \\chi^2_{(2-1)(2-1)}\\), p-value \\(&lt;0.0001\\)</p> <p>Strong evidence that the two factors are not independent, CVD status depends on cholesterol level.</p> <p>When \\(I=J=2\\), the chi-square test of independence is equivalent to comparing two proportions.</p>"},{"location":"sta303/contingency.html#formal-approach","title":"Formal approach","text":"<p>Let \\(Y_{ij}\\) be r.v. representing the number of observations in cell \\((i,j)\\).</p> <p>Observe \\(y_{ij}\\) be observed cell counts</p> <p>Then multinomial </p> \\[P(Y=y)=\\frac{n!\\pi_{11}^{y_{11}}\\pi_{12}^{y_{12}}\\pi_{21}^{y_{21}}\\pi_{22}^{y_{22}}} {y_{11}!y_{12}!y_{21}!y_{22}!}\\sim Multinomial(n,\\pi_{11},\\pi_{12},\\pi_{21},\\pi_{22})\\] <p>Log-likelihood is </p> \\[\\log\\mathcal{L}=\\sum_{j=1}^2\\sum_{i=1}^2 y_{ij}\\log\\pi_{ij}+\\log{n\\choose y_{11}y_{12}y_{21}y_{22}}\\] <p>Maximize \\(\\log\\mathcal{L}\\) w.r.t. \\(\\pi\\)'s and \\(\\sum\\sum\\pi_{ij}=1\\), then \\(\\hat\\pi_{ij}=y_{ij}/n\\)</p> <p>Under \\(H_0: \\pi_{ij}=\\pi_{i\\cdot}\\pi_{\\cdot j}\\), can substitute \\(\\pi_{ij}\\) and maximize the column and row \\(\\pi\\)'s. </p> \\[G^2 = -2\\log(\\mathcal{L}_R/\\mathcal{L}_F)\\sim\\chi^2_{(I-1)(J-1)}\\] <p>To obtain the d.f. <code>df(Unrestrcited / FULL)</code>\\(-\\)<code>df(Independence/REDUCED)</code> \\(=\\)<code>#parameters in FULL</code>(\\(\\pi_{ij}\\)) \\(-\\) <code>#parameters in REDUCED</code>(\\(\\pi_{i\\cdot},\\pi_{\\cdot j}\\)) \\(= IJ-1-(I+J-2)\\) \\(-1\\) because constraint \\(\\sum\\sum\\pi_{ij}=1\\) \\(-2\\) because constraint \\(\\sum\\pi_{i\\cdot}=1,\\sum\\pi_{\\cdot j}=1\\)</p>"},{"location":"sta303/contingency.html#case-study-7-three-way-contingency","title":"Case Study 7: Three Way Contingency","text":"<p>A alcohol use 1 = True, 2 = False  </p> <p>M marijuana use</p> <p>C cigarette use</p> <pre><code>A=c(1,1,1,1,2,2,2,2)\nC=c(1,1,2,2,1,1,2,2)\nM=c(1,2,1,2,1,2,1,2)\nY=c(911,538,44,456,3,43,2,279)\nA=as.factor(A)\nC=as.factor(C)\nM=as.factor(M)\nACM=cbind(A,C,M, Y)\nACM\n</code></pre> A matrix: 8 \u00d7 4 of type dbl ACMY 111911 112538 121 44 122456 211  3 212 43 221  2 222279"},{"location":"sta303/contingency.html#model-1-complete-independence","title":"Model 1. Complete Independence","text":"<p>\\(P(ACM)=P(A)P(C)P(M)\\) - the three factors are mutually independent</p> <p>\\(H_0: \\pi_{ijk} = \\pi_{i..}\\pi_{.j.}\\pi_{k..}\\) for all \\(i,j,k\\).  \\(H_a: \\pi_{ijk} \\neq \\pi_{i..}\\pi_{.j.}\\pi_{k..}\\)</p> <p>\\(\\log(\\mu_{ijk}) = \\beta_0+\\beta_1 \\mathbb{I}_C + \\beta_2 \\mathbb{I}_C + \\beta_3 \\mathbb{I}_M\\)</p> <p>MLE </p> \\[\\hat\\mu_{ijk}=n\\hat\\pi_{ijk} = n\\hat\\pi_{i..}\\hat\\pi_{.j.}\\hat\\pi_{..k}=\\frac{ny_{i..}y_{.j.}y_{..k}}{nnn}\\] <pre><code>mod_A.C.M=glm(Y~A+C+M, family=poisson) # Additive\nsummary(mod_A.C.M)\n</code></pre> <pre><code>Call:\nglm(formula = Y ~ A + C + M, family = poisson)\n\nDeviance Residuals: \n      1        2        3        4        5        6        7        8  \n 14.522   -7.817  -17.683    3.426  -12.440   -8.436   -8.832   19.639\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  6.29154    0.03667 171.558  &lt; 2e-16 ***\nA2          -1.78511    0.05976 -29.872  &lt; 2e-16 ***\nC2          -0.64931    0.04415 -14.707  &lt; 2e-16 ***\nM2           0.31542    0.04244   7.431 1.08e-13 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 2851.5  on 7  degrees of freedom\nResidual deviance: 1286.0  on 4  degrees of freedom\nAIC: 1343.1\n\nNumber of Fisher Scoring iterations: 6\n</code></pre>"},{"location":"sta303/contingency.html#model-2-block-independence","title":"Model 2. Block Independence","text":"<p>\\(P(AC\\mid M)= P(AC)\\) - Joint probability of alcohol and cigarette use is independent of marijuana use; Alcohol and cigarette use are associated. </p> <p>\\(H_0: \\pi_{ijk} = \\pi_{ij.}\\pi_{k..}\\) for all \\(i,j,k\\).  \\(H_a: \\pi_{ijk} \\neq \\pi_{ij.}\\pi_{k..}\\)</p> <p>\\(\\log(\\mu_{ijk}) = \\beta_0+\\beta_1 \\mathbb{I}_C + \\beta_2 \\mathbb{I}_C + \\beta_3 \\mathbb{I}_M+\\beta_4\\mathbb{I}_A\\mathbb{I}_C\\)</p> <p>MLE</p> \\[\\hat\\mu_{ijk}=n\\hat\\pi_{ijk} = n\\hat\\pi_{ij.}\\hat\\pi_{..k}=\\frac{ny_{ij.}y_{..k}}{nn}\\] <pre><code>mod_AC.M=glm(Y~M+A*C, family=poisson) #Block AC\nsummary(mod_AC.M)\n</code></pre> <pre><code>Call:\nglm(formula = Y ~ M + A * C, family = poisson)\n\nDeviance Residuals: \n      1        2        3        4        5        6        7        8  \n 11.297  -11.092  -13.996    9.045   -4.648    2.917  -14.721    8.286\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  6.41539    0.03595 178.451  &lt; 2e-16 ***\nM2           0.31542    0.04244   7.431 1.08e-13 ***\nA2          -3.44999    0.14976 -23.036  &lt; 2e-16 ***\nC2          -1.06402    0.05187 -20.515  &lt; 2e-16 ***\nA2:C2        2.87373    0.16730  17.178  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 2851.46  on 7  degrees of freedom\nResidual deviance:  843.83  on 3  degrees of freedom\nAIC: 902.87\n\nNumber of Fisher Scoring iterations: 6\n</code></pre> <pre><code>mod_AM.C=glm(Y~C+A*M, family=poisson) #Block AM\nsummary(mod_AM.C)\n</code></pre> <pre><code>Call:\nglm(formula = Y ~ C + A * M, family = poisson)\n\nDeviance Residuals: \n       1         2         3         4         5         6         7         8  \n 10.6031   -4.6398  -19.7664    5.9142   -0.1592  -14.1425    0.2114   13.4104\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  6.44142    0.03573 180.280   &lt;2e-16 ***\nC2          -0.64931    0.04415 -14.707   &lt;2e-16 ***\nA2          -5.25227    0.44838 -11.714   &lt;2e-16 ***\nM2           0.04003    0.04531   0.883    0.377    \nA2:M2        4.12509    0.45294   9.107   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 2851.46  on 7  degrees of freedom\nResidual deviance:  939.56  on 3  degrees of freedom\nAIC: 998.61\n\nNumber of Fisher Scoring iterations: 5\n</code></pre> <pre><code>mod_A.CM=glm(Y~A+C*M, family=poisson) #Block CM\nsummary(mod_A.CM)\n</code></pre> <pre><code>Call:\nglm(formula = Y ~ A + C * M, family = poisson)\n\nDeviance Residuals: \n       1         2         3         4         5         6         7         8  \n  4.4691    1.7907    0.7207   -7.2723  -15.2958   -4.8889   -2.1064   13.9760\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  6.66273    0.03417 194.969   &lt;2e-16 ***\nA2          -1.78511    0.05976 -29.872   &lt;2e-16 ***\nC2          -2.98919    0.15111 -19.782   &lt;2e-16 ***\nM2          -0.45308    0.05306  -8.539   &lt;2e-16 ***\nC2:M2        3.22431    0.16098  20.029   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 2851.46  on 7  degrees of freedom\nResidual deviance:  534.21  on 3  degrees of freedom\nAIC: 593.26\n\nNumber of Fisher Scoring iterations: 6\n</code></pre>"},{"location":"sta303/contingency.html#model-3-partial-independence","title":"Model 3. Partial Independence","text":"<p>\\(P(AC\\mid M)=P(A\\mid M)P(C\\mid M)\\). Alcohol and cigarette use are conditionally independent given marijuana use; Alcohol and marijuana use are associated, and cigarette and marijuana use are associated</p> <p>\\(H_0:\\pi_{ijk} = \\pi_{i.k}\\pi_{.jk}/\\pi_{..k}\\) \\(H_a:\\pi_{ijk} \\neq \\pi_{i.k}\\pi_{.jk}/\\pi_{..k}\\)</p> <p>\\(\\log(\\mu_{ijk}) = \\beta_0+\\beta_1 \\mathbb{I}_C + \\beta_2 \\mathbb{I}_C + \\beta_3 \\mathbb{I}_M+\\beta_4\\mathbb{I}_A\\mathbb{I}_M+\\beta_5\\mathbb{I}_C\\mathbb{I}_M\\)</p> <p>MLE</p> \\[\\hat\\mu_{ijk}=n\\hat\\pi_{ijk} = n\\hat\\pi_{.jk}\\hat\\pi_{i.k} / \\hat\\pi_{..k}=y_{ijk}y_{i.k}/y_{..k}\\] <pre><code>mod_AC.AM=glm(Y~A*C+A*M, family=poisson) #Partial A\nsummary(mod_AC.AM)\n</code></pre> <pre><code>Call:\nglm(formula = Y ~ A * C + A * M, family = poisson)\n\nDeviance Residuals: \n       1         2         3         4         5         6         7         8  \n  7.2238   -7.7739  -15.8396   11.3171    2.0272   -0.3442   -1.2388    0.1379\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  6.56527    0.03499 187.643   &lt;2e-16 ***\nA2          -6.91715    0.46893 -14.751   &lt;2e-16 ***\nC2          -1.06402    0.05187 -20.515   &lt;2e-16 ***\nM2           0.04003    0.04531   0.883    0.377    \nA2:C2        2.87373    0.16730  17.178   &lt;2e-16 ***\nA2:M2        4.12509    0.45294   9.107   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 2851.46  on 7  degrees of freedom\nResidual deviance:  497.37  on 2  degrees of freedom\nAIC: 558.41\n\nNumber of Fisher Scoring iterations: 5\n</code></pre> <pre><code>mod_AC.CM=glm(Y~A*C+C*M, family=poisson) #Partial C\nsummary(mod_AC.CM)\n</code></pre> <pre><code>Call:\nglm(formula = Y ~ A * C + C * M, family = poisson)\n\nDeviance Residuals: \n      1        2        3        4        5        6        7        8  \n 0.8401  -1.0667   2.4964  -0.6743  -6.0678   5.0235  -4.5440   0.8867\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  6.78658    0.03340 203.212   &lt;2e-16 ***\nA2          -3.44999    0.14976 -23.036   &lt;2e-16 ***\nC2          -3.40390    0.15354 -22.170   &lt;2e-16 ***\nM2          -0.45308    0.05306  -8.539   &lt;2e-16 ***\nA2:C2        2.87373    0.16730  17.178   &lt;2e-16 ***\nC2:M2        3.22431    0.16098  20.029   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 2851.461  on 7  degrees of freedom\nResidual deviance:   92.018  on 2  degrees of freedom\nAIC: 153.06\n\nNumber of Fisher Scoring iterations: 6\n</code></pre> <pre><code>mod_AM.CM=glm(Y~A*M+C*M, family=poisson) #Partial M\nsummary(mod_AM.CM)\n</code></pre> <pre><code>Call:\nglm(formula = Y ~ A * M + C * M, family = poisson)\n\nDeviance Residuals: \n      1        2        3        4        5        6        7        8  \n 0.0584   4.5702  -0.2619  -4.3441  -0.8663  -9.7716   2.2287   6.8353\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  6.81261    0.03316 205.450   &lt;2e-16 ***\nA2          -5.25227    0.44837 -11.714   &lt;2e-16 ***\nM2          -0.72847    0.05538 -13.154   &lt;2e-16 ***\nC2          -2.98919    0.15111 -19.782   &lt;2e-16 ***\nA2:M2        4.12509    0.45294   9.107   &lt;2e-16 ***\nM2:C2        3.22431    0.16098  20.029   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 2851.46  on 7  degrees of freedom\nResidual deviance:  187.75  on 2  degrees of freedom\nAIC: 248.8\n\nNumber of Fisher Scoring iterations: 5\n</code></pre>"},{"location":"sta303/contingency.html#model-4-uniform-association","title":"Model 4. Uniform Association","text":"<p>Association among all pairs</p> \\[\\log(\\mu_{ijk}) = \\beta_0+\\beta_1 \\mathbb{I}_C + \\beta_2 \\mathbb{I}_C + \\beta_3 \\mathbb{I}_M+\\beta_4\\mathbb{I}_A\\mathbb{I}_M+\\beta_5\\mathbb{I}_C\\mathbb{I}_M + \\beta_6\\mathbb{I}_A\\mathbb{I}_C\\] <pre><code>mod_AM.AC.CM=glm(Y~A*M+A*C+C*M, family=poisson) #Uniform\nsummary(mod_AM.AC.CM)\n</code></pre> <pre><code>Call:\nglm(formula = Y ~ A * M + A * C + C * M, family = poisson)\n\nDeviance Residuals: \n       1         2         3         4         5         6         7         8  \n 0.02044  -0.02658  -0.09256   0.02890  -0.33428   0.09452   0.49134  -0.03690\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  6.81387    0.03313 205.699  &lt; 2e-16 ***\nA2          -5.52827    0.45221 -12.225  &lt; 2e-16 ***\nM2          -0.52486    0.05428  -9.669  &lt; 2e-16 ***\nC2          -3.01575    0.15162 -19.891  &lt; 2e-16 ***\nA2:M2        2.98601    0.46468   6.426 1.31e-10 ***\nA2:C2        2.05453    0.17406  11.803  &lt; 2e-16 ***\nM2:C2        2.84789    0.16384  17.382  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 2851.46098  on 7  degrees of freedom\nResidual deviance:    0.37399  on 1  degrees of freedom\nAIC: 63.417\n\nNumber of Fisher Scoring iterations: 4\n</code></pre>"},{"location":"sta303/contingency.html#saturated-model","title":"Saturated Model","text":"\\[\\log(\\mu_{ijk}) = \\beta_0+\\beta_1 \\mathbb{I}_C + \\beta_2 \\mathbb{I}_C + \\beta_3 \\mathbb{I}_M+\\beta_4\\mathbb{I}_A\\mathbb{I}_M+\\beta_5\\mathbb{I}_C\\mathbb{I}_M + \\beta_6\\mathbb{I}_A\\mathbb{I}_C + \\beta_7\\mathbb{I}_A\\mathbb{I}_C\\mathbb{I}_M\\] <p>this always fits the data perfectly</p> <pre><code>mod_ACM=glm(Y~A*C*M, family=poisson) #Saturated\nsummary(mod_ACM)\n</code></pre> <pre><code>Call:\nglm(formula = Y ~ A * C * M, family = poisson)\n\nDeviance Residuals: \n[1]  0  0  0  0  0  0  0  0\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  6.81454    0.03313 205.682  &lt; 2e-16 ***\nA2          -5.71593    0.57830  -9.884  &lt; 2e-16 ***\nC2          -3.03035    0.15435 -19.633  &lt; 2e-16 ***\nM2          -0.52668    0.05437  -9.686  &lt; 2e-16 ***\nA2:C2        2.62489    0.92583   2.835  0.00458 ** \nA2:M2        3.18927    0.59962   5.319 1.04e-07 ***\nC2:M2        2.86499    0.16696  17.159  &lt; 2e-16 ***\nA2:C2:M2    -0.58951    0.94236  -0.626  0.53160    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance:  2.8515e+03  on 7  degrees of freedom\nResidual deviance: -4.1522e-14  on 0  degrees of freedom\nAIC: 65.043\n\nNumber of Fisher Scoring iterations: 3\n</code></pre> <pre><code>muhats&lt;-predict.glm(mod_AM.AC.CM, type=\"response\")\ncbind(A, C, M, Y, muhats)\n</code></pre> A matrix: 8 \u00d7 5 of type dbl ACMYmuhats 1111911910.38317 2112538538.61683 3121 44 44.61683 4122456455.38317 5211  3  3.61683 6212 43 42.38317 7221  2  1.38317 8222279279.61683"},{"location":"sta303/contingency.html#inference-for-log-linear-models","title":"Inference for log-linear models","text":""},{"location":"sta303/contingency.html#model-assumptions","title":"Model assumptions","text":"<ul> <li>independent quantities being counted</li> <li>Large enough sample size for MLE asymptotic tests to hold (most \\(\\hat\\mu_{ijk}\\geq 5\\))</li> <li>Cross-classified counts follow a Poisson distribution \\(var(y_{ijk})=\\mu_{ijk}\\). If not, the deviance is very large(\"extra-Poisson\" variation)</li> <li>Correct form <ul> <li>\\(\\log(E(Y))\\) is linear in \\(\\beta\\)'s</li> <li>All relevant variables included</li> <li>No outliers</li> <li>Agreement of predicted and observed count</li> <li>Check deviance GOF test</li> </ul> </li> </ul>"},{"location":"sta303/contingency.html#model-comparisons","title":"Model comparisons","text":"<p>Example For the uniform association model, is the CM interaction needed? / Does the (AC,CM) model fit just as well?</p> <p>Uniform association model \\(\\log(\\mu_{ijk}) = \\beta_0+\\beta_1 \\mathbb{I}_C + \\beta_2 \\mathbb{I}_C + \\beta_3 \\mathbb{I}_M+\\beta_4\\mathbb{I}_A\\mathbb{I}_M+\\beta_5\\mathbb{I}_C\\mathbb{I}_M + \\beta_6\\mathbb{I}_A\\mathbb{I}_C\\)</p> <p>\\(H_0: \\beta_6=0\\), reduced model is better no CM interaction \\(H_a: \\beta\\neq 0\\) full model is better</p> <p>Wald: test-statistic: 17.382.  \\(p&lt;2\\times 10^{-16}\\).</p> <p>LRT: test-statistic: 497.37 - 0.37399 = 497 \\(p&lt;2\\times 10^{-16}\\)</p> <p>Conclusion: Very strong evidence that we should keep the CM interaction term. The uniform association model is better than the model without CM interaction term. </p> <pre><code>mod_AM.AC=glm(Y~A+M+C+A:C + A*M, family=poisson) #Uniform\nsummary(mod_AM.AC)\n</code></pre> <pre><code>Call:\nglm(formula = Y ~ A + M + C + A:C + A * M, family = poisson)\n\nDeviance Residuals: \n       1         2         3         4         5         6         7         8  \n  7.2238   -7.7739  -15.8396   11.3171    2.0272   -0.3442   -1.2388    0.1379\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  6.56527    0.03499 187.643   &lt;2e-16 ***\nA2          -6.91715    0.46893 -14.751   &lt;2e-16 ***\nM2           0.04003    0.04531   0.883    0.377    \nC2          -1.06402    0.05187 -20.515   &lt;2e-16 ***\nA2:C2        2.87373    0.16730  17.178   &lt;2e-16 ***\nA2:M2        4.12509    0.45294   9.107   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 2851.46  on 7  degrees of freedom\nResidual deviance:  497.37  on 2  degrees of freedom\nAIC: 558.41\n\nNumber of Fisher Scoring iterations: 5\n</code></pre>"},{"location":"sta303/contingency.html#deviance-gof-test","title":"Deviance GOF test","text":"<p>Compare fitted model to saturated model Small deviance / large p-values</p> <ul> <li>fitted model is adequate</li> <li>test is not powerful enough to detect inadequacies</li> </ul> <p>Large deviance / small p-value</p> <ul> <li>Fitted model is not adequate, consider a more complex model OR</li> <li>underlying distribution is not adequately modeled by the Poisson distribution OR</li> <li>There are severe outliers </li> </ul>"},{"location":"sta303/contingency.html#outliers","title":"Outliers","text":"<ul> <li>raw residual: \\(y_{ijk} - \\hat\\mu_{ijk}\\)</li> <li>Pearson residuals: \\((y_{ijk} - \\hat\\mu_{ijk})/\\sqrt{\\hat\\mu_{ijk}}\\)</li> <li>Deviance residuals: \\(sign(y_{ijk} - \\hat\\mu_{ijk})\\sqrt{2(y_{ijk}\\log(y_{ijk}/\\hat\\mu_{ijk}))-y_{ijk}+\\hat\\mu_{ijk}}\\)</li> </ul> <p>Rule of thumb: outliers if Pearson or Deviance residual \\(&gt;3\\), is the sample size if small, then \\(&gt;2\\). </p>"},{"location":"sta303/contingency.html#extra-poisson-variation","title":"Extra-Poisson variation","text":"<ul> <li>Check if \\(Deviance/df &gt; 1\\)</li> <li>If other problems are ruled out, then include a dispersion paramter OR use negative binomial regression</li> </ul>"},{"location":"sta303/multi_comparisons.html","title":"Multi Comparisons and One Way ANOVA","text":""},{"location":"sta303/multi_comparisons.html#multiple-comparisons","title":"Multiple Comparisons","text":"<ul> <li>post hoc procedure: further comparisons after significant result from overall One-way ANOVA</li> <li>If the result for One-way ANOVA is good enough, i.e.some pairs are evidently true, we may omit some pairs to remove the number of tests</li> <li>Max of \\(G \\choose 2\\) pairwise comparisons</li> <li>Major issue: increased chance of making at least one Type I error when carrying out many tests, \\(E(\\#errors)=\\#tests \\times \\alpha\\)</li> <li>Two common solutions: based on controlling family Type I error rate, choose <ul> <li>Bonferroni</li> <li>Tukey's</li> </ul> </li> </ul> <p>Example \\(P(\\text{committing at least 1 Type I error})\\) ?</p> <ul> <li>For n independent tests: \\(P = 1-(1-\\alpha)^n\\)</li> </ul>"},{"location":"sta303/multi_comparisons.html#bonferronis-method","title":"Bonferroni's Method","text":"<ul> <li>Based on Bonferroni's inequality \\(P(A\\cup B) \\leq P(A)+P(B)\\), hence \\(P(\\cup A_i)\\leq \\sum P(A_i), A_i:=\\) the event that \\(i\\)th test results in a Type I error. </li> <li>Method: conduct each of \\(k=G \\choose 2\\) pairwise tests at level \\(\\alpha / k\\)</li> <li>CI: \\(|\\bar{y}_i - \\bar{y}_j|\\pm t_{\\alpha/2k} S_p \\sqrt{n_i^{-1} + n_j^{-1}}\\) </li> <li>Conservative: overall Type I error rate is usually much less than \\(\\alpha\\) if tests are not mutually independent. </li> <li>Type II error inflation.</li> </ul>"},{"location":"sta303/multi_comparisons.html#tukeys-approach","title":"Tukey's Approach","text":"<ul> <li>Usually less conservative than Bonferroni, particularly if group sample size are similar. Controls the overall Type I error rate of \\(\\alpha\\), simultaneous CI converage rate is \\(1-\\alpha\\). </li> </ul>"},{"location":"sta303/multi_comparisons.html#studentized-range-distribution","title":"Studentized Range distribution","text":"<ul> <li>\\(\\mathcal{X}= \\{X_1,...,X_n\\}, X_i\\in N(\\mu,\\sigma^2)\\). Determine the distribution of the max and min of \\(\\mathcal{X}\\). </li> <li>\\(X_{(n)}:=\\max{\\mathcal{X}}, X_{(1)};=\\min{\\mathcal{X}}\\), Range \\(:= X_{(n)}-X{(1)}\\)</li> <li>Based on \\(n\\) observations from \\(X\\), the Studentized range statistic is \\(Q_{stat} = Range / s, s=\\) sample std. </li> <li> <p>Based on \\(G\\) group means, with \\(n\\) observations per group: </p> \\[\\bar{Q}_g = \\frac{\\sqrt{n}(\\bar{y}_{(g)} - \\bar{y}_{(1)})}{s_v}\\] </li> </ul> <p>\\(s_v\\) estimator of the pooled std. \\(v=N-G=G(n-1)\\) d.f. </p> <ul> <li>If there are \\(G\\) groups, then there is a max of \\(k\\) pairwise differences</li> <li>Controlling overall simultaneous Type I error rate v.s. Individual Type I error rate</li> <li>Find a pairwise significant difference<ul> <li>Compare method-wise significant difference, \\(c(\\alpha)\\) with \\(|\\bar{y}_i - \\bar{y}_j|\\) OR </li> <li>determine whether CI contains 0 OR </li> <li>Compare \\(p\\) with \\(\\alpha\\)</li> </ul> </li> <li>\\(s=\\sqrt{MSE}\\) with d.f. \\(=v=d.f.Error\\)</li> </ul>"},{"location":"sta303/multi_comparisons.html#tukeys-honestly-significant-difference-hsd","title":"Tukey's Honestly Significant Difference (HSD)","text":"<ul> <li>let \\(q(G,v,\\alpha)/t^*:=\\) the critical value from the Studentized Range distribution. </li> <li>Family rate \\(=\\alpha\\)</li> <li>Tukey's HSD \\(=q(G,v,\\alpha) \\frac{s}{\\sqrt{n}}\\)</li> </ul>"},{"location":"sta303/multi_comparisons.html#case-study","title":"Case Study","text":"<p>Bonferroni</p> <ul> <li>\\(H_0:\\mu_i - \\mu_j = 0, H_a: \\mu_i\\neq \\mu_j\\) </li> <li>\\(t = \\frac{\\bar{x}_i - \\bar{x}_j}{S_p \\sqrt{n_i^{-1} + n_j^{-1}}}\\)</li> <li>\\(p = 2P(T_{\\alpha/2k}&gt; |t|)\\)</li> </ul> <pre><code># import data\nlibrary(Sleuth2)\njury = case0502\npercent = case0502$Percent\njudge = case0502$Judge\n</code></pre> <pre><code>judge = relevel(judge, ref=\"Spock's\")\npairwise.t.test(percent, judge, p.adj='bonf')\n</code></pre> <pre><code>    Pairwise comparisons using t tests with pooled SD\n\ndata:  percent and judge\n\n  Spock's A       B       C       D       E      \nA 0.00022 -       -       -       -       -      \nB 0.00013 1.00000 -       -       -       -      \nC 0.00150 1.00000 1.00000 -       -       -      \nD 0.57777 1.00000 1.00000 1.00000 -       -      \nE 0.03408 1.00000 1.00000 1.00000 1.00000 -      \nF 0.01254 1.00000 1.00000 1.00000 1.00000 1.00000\n\nP value adjustment method: bonferroni\n</code></pre> <pre><code># CI\nlmod=lm(percent~judge)\nconfint(lmod, level = 1- 0.05/nlevels(judge))\n</code></pre> A matrix: 7 \u00d7 2 of type dbl 0.357 %99.643 % (Intercept) 8.07808521.16636 judgeA 8.54734130.44821 judgeB 8.64725429.34163 judgeC 5.22296923.73259 judgeD-2.96958527.72514 judgeE 1.99725522.69163 judgeF 2.92297021.43259 <pre><code>amod = aov(percent~judge)\nhsd = TukeyHSD(amod, 'judge')\nhsd\n</code></pre> <pre><code>  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = percent ~ judge)\n\n$judge\n                 diff        lwr       upr     p adj\nA-Spock's 19.49777727   7.514685 31.480870 0.0001992\nB-Spock's 18.99444405   7.671486 30.317402 0.0001224\nC-Spock's 14.47777732   4.350216 24.605339 0.0012936\nD-Spock's 12.37777758  -4.416883 29.172438 0.2744263\nE-Spock's 12.34444443   1.021486 23.667402 0.0248789\nF-Spock's 12.17777766   2.050216 22.305339 0.0098340\nB-A       -0.50333322 -13.512422 12.505755 0.9999997\nC-A       -5.01999995 -17.003092  6.963092 0.8470098\nD-A       -7.11999969 -25.094638 10.854639 0.8777485\nE-A       -7.15333284 -20.162421  5.855755 0.6146239\nF-A       -7.31999961 -19.303092  4.663093 0.4936380\nC-B       -4.51666673 -15.839625  6.806291 0.8742030\nD-B       -6.61666648 -24.158118 10.924785 0.9003280\nE-B       -6.64999962 -19.053679  5.753679 0.6418003\nF-B       -6.81666639 -18.139624  4.506292 0.5109582\nD-C       -2.09999975 -18.894661 14.694661 0.9996956\nE-C       -2.13333289 -13.456291  9.189625 0.9968973\nF-C       -2.29999966 -12.427561  7.827562 0.9914731\nE-D       -0.03333314 -17.574784 17.508118 1.0000000\nF-D       -0.19999992 -16.994661 16.594661 1.0000000\nF-E       -0.16666677 -11.489625 11.156291 1.0000000\n</code></pre> <pre><code>plot(hsd)\n</code></pre> <p>\u200b </p>"},{"location":"sta303/multi_comparisons.html#linear-regression-model","title":"Linear Regression Model","text":"<ul> <li>\\(Y_{N\\times 1}=X_{N\\times(p+1)}\\beta_{(p+1)\\times 1} + \\epsilon_{N\\times 1}\\), response \\(Y\\) continuous, explanatory \\(X\\) categorical and/or continuous </li> <li>\\(Y\\) is linear in the \\(\\beta\\)'s i.e. no predictor is a linear function or combination of other predictors</li> <li>\\(\\hat{\\beta}=(X'X)^{-1}X'Y\\) Least square Estimate, need\\(rank(X'X)=rank(X)\\Rightarrow\\) columns of \\(X\\) must be linear independent</li> <li>Null hypothesis \\(H_0: \\beta = \\vec{0}\\). Assumptions</li> <li>Appropriate Linear Model</li> <li>Uncorrelated Errors</li> <li>\\(\\vec{\\epsilon}\\sim N(\\vec{0},\\sigma^2I)\\). </li> <li> <p>Sum of Squares Decomposition </p> \\[\\begin{align*}SST&amp;=SSE+SSR\\\\ \\sum_i^N(Y_i-\\bar{Y})^2 &amp;= \\sum_i^N (Y_i-\\hat{Y}_i)^2 + \\sum_i^N(\\hat{Y}_i - \\bar{Y})^2 \\end{align*}\\] </li> </ul>"},{"location":"sta303/multi_comparisons.html#one-way-anova","title":"One-Way ANOVA","text":"<ul> <li>Need one factor (categorical variable) with at least 2 levels \\((G\\geq 2)\\)</li> <li> <p>Aim: Compare \\(G\\) group means</p> \\[H_0:\\mu_1=\\mu_2=...=\\mu_G, H_a:\\exists i\\neq j. \\mu_i\\neq \\mu_j\\] </li> <li> <p>Predictors are indicator variables that classify the observations one way (into \\(G\\) groups)</p> <ul> <li>special case of a general linear model</li> <li>equivalent to GLM with one-way classification (one factor)</li> <li>GLM uses \\(G-1\\) dummy variables</li> </ul> </li> <li>ANOVA: compare means by analyzing variability</li> </ul>"},{"location":"sta303/multi_comparisons.html#one-way-expectations-and-estimates","title":"One Way Expectations and Estimates","text":"\\[\\begin{align*} E(Y_i)&amp;=(\\beta_0 + \\beta_1,...,\\beta_0 +\\beta_{G-1}, \\beta_0)^T\\\\ \\hat{Y}_i &amp;= (b_0+b_1,...,b_0 + b_{G-1}, b_0)^T\\\\ \\hat{\\beta} &amp;= (b_0,...,b_{G-1})^T = (\\bar{y}_G, \\bar{y}_1-\\bar{y}_G,...,\\bar{y}_{G-1}-\\bar{y}_G)^T \\end{align*}\\] <p>Then, the null hypothesis is \\(H_0: \\beta_i=\\mu_i-\\mu_0 = 0\\Rightarrow\\) the equal mean of \\(i\\)th group and the compared group</p> <p>In this case, for \\(SST\\), \\(N=\\sum_1^G n_i,\\hat{Y}_i=\\) mean of observations for group \\(g\\) from which the \\(i\\)th observation belongs, \\(\\bar{Y}=\\frac{\\sum_{g=1}^{G}\\sum_{j=1}^{n_g} y_{gj}}{N}\\) is the grand mean </p> \\[\\begin{align*} SSReg &amp;= \\sum_{g=1}^G n_g(\\bar{Y}_g-\\bar{Y})^2\\\\ RSS &amp;= \\sum_{g=1}^G \\sum_{g} (Y_i-\\bar{Y}_g)^2 \\end{align*}\\] <pre><code># import data\nlibrary(Sleuth2)\njury = case0502\npercent = case0502$Percent\njudge = case0502$Judge\n</code></pre> <pre><code>table(judge)\n</code></pre> <pre><code>judge\nSpock's       A       B       C       D       E       F \n      9       5       6       9       2       6       9\n</code></pre> <pre><code>with(jury, tapply(percent, judge, mean))\n</code></pre> <pre><code>Spock's     14.6222224235535\nA           34.1199996948242\nB           33.6166664759318\nC           29.0999997456868\nD           27\nE           26.9666668574015\nF           26.800000084771\n</code></pre>"},{"location":"sta303/multi_comparisons.html#compare-6-judges-exclude-spock","title":"Compare 6 judges (exclude Spock)","text":"<pre><code>others = subset(jury, judge != \"Spock's\") \nboxplot(others$Percent~others$Judge, data=others)\n</code></pre> <p>\u200b </p> <p> <pre><code>summary(aov(others$Percent~others$Judge))\n</code></pre></p> <pre><code>             Df Sum Sq Mean Sq F value Pr(&gt;F)\nothers$Judge  5  326.5   65.29   1.218  0.324\nResiduals    31 1661.3   53.59\n</code></pre>"},{"location":"sta303/multi_comparisons.html#rule-of-thumb","title":"Rule of thumb","text":"<p>Not a formal way, not a quick check whether equal variance</p> <pre><code>sss &lt;- with(others, tapply(others$Percent, others$Judge, sd))\nsss\n</code></pre> <pre><code>Spock's     &lt;NA&gt;\nA           11.9418181005452\nB           6.58222301297842\nC           4.59292918339244\nD           3.81837769736668\nE           9.01014223675898\nF           5.96887758208529\n</code></pre> <pre><code>max(sss, na.rm=T); min(sss, na.rm=T)\n</code></pre> <pre><code>11.9418181005452\n3.81837769736668\n</code></pre> <pre><code>max(sss, na.rm=T) / min(sss, na.rm=T) &gt; 2\n</code></pre> <pre><code>TRUE\n</code></pre> <pre><code>bartlett.test(others$Percent, others$Judge)\n</code></pre> <pre><code>    Bartlett test of homogeneity of variances\n\ndata:  others$Percent and others$Judge\nBartlett's K-squared = 6.3125, df = 5, p-value = 0.277\n</code></pre> <p>The small p-value may due to the uneven and small group sizes</p>"},{"location":"sta303/multi_comparisons.html#compare-all-7-judges","title":"Compare all 7 judges","text":"<pre><code>boxplot(percent~judge)\n</code></pre> <p>\u200b </p> <pre><code>summary(aov(percent~judge))\n</code></pre> <pre><code>            Df Sum Sq Mean Sq F value  Pr(&gt;F)    \njudge        6   1927   321.2   6.718 6.1e-05 ***\nResiduals   39   1864    47.8                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n</code></pre> <pre><code>summary(lm(percent~judge))\n</code></pre> <pre><code>Call:\nlm(formula = percent ~ judge)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-17.320  -4.367  -0.250   3.319  14.780\n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   14.622      2.305   6.344 1.72e-07 ***\njudgeA        19.498      3.857   5.056 1.05e-05 ***\njudgeB        18.994      3.644   5.212 6.39e-06 ***\njudgeC        14.478      3.259   4.442 7.15e-05 ***\njudgeD        12.378      5.405   2.290 0.027513 *  \njudgeE        12.344      3.644   3.388 0.001623 ** \njudgeF        12.178      3.259   3.736 0.000597 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.914 on 39 degrees of freedom\nMultiple R-squared:  0.5083,    Adjusted R-squared:  0.4326 \nF-statistic: 6.718 on 6 and 39 DF,  p-value: 6.096e-05\n</code></pre> <pre><code>par(mfrow=c(2,2))\nplot(lm(percent~judge))\n</code></pre> <p>\u200b </p> <pre><code>ssa = with(jury, tapply(percent,judge,sd))\nssa\n</code></pre> <pre><code>Spock's     5.03879411343757\nA           11.9418181005452\nB           6.58222301297842\nC           4.59292918339244\nD           3.81837769736668\nE           9.01014223675898\nF           5.96887758208529\n</code></pre> <pre><code>max(ssa) / min(ssa) &gt; 2\n</code></pre> <pre><code>TRUE\n</code></pre> <pre><code>bartlett.test(percent~judge)\n</code></pre> <pre><code>    Bartlett test of homogeneity of variances\n\ndata:  percent by judge\nBartlett's K-squared = 7.7582, df = 6, p-value = 0.2564\n</code></pre> <ul> <li>Residual vs Fitted: No obvious pattern, assume equal variance (also by rule of thumb and Bartlett)</li> <li>Normal Q-Q: overall OK</li> <li>Outliers: see Residual vs leverage, not influential point. </li> </ul> <p>\u200b </p>"},{"location":"sta303/poisson_regression.html","title":"Log-linear Regression (Poisson Regression)","text":""},{"location":"sta303/poisson_regression.html#case-study-mating-success-of-elephants","title":"Case Study: Mating Success of Elephants","text":"<p>Predictor: Age: age at beginning in years (range from 27-52) Outcome: Matings: #successful matings</p> <p>Question:  - What's the relationship between mating success and age? - Do males have diminished success after reaching some optimal age?</p> <pre><code># load data\nlibrary(Sleuth3)\nelmasu = case2201\nAge = elmasu$Age\nMatings = elmasu$Matings\n</code></pre> <pre><code>plot(Age, Matings)\n</code></pre> <p>\u200b </p>"},{"location":"sta303/poisson_regression.html#why-not-linear-regression","title":"Why not Linear Regression","text":"<ul> <li>outcome is counts and small numbers</li> <li>won't have a normal distribution conditional on age</li> </ul>"},{"location":"sta303/poisson_regression.html#poisson-distribution","title":"Poisson distribution","text":"<p>Useful for counts of rare events</p> <p>Poisson link function: \\(g(\\mu)=log(\\mu)\\), also called a log-linear model</p> <p>Interpretation of \\(\\beta\\)'s: Increase \\(x_j\\) by one unit, and holding other predictors constant, \\(\\mu_j\\) changes by a factor of \\(e^{\\beta_j}\\)</p> <p>Then, \\(\\mu = e^{X\\beta}\\)</p> <p>Estimation method: MLE by IRLS algorithm</p> <p>Inference: Wald procedures and likelihood ratio test (as in logistic regression)</p>"},{"location":"sta303/poisson_regression.html#checking-model-adequacy","title":"Checking Model Adequacy","text":"<p>(similar to binomial logistic regression)</p> <ul> <li>Linear in \\(\\beta\\)'s: Plot \\(\\log(y_i)\\) vs. \\(x\\)'s to see if linear relationship is appropriate. Jitter if many \\(y_i = 0\\) (by using \\(\\log(y_i+k),k\\) is a small positive value)</li> <li>Outliers: Deviance and Pearson residuals </li> <li>Correct form: Wald \\((\\hat\\beta_j/se(\\hat\\beta_j)^2\\sim \\chi^2_{p+1})\\) and LRT tests</li> <li>Adequate: Deviance GOF test</li> </ul> <p>Common problem: \\(var(Y_i) &gt; E(Y_i)\\) Solution: add an extra dispersion</p>"},{"location":"sta303/poisson_regression.html#model-log-likelihood","title":"Model log likelihood","text":"\\[\\mathcal{L} = \\prod_1^n P(Y_i=y_i)=\\prod_1^n \\frac{e^{-\\mu_i}\\mu_i^{y_i}}{y_i!}\\] \\[\\log\\mathcal{L}=\\sum_1^n (-\\mu_i + y_i\\log(\\mu_i)-\\log(y_i!))\\] <p>Since \\(\\log(y_i)\\) is a constant for \\(\\mu_i\\), we can drop it when maximizing likelihood.</p> <pre><code>fitllm = glm(Matings~Age, family=poisson)\nsummary(fitllm)\n</code></pre> <pre><code>Call:\nglm(formula = Matings ~ Age, family = poisson)\n\nDeviance Residuals: \n     Min        1Q    Median        3Q       Max  \n-2.80798  -0.86137  -0.08629   0.60087   2.17777\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -1.58201    0.54462  -2.905  0.00368 ** \nAge          0.06869    0.01375   4.997 5.81e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 75.372  on 40  degrees of freedom\nResidual deviance: 51.012  on 39  degrees of freedom\nAIC: 156.46\n\nNumber of Fisher Scoring iterations: 5\n</code></pre> <pre><code>plot(Age, log(Matings + 1))\nabline(fitllm$coefficients[1], fitllm$coefficients[2])\n</code></pre> <p>\u200b </p> <pre><code>fitllm2 = glm(Matings~Age+I(Age^2), family=poisson)\nsummary(fitllm2)\n</code></pre> <pre><code>Call:\nglm(formula = Matings ~ Age + I(Age^2), family = poisson)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-2.8470  -0.8848  -0.1122   0.6580   2.1134\n\nCoefficients:\n              Estimate Std. Error z value Pr(&gt;|z|)\n(Intercept) -2.8574060  3.0356383  -0.941    0.347\nAge          0.1359544  0.1580095   0.860    0.390\nI(Age^2)    -0.0008595  0.0020124  -0.427    0.669\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 75.372  on 40  degrees of freedom\nResidual deviance: 50.826  on 38  degrees of freedom\nAIC: 158.27\n\nNumber of Fisher Scoring iterations: 5\n</code></pre> <pre><code>print(AIC(fitllm))\nprint(BIC(fitllm))\nprint(AIC(fitllm2))\nprint(BIC(fitllm2))\n</code></pre> <pre><code>[1] 156.4578\n[1] 159.8849\n[1] 158.2723\n[1] 163.4131\n</code></pre> <pre><code>yhats = predict.glm(fitllm, type=\"response\")\nrres = residuals(fitllm, type=\"response\")\npres = residuals(fitllm, type=\"pearson\")\ndres = residuals(fitllm, type=\"deviance\")\ncbind(Matings, yhats, rres)\n</code></pre> A matrix: 41 \u00d7 3 of type dbl Matingsyhatsrres 101.313503-1.31350336 211.406903-0.40690281 311.406903-0.40690281 411.406903-0.40690281 531.406903 1.59309719 601.506944-1.50694362 701.506944-1.50694362 801.506944-1.50694362 921.506944 0.49305638 1021.506944 0.49305638 1121.506944 0.49305638 1211.614098-0.61409805 1321.851807 0.14819297 1441.983484 2.01651630 1531.983484 1.01651630 1631.983484 1.01651630 1731.983484 1.01651630 1821.983484 0.01651630 1912.124524-1.12452352 2012.124524-1.12452352 2122.124524-0.12452352 2232.124524 0.87547648 2352.437403 2.56259689 2462.437403 3.56259689 2512.610720-1.61071983 2612.610720-1.61071983 2762.610720 3.38928017 2822.796361-0.79636061 2912.995202-1.99520177 3033.436307-0.43630655 3143.680652 0.31934757 3203.942373-3.94237304 3323.942373-1.94237304 3433.942373-0.94237304 3543.942373 0.05762696 3693.942373 5.05762696 3734.222704-1.22270385 3854.522968 0.47703182 3975.189068 1.81093216 4025.558048-3.55804753 4197.315666 1.68433367 <pre><code>par(mfrow=c(1,2))\nplot(Age, pres, main=\"Pearson Residuals\")\nplot(Age, dres, main=\"Deviance Residuals\")\n</code></pre> <p>\u200b </p> <pre><code>psihat = sum(pres^2 / fitllm$df.residual)\nsummary(fitllm, dispersion=psihat)\n</code></pre> <pre><code>Call:\nglm(formula = Matings ~ Age, family = poisson)\n\nDeviance Residuals: \n     Min        1Q    Median        3Q       Max  \n-2.80798  -0.86137  -0.08629   0.60087   2.17777\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -1.58201    0.58590  -2.700  0.00693 ** \nAge          0.06869    0.01479   4.645  3.4e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1.157334)\n\n    Null deviance: 75.372  on 40  degrees of freedom\nResidual deviance: 51.012  on 39  degrees of freedom\nAIC: 156.46\n\nNumber of Fisher Scoring iterations: 5\n</code></pre> <pre><code>fitlm = lm(Matings~Age)\nsummary(fitlm)\n</code></pre> <pre><code>Call:\nlm(formula = Matings ~ Age)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.1158 -1.3087 -0.1082  0.8892  4.8842\n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -4.50589    1.61899  -2.783  0.00826 ** \nAge          0.20050    0.04443   4.513 5.75e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.849 on 39 degrees of freedom\nMultiple R-squared:  0.343, Adjusted R-squared:  0.3262 \nF-statistic: 20.36 on 1 and 39 DF,  p-value: 5.749e-05\n</code></pre> <pre><code>fitlml = lm(log(Matings+1)~Age)\nsummary(fitlml)\n</code></pre> <pre><code>Call:\nlm(formula = log(Matings + 1) ~ Age)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.49087 -0.33939  0.06607  0.35376  0.81171\n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -0.69893    0.45861  -1.524 0.135567    \nAge          0.05093    0.01259   4.046 0.000238 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5237 on 39 degrees of freedom\nMultiple R-squared:  0.2957,    Adjusted R-squared:  0.2776 \nF-statistic: 16.37 on 1 and 39 DF,  p-value: 0.0002385\n</code></pre> <pre><code>par(mfrow=c(3,2))\nplot(fitllm, which=1:2, main=\"Poisson\")\nplot(fitlm, which=1:2, main=\"SLR\")\nplot(fitlml, which=1:2, main=\"log trans\")\n</code></pre> <p>\u200b </p>"},{"location":"sta303/poisson_regression.html#deviance-gof-test","title":"Deviance GOF Test","text":"<pre><code># test statistic\nfitllm$deviance\n</code></pre> <pre><code>51.0116278635223\n</code></pre> <pre><code># p-value\n1 - pchisq(51.012, 39)\n</code></pre> <pre><code>0.0942563842711481\n</code></pre> <p>Question: determine whether the fitted model fits as well as the saturated model</p> <p>Hypotheses:  \\(H_0:\\) fitted model fits as well as saturated model \\(H_a:\\) Saturated model fits better</p> <p>Test Statistic: \\(51.012\\sim \\chi^2_{41-2}\\)</p> <p>p-value: 0.094</p> <p>Conclusion: weak evidence that fitted model is adequate. </p>"},{"location":"sta303/poisson_regression.html#wald-or-lrt","title":"Wald or LRT","text":"<pre><code>summary(fitllm)\n</code></pre> <pre><code>Call:\nglm(formula = Matings ~ Age, family = poisson)\n\nDeviance Residuals: \n     Min        1Q    Median        3Q       Max  \n-2.80798  -0.86137  -0.08629   0.60087   2.17777\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -1.58201    0.54462  -2.905  0.00368 ** \nAge          0.06869    0.01375   4.997 5.81e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 75.372  on 40  degrees of freedom\nResidual deviance: 51.012  on 39  degrees of freedom\nAIC: 156.46\n\nNumber of Fisher Scoring iterations: 5\n</code></pre> <pre><code>summary(fitllm2)\n</code></pre> <pre><code>Call:\nglm(formula = Matings ~ Age + I(Age^2), family = poisson)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-2.8470  -0.8848  -0.1122   0.6580   2.1134\n\nCoefficients:\n              Estimate Std. Error z value Pr(&gt;|z|)\n(Intercept) -2.8574060  3.0356383  -0.941    0.347\nAge          0.1359544  0.1580095   0.860    0.390\nI(Age^2)    -0.0008595  0.0020124  -0.427    0.669\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 75.372  on 40  degrees of freedom\nResidual deviance: 50.826  on 38  degrees of freedom\nAIC: 158.27\n\nNumber of Fisher Scoring iterations: 5\n</code></pre> <p>Question: determine whether the mean number of successful matings tends to peak at some age and then decrease or whether the mean continues to increase with age</p> <p>Wald Hypotheses: \\(H_0: \\beta_2 = 0, H_a:\\beta_3\\neq 0\\)</p> <p>Test Statistic: \\(-0.427\\sim N(0,1)\\)</p> <p>p-value: \\(0.669\\)</p> <p>LRT Hypothesis: $H_0: $ the model without higher order term is better. $H_a: $ the model with higher order term is better.</p> <p>Test statistic: \\(51.012 - 50.826 = 0.186\\sim \\chi^2_1\\)</p> <p>p-value: \\(0.667\\)</p> <p>Conclusion: There is strong evidence that the quadratic term does not contribute to the model. The data provides no evidence that the mean number of successful matings peak at some age</p>"},{"location":"sta303/poisson_regression.html#practice-example","title":"Practice Example","text":"<pre><code>library(Sleuth3)\nelmasu = case2201\nAge = elmasu$Age\nMatings = elmasu$Matings\n\nfitllm = glm(Matings~Age, family=poisson)\nsummary(fitllm)\n</code></pre> <pre><code>Call:\nglm(formula = Matings ~ Age, family = poisson)\n\nDeviance Residuals: \n     Min        1Q    Median        3Q       Max  \n-2.80798  -0.86137  -0.08629   0.60087   2.17777\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -1.58201    0.54462  -2.905  0.00368 ** \nAge          0.06869    0.01375   4.997 5.81e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 75.372  on 40  degrees of freedom\nResidual deviance: 51.012  on 39  degrees of freedom\nAIC: 156.46\n\nNumber of Fisher Scoring iterations: 5\n</code></pre> <pre><code>fitho = glm(Matings~Age + I(Age^2), family=poisson)\nsummary(fitho)\n</code></pre> <pre><code>Call:\nglm(formula = Matings ~ Age + I(Age^2), family = poisson)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-2.8470  -0.8848  -0.1122   0.6580   2.1134\n\nCoefficients:\n              Estimate Std. Error z value Pr(&gt;|z|)\n(Intercept) -2.8574060  3.0356383  -0.941    0.347\nAge          0.1359544  0.1580095   0.860    0.390\nI(Age^2)    -0.0008595  0.0020124  -0.427    0.669\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 75.372  on 40  degrees of freedom\nResidual deviance: 50.826  on 38  degrees of freedom\nAIC: 158.27\n\nNumber of Fisher Scoring iterations: 5\n</code></pre>"},{"location":"sta303/poisson_regression.html#part-a","title":"Part (a)","text":"<p>Consider the elephant mating example from lecture.</p> <ol> <li>Both the binomial and the Poisson distributions provide probability models for counts. Is the binomial distribution appropriate for the number of successful matings of the male African elephants? No, the binomial distribution has a upper bound for the number of success. (Number of success in a definite number of trials). </li> <li>For the model fit in lecture, interpret the coefficient of age.  Keeping other explanatory variables constant, one year increase in age will increase the number of matings by \\(e^{\\beta_1}-1 = e^{0.0687}-1=0.07\\)</li> <li>Consider the plot of the number of matings versus age. The spread of the responses is larger for larger values of the mean response. Should we be concerned? No, the model assumes the underlying distribution is Poisson, where \\(E(X)=var(X)=\\mu\\). Therefore, the spread will increase as the increase of of the mean response </li> <li>From the estimated log-linear regression of the elephants' successful matings on age, what are the mean and variance of counts of successful matings (in the 8 years of the study) for the elephants who are aged 25 years at the beginning of the observation period? What are the mean and variance for elephants who are aged 45 years?  For 25 years old: mean = variance = \\(e^{-1.582+(0.0687)25}=1.15\\)  For 25 years old: mean = variance = \\(e^{-1.582+(0.0687)45}=4.53\\)</li> <li>While it is hypothesized that the number of matings increases with age, there may be an optimal age for matings where, for older elephants the number of matings starts to decline. One way to investigate this is to add a quadratic term for age into the model to allow the log of the mean number of matings to reach a peak. Does the inclusion in the model of \\(age^2\\) improve the fit?  By Wald test, since \\(p=0.669\\), we cannot reject the null hypothesis, there is no evidence that the higher order term improve the fit</li> </ol>"},{"location":"sta303/poisson_regression.html#part-b","title":"Part (b)","text":"<p>What is the difference between a log-linear model and a linear model after the log transformation of the response?</p> <p>For log-linear model, mean is \\(\\mu\\), the model is \\(\\log(\\mu)=X\\beta\\).</p> <p>For SLR with transformation, the model is expressed in terms of the mean of the log of \\(Y\\) with different model assumptions.   - different variance  - different test procedures and diagnosis</p>"},{"location":"sta303/poisson_regression.html#part-c","title":"Part (c)","text":"<p>Why are ordinary residuals \\(y_i-\\hat\\mu_i\\) not particularly useful for Poisson regression?</p> <p>Because the variance in Poisson model is not constant. The residuals with larger means will have larger variances. So if an observation has a large residual it is difficult to know whether it is an outlier or an observation from a distribution with larger variance than the others. Residuals that are studentized so that they have the same variance are more useful for identifying outliers.</p>"},{"location":"sta303/poisson_regression.html#part-d","title":"Part (d)","text":"<p>Consider the deviance goodness-of-t test.</p> <ol> <li>Under what conditions is it valid for Poisson regression?  The sample size for each group is large enough. </li> <li>When it is valid, what possibilities are suggested by a small p-value?  The Poisson distribution is an inadequate model, the explanatory variables are inadequate, or there are some outliers. </li> <li>Large p-value?  Model is correct, or there is insufficient data to detect any inadequacies. </li> </ol> <p>Because the underlying distribution is Poission. The likelihood function is</p> \\[\\prod_1^n \\frac{\\mu_i^{y_i}e^{-\\mu_i}}{y_i!}=\\frac{e^{-\\sum \\mu_i}\\prod \\mu_i^{y_i}}{\\prod y_i!}\\] <p>Take log</p> \\[=-\\sum \\mu_i + \\sum y_i \\log(mu_i) - \\sum \\log(y_i!)\\] <p>Where \\(\\mu_i = e^{X_{\\cdot,i}\\beta}\\) (X: the ith observation)</p>"},{"location":"sta303/two_way_ANOVA.html","title":"Two Way ANOVA","text":""},{"location":"sta303/two_way_ANOVA.html#two-way-anova_1","title":"Two Way ANOVA","text":"<ul> <li>Extension of One-way ANOVA, a special case of a GLM.</li> <li>Two factors, each with \\(\\geq 2\\) levels. </li> <li>Uses a maximum of \\((G_1-1) + (G_2 - 1) +(G_1-1)(G_2 - 1)\\) (individual + interactions)indicator variables</li> </ul>"},{"location":"sta303/two_way_ANOVA.html#two-types-of-factors","title":"Two types of factors","text":"<ul> <li>FIXED effect: data has been gathered from all the levels of the factor that are of interest</li> <li>Random effect: interest is in all possible levels of factor, but only a random sample of levels is included in the data. </li> </ul>"},{"location":"sta303/two_way_ANOVA.html#case-study-the-pygmalion-effect","title":"Case Study The Pygmalion Effect","text":"<pre><code>library(Sleuth2)\ndata = case1302\nhead(data)\nscore = data$Score\ncompany = as.factor(data$Company)\ntreat = as.factor(data$Treat)\n</code></pre> A data.frame: 6 \u00d7 3 CompanyTreatScore &lt;fct&gt;&lt;fct&gt;&lt;dbl&gt; 1C1Pygmalion80.0 2C1Control  63.2 3C1Control  69.2 4C2Pygmalion83.9 5C2Control  63.1 6C2Control  81.5 <pre><code>library(ggplot2)\nggplot(data, aes(x=company, y=score, fill=company)) + geom_boxplot()\n</code></pre> <p>\u200b </p> <pre><code>ggplot(data, aes(x=treat, y=score, fill=treat))+geom_boxplot()\n</code></pre> <p>\u200b </p> <pre><code>interaction.plot(company, treat, score, col=c('blue','red'))\n</code></pre> <p>\u200b </p> <pre><code>interaction.plot(treat, company, score, col=c('blue','red','green','purple','black'))\n</code></pre> <p>\u200b </p> <ul> <li>Response: score on a test </li> <li>Factors: <ol> <li>Company: 10 levels (C1,...,C10)</li> <li>Treatment: 2 levels (Pygmalion, Control)</li> </ol> </li> <li>Aim: investigate the interaction between Company and Treatment</li> <li>Method: Fit a Two-Way ANOVA</li> </ul>"},{"location":"sta303/two_way_ANOVA.html#variables","title":"Variables","text":"<ul> <li>\\(Y_i\\) score for \\(i\\)th platoon</li> <li>Explanatory: \\(9(\\mathbb{I}_{C_m, i})+1(\\mathbb{I}_{P_n,i})+9(Interaction)\\)  For each indicter: \\(\\mathbb{I_{C_m,i}}=I\\)(ith platoon is from Company m), \\(\\mathbb{I_{P_n,i}}=I\\)(ith platoon is 'Pygmalion')</li> </ul>"},{"location":"sta303/two_way_ANOVA.html#overall-test-and-partial-test","title":"Overall test and Partial Test","text":"<p>Overall test \\(H_0: \\beta = \\vec{0}\\)  Partial test $H_0: $ subset of \\(\\beta\\) are \\(0\\)  Test statistic: </p> \\[\\begin{align*} F &amp;= \\frac{(SSReg_{\\text{full}} - SSReg_{\\text{reduced}}) / d.f._{\\text{reduced}}} {MSE_{\\text{full}}}\\\\   &amp;= \\frac{(RSS_{\\text{full}} - RSS_{\\text{reduced}}) / d.f._{\\text{reduced}}} {MSE_{\\text{full}}} \\\\   &amp;\\sim F_{d.f._{\\text{full}}, d.f._{\\text{error}}} \\end{align*}\\] <p>We can use partial test to see if the interaction terms are needed</p>"},{"location":"sta303/two_way_ANOVA.html#interactive-model","title":"Interactive Model","text":"<pre><code>model_i = lm(score~company*treat)\nsummary(model_i)\n</code></pre> <pre><code>Call:\nlm(formula = score ~ company * treat)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n  -9.2   -2.3    0.0    2.3    9.2\n\nCoefficients:\n                        Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)               80.000      7.204  11.105 1.49e-06 ***\ncompanyC2                  3.900     10.188   0.383    0.711    \ncompanyC3                -11.800     10.188  -1.158    0.277    \ncompanyC4                 -3.500     10.188  -0.344    0.739    \ncompanyC5                  7.800     10.188   0.766    0.463    \ncompanyC6                  9.800     10.188   0.962    0.361    \ncompanyC7                 -3.900     10.188  -0.383    0.711    \ncompanyC8                 -8.500     10.188  -0.834    0.426    \ncompanyC9                -10.500     10.188  -1.031    0.330    \ncompanyC10                 3.700     10.188   0.363    0.725    \ntreatControl             -13.800      8.823  -1.564    0.152    \ncompanyC2:treatControl     2.200     12.477   0.176    0.864    \ncompanyC3:treatControl    21.800     13.477   1.618    0.140    \ncompanyC4:treatControl     3.800     12.477   0.305    0.768    \ncompanyC5:treatControl     2.200     12.477   0.176    0.864    \ncompanyC6:treatControl     5.800     12.477   0.465    0.653    \ncompanyC7:treatControl     2.800     12.477   0.224    0.827    \ncompanyC8:treatControl    12.800     12.477   1.026    0.332    \ncompanyC9:treatControl    17.400     12.477   1.395    0.197    \ncompanyC10:treatControl    0.800     12.477   0.064    0.950    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7.204 on 9 degrees of freedom\nMultiple R-squared:  0.7388,    Adjusted R-squared:  0.1875 \nF-statistic:  1.34 on 19 and 9 DF,  p-value: 0.3358\n</code></pre> <pre><code>anova(model_i)\n</code></pre> A anova: 4 \u00d7 5 DfSum SqMean SqF valuePr(&gt;F) &lt;int&gt;&lt;dbl&gt;&lt;dbl&gt;&lt;dbl&gt;&lt;dbl&gt; company9670.9755 74.552841.43665580.29901878 treat1338.8828338.882796.53037390.03091657 company:treat9311.4640 34.607110.66688950.72211538 Residuals9467.0399 51.89332       NA        NA <pre><code>plot(model_i, which=1:2)\n</code></pre> <pre><code>Warning message:\n\"not plotting observations with leverage one:\n  1, 4, 7, 8, 9, 12, 15, 18, 21, 24, 27\"\n</code></pre> <p>\u200b </p> <p>\u200b </p>"},{"location":"sta303/two_way_ANOVA.html#additive-model","title":"Additive Model","text":"<pre><code>model_a = lm(score~company+treat)\nsummary(model_a)\n</code></pre> <pre><code>Call:\nlm(formula = score ~ company + treat)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-10.660  -4.147   1.853   3.853   7.740\n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  75.61367    4.16822  18.141 5.16e-13 ***\ncompanyC2     5.36667    5.36968   0.999   0.3308    \ncompanyC3     0.19658    6.01886   0.033   0.9743    \ncompanyC4    -0.96667    5.36968  -0.180   0.8591    \ncompanyC5     9.26667    5.36968   1.726   0.1015    \ncompanyC6    13.66667    5.36968   2.545   0.0203 *  \ncompanyC7    -2.03333    5.36968  -0.379   0.7094    \ncompanyC8     0.03333    5.36968   0.006   0.9951    \ncompanyC9     1.10000    5.36968   0.205   0.8400    \ncompanyC10    4.23333    5.36968   0.788   0.4407    \ntreatControl -7.22051    2.57951  -2.799   0.0119 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.576 on 18 degrees of freedom\nMultiple R-squared:  0.5647,    Adjusted R-squared:  0.3228 \nF-statistic: 2.335 on 10 and 18 DF,  p-value: 0.0564\n</code></pre>"},{"location":"sta303/two_way_ANOVA.html#partial-f-test-anova","title":"Partial F test (ANOVA)","text":"<p>Test \\(H_0:\\beta_1 = 0\\) (treat Control), \\(H_a:\\beta_1\\neq 1\\)  (T-test or partial F test) \\(F\\sim F_{1, 18}\\) \\(p=0.0119\\)</p> <p>Test \\(H_0: \\beta_2=...=\\beta_{10} = 0\\), $H_a: $ at least one of \\(\\beta_2,...,\\beta_{10} \\neq 0\\) \\(F\\sim F_{9,18}\\)</p> <pre><code>anova(model_a)\n</code></pre> A anova: 3 \u00d7 5 DfSum SqMean SqF valuePr(&gt;F) &lt;int&gt;&lt;dbl&gt;&lt;dbl&gt;&lt;dbl&gt;&lt;dbl&gt; company 9670.9755 74.552841.7237560.15556403 treat 1338.8828338.882797.8354010.01185757 Residuals18778.5039 43.25022      NA        NA <p>Confidence Interval for two sample t test</p> \\[(\\bar{x}_1-\\bar{x_2}\\pm t\\cdot s_p \\sqrt{n_1^{-1}+n_2^{-1}} )\\] <p>Least square CI</p> \\[\\hat{\\beta}_1 \\pm t\\cdot se(\\hat\\beta_1)\\]"},{"location":"sta303/two_way_ANOVA.html#model-checking","title":"Model Checking","text":"<ul> <li>Model is appropriate (include relevant or exclude irrelevant factors)</li> <li>No outliers</li> <li>Normality</li> </ul> <pre><code>par(mfrow=c(2,2))\nplot(model_a)\n</code></pre> <p>\u200b </p> <ul> <li>Variance assumption may not be satisfied (variance is decreasing), consider weighted least square regression</li> <li>Normality satisfied, no dramatic pattern</li> <li>No outliers</li> </ul>"},{"location":"sta347/distribution.html","title":"Random Variables and Single Variable Distributions","text":""},{"location":"sta347/distribution.html#random-variable-and-distribution","title":"Random Variable and Distribution","text":"<p>A random variable is a function \\(X:S\\rightarrow \\mathbb R\\) s.t. \\(\\{s\\in S: X(s)\\leq r\\}\\) is an event for all \\(r\\in\\mathbb R\\). </p> <p>Theorem If \\(X, Y\\) are random variables, then \\(aX, X+Y, XY\\) are all random variables. </p> <p>The distribution of \\(X\\) is the collection of all probabilities of all events induced by \\(X\\), i.e. \\((B, P(X\\in B))\\), \\(B\\) is the Borel set. Two random variables \\(X, Y\\) are identically distributed if they have the same distribution.</p>"},{"location":"sta347/distribution.html#discrete-distribution","title":"Discrete Distribution","text":"<p>\\(X\\) is discrete if \\(P(X=x)=0\\) or \\(P(X=x) &gt; 0\\) and \\(\\sum_x P(X=x)=1\\), i.e. they takes at most countably many values \\(x_1,x_2,...\\) s.t. \\(P(X=x_i) &gt; 0\\) and \\(\\sum P(X=x_i) = 1\\).</p> <p>Then, probability mass function is defined as </p> \\[\\text{pmf}_X(x):X(S)\\rightarrow \\mathbb R:=P(X=x)\\] <p>Theorem Let \\(X(S) = \\{x_1,x_2,...\\}\\) be the set of possible values of a discrete random variable \\(X\\). Then for any \\(A\\subset\\mathbb R\\), </p> \\[P(X\\in A) = \\sum_{x_i\\in A} P(X=x_i) = \\sum_{x_i\\in A}\\text{pmf}_X(x_i)\\] <p>Bernoulli distribution \\(X\\sim Bernoulli(p)\\) if \\(X\\) taking values \\(\\{0,1\\}\\) with \\(P(X=1) = p\\) and \\(P(X=0) = 1-p\\) for \\(p\\in[0,1]\\).</p> <p>Uniform distribution \\(X\\sim uniform(\\mathcal X)\\) is \\(\\mathcal X\\) is a non-empty finite set and \\(X\\) takes values in \\(\\mathcal X\\) with equal probability. </p> \\[\\text{pmf}_X(x) = |\\mathcal X|^{-1}\\mathbb I(x\\in\\mathcal X)\\] <p>Binomial distribution \\(X\\sim binomial(n, p)\\) if \\(X\\) is identically distributed to the number of success in \\(n\\) independent trails with success probability \\(p\\). </p> \\[\\text{pmf}_X(x) = {n\\choose x} p^x(1-p)^{n-x} \\mathbb I(x=0,...,n)\\] <p>Geometric distribution \\(X \\sim geometric(p)\\) the number of independent Bernoulli trials until the first success. </p> \\[\\text{pmf}_X(n) = (1-p)^{n-1}p\\] <p>Negative binomial distribution \\(X\\sim neg-bin(k, p)\\) the number of independent Bernoulli trials until the kth success. </p> \\[\\text{pmf}_X(n) = {n-1\\choose k-1}(1-p)^{n-k}p^k\\] <p>Hypergeometric distribution \\(X\\sim hypergeo(n, r, m)\\) the number of black balls when \\(m\\) balls are drawn without replacement from a jar or \\(n\\) balls, and \\(r\\) of them are black.</p> \\[\\text{pmf}_X(k) = \\frac{ {r\\choose k}{n-r\\choose m-k}}{n\\choose m} \\mathbb I(k=0,...,\\min(r,m))\\] <p>Poission distribution \\(X\\sim Poisson(\\lambda)\\) the number of events occurring in a fixed interval of time or space if these events occur with a known constant mean rate and independently of the time since the last event. </p> \\[\\text{pmf}_X(k) = \\frac{\\lambda^k e^{-\\lambda}}{k!}\\]"},{"location":"sta347/distribution.html#continuous-distribution","title":"Continuous Distribution","text":"<p>\\(X\\) is continuous if for all real number interval \\([a,b]\\), there is some probability density function \\(f: \\mathbb R\\rightarrow \\mathbb R, f&gt; 0\\) s.t. </p> \\[P(a &lt; X\\leq b) = \\int_a^b f(x)dx\\] <p>Note that if we can find a well defined pdf for a continuous distribution, we have its definition. </p> <p>Uniform distribution \\(X\\sim uniform(a, b)\\) if \\(P(c&lt;X&lt;d) = \\frac{d-c}{b-a}\\) for \\(a\\leq c\\leq d\\leq b\\).</p> \\[\\text{pdf}_X(x) = (b-a)^{-1} \\mathbb I (a&lt;x&lt;b)\\] <p>Exponential distribution \\(X\\sim exponential(\\lambda)\\) the time between events in a Poisson point process</p> \\[\\text{pdf}_X(x) = \\lambda e^{-\\lambda x}\\mathbb I(w &gt; 0)\\] <p>Gamma distribution \\(X\\sim Gamma(\\alpha, \\beta)\\) is a family of continuous distributions parameterized by  shape \\(\\alpha\\), rate \\(\\beta\\) (or scale \\(\\theta = \\beta^{-1}\\))</p> \\[\\text{pdf}_X(x) = \\frac{\\beta^\\alpha}{\\Gamma(\\alpha)} x^{\\alpha-1}e^{-\\beta x} \\mathbb I(x&gt;0)\\] <p>Normal distribution \\(Z \\sim \\mathcal N(\\mu, \\sigma^2)\\) the most common continuous distribution parameterized by mean \\(\\mu\\in\\mathbb R\\) and standard deviation \\(\\sigma &gt; 0\\)</p> \\[\\text{pdf}_Z(z) = (2\\pi\\sigma^2)^{-1/2} \\exp\\big(-\\frac{(x-\\mu)^2}{2\\sigma^2}\\big)\\]"},{"location":"sta347/distribution.html#cumulative-distribution-function","title":"Cumulative Distribution Function","text":"<p>The cumulative distribution function of \\(X\\) is the function </p> \\[\\text{cdf}_X(x) = F_X(x) = P(X \\leq x)\\] <p>\\(F\\) has the following properties  1. nondecreasing: since $\\forall x\\leq y, {X\\leq x} \\subset {X\\leq y} $  2. \\(\\lim_\\infty F(x) = 1, \\lim_{-\\infty} F(x) = 0\\): consider the two events \\(\\{X\\leq \\infty\\} = S, \\{X\\leq -\\infty\\} = \\emptyset\\)  3. \\(F\\) is right continuous, \\(\\lim_{y\\rightarrow x-} F(y) = F(x)\\)  4. \\(F(x-):=\\lim_{y\\rightarrow x+} F(y) = P(X &lt; x)\\)  5. \\(P(X=x) = F(x) - F(x-)\\)</p> <p>Note that for any function \\(F\\) that satisfies properties 1., 2., 3. It is a cdf. </p> <p>p-quantile of a r.v. \\(X\\) is \\(x\\) s.t. \\(P(X\\leq x) \\geq p\\) and \\(P(X\\geq x) \\geq 1-p\\), and lower quartile, median, upper quartile is the \\(0.25\\)-quantile, \\(0.5\\)-quantile, \\(0.75\\)-quantile, respectively. Inter quartile range (IQR) is the difference between upper and lower quartile.</p>"},{"location":"sta347/expectations.html","title":"Expectations","text":"<p>(From now on, I will use \\(f\\) for pdf/pmf and \\(F\\) for cdf for simplicity)</p>"},{"location":"sta347/expectations.html#expectations-of-random-vairables","title":"Expectations of Random Vairables","text":""},{"location":"sta347/expectations.html#defn-with-density-function","title":"Def'n with density function","text":"<p>The expectation of a r.v. \\(X\\) is defined as </p> \\[\\mathbb E(X) = \\sum_{x} x\\text{pmf}_X(x) = \\int_{-\\infty}^\\infty x\\text{pdf}_X(x)dx\\] <p>if the summation/integral converges absolutely. </p> <p>For \\(X\\) without defined \\(pdf\\) or \\(pmf\\), we can define it via its probability space \\((S, \\mathcal E, P)\\)</p> \\[\\mathbb E(X) = \\int_{S} X(s)dP(s)\\] <p>For \\(X\\) defined on \\(\\mathbb R^{\\geq 0}\\), </p> \\[\\mathbb E(X) = \\int_0^\\infty xdF(x)\\] <p>This can be extended to negative reals by multiplying \\(-1\\), and it's why we need the summation/integral to converge absolutely. </p> <p>Also, note that </p> \\[\\mathbb E(X) = \\int_0^\\infty P(X&gt; z)dz  = \\int_0^\\infty xdF(x)\\] <p>If we do the integral via \\(y\\) axis, instead of \\(x\\) axis.</p> <p>Example Expectations may not exist. Consider Cauchy distribution </p> \\[\\text{pdf}_X(x) = \\frac{1}{\\pi (1+x^2)}\\] <p>This is a well-defined distribution, we can check via its \\(cdf\\), note that </p> \\[\\text{cdf}_X(x) = \\frac{1}{\\pi} \\arctan(x) + \\frac{1}{2}\\] <p>so that \\(\\text{cdf}_X(\\infty) = 1, \\text{cdf}_X(-\\infty) = 0\\)</p> <p>However, the expectation</p> \\[\\mathbb E(X) = \\int_{-\\infty}^\\infty \\frac{x}{\\pi (1+x^2)}\\] <p>Note that </p> \\[\\int_{-\\infty}^\\infty |\\frac{x}{\\pi (1+x^2)}| \\geq 2\\int_1^\\infty \\frac{1}{\\pi 2x} = \\infty\\] <p>Since the integral does not converge absolutely, its expectation is not defined</p>"},{"location":"sta347/expectations.html#defn-with-probability","title":"Def'n with Probability","text":"<p>Theorem For any r.v. \\(X\\) with finite expectation</p> \\[\\mathbb E(X) = \\int_0^\\infty P(X&gt;z)dz - \\int_{-\\infty}^0 P(X&lt;z)dz = \\int_{-\\infty}^\\infty xdF(x)\\]"},{"location":"sta347/expectations.html#theorem-1-expectation-and-probability","title":"Theorem 1. Expectation and Probability","text":"<p>\\(P(X\\in A) = \\mathbb E(\\mathbb I(X\\in A))\\)</p> <p>proof. </p> \\[\\mathbb E(\\mathbb I(X\\in A)) = 1P(X\\in A) + 0 P(X\\not\\in A) = P(X\\in A)\\]"},{"location":"sta347/expectations.html#theorem-2-functions-of-random-variables","title":"Theorem 2. Functions of Random Variables","text":"<p>For r.v. \\(X\\) and some transformation \\(g\\), if \\(\\mathbb E(g(X))\\) is defined, then</p> \\[\\mathbb E(g(X)) = \\int_{-\\infty}^\\infty g(x)\\text{pdf}_X(x)dx = \\sum_{x} g(x) \\text{pmf}_X(x)\\] <p>proof. We will assume \\(g\\) is non-negative, \\(Y=g(X)\\)</p> \\[\\begin{align*} \\mathbb E(Y) &amp;= \\int_0^\\infty P(Y &gt; z) dz \\\\ &amp;= \\int_0^\\infty P(g(X) &gt; z)dz \\\\ &amp;= \\int_0^\\infty \\mathbb I(g(x) &gt; z) d[\\text{cdf}_X(x)]dz\\\\ &amp;= \\int_{-\\infty}^\\infty g(x)d[\\text{cdf}_X(x)] \\end{align*}\\]"},{"location":"sta347/expectations.html#properties-of-expectations","title":"Properties of Expectations","text":""},{"location":"sta347/expectations.html#linearity","title":"Linearity","text":"<p>Claim \\(Y=aX+b\\implies \\mathbb E(Y) = a\\mathbb E(X) + b\\)</p> <p>proof 1. Using Riemann Stieltjes integral, </p> \\[F_Y(y) = P(Y \\leq y) = P(aX+b \\leq y) = P(X\\leq \\frac{y-b}{a}) = F_X(\\frac{y-b}{a})\\] <p>Thus, we have that </p> \\[\\begin{align*} \\mathbb E(Y) &amp;= \\int_{-\\infty}^\\infty y dF_Y(y)\\\\ &amp;= \\int ydF_X(\\frac{y-b}{a})\\\\ &amp;= \\int (az+b)dF_X(z)&amp;z=\\frac{y-b}{a}\\\\ &amp;= a\\int zdF_X(z) + b\\\\ &amp;=  a\\mathbb E(X) + b \\end{align*}\\] <p>proof 2. Using the alternative definition of</p> \\[\\begin{align*} \\mathbb E(Y) &amp;= \\int_0^\\infty P(Y &gt; z) dz - \\int_{-\\infty}^{0}P(Y&lt;z)dz\\\\ &amp;= \\int_0^\\infty P(X &gt; \\frac{z-b}{a}) dz - \\int_{-\\infty}^{0}P(X&lt;\\frac{z-b}{a})dz\\\\ &amp;= \\int_{-b/a}^\\infty aP(X &gt; w) dw - \\int_{-\\infty}^{-b/a}aP(X&lt;w)dw &amp;w=\\frac{z-b}{a}\\\\ &amp;= a(\\int_{0}^\\infty P(X &gt; w) dw -  \\int_{-\\infty}^0 P(X&lt;w)dw) \\\\ &amp;\\quad+ a(\\int_{-b/a}^0 P(X&gt;z)dz + \\int_{-b/a}^0 P(X&lt;z)dz)\\\\ &amp;= a\\mathbb E(X) + a\\int_{-b/a}^0 [P(X&gt;z) +P(X&lt;z)]dz\\\\ &amp;= a\\mathbb E(X)+b \\end{align*}\\] <p>Claim \\(\\mathbb E(X+Y) = \\mathbb E(X)+\\mathbb E(Y)\\)</p> <p>proof. For simplicity, we will prove only the discrete case, where the set of values is \\(S = \\{x + y: x\\in \\mathcal X, y\\in\\mathcal Y\\}\\).  </p> \\[\\begin{align*} \\mathbb E(X+Y) &amp;= \\sum_{z\\in S} z P(X+Y=z)\\\\ &amp;= \\sum_{z\\in S} z \\sum_{x\\in\\mathcal X, z-x\\in\\mathcal Y} P(X=x)P(Y=z-x)\\\\ &amp;= \\sum_{z\\in S} \\sum_{x\\in\\mathcal X, z-x\\in\\mathcal Y} (x+(z-x)) P(X=x)P(Y=z-x)\\\\ &amp;= \\sum_{x\\in \\mathcal X}\\sum_{y\\in\\mathcal Y} (x+y)P(X=x)P(Y=y)\\\\ &amp;= \\sum_{x\\in \\mathcal X}xP(X=x)\\sum_{y\\in\\mathcal Y}P(Y=y) + \\sum_{y\\in \\mathcal Y}yP(Y=y)\\sum_{x\\in\\mathcal X}P(X=x)\\\\ &amp;= \\sum_{x\\in \\mathcal X}xP(X=x) + \\sum_{y\\in \\mathcal Y}yP(Y=y)\\\\ &amp;= \\mathbb E(X)+\\mathbb E(Y) \\end{align*}\\]"},{"location":"sta347/expectations.html#positive-expectation","title":"Positive Expectation","text":"<p>Claim \\(X\\geq 0\\implies \\mathbb E(X)\\geq 0\\)</p> <p>proof. By definition,</p> \\[\\mathbb E(X) = \\int_{-\\infty}^{\\infty}P(X&gt;z)dz = \\int_{-\\infty}^{0}P(X&gt;z)dz + \\int_{0}^{\\infty}P(X&gt;z)dz\\] <p>Since \\(X\\geq 0\\), we have \\(\\int_{-\\infty}^{0}P(X&gt;z)dz = \\int 0 dz = 0\\) By non-negativity of probability, </p> \\[\\mathbb E(X) = \\int_{0}^{\\infty}P(X&gt;z)dz \\geq 0\\]"},{"location":"sta347/expectations.html#expectation-of-constant","title":"Expectation of constant","text":"<p>Claim \\(\\mathbb E(c) = 1\\)</p> <p>proof. Let r.v. \\(X=c\\), \\(\\mathbb E(c) = 1P(X=c) = 1\\)</p>"},{"location":"sta347/expectations.html#range-of-expectation","title":"Range of Expectation","text":"<p>Claim \\(a\\leq X\\leq b \\implies a\\leq \\mathbb E(X)\\leq b\\)</p> <p>proof. WLOG assume \\(F\\) is continuously definedm</p> \\[\\mathbb E(X) = \\int_{-\\infty}^\\infty x dF(x) = \\int_a^b xdF(x)\\] <p>Then, note that </p> \\[\\int dF(x) = F(\\infty)-F(-\\infty) = 1 - 0 = 1\\] <p>using \\(a\\leq X\\leq b\\), we have that </p> \\[a =a\\int dF(x) \\leq \\int xdF(x) \\leq b\\int dF(x) = b\\]"},{"location":"sta347/expectations.html#product-of-independent-random-variables","title":"Product of Independent Random Variables","text":"<p>Claim If \\(X,Y\\) are independent, \\(g(X), h(Y)\\) are r.v. with finite expectations. Then</p> \\[\\mathbb E(g(X)h(Y)) = \\mathbb E(g(X))\\mathbb E(h(Y))\\] <p>proof. By definition of expecatation,</p> \\[\\mathbb E(g(X)h(Y)) = \\int_{\\mathbb R^2} g(x)h(y) dF_{X,Y}(x,y)\\] <p>Since independence, </p> \\[\\int_{x,y} g(x)h(y) dF_{X,Y}(x,y) = \\iint g(x)h(y) dF_X(x)F_Y(y) = \\int g(x)F_X(x)\\int h(y)F_Y(y)\\] <p>Thus, \\(\\mathbb E(g(X)h(Y)) = \\mathbb E(g(X))\\mathbb E(h(Y))\\)</p>"},{"location":"sta347/expectations.html#conditional-expectation","title":"Conditional Expectation","text":"<p>The conditional expectation of \\(Y\\) given \\(X=x\\) is defined as </p> \\[\\mathbb E(Y | X = x) = \\int y dF_{Y|X}(y|x)\\] <p>The alternative definitions for conditional expectation is very similar to unconditional ones</p> \\[\\mathbb E(Y|X=x) = \\int_0^\\infty P(Y&gt;z|X=x)dz -\\int_{-\\infty}^0 P(Y&lt;z|X=x)dz\\] \\[\\mathbb E(Y|X=x) = \\sum yf_{Y|X}(y|x) = \\int y f_{Y|X}(y|x)dy\\] <p>Also, the properties hold </p> \\[\\mathbb E(aY+bZ | X) = a\\mathbb E(Y|X) + b\\mathbb E(Z|X)\\] \\[P(Y\\geq 0 | X) = 1\\implies \\mathbb E(Y|X) &gt; 0\\] \\[\\mathbb E(1|X) = 1\\]"},{"location":"sta347/expectations.html#expectation-of-conditional-expectation","title":"Expectation of Conditional Expectation","text":"<p>Claim \\(\\mathbb E(\\mathbb E(Y|X)) = \\mathbb E(Y)\\)</p> <p>proof. First, note that \\(\\mathbb E(Y|X)\\) can be seen as a function of \\(x\\), so that</p> \\[\\begin{align*} \\mathbb E(\\mathbb E(Y|X)) &amp;= \\iint ydF_{Y|X}(y|x) dF_X(x)\\\\ &amp;= \\int_{\\mathbb R^2} y dF_{X,Y}(x,y)\\\\ &amp;= \\iint ydF_{X|Y}(x|y) dF_Y(y)\\\\ &amp;= \\int y dF_Y(y) = \\mathbb E(Y) \\end{align*}\\]"},{"location":"sta347/expectations.html#conditional-variance","title":"Conditional Variance","text":"<p>Conditional variance is given by </p> \\[var(Y|X=x) = \\mathbb E((Y- \\mathbb E(Y|X=x))^2 | X=x)\\] <p>The alternative form is very similar to unconditonal ones</p> \\[var(Y|X) = \\mathbb E(Y^2 | X) - \\mathbb E(Y|X)^2\\]"},{"location":"sta347/expectations.html#variance-of-conditioanl-expectation","title":"Variance of Conditioanl Expectation","text":"<p>Claim \\(var(Y) = \\mathbb E(var(Y|X)) + var(\\mathbb E(Y|X))\\)</p> <p>proof. Known that \\(\\mathbb E(Y) = \\mathbb E(\\mathbb E(Y|X))\\), we have that </p> \\[\\mathbb E(var(Y|X)) = \\mathbb E(\\mathbb E(Y^2 | X)) - \\mathbb E(\\mathbb E(Y|X)^2) = \\mathbb E(Y^2) - \\mathbb E(\\mathbb E(Y|X)^2)\\] <p>and also</p> \\[var(\\mathbb E(Y|X)) = \\mathbb E(\\mathbb E(Y|X)^2) - \\mathbb E(\\mathbb E(Y|X))^2 = \\mathbb E(\\mathbb E(Y|X)^2) - \\mathbb E(Y)^2\\] <p>summing them together, we have the claim</p> \\[\\mathbb E(var(Y|X)) + var(\\mathbb E(Y|X)) =  \\mathbb E(Y^2) - \\mathbb E(Y)^2 = var(Y)\\]"},{"location":"sta347/function_rv.html","title":"Functions of Random Variable","text":""},{"location":"sta347/function_rv.html#change-of-variable","title":"Change of Variable","text":"<p>Theorem Let \\(X\\) be a r.v. and \\(Y=g(X)\\) where \\(g:\\mathbb R\\rightarrow\\mathbb R\\) is a funciton. Then</p> <ul> <li>\\(X\\) is discrete, </li> </ul> \\[\\text{pmf}_Y(y) = \\sum_{x: g(x) = y} \\text{pmf}_X(x)\\] <ul> <li>\\(X\\) is continuous and \\(g\\) is an appropriate transformation, </li> </ul> \\[\\text{cdf}_Y(y) = \\int_{x: g(x)\\leq y}\\text{pdf}_X(x)dx = P(\\{x: g(x) \\leq y\\})\\] \\[\\text{pdf}_Y(y) = \\frac{d}{dy} \\text{cdf}_Y(y)\\] <p>Theorem Let \\(F(x) = \\text{cdf}_X(x)\\), then \\(F(X)\\sim uniform(0, 1)\\)</p> <p>proof. Let \\(Y=F(X)\\), </p> \\[\\text{cdf}_Y(y) = P(\\{x:F(x) &lt; y\\}) = P(X \\leq x) = F(x) = y\\]"},{"location":"sta347/function_rv.html#change-of-single-variable","title":"Change of Single Variable","text":"<p>Theorem (change of variable) Let \\(X\\) be continuous r.v. and function \\(g\\) be differentiable and injective. Then </p> \\[\\text{pdf}_Y(y) = \\text{pdf}_X(g^{-1}(y))|\\frac{d}{dy}g^{-1}(y)|\\] <p>proof. wlog assume \\(g\\) is increasing (as a appropriate transformation function), then  \\begin{align} \\text{pdf}Y(y) &amp;= \\frac{d}{dy}\\text{cdf}_Y(y)\\ &amp;= \\frac{d}{dy}\\int_X(x)dx\\ &amp;= \\frac{d}{dy} \\text{cdf}_X(g^{-1}(y)) \\ &amp;= \\text{pdf}_X(g^{-1}(y))|\\frac{d}{dy}g^{-1}(y)| \\end{align}^{g^{-1}(y)} \\text{pdf}</p>"},{"location":"sta347/function_rv.html#change-of-variables-for-multivariate-functions","title":"Change of Variables for Multivariate Functions","text":"<p>Theorem For discrete random variables \\(\\mathbf X = (X_1,...,X_n)\\), Let \\(\\mathbf G:\\mathbb R^n\\rightarrow\\mathbb R^m\\) be the transformation s.t. \\(\\mathbf Y = (Y_1,...,Y_m), \\mathbf Y = \\mathbf G(\\mathbf X), Y_i = g_i(X_1, ..., X_n)\\). Then</p> \\[\\text{pmf}_{\\mathbf Y} = \\sum_{\\mathbf x: \\mathbf G(\\mathbf x) = \\mathbf y}\\text{pmf}_{\\mathbf X}(\\mathbf x)\\] <p>random variables \\(X_1,...,X_n\\) are said to be independent and identically distributed (iid.) if \\(X_i\\)'s are independent and have the same distribution. </p>"},{"location":"sta347/function_rv.html#example","title":"Example","text":"<p>the sum of independent Bernoulli trails follows binomial distribution. </p> <p>proof. Let \\(X_i \\sim \\text{Bern.}(p)\\). \\(Y_n = \\sum^n X_i\\). We will prove by induction. </p> <p>Obviously \\(Y_1 = X_1 \\sim \\text{Bern.}(p)\\equiv \\text{binomial}(1, p)\\)</p> <p>Assume \\(Y_k \\sim \\text{binomial}(k, p)\\). Then</p> \\[\\begin{align*} P(Y_{k+1} = 0) &amp;= P(Y_k=0, X_{k+1} - 0) \\\\ &amp;= P(Y_k = 0)P(X_{k+1}=0)\\\\ &amp;= {k\\choose 0}(1-p)^k (1-p) \\\\ &amp;= {k+1\\choose 0}(1-p)^{k+1}\\\\ P(Y_{k+1}=j) &amp;= P((Y_k = j, X_{k+1} = 0)\\cup (Y_k = j-1, X_{k+1} = 1))\\\\ &amp;= P(Y_k = j)P(X_{k+1} = 0) + P(Y_k = j-1)P(X_{k+1} = 1)\\\\ &amp;= {k\\choose j}p^j(1-p)^{k-j}(1-p) + {k\\choose j-1}p^{j-1}(1-p)^{k-(j-1)}p\\\\ &amp;= {k+1\\choose j}p^j(1-p)^{k+1-j}\\\\ Y_{k+1}&amp;\\sim \\text{binomial} (k+1,p) \\end{align*}\\]"},{"location":"sta347/function_rv.html#theorem","title":"Theorem","text":"<p>If \\(X,Y\\) are independent continuous r.v. then </p> \\[\\text{pdf}_{X+Y}(z) = \\int \\text{pdf}_X(x)\\text{pdf}_Y(z-x)dx\\] <p>proof. Let \\(Z=X+Y\\)</p> \\[\\begin{align*} \\text{cdf}_Z(z) &amp;= P(X+Y \\leq z)\\\\ &amp;= P(X \\leq x, Y \\leq z-x)\\\\ &amp;=P(X \\leq x)P(Y \\leq z-x)\\\\ &amp;= \\int_{-\\infty}^{\\infty} \\int_{-\\infty}^{z-x}\\text{pdf}_X(x)\\text{pdf}_Y(y)dydx\\\\ &amp;= \\int_{-\\infty}^{\\infty} \\text{pdf}_X(x) \\text{cdf}_Y(z-x)dx\\\\ \\text{pdf}_Z(z) &amp;= \\frac{d}{dz} \\int_{-\\infty}^{\\infty} \\text{pdf}_X(x) \\text{cdf}_Y(z-x)dx\\\\ &amp;= \\int_{-\\infty}^{\\infty} \\text{pdf}_X(x) \\text{pdf}_Y(z-x)dx\\\\ \\end{align*}\\]"},{"location":"sta347/function_rv.html#example_1","title":"Example","text":"<p>For \\(X_1,...,X_n\\) iid, \\(Y_n = \\max(X_1,...,X_n), Y_1 = \\min(X_1,...,X_n)\\). </p> \\[\\begin{align*} \\text{cdf}_{Y_n}(y) &amp;= P(\\max(X_1,...,X_n) \\leq y) \\\\ &amp;= P(X_1 \\leq y,...,X_n\\leq y)\\\\ &amp;= \\prod^n P(X_i \\leq y) \\\\ &amp;= \\text{cdf}_X(y)^n\\\\ \\text{cdf}_{Y_1}(y) &amp;= 1 - P(Y_1 &gt; y)\\\\ &amp;= 1 - P(\\min(X_1,...,X_n) &gt; y) \\\\ &amp;= 1 - P(X_1 &gt; y,...,X_n &gt; y)\\\\ &amp;= 1 - \\prod^n P(X_i &gt; y) \\\\ &amp;= 1 - (1-\\text{cdf}_X(y))^n\\\\ \\text{cdf}_{Y_1,Y_n}(y_1,y_2)&amp;= P(Y_1\\leq y_1, Y_2\\leq y2)\\\\ &amp;= P(y_2\\leq y_2) - P(Y_1 &gt; y_1, Y_2\\leq y_2)\\\\ &amp;= \\text{cdf}_X(y_2)^n - \\prod^n P(y_1 &lt; X_i \\leq y_2)\\\\ &amp;= \\text{cdf}_X(y_2)^n - (\\text{cdf}_X(y_2) - \\text{cdf}_X(y_1))^n \\end{align*}\\]"},{"location":"sta347/function_rv.html#theorem-change-of-variables","title":"Theorem (change of variables)","text":"<p>For random variables \\(\\mathbf X = (X_1,...,X_n)\\), Let \\(\\mathbf G:\\mathbb R^n\\rightarrow\\mathbb R^m\\) be the transformation s.t. \\(\\mathbf Y = (Y_1,...,Y_m), \\mathbf Y = \\mathbf G(\\mathbf X), Y_i = g_i(X_1, ..., X_n)\\). IF \\(\\mathbf G\\) is injective and differentiable, then</p> \\[\\text{pdf}_{\\mathbf Y}(\\mathbf y) = \\text{pdf}_{\\mathbf X}(\\mathbf G^{-1}(\\mathbf y))|\\det(\\frac{D}{D\\mathbf y}\\mathbf G^{-1}(\\mathbf y))|\\]"},{"location":"sta347/ineq.html","title":"Inequalities Related to Expecations","text":""},{"location":"sta347/ineq.html#markovs-inequality","title":"Markov's Inequality","text":"<p>Claim If \\(X\\geq 0\\) with \\(\\mu &lt;\\infty\\). Then for all \\(a&gt;0\\)</p> \\[P(X\\geq a) \\leq \\mu/a\\] <p>proof. </p> \\[\\begin{align*} P(X\\geq a) &amp;= \\int \\mathbb I(x\\geq a) dF_X(x)\\\\ &amp;\\leq \\int \\frac{x}{a}\\mathbb I(x\\geq a) dF_X(x)\\\\ &amp;\\leq \\mathbb E(X/a)\\\\ &amp;= \\mu /a \\end{align*}\\]"},{"location":"sta347/ineq.html#chebychevs-inequality","title":"Chebychev's Inequality","text":"<p>Claim Let \\(X\\) be a r.v. with mean \\(\\mu\\) and \\(\\sigma^2\\). Then, for all \\(a &gt; 0\\)</p> \\[P(|X-\\mu| \\geq a\\sigma) \\leq a^{-2}\\] <p>proof 1. </p> \\[\\begin{align*} P(|X-\\mu | \\geq a\\sigma) &amp;= \\int \\mathbb I(|x-\\mu |\\geq a\\sigma) dF_X(x)\\\\ &amp;\\leq \\int (\\frac{|x-\\mu|}{a\\sigma})^2\\mathbb I(|x-\\mu |\\geq a\\sigma) dF_X(x)\\\\ &amp;\\leq \\mathbb E(\\frac{|x-\\mu|}{a\\sigma})^2\\\\ &amp;= \\sigma^2 \\frac{1}{a^2\\sigma^2}\\\\ &amp;= a^{-2} \\end{align*}\\] <p>proof 2. Taking \\(Y = (X-\\mu)^2/\\sigma^2\\) and \\(a = k^2\\), using Markov's Inequality, in this case</p> \\[P(|X-\\mu| \\geq a\\sigma) = P((X-\\mu)^2 \\geq a^2\\sigma^2) \\leq \\mathbb E((X-\\mu)^2) / {a^2\\sigma^2} = a^{-2}\\]"},{"location":"sta347/ineq.html#cauchy-schwartz-inequality","title":"Cauchy-Schwartz Inequality","text":"<p>Claim If \\(X,Y\\) are r.v. with finite second moment</p> \\[\\mathbb E(XY)^2 \\leq \\mathbb E(X^2)\\mathbb E(Y^2)\\] <p>where the equality holds IFF \\(P(aX=bY) = 1\\) for some \\(a,b\\in\\mathbb R\\)</p> <p>proof. Note that we can consider the expectation as a inner product. </p> <p>Corollary \\(cov(X, Y)^2 \\leq var(X)var(Y)\\)</p> <p>proof. By CS Ineq.</p> \\[\\begin{align*} cov(X, Y)^2 &amp;= \\mathbb E[(X-\\mathbb E(X))(Y-\\mathbb E(Y))]^2 \\\\ &amp;\\leq \\mathbb E[(X-\\mathbb E(X))^2]\\mathbb E[(Y-\\mathbb E(Y))^2]\\\\ &amp;= var(X)var(Y) \\end{align*}\\]"},{"location":"sta347/ineq.html#range-of-correlation","title":"Range of Correlation","text":"<p>Claim \\(corr(X, Y) \\in [-1, 1]\\)</p> <p>proof. By the definition of correlation and the corollary</p> \\[[corr(X, Y)]^2 = \\frac{cov(X, Y)^2}{var(X)var(Y)} \\leq 1\\] <p>Corollary \\(Y=aX+b\\) IFF \\(corr(X, Y) = 1\\times \\text{sign}(a)\\)</p>"},{"location":"sta347/ineq.html#additional-inequalities","title":"Additional Inequalities","text":"<p>Note that the following inequalities are given without proofs. </p>"},{"location":"sta347/ineq.html#youngs-inequality","title":"Young's Inequality","text":"<p>For \\(p, q &gt; 1\\) with \\(p^{-1} + q^{-1} = 1\\) and \\(x, y\\geq 0\\). </p> \\[xy \\leq \\frac{x^p}{p} + \\frac{y^q}{q}\\] <p>with equality holds IFF \\(x^p = y^q\\)</p>"},{"location":"sta347/ineq.html#holders-inequality","title":"Holder's Inequality","text":"<p>For \\(p, q &gt; 1\\) with \\(p^{-1}+q^{-1} = 1\\). </p> \\[\\mathbb E(XY) \\leq \\mathbb E(|X|^p)^{-p} \\mathbb E(|Y|^q)^{-q}\\] <p>proof. The statement is obtained by treating expectation as an inner product, and CS inequality is a special case where \\(p=q=2\\).</p>"},{"location":"sta347/ineq.html#jensens-inequality","title":"Jensen's Inequality","text":"<p>For a convex function \\(\\psi\\), \\(\\psi(\\mathbb E(X)) \\leq \\mathbb E(\\psi(X))\\)</p> <p>proof. This inequality can be easily derived from convexity definition</p>"},{"location":"sta347/ineq.html#lyapounovs-inequality","title":"Lyapounov's Inequality","text":"<p>If \\(\\mathbb E(|X|^p)\\) is finite for some \\(p&gt;0\\), then \\(\\mathbb E(|X|^q) \\leq \\mathbb E(|X|^p)^{q/p}\\) for all \\(0 &lt; q \\leq p\\). </p> <p>proof. \\(x^{p/q}\\) is convex, apply Jensen's Inequality</p>"},{"location":"sta347/ineq.html#minkowskis-inequality","title":"Minkowski's Inequality","text":"<p>For \\(p\\geq 1, \\|X+Y\\|_p \\leq \\|X\\|_p + \\|Y\\|_p\\)</p>"},{"location":"sta347/mv_distribution.html","title":"Multivariate Distributions","text":""},{"location":"sta347/mv_distribution.html#joint-distributions","title":"Joint Distributions","text":"<p>The joint distribution of random variables \\(X_1,...,X_n\\) is defined as the collection of all possible probabilities, \\(P((X_1,...,X_n)\\in B)\\) where \\(B\\) is a Borel set in \\(\\mathbb R^n\\). When, \\(n=2\\), we can call it bivariate distribution.</p> <p>Then, we can define the joint pmf as </p> \\[\\text{pmf}_{X,Y}(x,y) = P(X=x,Y=y)\\] <p>Simiarly, the joint pdf is </p> \\[P((X,Y) \\in B) = \\iint_B f(x,y)dxdy\\] <p>Both of them still satisifies non-negativity and total probability.</p> <p>Similarly, if we have more variables, the joint pmf is </p> \\[\\text{pmf}_{X_1,...,X_n}(x_1,...,x_n) = P(X_1=x_1, ..., X_n=x_n)\\] \\[\\text{pdf}_{X_1,...,X_n}(x_1,...,x_n) = \\int\\cdots\\int_B f(x,y)dxdy\\] <p>joint cdf is then defined as </p> \\[\\text{cdf}_{X,Y}(x,y) = P(X\\leq x, Y\\leq y)\\]"},{"location":"sta347/mv_distribution.html#marginal-distributions","title":"Marginal Distributions","text":"<p>Suppose \\(X,Y\\) are r.v. We can marginalize the density functions for \\(X\\) or \\(Y\\) from the joint density. </p> \\[\\text{cdf}_X(x) = \\lim_{y\\rightarrow\\infty} \\text{cdf}_{X,Y}(x,y)\\] \\[\\text{pdf}_X(x) = \\int \\text{pdf}_{X,Y}(x,y)dy\\] \\[\\text{pmf}_X(x) = \\sum_{y} \\text{pmf}_{X,Y}(x,y)\\] <p>If we have more variables, we can marginalize one variable by taking its limit / integral / sum just as in bivariate case</p> <p>random variable \\(X,Y\\) is independent (independence of r.v.) (\\(X\\perp Y\\)) IFF \\(P(X\\in A, Y\\in B) = P(X\\in A)P(Y\\in B)\\)</p> <p>Theorem The following statements are equivalent:</p> \\[X\\perp Y\\] \\[\\text{cdf}_{X,Y}(x,y) = \\text{cdf}_X(x) \\text{cdf}_Y(y)\\] \\[\\text{pmf}_{X,Y}(x,y) = \\text{pmf}_X(x) \\text{pmf}_Y(y)\\] \\[\\text{pdf}_{X,Y}(x,y) = \\text{pdf}_X(x) \\text{pdf}_Y(y)\\] <p>proof. (We will only prove for cdf since the idea is very similar using the definition)</p> \\[\\text{cdf}_{X,Y}(x,y) = P(X\\leq x, Y\\leq y) = P(X\\in A)P(Y\\in B) = \\text{cdf}_X(x) \\text{cdf}_Y(y)\\] <p>If we have more variables, then they are (mutually) independent IFF</p> \\[[cdf/pmf/pdf]_{X_1,...,X_n}(x_1,...,x_n) = \\prod_i [cdf/pmf/pdf]_{X_i}(x_i)\\]"},{"location":"sta347/mv_distribution.html#example","title":"Example","text":"<p>\\(X,Y\\) are independent and has the same density \\(2x \\mathbb I(0\\leq x\\leq 1)\\), compute \\(P(X+Y\\leq 1)\\)</p> \\[\\begin{align*} P(X+Y\\leq 1) &amp;= P(0\\leq X\\leq 1, 0\\leq y\\leq 1-x)\\\\ &amp;= \\int_0^1\\int_0^{1-x}2x2ydydx\\\\ &amp;= 4\\int_0^1\\int_0^{1-x}xydydx\\\\ &amp;= 1/6 \\end{align*}\\]"},{"location":"sta347/mv_distribution.html#example_1","title":"Example","text":"<p>\\(\\text{pdf}_{X,Y}(x,y) = kx^2y^2 \\mathbb I(x^2+y^2\\leq 1)\\), show \\(X,Y\\) is not independent. </p> <p>Marginalize \\(X\\), </p> \\[\\begin{align*} \\text{pdf}_X(x) &amp;= \\int_{-\\infty}^\\infty  kx^2y^2 \\mathbb I(x^2+y^2\\leq 1)dy\\\\ &amp;= kx^2\\int_{-\\infty}^\\infty  y^2 \\mathbb I(x^2+y^2\\leq 1)dy \\\\ &amp;= kx^2\\int_{-\\sqrt{1-x^2}}^{\\sqrt(1-x^2)}y^2dy\\\\ &amp;= \\frac{2}{3}kx^2(1-x^2)^\\frac{3}{2} \\end{align*}\\] <p>Note that \\(X,Y\\) is symmetric, so that </p> \\[\\text{pdf}_X(x)\\text{pdf}_Y(y) = \\frac{2}{3}kx^2(1-x^2)^\\frac{3}{2}\\frac{2}{3}ky^2(1-y^2)^\\frac{3}{2}\\neq \\text{pdf}_{X,Y}(x,y)\\]"},{"location":"sta347/mv_distribution.html#example_2","title":"Example","text":"<p>\\(\\text{pdf}_{X,Y}(x,y) = ke^{-(x+2y)}\\mathbb I(x\\geq 0, y\\geq 0)\\), determine \\(k\\) and whether \\(X,Y\\) independent. </p> <p>To determine \\(k\\), using the total probability </p> \\[\\int_{-\\infty}^\\infty \\int_{-\\infty}^\\infty \\text{pdf}_{X,Y}(x,y) dxdy = \\int_{0}^\\infty \\int_{0}^\\infty ke^{-(x+2y)} dxdy = [-ke^{-2y}/2]^\\infty_0 = \\frac{0-(-k)}{2} = \\frac{k}{2}\\] <p>so that \\(k/2 = 1\\implies k=2\\)</p> <p>Marginalize \\(X\\), we have </p> \\[\\text{pdf}_Y(y) = \\int_{0}^\\infty 2e^{-(x+2y)} dx = 2e^{-2y}\\] <p>Marginalize \\(Y\\), we have </p> \\[\\text{pdf}_X(x) = \\int_{0}^\\infty 2e^{-(x+2y)} dy = e^{-x}\\] <p>Indeed, they are independent since </p> \\[\\text{pdf}_Y(y)\\text{pdf}_X(x) = 2e^{-2y}\\mathbb I(y\\geq 0) e^{-x}\\mathbb I(x\\geq 0) = 2e^{-(x+2y)}\\mathbb I(x\\geq 0, y\\geq 0) = \\text{pdf}_{X,Y}(x,y)\\]"},{"location":"sta347/mv_distribution.html#conditional-distributions","title":"Conditional Distributions","text":"<p>The conditional pmf is defined as </p> \\[\\text{pmf}_{X|Y}(x|y) = P(X=x | Y=y) = \\frac{P(X=x, Y=y)}{P(Y=y)} = \\frac{\\text{pmf}_{X,Y}(x,y)}{\\text{pmf}_Y(y)}\\] <p>The conditional pdf is given by </p> \\[\\text{pdf}_{X|Y}(x|y) =  \\frac{\\text{pdf}_{X,Y}(x,y)}{\\text{pdf}_Y(y)}\\] <p>Since in continuous case, \\(P(Y=y) = 0\\) in general, we need to define a very small \\(\\delta\\) so that </p> \\[P(X\\in A|Y\\in B_\\delta) =\\frac{P(X\\in A, Y\\in B_\\delta)}{P(Y\\in B_\\delta)} = \\frac{\\int_{B_\\delta}\\int_A \\text{pdf}_{X,Y}(x,y)dxfy}{\\int_{B_\\delta}\\text{pdf}_Y(y)dy}\\] <p>Note that since \\(B_\\delta\\) is arbitrarily small, we can approximate \\(\\int_{B_\\delta}f(x)dx = 2\\delta f(x)\\)</p>"},{"location":"sta347/probability.html","title":"Introduction to Probability","text":""},{"location":"sta347/probability.html#frequentism-vs-bayesian","title":"Frequentism vs. Bayesian","text":"<p>Frequentists' view point of probability</p> <p>The probability that some specific outcome of a process will be obtained can be interpreted to mean the relative frequency with which that outcome would be obtained if the process were repeated a large number of times under similar conditions.</p> <p>Bayesian</p> <p>Each person has beliefs about random procedure which presumed to be different. It is inferential point of view</p> <p>The difference is that in frequentism, samples are coming from one unknown true distribution; in Bayesian, all unknowns must have probability structure. </p>"},{"location":"sta347/probability.html#sample-spaces-experiments-and-events","title":"Sample Spaces, Experiments, and Events","text":"<p>An experiment is any process, real or hypothetical, in which the possible outcomes can be identified ahead of time.  The collection of all possible outcomes of an experiment is called the sample space of the experiment which is often denoted by S.  An event is a well-defined subset of sample space. </p> <p>For example, an experiment can be \"tossing a fair coin 3 time\". Then, the sample space \\(S = \\{TTT, TTH, THT, THH, HTT, HTH, HHT, HHH\\}\\) is the collection of all outcomes, and \\(E = \\{THH, HTH, HHT\\} \\subseteq S\\) is an event that two heads are tossed. </p>"},{"location":"sta347/probability.html#probability","title":"Probability","text":"<p>Probability is a function \\(P: \\mathbb P(S)\\rightarrow \\mathbb R\\) such that </p> <ol> <li>Non-negativity \\(\\forall E\\subseteq S. P(E)\\geq 0\\)</li> <li>\\(P(S) = 1\\)</li> <li>Countable additivity \\(\\forall E_1, E_2 \\subseteq S. E_1\\cap E_2 = \\emptyset\\Rightarrow P(\\bigcup^\\infty E_i) = \\sum^\\infty P(E_i)\\). </li> </ol> <p>Theorem 1 \\(P(\\emptyset) = 0\\).  proof. Let \\(E_1=\\emptyset=E_2 = E_3 = ...\\), so that they are disjoint. </p> \\[P(\\emptyset) = P(\\bigcup^\\infty E_i) = \\sum^\\infty P(E_i) \\geq P(E_1)+P(E_2) = 2P(\\emptyset)\\] <p>so that \\(P(\\emptyset) \\leq 0\\), using non-negativity, \\(P(\\emptyset) = 0\\)</p> <p>Theorem 2 (Finite additivity) For any disjoint events \\(E_1,...,E_n\\), \\(P(\\bigcup^n E_i) = \\sum^n P(E_i)\\) proof. Define \\(E_i = \\emptyset\\) for \\(i&gt;n\\). Then by countable additivity.</p> <p>Theorem 3 \\(\\forall A\\subseteq S. P(A^c) = 1-P(A)\\) proof. By finite additivity, \\(P(S) = P(A^c\\cup A) = P(A^c)+P(A) = 1\\). </p> <p>Theorem 4 \\(\\forall A, B \\subseteq S. A\\subseteq B \\Rightarrow P(A)\\leq P(B)\\). proof. By non-negativity and finite additivity. \\(P(B)=P(A+(B-A))=P(A)+P(B-A)\\geq P(A)\\). </p> <p>Theorem 5 \\(\\forall A\\subseteq S. 0\\leq P(A)\\leq 1\\). proof. \\(A\\subseteq S\\Rightarrow P(A)\\leq P(S) = 1\\).</p> <p>Theorem 6 \\(P(A-B)=P(A)-P(A\\cap B)\\). proof. \\(P(A) = P((A-B)\\cup (A\\cap B)) = P(A-B)+P(A\\cap B)\\).</p> <p>Theorem 7 \\(P(A\\cup B) = P(A) + P(B) - P(A\\cap B)\\).  proof. \\(P(A\\cup B) = P(A\\cup B-A) = P(A) + P(B-A) = P(A)+P(B)-P(A\\cap B)\\)</p> <p>Theorem 8 (sub-additivity) For \\(E_1,...,E_n\\). \\(P(\\bigcup^n E_i) \\leq \\sum^n P(E_i)\\). proof. Define \\(A_1 = E_1, A_i = E_i - \\cup^{i-1}_j E_j\\) so that \\(\\cup^n A_i = \\cup^n E_i\\) and \\(A_i\\) are mutually disjoint. and \\(A_i\\subset E_i\\). Therefore, </p> \\[P(\\bigcup^n E_i) = P(\\bigcup^n A_i) = \\sum^n P(A_i)\\leq \\sum^n P(E_i)\\]"},{"location":"sta347/probability.html#continuity-of-probability","title":"Continuity of Probability","text":"<p>For a sequence of events \\(A_1, A_2,...\\) Continuity from below If \\(A_1,A_2\\) is increasing to \\(A\\) (\\(A_1\\subset A_2... , \\bigcup A_i = A\\)) then \\(\\lim_{n\\rightarrow\\infty}P(A_n) = P(A)\\). proof. \\(A_j\\subset A_{j+1}\\) so that \\(A_j\\cap A_{j+1} = A_j\\). Therefore, define \\(C_{j+1} = A_{j+1}\\cap A_j^c\\) and \\(C_1 = A_1\\) so that \\(C_j\\) is the part of \\(A_{j+1}\\) that is not in \\(A_j\\) and \\(C_j\\) are disjoint. Also, \\(C_j\\) is increasing to \\(A\\) by its construction. Therefore, we can prove by countable additivity. </p> <p>Continuity from above If \\(A_1,A_2\\) is decreasing to \\(A\\) (\\(A_1\\supset A_2... , \\bigcap A_i = A\\)) then \\(\\lim_{n\\rightarrow\\infty}P(A_n) = P(A)\\). proof. Note that \\(A_1 - A_n\\) is increasing to \\(A_1-A\\). so that </p> \\[\\lim_{n\\rightarrow\\infty} P(A_1-A_n) = P(A_1-A)\\] <p>note that \\(P(A_1-A_n) = P(A_1)-P(A_n)\\) since \\(A_n\\subset A_1\\), similarly \\(P(A_1-A) = P(A_1)-P(A)\\)</p> \\[\\lim_{n\\rightarrow\\infty} P(A_1)-P(A_n) = P(A_1)-P(A)\\Rightarrow \\lim_{n\\rightarrow\\infty} P(A_n) = P(A)\\]"},{"location":"sta347/probability.html#combinatorics","title":"Combinatorics","text":"<p>Permutation When there are \\(n\\) elements, the number of events pulling \\(k\\) elements out of \\(n\\) elements is called a permutation of \\(n\\) elements taken \\(k\\) at a time and denoted by \\(nPk\\).</p> \\[nPk = \\frac{n!}{(n-k)!}\\] <p>Combination The number of combinations of n elements taken k at a time is denoted by \\(nCk\\) or \\(n\\choose k\\)</p> \\[{n\\choose k} = \\frac{n!}{k!(n-k)!}\\] <p>Binomial coefficients </p> \\[(x+y)^n = \\sum_{k=0}^n {n\\choose k} x^k y^{n-k}\\] <p>Note that the coefficient is determined by the number of combinations choosing \\(k\\) \\(x\\)-terms among \\(n\\) \\((x + y)\\) terms. This result can be expanded to infinity as Newton's generalized binomial theorem.</p> <p>Multinomial Coefficients </p> \\[(x_1+...+x_k)^n = \\sum {n\\choose n_1,...,n_k} x_1^{n_1}\\cdot...\\cdot x_k^{n_k}\\] <p>where the coefficient</p> \\[{n\\choose n_1,...,n_k} = \\frac{n!}{n_1!\\cdot...\\cdot n_k!}\\]"},{"location":"sta347/probability.html#inclusion-exclusion-formula","title":"Inclusion Exclusion Formula","text":"\\[\\begin{align*} P(\\bigcup^n A_i) =  &amp;\\sum^n P(A_i) \\\\ &amp;+(-1)\\sum_{i&lt;j} P(A_i\\cap A_j) \\\\ &amp;+ \\sum_{i&lt;j&lt;k} P(A_i\\cap A_j\\cap A_k) \\\\ &amp;+ \\cdots \\\\ &amp; + (-1)^{n-1}P(A_1\\cap ... \\cap A_n) \\end{align*}\\] <p>proof Using induction on the number of events \\(n\\). </p>"},{"location":"sta347/probability.html#conditional-probability","title":"Conditional Probability","text":"<p>The conditional probability of an event \\(A\\) given \\(B\\) is defined by \\(P(A\\mid B) = \\frac{P(A\\cap B)}{P(B)}\\), if \\(P(B) &gt; 0\\). </p>"},{"location":"sta347/probability.html#independent-events","title":"Independent Events","text":"<p>\\(A\\) and \\(B\\) are independent if \\(P(A\\cap B) = P(A)P(B)\\).  A collection of events \\(\\{A_i\\}\\) are (mutually) independent if \\(P(\\bigcap A_i) = \\prod P(A_i)\\) for \\(A_i\\neq \\emptyset\\), are pairwise independent if \\(\\forall A_i\\neq A_j\\), \\(A_i, A_j\\) are independent. </p> <p>Theorem \\(A\\perp B\\) IFF \\(A\\perp B^c\\).  proof. </p> <p>\\(\\Rightarrow\\)</p> \\[\\begin{align*} P(A\\cap B^c) &amp;= P(A-B) = P(A) - P(A\\cap B) \\\\&amp;= P(A)-P(A)P(B)=P(A)(1-P(B))=P(A)P(B^c) \\end{align*}\\] <p>\\(\\Leftarrow\\)</p> \\[\\begin{align*} P(A\\cap B) &amp;= P(A)-P(A\\cap B^c) = P(A) - P(A)P(B^c) \\\\&amp;= P(A)(1-(1-P(B))) = P(A)P(B) \\end{align*}\\] <p>\\(A\\) and \\(B\\) are conditional independent given \\(C\\) if </p> \\[P(A\\cap B\\mid C) = P(A\\mid C)P(B\\mid C)\\]"},{"location":"sta347/probability.html#bayes-theorem","title":"Bayes Theorem","text":"<p>Theorem (Law of total probability) Let \\(B_1,...,B_k\\) be a partition of the sample space \\(S\\). For any event \\(A\\) </p> \\[P(A) = \\sum^k {P(B_i)}{P(A\\mid B_i)}\\] <p>proof. Note that </p> \\[A = A\\cap S = A \\cap \\bigcup^k B_i = \\bigcup^k A\\cap B_i\\] <p>are \\(A\\cap B_i\\) are disjoint since \\(B_i\\)'s are disjoint, so that finite additivity and conditional probability gives</p> \\[P(A) = P(\\bigcup^k A\\cap B_i) = \\sum^k P(A\\cap B_i) = \\sum^k {P(B_i)}{P(A\\mid B_i)}\\] <p>Theorem (Bayes Theorem) \\(P(B\\mid A) = \\frac{P(A\\mid B)P(B)}{P(A\\mid B)P(B) + P(A\\mid B^c)P(B^c)}\\). proof. Direct from law of total probability.</p> <p>Therefore, Bayes Theorem says that Posterior probability can be expressed with respect to prior probabilities</p>"},{"location":"sta347/var.html","title":"Variance and Covariance","text":""},{"location":"sta347/var.html#moments","title":"Moments","text":"<p>For \\(k\\in\\mathbb Z^+\\), the kth moment of \\(X\\) is defined as \\(\\mathbb E(X^k)\\) if it is finite and the kth central moment is \\(\\mathbb E((X-\\mathbb E(X))^k)\\) (if it is finite). </p> <p>Mean is defined as \\(\\mu := \\mathbb E(X)\\) is the expectation and as the the first moment. </p> <p>Variance is defined as the 2nd central moment, a.k.a.</p> \\[\\sigma^2 = var(X) = \\mathbb E[(X-\\mathbb E(X))^2]\\] <p>skewness is the standardized third moment</p> \\[\\mathbb E[(X-\\mu)^3] / \\sigma^3\\] <p>kurtosis is the standardized fourth moment</p> \\[\\mathbb E[(X-\\mu)^4] / \\sigma^4\\]"},{"location":"sta347/var.html#theorem-1","title":"Theorem 1","text":"<p>Claim. </p> \\[\\mathbb E(|X|^k) &lt;\\infty \\implies \\forall s \\leq k. \\mathbb E(|X|^s) &lt;\\infty\\] <p>proof.</p> \\[|x|^s \\leq \\max(1, |x|)^{k-s} |x|^s \\leq 1 + |x|^k\\] <p>so that </p> \\[\\mathbb E(1+|X|^k) \\leq 1 + \\mathbb E(|X|^k) &lt;\\infty\\]"},{"location":"sta347/var.html#variance","title":"Variance","text":"<p>Variance is defined as the 2nd central moment, a.k.a.</p> \\[var(X) = \\mathbb E[(X-\\mathbb E(X))^2]\\] <p>Covariance between \\(X,Y\\) is </p> \\[cov(X, Y) = \\mathbb E[(X-\\mathbb E(X))(Y-\\mathbb E(Y))]\\] <p>\\(X, Y\\) are uncorrelated if \\(cov(X, Y) = \\mathbb E(XY)-\\mathbb E(X)\\mathbb E(Y) = 0\\)</p> <p>Correlation of \\(X,Y\\) (defined when \\(X,Y\\) have finite secomd moment)</p> \\[cor(X,Y) = \\frac{cov(X,Y)}{\\sqrt{var(X)var(Y)}}\\]"},{"location":"sta347/var.html#alternative-form-of-variance","title":"Alternative Form of Variance","text":"<p>Claim \\(var(X) = \\mathbb E(X^2)- \\mathbb E(X)^2\\)</p> <p>proof. </p> \\[\\begin{align*} var(X) &amp;= \\mathbb E((X-\\mu)^2) \\\\ &amp;= \\mathbb E(X^2 - 2X\\mu + \\mu^2) \\\\ &amp;= \\mathbb E(X^2) - 2\\mu \\mathbb E(X) + \\mu^2\\\\ &amp;= \\mathbb E(X^2) - \\mu^2 \\end{align*}\\]"},{"location":"sta347/var.html#alternative-form-of-covariance","title":"Alternative Form of Covariance","text":"<p>Claim \\(cov(X,Y) = \\mathbb E(XY)-\\mathbb E(X)\\mathbb E(Y)\\)</p> <p>proof. </p> \\[\\begin{align*} cov(X,Y) &amp;= \\mathbb E((X-\\mathbb E(X))(Y - \\mathbb E(Y)))\\\\ &amp;= \\mathbb E(XY - \\mathbb E(X)Y - \\mathbb E(Y) X + \\mathbb E(X)\\mathbb E(Y))\\\\ &amp;= \\mathbb E(XY) - \\mathbb E(X)\\mathbb E(Y) - \\mathbb E(Y)\\mathbb E(X) + \\mathbb E(X)\\mathbb E(Y)\\\\ &amp;= \\mathbb E(XY) - \\mathbb E(X)\\mathbb E(Y) \\end{align*}\\]"},{"location":"sta347/var.html#variance-under-linear-transfomation","title":"Variance under Linear transfomation","text":"<p>Claim \\(var(aX+b) = a^2var(X)\\)</p> <p>proof.</p> \\[\\begin{align*} var(aX+b) &amp;= \\mathbb E((aX+b)^2) -\\mathbb E(aX+b)^2 \\\\ &amp;= a^2\\mathbb E(X^2) + 2ab\\mathbb E(X) + b^2 - (a\\mathbb E(X) + b)^2\\\\ &amp;= a^2\\mathbb E(X^2) + 2ab\\mathbb E(X) + b^2 - a^2\\mathbb E(X)^2 - 2ab\\mathbb E(X) - b^2\\\\ &amp;= a^2\\mathbb E(X^2) - a^2\\mathbb E(X)^2\\\\ &amp;= a^2var(X) \\end{align*}\\]"},{"location":"sta347/var.html#variance-of-sums","title":"Variance of Sums","text":"<p>Claim \\(var(X+Y) = var(X)+ var(Y) + 2cov(X,Y)\\)</p> <p>proof. </p> \\[\\begin{align*} var(X+Y) &amp;= \\mathbb E((X+Y)^2) - \\mathbb E(X+Y)^2\\\\ &amp;= \\mathbb E(X^2) + 2\\mathbb E(XY) + \\mathbb E(Y)^2 - \\mathbb E(X)^2 - 2\\mathbb E(X)\\mathbb E(Y) - \\mathbb E(Y)^2\\\\ &amp;= \\mathbb E(X^2) - \\mathbb E(X)^2 + \\mathbb E(Y)^2 - \\mathbb E(Y)^2 + 2(\\mathbb E(XY) - \\mathbb E(X)\\mathbb E(Y))\\\\ &amp;= var(X)+ var(Y) + 2cov(X,Y) \\end{align*}\\] <p>Corollary \\(var(X+Y) = var(X) + var(Y)\\) IFF \\(X,Y\\) uncorrelated. </p> <p>Corollary \\(var(\\sum^n X_i) = \\sum^n var(X_i)\\) if \\(X_i\\) are pairwise uncorrelated. </p>"},{"location":"sta347/var.html#bounded-random-variable","title":"Bounded random variable","text":"<p>Claim If \\(X\\) is bounded, then its variance is finite</p> <p>proof. \\(X\\) bounded implies \\(\\mathbb E(X)\\) bounded, and \\(X^2\\) bounded, so that  \\(\\sigma^2 = \\mathbb E(X^2) - \\mathbb E(X)^2\\) is also bounded</p>"},{"location":"sta347/var.html#zero-variance","title":"Zero Variance","text":"<p>Claim \\(var(X) = 0\\) IFF \\(P(X=c) = 1\\)</p> <p>proof. \\(\\mathbb E(X - \\mathbb E(X)^2) = 0\\) IFF \\(\\mathbb E(X) = c\\)</p>"},{"location":"sta355/bayes_inference.html","title":"Bayes Inference","text":"<p>The MoM of MLE approach cannot incorporate other information about \\(\\theta\\) into the inference. For example, information from previous studies or \"common sense\" on the parameters. </p>"},{"location":"sta355/bayes_inference.html#prior-and-posterior","title":"Prior and Posterior","text":"<p>Quantify a priori information about \\(\\theta\\) via a distribution.  We can think of \\(f(x_1...x_n; \\theta)\\) as representing the conditional pdf of \\(X_1,...,X_n\\) given \\(\\theta\\), where \\(\\theta \\sim \\Theta\\) so that </p> \\[f(x_1,...,x_n; \\theta) = f(x_1,...x_n | \\theta)\\] <p>Such information of \\(\\theta\\) is given via a prior \\(\\pi(\\theta)\\). Then, the posterior density function combines the information from the prior with the information from the data. By Bayes Theorem</p> \\[\\begin{align*} \\pi(\\theta|x_1,...,x_n) &amp;= \\frac{\\pi(\\theta)f(x_1,...,x_n; \\theta)}{\\int_\\Theta \\pi(s)f(x_1,...,x_n; s) ds}\\\\ &amp;= c(x_1,...,x_n)\\pi(\\theta)\\mathcal L(\\theta)\\\\ &amp;\\propto \\pi(\\theta)\\mathcal L(\\theta) \\end{align*}\\] <p>The denominator, \\(c(x_1,...,x_n)\\) is often called the normalizer, which depends only on the data and in practice, intractable to compute. </p>"},{"location":"sta355/bayes_inference.html#bayesian-interval-estimation","title":"Bayesian Interval Estimation","text":"<p>Given a posterior \\(\\pi(\\theta|x)\\), an interval \\(\\mathcal I(x)\\) is \\(100 p\\%\\) credible interval for \\(\\theta\\) if </p> \\[\\int_{\\mathcal I(x)} \\pi(\\theta|x)d\\theta = p\\] <p>A \\(100p\\%\\) credible interval is called a \\(100p\\%\\) highest posterior density interval for \\(\\theta\\) if </p> \\[\\forall \\theta \\in \\mathcal I. \\forall \\theta' \\not\\in \\mathcal I. \\pi(\\theta|x) &gt; \\pi(\\theta'|x)\\] <p>This measurement is in real time, very close to CI</p>"},{"location":"sta355/bayes_inference.html#maximum-a-posteriori-map-estimate","title":"Maximum a posteriori (MAP) estimate","text":"<p>\\(\\hat\\theta\\) is the posterior mode</p> \\[\\pi(\\hat\\theta|x) \\geq \\pi(\\theta|x), \\forall \\theta\\in\\Theta\\] <p>MAP are often used in situation where MLE is unstable or undefined.  The prior density is used to \"regularize\" the problem, i.e. force the distribution of \\(\\hat \\theta\\) to stay within a bounded subset of \\(\\Theta\\). so that we reduce the variance of \\(\\hat\\theta\\), in exchange of possibly increasing the bias. </p>"},{"location":"sta355/bayes_inference.html#example-non-regular-location-estimation","title":"Example: Non-regular location estimation","text":"<p>Let \\(X_1,...,X_n\\) indep. r.v. with </p> \\[f(x;\\theta) = \\frac{|x-\\theta|^{-1/2}}{2\\sqrt \\pi} \\exp(-|x-\\theta|)\\] <p>The likelihood function is </p> \\[\\mathcal L(\\theta) = \\prod^n \\bigg\\{\\frac{|x_i-\\theta|^{-1/2}}{2\\sqrt \\pi} \\exp(-|x_i-\\theta|)\\bigg\\}\\] <p>MLE is undefined since \\(\\lim_{\\theta\\rightarrow x_i}\\mathcal L(\\theta) = \\infty\\)</p> <p>Consider a Cauchy prior \\(\\pi(\\theta) = \\frac{10}{\\pi(100+\\theta^2)}\\) so that the posterior is </p> \\[\\pi(\\theta|x_1,...,x_n) = c(x)(100+\\theta^2)^{-1}\\prod^n \\frac{|x_i-\\theta|^{-1/2}}{2\\sqrt \\pi} \\exp(-|x_i-\\theta|)\\] <p>so that we can compute the pdf</p>"},{"location":"sta355/bayes_inference.html#multiparameter-model-bayesian-inference","title":"Multiparameter Model Bayesian Inference","text":"<p>For the interested parameter, we can determine the posterior density of the subset of interested parameters by integrating over the others, i.e. </p> \\[\\pi(\\theta_1 | x) = \\int \\pi(\\theta_1, \\theta_1,...,\\theta_k | x)d\\theta_2...d\\theta_k\\]"},{"location":"sta355/bayes_inference.html#example-two-state-markov-chain","title":"Example: Two state Markov Chain","text":"<p>Consider a (first order) Markov Chain \\(\\{X_i\\}\\) where each \\(X_i \\in \\{0, 1\\}\\), by Markov assumption, </p> \\[f(x_1,...,x_n) = f_1(x_1)\\prod_{i=2}^n f_{i | {i-1}}(x_i | x_{i-1})\\] <p>Then, we parameterize the transition (conditional) probabilities as </p> \\(X_{i} = 0\\) \\(X_{i} = 1\\) \\(X_{i+1} = 0\\) \\(1-a\\) \\(a\\) \\(X_{i+1} = 1\\) \\(\\beta\\) \\(1-\\beta\\) <p>Let the table above be our transition matrix \\(P\\), note that  \\(P^k\\) is used to determine \\(P(X_{i+k} = x | X_i = y)\\), also note that the eigenvalues are \\(1, \\rho = 1-a-\\beta\\) for \\(P\\), which gives \\(1, \\rho^k\\) for \\(P^k\\).</p> <p>We can obtain the stationary distribution of the Markov Chain by looking at </p> \\[\\lim_{k\\rightarrow \\infty} P^k = P_0 = \\begin{bmatrix} \\frac{\\beta}{a+\\beta} &amp; \\frac{a}{a+\\beta}\\\\ \\frac{\\beta}{a+\\beta} &amp; \\frac{a}{a+\\beta} \\end{bmatrix}\\] <p>Therefore, the stationary distribution of \\(X_i\\) is </p> \\[P(X_i = 1) = f(1; a, \\beta) = \\frac{a}{a+\\beta} = \\theta\\] \\[P(X_i = 0) = f(0; a,\\beta) = \\frac{\\beta}{a+\\beta} = 1-\\theta\\] <p>So that </p> \\[corr(X_i, X_{i+1}) = \\frac{cov(X_i, X_{i+1})}{var(X_i)} = 1-a-\\beta = \\rho\\] <p>Then, we define a run as the number of subsequence consisting of all \\(0\\) or \\(1\\), for example </p> \\[000 11 0 1111 00 111 0\\] <p>gives 7 runs. </p> <p>If \\(X_1,...,X_n\\) come from the two state MC, then the number of runs is </p> \\[R = 1 + \\sum_{i=1}^{n-1} \\mathbb I(X_i \\neq X_{i+1})\\] <p>\\(R\\) provide some information about \\(\\rho\\), i.e. \\(\\rho\\rightarrow 1\\Rightarrow R\\downarrow, \\rho\\rightarrow -1\\Rightarrow R\\uparrow\\) </p>"},{"location":"sta355/bayes_inference.html#mom-estimator","title":"MoM estimator","text":"<p>Note that </p> \\[\\begin{align*} E(R) &amp;= 1 + \\sum^{n-1}P(X_i\\neq X_{i+1})\\\\ &amp;=  1 + \\frac{2(n-1)a\\beta}{a+\\beta}\\\\ &amp;= 1 + 2(n-1)\\theta(1-\\theta)(1-\\rho) \\end{align*}\\] <p>We can use the proportion of 1s and number of runs to obtain the MoM estimator of \\(\\theta\\) and \\(\\rho\\)</p> \\[\\hat\\theta = n^{-1}\\sum^n \\mathbb I(X_i = 1), \\hat\\rho = 1 - \\frac{R-1}{2(n-1)\\hat\\theta(1-\\hat\\theta)}\\]"},{"location":"sta355/bayes_inference.html#mle-mcle","title":"MLE (MCLE)","text":"<p>Note that </p> \\[\\begin{align*} \\mathcal L(a, \\beta) &amp;= f(x_1; a, \\beta) \\prod^{n-1}f(x_{i+1} | x_i ; a, \\beta)\\\\ &amp;= (\\frac{a}{a+\\beta})^{x_1}(\\frac{\\beta}{a+\\beta})^{1-x_1} \\\\ &amp;\\quad\\prod_{n-1} [a^{(1-x_i)x_{i+1}}(1-a)^{(1-x_i)(1-{x_{i+1}})}][\\beta^{x_i(1-x_{i+1})}(1-\\beta)^{x_ix_{x_i+1}}] \\end{align*}\\] <p>We can also define a conditional likelihood function</p> \\[\\mathcal L_{cond}(a,\\beta) = \\prod^{n-1}f(x_{i+1}|x_i; a, \\beta)\\] <p>Maximizing \\(\\mathcal L\\) w.r.t. to \\(a,\\beta\\) individually is impossible, while we can maximize \\(\\mathcal L_{cond}\\), which gives </p> \\[\\hat a = \\frac{\\sum^{n-1}(1-X_i)(X_{i+1})}{\\sum^{n-1}(1-X_i)}; \\hat\\beta = \\frac{\\sum^{n-1}(1-X_{i+1})(X_{i})}{\\sum^{n-1}X_i}\\]"},{"location":"sta355/bayes_inference.html#bayesian-analysis","title":"Bayesian Analysis","text":"<p>Consider a prior \\(\\pi(a,\\beta)\\), then the prior density for \\((\\theta, \\rho)\\) is </p> \\[\\pi(\\theta(1-\\rho), (1-\\theta)(1-\\rho))\\times \\underset{jacobian}{(1-\\rho)}\\] <p>on the set \\(\\{(\\theta, \\rho) : \\rho \\in (-1, 1),  \\theta\\in \\frac{-\\min(\\rho, 0)}{1-\\min(\\rho, 0)}, \\frac{1}{1-\\min(\\rho, 0)}\\}\\)</p> <p>However, it's hard for Bayesian to compute the normalizer. </p>"},{"location":"sta355/bayes_inference.html#markov-chain-monte-carlo-mcmc","title":"Markov Chain Monte Carlo (MCMC)","text":"<p>To draw samples \\((a_1,\\beta_1), ..., (a_N, \\beta_N)\\) from the posterior density, gives \\(\\rho_1,...,\\rho_N\\) from posterior for \\(\\rho\\) so that we can estimate the posterior on \\(\\rho\\) using  kernel density estimation. </p>"},{"location":"sta355/density_estimation.html","title":"Density Estimation","text":""},{"location":"sta355/density_estimation.html#histograms","title":"Histograms","text":"<p>Given data \\(x_1,...,x_n\\) and \"bins\" \\(B_k = [u_{k-1}, u_k), k=1,...,m\\), then </p> \\[\\text{hist}(x)=\\frac{1}{n(u_k-u_{k-1})}\\sum^n \\mathbb I(x_i \\in B_k)\\] <p>Note that \\(\\text{hist}\\) is a pdf since \\(\\text{hist}(x)\\geq 0\\) and </p> \\[\\begin{align*} \\int_{-\\infty}^\\infty \\text{hist}(x)dx &amp;= \\sum^m \\int_{B_k} \\text{hist}(x)dx\\\\ &amp;= n^{-1}\\sum^n_{i=1}\\sum_{k=1}^m \\mathbb I(x_i \\in B_k) \\int_{B_k} \\frac{1}{u_k - u_{k-1}}dx \\\\ &amp;= 1 \\end{align*}\\] <p>Histogram depends on number of bins and boundaries of the bins. </p>"},{"location":"sta355/density_estimation.html#substitution-principle-estimation","title":"Substitution principle estimation","text":"<p>Consider the empirical distribution function </p> \\[\\hat F(x) = \\frac{1}{n}\\sum_{i=1}^n \\mathbb I(X_i \\leq x)\\] <p>However, we cannot derive any \\(f\\) since this is not differentiable. </p>"},{"location":"sta355/density_estimation.html#kernel-density-estimation","title":"Kernel Density estimation","text":"<p>Start with a density function \\(w(x)\\), i.e. a kernel</p> <p>Given \\(w\\) and a bandwidth \\(h\\), define the kernel density estimator </p> \\[\\hat f_h (x) = \\frac{1}{nh}\\sum^n w(\\frac{x-xX_i}{h})\\] <p>bandwidth controls the amount of smoothing, as \\(h\\) increases, the estimator \\(\\hat f_h(x)\\) becomes smoother. </p> <p>Some examples of kernels are </p> <ul> <li>Gaussian kernel \\(w(x) = \\sqrt{2\\pi}^{-1}\\exp(\\frac{-x^2}{2})\\)</li> <li>Epanechinkov kernel \\(w(x) = \\frac{3}{4\\sqrt 5}(1-\\frac{x^2}{5}), |x|\\leq \\sqrt 5\\)</li> <li>Rectangular \\(w(x) = \\frac{1}{2\\sqrt{3}}, |x| \\leq \\sqrt 3\\)</li> <li>Triangular \\(w(x) = \\frac{1}{\\sqrt 6}(1-\\frac{|x|}{\\sqrt 6}), |x|\\leq \\sqrt 6\\)</li> </ul>"},{"location":"sta355/density_estimation.html#example","title":"Example","text":"<p>Draw 500 observations from </p> \\[f(x) = \\begin{cases} \\sqrt{2\\pi}^{-1}\\exp(\\frac{-(x-2)^2}{2})&amp;k\\leq 0.7 \\\\ \\sqrt{2\\pi}^{-1}\\exp(\\frac{-(x+2)^2}{2})&amp;k &gt; 0.7 \\end{cases}\\] <pre><code>N &lt;- 500\ncomponents &lt;- sample(1:2,prob=c(.7, .3),size=N,replace=TRUE)\nmus &lt;- c(2, -2)\n\nx &lt;- rnorm(n=N,mean=mus[components],sd=1)\npar(mfrow=c(2, 2))\n.5387 - plot(density(x))\n.8 - plot(density(x, bw=.8))\n.4 - plot(density(x, bw=.4))\n.2 - plot(density(x, bw=.2))\n</code></pre> <p>\u200b </p>"},{"location":"sta355/density_estimation.html#redistribution","title":"Redistribution","text":"<p>The empirical distribution function \\(\\hat F\\) puts probability mass \\(1/n\\) at each of the points \\(X_1,...,X_n\\), use the kernel with bandwidth \\(h\\) to redistribute this mass around each \\(X_i\\), probability density around \\(X_i = \\frac{1}{nh}w(\\frac{x-X_i}{h})\\) where </p> \\[\\int_{-\\infty}^\\infty \\frac{1}{nh}w(\\frac{x-X_i}{h})dx = \\frac{1}{n}\\int_{-\\infty}^\\infty w(t)dt = \\frac{1}{n}\\] <p>The density estimate is now simply the sum of these densities over all observations </p> \\[\\hat f_h(x) = \\frac{1}{nh}\\sum_{i=1}^n w(\\frac{x-X_i}{h})\\]"},{"location":"sta355/density_estimation.html#convolution","title":"Convolution","text":"<p>Look at the distribution of \\(Y_h = U+hV\\) where \\(U\\sim \\hat F\\) and \\(V\\) has density \\(w\\) with \\(V,U\\) independent. If \\(h\\) is small then \\(Y_h \\approx U \\sim \\hat F\\). </p> <p>Unlike \\(U, Y_h\\) is a continuous r.v. for each \\(h&gt;0\\): </p> \\[\\begin{align*} P(Y_h\\leq x) &amp;= \\sum_{i=1}^n P(U+hV\\leq x \\mid U = X_i)\\underset{=1/n}{P(U=X_i)}\\\\ &amp;= \\frac{1}{n}\\sum_{i=1}^n P(hV\\leq x-X_i) &amp;U,V\\text{ indep.}\\\\ &amp;= \\frac{1}{n}\\sum_{i=1}^n P(V\\leq \\frac{x-X_i}{h}) \\end{align*}\\]"},{"location":"sta355/density_estimation.html#choice-of-bandwidth","title":"Choice of bandwidth","text":"<p>The choice of \\(h\\) depends on what we believe the underlying density looks like, if \\(f\\) is believed to be smooth, then take larger \\(h\\). If \\(f\\) is believed to have a number of modes (i.e. local maxima) then we should take smaller \\(h\\). </p> <p>Consider the bias-variance decomposition  </p> \\[MSE(\\hat{f_h}(x)) = E[(\\hat{f_h}(x) - f(x))^2] = var(\\hat{f_h}(x)) + \\bigg[E[\\hat{f_h}(x)] - f(x)\\bigg]\\] <p>As \\(h\\) decreases, the squared bias term also decrease but the variance increases.  </p>"},{"location":"sta355/density_estimation.html#example-the-rectangular-kernel","title":"Example: The rectangular kernel","text":"<p>Take \\(w(x) = 1/2\\) for \\(|x|\\leq 1\\). Then, </p> \\[\\hat{f_h}(x) = \\frac{1}{2nh}\\sum_{i=1}^n \\mathbb I(x-h\\leq X_i \\leq x+h)\\] <p>The mean of \\(\\hat{f_h}(x)\\) is </p> \\[E[\\hat{f_h}(x)] = \\frac{F(x+h) - F(x-h)}{2h}\\approx f(x) + \\frac{h^2}6 f''(x)\\] <p>the squared bias is </p> \\[\\big[E[\\hat{f_h}(x)]-f(x)\\big]^2 \\approx \\frac{h^4}{36}[f''(x)]^2\\] <p>For the variance, </p> \\[var[\\hat{f_h}(x)] = \\frac{1}{4h^2n}var[\\mathbb I(x-h\\leq X_i \\leq x+h)]\\] <p>Thus, we can approximate the variance by </p> \\[var[\\hat{f_h}(x)]\\approx \\frac{f(x)}{2hn}\\] <p>The MSE is approximately</p> \\[MSE[\\hat{f_h}(x)]\\approx \\frac{f(x)}{2hn} + \\frac{h^4}{36}[f''(x)]^2\\]"},{"location":"sta355/density_estimation.html#uncertainty-estimating","title":"Uncertainty Estimating","text":"<p>Let \\((X_1,..., X_n)\\sim F_\\theta\\) for some \\(\\theta \\in \\Theta \\subset R\\), want to estimate \\(\\theta\\) using \\(X_1,...,X_n\\). </p> <p>Let \\(\\hat\\theta\\) be the estimator of the true \\(\\theta\\), but we don't know what \\(\\theta\\) is, so how can we say about the estimation error?</p>"},{"location":"sta355/density_estimation.html#example_1","title":"Example","text":"<p>Let \\(X_1,...,X_n\\) indep. \\(N(\\mu, \\sigma^2)\\) random variables. Estimate \\(\\mu = E(X_i)\\) by \\(\\hat\\mu = \\bar X\\) (substitution principle estimator).</p> <p>We know that \\(\\hat\\mu \\sim N(\\mu, \\sigma^2/n)\\). Then </p> \\[P(|\\hat\\mu - \\mu|\\leq 1.96\\frac{\\sigma}{\\sqrt n}) = 0.95\\] <p>In this example, knowing the distribution of \\(\\hat\\mu\\) tells us a lot about the uncertainty of \\(\\hat\\mu\\) of an estimator of \\(\\mu\\). If \\(\\sigma^2\\) is unknown then we can estimate </p> \\[S^2 = \\frac 1{n-1} \\sum^n (X_i - \\bar X)^2\\] <p>and replace 1.96 by some \\(t\\). </p> <p>Sampling distribution of \\(\\hat\\theta\\) is its probability distribution; this will depend on \\(\\theta\\). </p> <p>Mean square error of \\(\\hat\\theta\\) is defined as </p> \\[MES_\\theta(\\hat\\theta) = E_\\theta[(\\hat\\theta - \\theta)^2] = var_\\theta(\\hat\\theta) + [E_\\theta(\\hat\\theta ) - \\theta]^2\\] <p>Unbiased if \\(bias_\\theta(\\hat \\theta) := E_\\theta(\\hat\\theta) - \\theta = 0\\)</p>"},{"location":"sta355/density_estimation.html#problem-with-unbiasedness","title":"Problem with unbiasedness","text":"<ul> <li>In many problems, unbiased estimators do not exist</li> <li>In some problems, the estimator lies outside the parameter space with positive probability</li> <li>If \\(\\hat\\theta\\) is an unbiased estimator of \\(\\theta\\) and \\(g\\) is a non-linear function then \\(E_\\theta[g(\\hat\\theta)] \\neq g(\\theta)\\) unless \\(P_\\theta(\\hat\\theta = \\theta) = 1\\). </li> </ul>"},{"location":"sta355/density_estimation.html#when-to-worry-about-bias","title":"When to worry about bias?","text":"<ul> <li>If \\(\\hat\\theta\\) is systematically larger or smaller than \\(\\theta\\)</li> <li>If the squared bias is approximately equal to or greater than the variance</li> </ul>"},{"location":"sta355/density_estimation.html#example-sample-variance","title":"Example: Sample variance","text":"<p>\\(X_1,...,X_n\\) indep. with \\(\\mu, \\sigma^2\\), using unbiased estimator of sample variance </p> \\[S^2 = \\frac1{n-1}\\sum_{i=1}^n (X_i - \\bar X)^2\\] <p>However, \\(S = \\sqrt{S^2}\\) is biased, if we assume that \\(X_1,...,X_n\\) are Normal then we can evaluate the bias explicitly</p> \\[\\begin{align*} E(S) &amp;= \\sigma (\\frac 2{n-1})^{1/2}\\frac{\\Gamma(n/2)}{\\Gamma(\\frac{n-1}2)}\\\\ &amp;= \\sigma \\bigg[1-\\frac1{4n} - \\frac 7{32n^2}\\bigg]\\\\ E(S) - \\sigma &amp;\\approx -\\frac\\sigma{4n} - \\frac{7\\sigma}{32n^2} \\end{align*}\\] <p>Then</p> \\[\\begin{align*} MSE(S) &amp;= var(S) + [E(S)-\\sigma]^2 \\\\ &amp;= E(S^2) - [E(S)]^2 + [E(S)-\\sigma]^2 \\\\ &amp;\\approx \\frac{\\sigma^2}{2n}  + (-\\frac\\sigma{4n})^2\\\\ &amp;= \\frac{\\sigma^2}{2n} + \\frac{\\sigma^2}{16n^2} \\end{align*}\\]"},{"location":"sta355/density_estimation.html#consistency","title":"Consistency","text":"<p>The sequence of estimators \\(\\hat\\theta_n\\) is consistent for \\(\\theta\\) if for each \\(\\epsilon &gt; 0\\) and \\(\\theta\\in\\Theta\\), </p> \\[\\lim_{n\\rightarrow\\infty}P_\\theta(|\\hat\\theta_n - \\theta| &gt; \\epsilon) = 0\\] <p>a.k.a. \\(\\hat\\theta_n \\rightarrow^p \\theta\\)</p> <p>Note that consistency is an aspirational property: </p> <ul> <li>if we have enough info. then we can estimate \\(\\theta\\) arbitrarily precisely. </li> <li>For a finite \\(n\\), consistency isn't meaningful</li> </ul>"},{"location":"sta355/density_estimation.html#sample-means-and-functions-thereof","title":"Sample means and functions thereof","text":"<p>\\(X_1,...,X_n\\) indep. with mean \\(\\mu\\) and \\(\\sigma^2\\), then estimate \\(\\mu\\) by \\(\\hat\\mu_n = n^{-1}\\sum_{i=1}^n X_i\\) By WLLN, \\(\\hat\\mu_n\\) is a consistent estimator of \\(\\mu\\) (i.e. \\(\\{\\hat\\mu_u\\}\\) is consistent). </p> <p>Likewise, if we want to estimate \\(\\theta = g(\\mu)\\) where \\(g\\) is a continuous function then \\(\\hat\\theta_n = g(\\hat\\mu_n)\\) is a consistent estimator of \\(\\theta\\). </p> <p>We can also approximate the sampling distributions of \\(\\hat\\mu_n\\) and \\(\\hat\\theta_n\\) by normal distributions </p> \\[\\hat\\mu_n \\sim N(\\mu, \\sigma^2/n), \\hat\\theta_n \\sim N(\\theta, [g'(\\mu)]^2 \\frac{\\sigma^2}{n})\\]"},{"location":"sta355/density_estimation.html#example-regression-design","title":"Example: Regression design","text":"\\[Y_i = \\beta_0 + \\beta_1x_i + \\epsilon_i, i = 1,..,n\\] <p>Assume for simplicity that \\(\\epsilon_1,...,\\epsilon_n\\) are indep. \\(N(0,\\sigma^2)\\) Least squares estimator of \\(\\beta_1\\):</p> \\[\\hat\\beta_1 = \\bigg[\\sum_{i=1}^n (x_i - \\bar x)Y_i\\bigg]/\\bigg[\\sum_{i=1}^n (x_i - \\bar x)^2\\bigg]\\sim N(\\beta_1, \\frac{\\sigma^2}{\\sum^n (x_i - \\bar x)^2})\\] <p>Thus \\(\\hat\\beta_1 = \\hat\\beta_1^{(n)}\\) will be consistent provided that </p> \\[\\sum^n (x_i - \\bar x)^2 \\rightarrow \\infty\\]"},{"location":"sta355/density_estimation.html#sampling-distributions-and-standard-errors","title":"Sampling distributions and standard errors","text":"<p>Assume \\(\\hat\\theta\\) is an estimator of some parameter \\(\\theta\\), then \\(se(\\hat\\theta)\\) is defined to be the standard deviation of the sampling deviation of the sampling distribution of \\(\\hat\\theta\\).  This is rarely known exactly but can usually be approximated somehow. </p> <p>If \\(\\hat\\mu = \\bar X\\) where \\(\\bar X\\) is based on \\(n\\) indep. observations with variance \\(\\sigma^2\\) then \\(se(\\hat\\mu) = \\sigma/\\sqrt n\\). </p> <p>If the sampling distribution is approximately normal then we can approximate the standard error by the standard deviation of the approximating normal distribution. </p>"},{"location":"sta355/density_estimation.html#example_2","title":"Example","text":"<p>\\(\\hat\\mu = \\bar X, se(\\hat\\mu) = \\sigma/\\sqrt n\\). estimated standard error is </p> \\[\\hat{se}(\\hat\\mu) = \\frac{S}{\\sqrt n}, S^2 = (n-1)^2\\sum^n (X_i -\\hat X)^2\\]"},{"location":"sta355/density_estimation.html#example-the-delta-method-estimator","title":"Example: the Delta Method estimator","text":"<p>\\(X_1,...,X_n\\) indep. with some unknown cdf \\(F\\), suppose \\(\\hat\\theta = g(\\bar X)\\). If \\(g\\) is differentiable then we can approximate the sampling distribution of \\(\\hat\\theta\\) by a normal distribution using Delta Method: </p> \\[\\hat\\theta = g(\\bar X)\\sim N(g(\\mu)=\\theta, [g'(\\mu)]^2 \\frac{\\sigma^2}{n})\\] <p>where \\(\\sigma^2 = var(X_i)\\) This suggests that we can estimate \\(se(\\hat\\theta)\\) using the Delta Method estimator </p> \\[\\hat{se}(\\hat\\theta) = \\frac{|g'(\\bar X)|S}{\\sqrt n}\\] <p>where \\(S^2\\) is the sample variance of \\(X_1,...,X_n\\).  We are using the substitution principle here to estimate the unknown \\(\\mu\\) and \\(\\sigma^2\\)</p> <p>Recall that the Delta Method follows from the Taylor series approximation</p> \\[\\begin{align*} \\hat\\theta - \\theta &amp;= g(\\bar X ) - g(\\mu)\\\\ &amp;\\approx g'(\\mu)(\\bar X - \\mu)\\\\ &amp;= \\frac1n\\sum^n g'(\\mu)(X_i - \\mu) \\end{align*}\\] <p>Thus </p> \\[\\hat\\theta \\approx \\frac1n \\sum^n [g(\\mu) + g'(\\mu)(X_i - \\mu)]\\] <p>We \"estimate\" \\(\\phi(X_i)\\) by a pseudo-value </p> \\[\\phi_i = g(\\bar X) + g'(\\bar X)(X_i - \\bar X), i=1,...,n\\] <p>Note that </p> \\[\\hat \\theta = g(\\bar X) = n^{-1} \\sum^n \\phi_i = \\bar\\phi\\] <p>The Delta Method estimator can now be written as </p> \\[\\hat{se}(\\hat\\theta) = \\bigg[\\frac1{n(n-1)}\\sum^n(\\phi_i - \\bar\\phi)^2\\bigg]^{1/2} = \\frac{|g'(\\bar X)|S}{\\sqrt n}\\]"},{"location":"sta355/density_estimation.html#example-trimmed-mean","title":"Example: Trimmed mean","text":"<p>\\(X_1,...,X_n\\) indep. continuous r.v. with density \\(f(x-\\theta)\\) where \\(f(x) = f(-x)\\) and \\(\\theta\\) is unknown. if suspecting that \\(f\\) is heavy-tailed, then we want to eliminate the effects of extreme observations.</p> <p>To minimize the effect of extreme observations, we estimate \\(\\theta\\) by a trimmed mean: </p> \\[\\hat\\theta = \\frac1{n-2r}\\sum_{r+1}^{n-r}X_{(k)}\\] <p>In general, the trimmed mean is a substitution principle estimator of </p> \\[\\theta(F) = \\frac1{1-2a}\\int^{1-a}_a F^{-1}(\\tau)d\\tau\\] <p>where \\(a = r/n\\). </p> <p>Then, the sampling distribution of trimmed mean is approximately normal </p> \\[\\hat\\theta\\sim N(\\theta, \\frac{v^2(F)}{n})\\] <p>where </p> \\[v^2(F) = \\frac1{(1-2a)^2}\\int_a^{1-a}\\int_a^{1-a}\\frac{\\min(s,t)-st}{f(F^{-1}(t))f(F^{-1}(s))}dsdt\\]"},{"location":"sta355/density_estimation.html#approximating-estimators-by-sample-means","title":"Approximating estimators by sample means","text":"<p>Suppose that \\(\\hat\\theta\\) is some complicated estimator like a trimmed mean. </p> <p>How to compute \\(\\hat{se}(\\hat\\theta)\\), suppose that we can approximate \\(\\hat\\theta\\) by an average \\(\\hat\\theta \\approx n^{-1}\\sum^n \\phi(X_i)\\) </p> <p>This suggests using </p> \\[\\hat{se}(\\hat\\theta) = \\big\\{\\frac1{n(n-1)}\\sum^n [\\phi(X_i) - \\bar\\phi]^2\\big\\}^{1/2}\\] <p>where \\(\\bar\\phi = \\frac1n \\sum^n \\phi(X_i)\\approx \\hat\\theta\\)</p>"},{"location":"sta355/density_estimation.html#leave-one-out-estimators","title":"Leave-one-out estimators","text":"<p>Suppose that \\(\\hat\\theta = \\hat\\theta(X_1,...,X_n)\\), then define \\(\\hat\\theta_{-i} = \\hat\\theta(X_1,...,X_{i-1}, X_{i+1},...,X_n)\\) if \\(\\hat\\theta = \\bar X\\) then \\(\\hat \\theta_{-i} = \\frac1{n-1}\\sum_{j\\neq i}X_j\\)</p>"},{"location":"sta355/density_estimation.html#example-theil-index","title":"Example: Theil Index","text":"<p>Define \\(\\theta(F) = E_F[\\frac{X_i}{\\mu(F)}\\ln (\\frac{X_i}{\\mu(F)})]\\) where \\(P(X_i &gt; 0) = 1\\) and \\(\\mu(F) = E_F(X_i)\\).  </p> <p>We estimate \\(\\theta(F)\\) by </p> \\[\\hat\\theta = n^{-1}\\sum^n \\frac{X_i}{\\bar X}\\ln(\\frac{X_i}{\\bar X})\\] <p>The leave-one-out estimators are </p> \\[\\hat\\theta_{-i} = \\frac1{n-1}\\sum_{j\\neq i}\\frac{X_j}{\\bar X_{-i}}\\ln(\\frac{X_j}{\\bar X_{-i}})\\] <p>where \\(\\bar X_{-i} = \\frac1{n-1}\\sum_{j\\neq i}X_j\\)</p>"},{"location":"sta355/density_estimation.html#from-leave-one-out-to-pseudo-values","title":"From leave-one-out to pseudo-values","text":"<p>Suppose that we can approximate \\(\\hat\\theta\\) by a sample mean</p> \\[\\hat\\theta \\approx \\frac1n\\sum^n \\phi(X_i)\\] <p>for some (unknown) function \\(\\phi\\). </p> <p>Then for the leave-one-out estimators, we have </p> \\[\\hat\\theta_{-i}\\approx \\frac1{n-1}\\sum_{j\\neq i}\\phi(X_j)\\] <p>This suggests that we can recover \\(\\phi(X_i)\\) (approximately) by the pseudo-value </p> \\[\\Phi_i = n\\hat\\theta - (n-1)\\hat\\theta_{-i}\\approx \\phi(X_i)\\] <p>The pseudo-values can be used to estimate the s.e. of \\(\\hat\\theta\\). </p>"},{"location":"sta355/density_estimation.html#jackknife-se-estimator","title":"Jackknife s.e. estimator","text":"<p>Given the pseudo-values \\(\\Phi_1,...,\\Phi_n\\), define the jackknife estimator of \\(se(\\hat\\theta)\\)</p> \\[\\hat{se}(\\hat\\theta) = \\big[\\frac1{n(n-1)}\\sum^n (\\Phi_i - \\bar\\Phi)^2 \\big]^{1/2} = \\big [\\frac{n-1}n\\sum^n (\\hat\\theta_{-i} - \\hat\\theta_{\\cdot})^2 \\big]^{1/2}\\] <p>where \\(\\hat\\theta_\\cdot = \\frac1n \\sum^n \\hat\\theta_{-i}\\) For many estimators, we have </p> \\[E_\\theta(\\hat\\theta) = \\theta + \\frac{a_1(\\theta)}n + \\frac{a_2(\\theta)}n^2\\] <p>We can use the jacknife to remove the \\(1/n\\) bias term.  </p> <p>Define the bias-corrected \\(\\hat\\theta\\):</p> \\[\\begin{align*} \\hat\\theta_{bc} &amp;= n\\hat\\theta - (n-1)\\hat\\theta_\\cdot\\\\ &amp;= \\hat\\theta - (n-1)(\\hat\\theta_\\cdot -\\hat\\theta)\\\\ &amp;= n^{-1}\\sum^n \\Phi_i \\end{align*}\\] <p>For \\(\\hat\\theta_{bc}\\), then </p> \\[E_\\theta(\\hat\\theta_{bc}) = \\theta + \\frac{a^*_2(\\theta)}n^2 + \\cdots\\] <pre><code>x &lt;- rgamma(100,2)\ny &lt;- x/mean(x)\ntheil &lt;- mean(y*log(y))\nsprintf(\"theil index %f\", theil)\n# Compute pseudo-values\npseud &lt;- NULL\nfor (i in 1:100) {\n    xi &lt;- x[-i]\n    yi &lt;- xi/mean(xi)\n    loo &lt;- mean(yi*log(yi))\n    pseud &lt;- c(pseud,100*theil - 99*loo)\n}\nsprintf(\"mean of pseudo %f\", mean(pseud)) # mean of pseudo-values - bias-corrected estimate\nsprintf(\"jackknife s.e. estimate %f\", sqrt(var(pseud)/100))\n</code></pre> <pre><code>theil index 0.188827\nmean of pseudo 0.190167\njackknife s.e. estimate 0.022019\n</code></pre>"},{"location":"sta355/density_estimation.html#delta-method-vs-jackknife","title":"Delta Method vs. Jackknife","text":"<p>Sample 100 observations from a Gamma distribution with \\(\\alpha = 2\\) and \\(\\lambda = 1\\). Estimate \\(\\theta = \\ln(\\mu) = g(\\mu); g'(\\mu) = 1/\\mu\\).  For our sample \\(\\bar x = 1.891\\) and \\(s^2 = 1.911\\), \\(\\hat \\theta = \\ln(\\bar x) = 0.637\\) </p> <p>Thus the Delta Method s.e. estimate is </p> \\[\\hat{se}(\\hat\\theta) = |g'(\\bar x)|\\frac s{\\sqrt n} = \\frac s{\\bar x\\sqrt n} = 0.0731\\] <p>Computing the jackknife estimate is somewhat more computationally intensive. </p> <pre><code>x &lt;- rgamma(100,2)\nthetaloo &lt;- NULL\nfor (i in 1:100) {\n    xi &lt;- x[-i]\n    thetaloo &lt;- c(thetaloo,log(mean(xi)))\n}\njackse &lt;- sqrt(99*sum((thetaloo-mean(thetaloo))^2)/100)\nsprintf(\"jackkniefe s.e. %f\", jackse)\n</code></pre> <pre><code>jackkniefe s.e. 0.070312\n</code></pre>"},{"location":"sta355/density_estimation.html#example-the-lorenz-curve","title":"Example: The Lorenz curve","text":"<p>Suppose that \\(F\\) is the cdf of a positive r.v. with finite mean \\(\\mu(F)\\), let \\(F\\) describes the income distribution within some population.  </p> <p>For each such \\(F\\), we can define its Lorenz curve: </p> \\[\\mathcal L_F(\\tau) = \\frac1{\\mu(F)} \\int_0^\\tau F^{-1}(s)ds, 0\\leq \\tau\\leq 1\\] <p>a.k.a. the fraction of total income held by poorest \\(\\tau\\). </p> <p>\\(\\mathcal L_F(\\tau) \\leq \\tau\\) with \\(\\mathcal L_F(0) = 0\\) and \\(\\mathcal L_F(1)=1\\). The difference between \\(\\tau\\) and \\(\\mathcal L_F(\\tau)\\) can be used to measure income inequality. </p>"},{"location":"sta355/density_estimation.html#example-the-gini-index","title":"Example: The Gini index","text":"<p>One measure of income inequality is the Gini index defined by </p> \\[Gini(F) = 2\\int_0^1 (\\tau - \\mathcal L_F(\\tau))d\\tau = \\frac{1}{\\mu(F)}\\int_0^1 (2\\tau - 1)F^{-1}(\\tau)d\\tau\\] <p>\\(Gini(F)\\in [0,1]\\Rightarrow 0\\) perfect equality, \\(1\\) perfect inequality. </p> <p>We estimate the quantiles \\(F^{-1}(\\tau)\\) by order statistics. Given indep. observations \\(X_1,...,X_n\\) from \\(F\\), we have </p> \\[\\hat{Gini}(F) = \\frac1{n\\bar X} \\sum^n (\\frac{2k-1}{n}-1)X_{(k)}\\] <pre><code>gini &lt;- function(x) {\n    # compute point estimate\n    n &lt;- length(x)\n    x &lt;- sort(x)\n    wt &lt;- (2*c(1:n)-1)/n - 1\n    g &lt;- sum(wt*x)/sum(x)\n    # compute leave-one-out estimates\n    wt1 &lt;- (2*c(1:(n-1))-1)/(n-1) - 1\n    gi &lt;- NULL\n    for (i in 1:n) {\n        x1 &lt;- x[-i] # data with x[i] deleted\n        gi &lt;- c(gi,sum(wt1*x1)/sum(x1))\n    }\n    # compute jackknife std error estimate\n    gbar &lt;- mean(gi)\n    se &lt;- sqrt((n-1)*sum((gi-gbar)^2)/n)\n    r &lt;- list(gini=g,se=se)\n}\n\n# generate 500 observations from a Gamma( a = 1/2 )\nx &lt;- rgamma(500,1/2) # Sample from a Gamma with alpha=1/2\nr &lt;- gini(x)\nsprintf(\"gini %f\", r$gini)\nsprintf(\"s.e. %f\", r$se)\n</code></pre> <pre><code>gini 0.626679\ns.e. 0.012652\n</code></pre> <pre><code>plot(c(1:500)/500,cumsum(sort(x))/sum(x),type=\"l\",lwd=4,col=\"red\")\nabline(0,1,lty=2,lwd=2)\n</code></pre> <p>\u200b </p>"},{"location":"sta355/methods_of_estimation.html","title":"Methods of Estimation","text":"<pre><code>import matplotlib.pyplot as plt\nimport numpy as np\n</code></pre>"},{"location":"sta355/methods_of_estimation.html#method-of-moments-estimation","title":"Method of moments estimation","text":"<p>Want to find a statistic \\(T(X_1,...,X_n)\\) s.t. \\(E_\\theta[T(X_1,...,X_n)] = h(\\theta)\\) where \\(h\\) has a well-defined inverse. Then, we can set \\(T(X_1,...,X_n) = h(\\hat\\theta)\\) s.t. \\(\\hat\\theta = h^{-1}(T)\\)</p> <p>If \\(X_1,..., X_n\\) indep. and \\(E_\\theta(X_i) = h(\\theta)\\), then by substitution principle, we can estimate \\(E_\\theta(X_i)\\) by \\(\\bar X\\) and then \\(\\bar X = h(\\hat\\theta)\\) and so \\(\\hat\\theta = h^{-1}(\\bar X)\\)</p>"},{"location":"sta355/methods_of_estimation.html#example-exponential-distribution","title":"Example: Exponential Distribution","text":"<p>\\(X_1,...,X_n\\) indep, \\(f(x;\\lambda) = \\lambda \\exp(-\\lambda x), x\\geq 0\\), \\(\\lambda &gt; 0\\) is unknown.</p> <p>Note that for \\(r &gt; 0, E_\\lambda (X_i^r) = \\lambda ^{-r}\\Gamma(r+1)\\) so that we have MoM estimator </p> \\[n^{-1}\\sum^n X_i^r = \\frac{\\Gamma(r+1)}{\\hat\\lambda^r}\\] \\[\\Rightarrow \\hat\\lambda(r) = \\bigg((n\\Gamma(r+1)^{-1}\\sum^nX_i^r)\\bigg)^{-1/r}\\] <p>Using \\(r = 1\\) gives the best estimation (minimized s.d.)</p>"},{"location":"sta355/methods_of_estimation.html#example-gamma-distribution","title":"Example: Gamma Distribution","text":"<p>\\(X_1,...,X_n\\) indep. \\(f(x;\\lambda, \\alpha) = \\lambda^a x^{a-1}exp(-\\lambda x) \\Gamma(a)^{-1}, x\\geq 0\\). \\(\\lambda, a &gt; 0\\) are unknown.  Note that \\(E(X_i) = a/\\lambda, var(X_i) = a/\\lambda^2\\), so that MoM gives </p> \\[\\bar X = \\hat a / \\hat \\lambda , S^2 = \\hat a / \\hat \\lambda^2\\] \\[\\Rightarrow \\hat a = \\bar X^2 / S^2 , \\hat \\lambda = \\bar X / S^2\\]"},{"location":"sta355/methods_of_estimation.html#confidence-interval","title":"Confidence Interval","text":"<p>An interval \\(\\mathcal I = [l(X_1,...,X_n), u(X_1,...,X_n)]\\) is a CI with coverage \\(100p\\%\\) if </p> \\[P[l(X_1,...,X_n)\\leq \\theta \\leq u(X_1,...,X_n)] = p, \\forall \\theta\\in\\Theta\\]"},{"location":"sta355/methods_of_estimation.html#the-pivotal-method","title":"The pivotal method","text":"<p>Is not that often that we can measure such probability directly. One way to work around is to find a r.v. \\(g(X_1,...,X_n,\\theta)\\) whose distribution is independent of \\(\\theta\\) and any other unknown params. </p>"},{"location":"sta355/methods_of_estimation.html#confidence-interval_1","title":"Confidence Interval","text":"<p>An interval \\(\\mathcal I = [l(X_1,...,X_n), u(X_1,...,X_n)]\\) is a CI with coverage \\(100p\\%\\) if </p> \\[P[l(X_1,...,X_n)\\leq \\theta \\leq u(X_1,...,X_n)] = p, \\forall \\theta\\in\\Theta\\]"},{"location":"sta355/methods_of_estimation.html#example","title":"Example","text":"<p>For \\(X_1,...,X_{20}\\) indep. \\(\\sim N(\\mu, \\sigma^2)\\), the \\(95\\%\\) CI is \\([\\bar X\\pm -2.093\\frac{S}{\\sqrt{20}}]\\). </p> <p>The following example is 100 samples of size 20 from \\(N(0, 1)\\) and we note that \\(95\\%\\) of the samples falls into the confidence interval. </p> <pre><code>samples = np.random.randn(200, 20)\nmean = samples.mean(axis=1)\nsd = samples.std(axis=1)\nnot_in_CI = np.concatenate((np.where(mean + 2.093 * sd / samples.shape[1]**0.5 &lt; 0)[0], \n                            np.where(mean - 2.093 * sd / samples.shape[1]**0.5 &gt; 0)[0]))\nplt.figure(figsize=(12, 4))\nplt.errorbar(x=np.arange(samples.shape[0]), y=mean, \n             yerr = 2.093 * sd / 20**0.5 , \n             fmt=\" \", label=\"CI covers true mean\")\nplt.errorbar(x=not_in_CI, y=mean[not_in_CI], \n             yerr = 2.093 * sd[not_in_CI] / 20**0.5 , \n             fmt=\" \", color=\"red\", label=\"CI does not cover\")\nplt.axhline(0, linestyle=\":\", color=\"grey\")\nplt.xlabel(\"sample\"); plt.ylabel(\"CI\")\nplt.title(r\"95% CIs for $\\mu$\"); plt.legend();\n</code></pre> <p>\u200b </p>"},{"location":"sta355/methods_of_estimation.html#the-pivotal-method_1","title":"The pivotal method","text":"<p>Is not that often that we can measure such probability directly. One way to work around is to find a r.v. \\(g(X_1,...,X_n,\\theta)\\) whose distribution is independent of \\(\\theta\\) and any other unknown params. </p>"},{"location":"sta355/methods_of_estimation.html#maximum-likelihood-estimation","title":"Maximum Likelihood Estimation","text":"<p>Given \\((X_1,...,X_n)\\) r.v. with joint pdf</p> \\[f(x_1,...,x_n; \\theta_1,...,\\theta_k)\\] <p>where \\(\\theta\\)'s are unknown parameters. The likelihood is defined as </p> \\[\\mathcal L(\\theta_1, ...,\\theta_k) = f(x_1,...,x_n; \\theta_1,...,\\theta_k)\\] <p>note that \\(x_1,...,x_n\\) are fixed observations</p> <p>Suppose that for each \\(\\vec x\\), \\((T_1(\\vec x), ..., T_k(\\vec x))\\) maximize \\(\\mathcal L (\\Theta)\\) . Then maximum likelihood estimators (MLEs) of \\(\\Theta\\) are </p> \\[\\hat \\theta_j = T_j(X_1,..., X_n), j = 1,..., k\\]"},{"location":"sta355/methods_of_estimation.html#existence-and-uniqueness","title":"Existence and uniqueness","text":"<ul> <li>MLE is essentially an ad hoc procedure albeit one that works very well in many problems. </li> <li>MLEs need not be unique, although in most cases, it is unique. </li> <li>MLEs may not exist, typically when the sample size is too small.</li> </ul>"},{"location":"sta355/methods_of_estimation.html#sufficient-statistic","title":"Sufficient Statistic","text":"<p>A statistic \\(T = (T_1(\\vec X), ..., T_m(\\vec X))\\) is sufficient for \\(\\theta\\) if the conditional distribution of \\(\\vec X\\) given \\(T = t\\) depends only on \\(t\\). </p>"},{"location":"sta355/methods_of_estimation.html#neyman-factorization-theorem","title":"Neyman Factorization Theorem","text":"<p>\\(T\\) is sufficient for \\(\\theta\\) IFF </p> \\[f(\\vec x;\\theta) = g(T(\\vec x); \\theta) h(\\vec x)\\]"},{"location":"sta355/methods_of_estimation.html#observed-fisher-information","title":"Observed Fisher Information","text":"<p>Given the MLE \\(\\hat \\theta\\), the observed Fisher information is </p> \\[\\mathcal I(\\hat\\theta) = -\\frac{d^2}{d\\theta^2}\\ln \\mathcal L(\\hat\\theta)\\] <p>Fisher information is an estimator for standard error, i.e. </p> \\[\\hat se(\\hat \\theta) = \\{\\mathcal I(\\hat\\theta)\\}^{-1/2}\\] <p>Mathematically, this is the absolute curvature of the log-likelihood function at its maximum. If this is small, then the estimator is more well-defined (hence with smaller estimated s.e.)</p>"},{"location":"sta355/methods_of_estimation.html#approximate-normality-of-mles","title":"Approximate normality of MLEs","text":"<p>Theorem For \\(X_1,...,X_n\\) indep. with pdf \\(f\\) for some real-valued \\(\\theta\\in \\Theta\\), if</p> <ul> <li>\\(\\Theta\\) is an open set</li> <li>\\(A = \\{x: f(x;\\theta)&gt; 0\\}\\) does not depend on \\(\\theta\\) (true for the exponential families) </li> <li>\\(l(x;\\theta)\\) is 3-time differentiable w.r.t. \\(\\theta\\) for each \\(x\\in A\\). </li> </ul> <p>Then, with \\(\\theta_0\\) being the true parameter, we have </p> \\[\\begin{align*} \\sqrt n(\\hat\\theta_n - \\theta_0) &amp;\\approx \\bigg\\{-n^{-1}\\sum^n l''(X_i; \\theta_0)\\bigg\\}^{-1}\\frac{1}{\\sqrt n} \\sum^n l'(X_i;\\theta_0)\\\\ &amp;\\approx \\frac{1}{\\sqrt n} \\sum^n \\frac{l'(X_i; \\theta_0)}{\\mathcal I(\\theta_0)}\\\\ &amp;\\rightarrow^d N(0, \\mathcal I(\\theta_0)^{-1}) \\end{align*}\\] <p>proof. This conclusion follows Taylor expansion </p> \\[0 = \\frac{1}{\\sqrt n} \\sum^n l'(X_i;\\theta_0) + \\{\\sqrt n (\\hat \\theta_n - \\theta_0)\\} \\frac{\\sum^n l''(X_i; \\theta_0)}{n} + \\sqrt n(\\hat\\theta_n - \\theta_0) \\times R_n\\] <p>where \\(R_n = \\frac{1}{2}(\\hat\\theta_n - \\theta_0) \\frac{\\sum^nl''(X_i; \\theta_n^*)}{n}\\) is the Taylor's remainder with \\(\\theta_n^*\\) in between \\(\\hat\\theta_n, \\theta_0\\) so that </p> \\[\\sqrt n (\\hat\\theta_n - \\theta_0) =\\bigg\\{-n^{-1}\\sum^n l''(X_i; \\theta_0) - R_n\\bigg\\}^{-1}\\frac{1}{\\sqrt n} \\sum^n l'(X_i;\\theta_0)\\] <p>Then, note that \\(\\hat\\theta_n - \\theta_0 \\rightarrow ^p 0\\) (i.e. \\(\\hat\\theta\\)) is a \"good\" estimator from the assumption, and \\(\\frac{1}{n}\\sum l''(X_i; \\theta_n^*)\\) is bounded. so that their product, </p> \\[ R_n = \\frac{1}{2}(\\hat\\theta_n - \\theta_0) \\frac{\\sum^nl''(X_i; \\theta_n^*)}{n}\\rightarrow^p 0\\] <p>Therefore, \\(\\sqrt{\\hat\\theta_n - \\theta_0}\\rightarrow^d (0, \\mathcal I(\\theta_0)^{-1})\\)</p>"},{"location":"sta355/order_statistics.html","title":"Order Statistics","text":""},{"location":"sta355/order_statistics.html#order-statistics_1","title":"Order Statistics","text":"<p>Let \\(X_1,...,X_n\\) indep. with unknown \\(F\\)</p> <p>order \\(X_1,...,X_n\\) in increasing order \\(X_{(1)}\\leq ... \\leq X_{(n)}\\). Due to the independence assumption, the order statistics carry the same info about \\(F\\) as the unordered.  </p> <p>Also, the order statistics can be used to estimate the quantiles \\(F^{-1}(\\tau)\\), such as median </p> \\[M = \\begin{cases} \\frac{1}{2}(X_\\frac n2 + X_{\\frac n2+1}) &amp;\\text{even}\\\\ \\frac 1 2 X_{\\frac{n+1}2} &amp;\\text{odd} \\end{cases}\\approx F^{-1}(1/2)\\] <p>Similarly, \\(F^{-1}(\\tau)\\approx X_{k}, k \\approx \\tau n\\)</p>"},{"location":"sta355/order_statistics.html#sample-extremums","title":"Sample Extremums","text":"<p>with the independence assumption, </p>"},{"location":"sta355/order_statistics.html#sample-minimum","title":"Sample Minimum","text":"\\[\\begin{align*} P(X_{(1)}\\leq x) &amp;= 1 - P(X_{1}&gt;x)\\\\ &amp;= 1 - P(X_1&gt;x, X_2&gt;x,..., X_n&gt;x)\\\\ &amp;= 1 - \\prod_{i=1}^n P(X_i&gt;x) &amp;\\text{independence}\\\\ &amp;= 1 - [1-F(x)]^n \\end{align*}\\] <p>so that the pdf is \\(g_1(x) = n[1-F(x)]^{n-1}f(x)\\)</p>"},{"location":"sta355/order_statistics.html#sample-maximum","title":"Sample Maximum","text":"\\[P(X_{n}\\leq x) = P(X_1\\leq x, ..., X_n\\leq x)= F(x)^n\\] <p>pdf is \\(g_n(x) = nF(x)^{n-1}F(x)\\)</p>"},{"location":"sta355/order_statistics.html#sample-distribution","title":"Sample Distribution","text":"<p>Consider the distribution of \\(X_{k}\\)</p> <p>First, define r.v. \\(Z(x) = \\sum^n\\mathbb I(X_i\\leq x) \\sim Binomial(n,F(x))\\) so that \\(X_{(k)}\\leq x = Z(x)\\geq k\\). </p> <p>Then, </p> \\[P(X_{(k)}\\leq x) = P(Z(x)\\geq k) = \\sum_{i=k}^n{n\\choose i}F(x)^i[1-F(x)]^{n-i}\\] <p>and </p> \\[\\begin{align*} g_k(x) &amp;= \\frac{d}{dx}\\sum_{i=k}^n {n\\choose i}F(x)^i[1-F(x)]^{n-i}\\\\ &amp;= \\frac{n!}{(k-1)!(n-k)!}F(x)^{k-1}[1-F(x)]^{n-k}f(x) \\end{align*}\\]"},{"location":"sta355/order_statistics.html#central-order-statistics","title":"Central order statistics","text":"<p>Let \\(k = k_n\\approx \\tau n, \\tau \\in (0,1)\\), then \\(X_{(k)}\\) is called a central order statistic. Intuitively, \\(X_{(k)}\\) is an estimator of the \\(\\tau\\)-quantile \\(F^{-1}(\\tau)\\), formally </p> \\[X_{(k)} \\rightarrow^p F^{-1}(\\tau), n\\rightarrow \\infty, k_n/n\\rightarrow \\tau\\]"},{"location":"sta355/order_statistics.html#convergence-in-distribution-of-central-order","title":"Convergence in distribution of central order","text":"\\[\\sqrt n(X_{(k)} - F^{-1}(\\tau))\\rightarrow^d N\\bigg(0, \\frac{\\tau(1-\\tau)}{f^2(F^{-1}(\\tau))}\\bigg)\\] <p>Proof by using \\(Unif(0,1)\\) order statistics and then Delta method to generalize.</p> <p>proof. Take \\(U_1,...,U_n\\) be independent \\(Unif(0,1)\\) r.v., and use the order statistics \\(U_{(1)}\\leq ... \\leq U_{(n)}\\). Take \\(E_1,E_2,...,E_{n+1}\\) to be independent r.v. \\(\\sim Exponential(1)\\). Let \\(S=\\sum_{i=1}^{n+1} E_i\\)Note that </p> \\[(U_{(1)},...,U_{(n)})=^d (\\frac{E_1}{S}, \\frac{E_1+E_2}S, ..., \\frac{E_1+...+E_n}S)\\] <p>Then, we can approximate the distribution by sum of exponential r.v. </p> \\[U_{(k)}=\\frac{(E_1+...+E_k)/n}{(E_1+...+E_{n+1})/n}\\approx n^{-1}(E_1+...+E_k)\\] <p>Assume \\(\\sqrt{n}(\\frac{k_n}{n}-\\tau)\\rightarrow 0\\), then </p> \\[\\sqrt{n}(U_{(k_n)}-\\tau)=^d \\sqrt n \\big(\\frac{E_1+...E_{k_n}}{S}-\\tau\\big)=\\sqrt n \\big(\\frac{E_1+...E_{k_n}-\\tau S}{S}\\big)\\] <p>Note that </p> \\[\\frac Sn = \\underset{\\rightarrow^p \\:1\\: (WLLN)}{\\big(\\frac{E_1+...E_{n+1}}{n+1}}\\big)\\underset{\\rightarrow 1}{\\frac{n+1}{n}}\\rightarrow^p 1\\] <p>WTS \\(\\sqrt n \\big(n^{-1}(E_1+...+E_{k_n}-\\tau S)\\big)\\rightarrow^d N(0,\\tau(1-\\tau))\\)</p> <p>Let \\(A\\) be the summation</p> \\[\\begin{align*} A&amp;:=E_1+...+E_{k_n}-\\tau S \\\\ &amp;= E_1 + ..+E_{k_n}-\\tau(E_1+...+E_n)\\\\ &amp;= (1-\\tau)(E_1 + ... + E_{k_n})+(-\\tau)(E_{k_n+1}+...+E_{n+1})\\\\ \\end{align*}\\] <p>Using CLT, </p> \\[\\begin{align*} E[\\frac A{\\sqrt n}] &amp;= \\frac{1}{\\sqrt n}\\big(k_n(1-\\tau)- (n-k_n+1)\\tau\\big)\\\\ &amp;=\\frac{1}{\\sqrt n}(k_n-n\\tau -\\tau)\\\\ &amp;= \\sqrt n(\\frac{k_n}{n}-\\tau) - \\frac{\\tau}{\\sqrt n}\\\\ &amp;\\rightarrow 0 - 0=0 \\end{align*}\\] \\[\\begin{align*} var\\bigg[\\frac A{\\sqrt n}\\bigg] &amp;= \\frac 1 n \\big (k_n(1-\\tau)^2 + (n-k_n+1)\\tau^2\\big)\\\\ &amp;= \\frac{1}{n}\\big(k_n - 2\\tau k_n + k_n \\tau^2 + n\\tau^2 - k_n \\tau^2 + \\tau^2\\big)\\\\ &amp;= \\frac{k_n} n - 2\\tau\\frac{k_n}n + \\tau^2 + \\frac{\\tau^2}{n}\\\\ &amp;\\rightarrow \\tau - 2\\tau\\tau + \\tau^2 + 0\\\\ &amp;= \\tau(1-\\tau) \\end{align*}\\] <p>Theorem If \\(U\\sim Unif(0,1)\\) and \\(F\\) is continuous cdf with pdf \\(f\\) with \\(f(x)&gt;0\\) for all \\(x\\) with \\(0&lt;F(x)&lt;1\\). Then \\(X=F^{-1}(U)\\sim F\\). Therefore, for some cdf </p> \\[F^{-1}(U_{(1)})\\leq ... \\leq F^{-1}(U_{(n)})\\] <p>are order statistics from \\(F\\). </p> <p>Then, </p> \\[\\sqrt n (X_{(k_n)}-F^{-1}(\\tau))=^d \\sqrt n (F^{-1}(U_{(k_n)}-F^{-1}(\\tau)))\\] <p>Then we can use Delta Method, note that</p> \\[\\begin{align*} \\frac{d}{d\\tau}F(F^{-1}(\\tau)) &amp;= \\frac d{d\\tau}\\tau\\\\ f(F^{-1}(\\tau))\\frac d{d\\tau}F^{-1}(\\tau)&amp;= 1\\\\ \\frac d{d\\tau}F^{-1}(\\tau)&amp;= f(F^{-1}(\\tau))^{-1} \\end{align*}\\] <p>So that </p> \\[\\sqrt n (X_{k_n}-F^{-1}(\\tau))\\rightarrow^d N\\big(0, \\frac{\\tau(1-\\tau)}{f^2(F^{-1}(\\tau))}\\big)\\]"},{"location":"sta355/order_statistics.html#quantile-quantile-plots","title":"Quantile-quantile plots","text":"<p>Plot \\(x_{(k)}\\) versus \\(F_0^{-1}(\\tau_k)\\) for \\(k=1,...,n\\). According to the theory, if the data come from a distribution of this form then </p> \\[x_{(k)} = \\mu + \\sigma F_0^{-1}(\\tau_k) + \\epsilon_k, k=1,...,n\\] <p>where </p> \\[\\epsilon_k \\sim N\\bigg(0, \\frac{\\sigma^2\\tau_k (1-\\tau_k)}{nf_0^2(F_0^{-1})(\\tau_k)}\\bigg)\\] <p>Then, note that for fixed \\(\\tau_k, var(\\tau_k)\\rightarrow^{n\\rightarrow \\infty}0\\) </p> <p>Assess if data \\(x_1,...,x_n\\) are well-modeled by a cdf of the form \\(F_0(\\frac{x-\\mu}{\\sigma})\\) for some \\(F_0\\). </p>"},{"location":"sta355/order_statistics.html#example-normal-qq-plot","title":"Example: Normal QQ Plot","text":"<p>Given \\(x_1,...,x_n\\) then the steps are </p> <ol> <li>order \\(x_1,...,x_n\\) into \\(x_{(1)}\\leq ...\\leq x_{(n)}\\)</li> <li>let \\(Z_{(1)}\\leq ... \\leq Z_{(n)}\\) be the order statistics of a sample of size \\(n\\) from \\(N(0,1)\\) and define \\(e_i = E(Z_{(i)})\\) to be the expected values of the order statistics; \\(e_i \\approx \\Phi^{-1}(\\frac{i-0.375}{n+0.25})\\) </li> <li>Plot \\(x_{(i)}\\) vs. \\(e_i\\). If \\(x_1,...,x_n\\) do com from a normal distribution then the points should fall close to a straight line. If the plot shows a certain degree of curvature then notifies this may not be a normal model. </li> </ol> <pre><code>x1 &lt;- rnorm(200) # generate random data from N(0,1)\nqqnorm(x1) \nx2 &lt;- rgamma(200, shape=.5) # generate gamma with shape=0.5\nqqnorm(x2)\n</code></pre> <p>\u200b </p> <p>\u200b </p>"},{"location":"sta355/order_statistics.html#shapiro-wilk-test","title":"Shapiro-Wilk Test","text":"<p>A formalized way of composing the normal QQ plot by the correlation between \\(\\{X_{(k)}\\}\\) and \\(\\{F_0^{-1}(\\tau_k)\\}\\) where \\(F_0 = N(0,1)\\)</p> <p>\\(H_0:\\) data come from \\(N(\\mu,\\sigma)\\) for some \\(\\mu,\\sigma\\) statistic </p> \\[W = \\frac{(\\sum^n a_ix_{(i)})^2}{\\sum^n(x_{(i)}-\\bar x)^2}, \\text{ where }\\begin{bmatrix}a_1\\\\...\\\\a_n\\end{bmatrix} = k_vV^{-1}\\begin{bmatrix}e_1\\\\...\\\\e_n\\end{bmatrix}\\] <p>where \\(V[i,j] = cov(Z_{(i)}, Z_{(j)})\\) and \\(k_v\\) is determined so that \\(\\sum a_i^2 = 1\\). </p> <p>For larger \\(n\\), then \\(W\\) is approximately </p> \\[W^* = \\frac{(\\sum^n e_ix_{(i)})^2}{\\sum_{i=1}^n e_i^2(\\sum_{i=1}^n (x_{(i)}-\\bar x))^2}\\] <pre><code>shapiro.test(x1)\nshapiro.test(x2)\n</code></pre> <pre><code>    Shapiro-Wilk normality test\n\ndata:  x1\nW = 0.99369, p-value = 0.5554\n\n\n\n\n\n    Shapiro-Wilk normality test\n\ndata:  x2\nW = 0.6741, p-value &lt; 2.2e-16\n</code></pre>"},{"location":"sta355/spacings.html","title":"Spacings","text":"<p>Given the order statistics \\(X_{(1)}\\leq ... \\leq X_{(n)}\\), define \\((n-1)\\) spacings (first order spacings) by </p> \\[D_k = X_{(k+1)}-X_{(k)}, k=1,...,n-1\\] <p>Intuitively, the spacings should carry some information about the pdf \\(f\\). </p> <p>Note that if \\(\\tau \\approx \\frac{k+1}{n}\\approx \\frac{k}{n}\\) then \\(X_{(k+1)}\\) and \\(X_{(k)}\\) estimate \\(F^{-1}(\\tau)\\). If \\(f(F^{-1}(\\tau))\\) is large then \\(D_k\\) is small, conversely, \\(f(F^{-1}(\\tau))\\) is small then \\(D_k\\) is large. </p>"},{"location":"sta355/spacings.html#exponential-spacings","title":"Exponential Spacings","text":"<p>\\(X_1,...,X_n\\sim Exp(\\lambda)\\) iid. </p> \\[f(x;\\lambda) = \\lambda \\exp(-\\lambda x)\\mathbb I(x\\geq 0)\\] <p>Given the order statistics \\(X_{(1)}\\leq ...\\leq X_{(n)}\\) define </p> \\[\\begin{align*} Y_1 &amp;= nX_{(1)}\\\\ Y_2 &amp;= (n-1)(X_{(2)}-X_{(1)}) = (n-1)D_1\\\\ Y_2 &amp;= (n-2)(X_{(3)}-X_{(2)}) = (n-2)D_2\\\\ \\vdots\\\\ Y_n &amp;= X_{(n)} - X_{(n-1)} = D_{n-1} \\end{align*}\\]"},{"location":"sta355/spacings.html#proposition-1","title":"Proposition 1","text":"<p>\\(Y_1,...,Y_n\\) are iid \\(\\sim Exp(\\lambda)\\) </p> <p>proof. Note that the join pdf of \\((X_{(1)}, ..., X_{(n)})\\) if </p> \\[f(x_1,...,x_n) = n!\\lambda^n\\exp(-\\lambda \\sum^n x_i)\\mathbb I(0\\leq x_1&lt;x_2&lt;...&lt;x_n)\\] <p>Also, note that </p> \\[\\begin{align*} X_{(1)} &amp;= Y_1/n \\\\ X_{(k)} &amp;= \\frac{Y_1}{n} + ... + \\frac{Y_k}{n-k+1}, k = 2,...,n \\end{align*}\\] <p>Therefore, </p> \\[g(y_1,...,y_n) = f\\big(\\frac{y_1}n, ..., \\frac{y_1}{n} + \\frac{y_2}{n-1} + ... + y_n\\big)|J(y_1,...,y_n)|\\] <p>Note that \\(|J|\\) is the absolute determinant of the matrix </p> \\[\\begin{bmatrix} 1/n&amp;0&amp;0&amp;...&amp;0\\\\ 1/n&amp;\\frac 1{n-1}&amp;0&amp;...&amp;0\\\\ \\vdots &amp;\\vdots &amp;\\ddots &amp;...&amp;\\vdots\\\\ 1/n&amp;\\frac{1}{n-1}&amp;\\frac 1{n-2}&amp;...&amp;1 \\end{bmatrix}\\] <p>which is \\(\\frac{1}{n!}\\)</p> \\[g(y_1,...,y_n)=n!\\lambda^n\\exp(-\\lambda \\sum^n x_i) \\frac 1{n!} = \\lambda^n\\exp(-\\lambda \\sum^n x_i)\\mathbb I(y_1,...,y_n\\geq 0)\\]"},{"location":"sta355/spacings.html#proposition-2","title":"Proposition 2","text":"<p>If \\(\\frac{k_n}n\\rightarrow\\tau\\in (0,1)\\) and \\(f(F^{-1}(\\tau)) &gt; 0\\), then </p> \\[nD_{k_n}\\rightarrow^d Exp(f(F^{-1}(\\tau)))\\] \\[\\implies P(D_{k_n}\\leq x )\\approx 1 - \\exp(-nf(F^{-1}(\\tau))x), x\\geq 0\\] <p>proof. Note that </p> \\[X_{(k_n+1)}=^d F^{-1}\\big(\\frac{E_1+...+E_{k_n+1}}{E_1+...+E_{n+1}}\\big), X_{(k_n)}=^d F^{-1}\\big(\\frac{E_1+...+E_{k_n}}{E_1+...+E_{n+1}}\\big)\\] <p>where \\(E_i \\sim Exp(1)\\) so that </p> \\[\\begin{align*} nD_{k_n} &amp;= ^d n\\bigg(F^{-1}\\big(\\frac{E_1+...+E_{k_n+1}}{E_1+...+E_{n+1}} - F^{-1}\\big(\\frac{E_1+...+E_{k_n}}{E_1+...+E_{n+1}}\\big)\\big)\\bigg)\\\\ &amp;\\approx \\frac{1}{f(F^{-1}(\\tau))}\\bigg(\\frac{nE_{k_n+1}}{E_1+...+E_{n+1}}\\bigg)\\\\ &amp;= \\frac{1}{f(F^{-1}(\\tau))}\\bigg(\\frac{E_{k_n+1}}{(E_1+...+E_{n+1})/n}\\bigg)\\\\ &amp;\\rightarrow^p \\frac{E_{k_n+1}}{f(F^{-1}(\\tau))} &amp;\\text{WLLN, }\\frac{E_1+...+E_{n+1}}n\\rightarrow^p 1\\\\ &amp;\\sim Exp(f(F^{-1}(\\tau))) \\end{align*}\\]"},{"location":"sta355/spacings.html#example-density-estimation-using-spacings","title":"Example: density estimation using spacings","text":"<p>Consider \\(D_1,...,D_{n-1}\\) are iid. exponential with \\(E(nD_k) = \\exp(g(V_k))\\) where \\(V_k = \\frac{X_{(k+1)} + X_{(k)}}{2}\\), then \\(V_k\\approx F^{-1}(\\tau), \\tau\\approx \\frac kn\\approx \\frac{k+1}n\\) and the density is \\(f(x)=\\exp(-g(x))\\)</p> <p>Using <code>B-spline</code> functions, we can estimate the function \\(g(x)\\)</p> \\[g(x)=\\beta_0 + \\sum_{i=1}^p \\beta_j \\psi_j(x)\\] <p>where \\(\\beta_i\\)'s are unknown parameters and \\(\\psi_j\\)'s are B-spline functions. </p> <pre><code># create the splines functions\nden.splines &lt;- function(x,p=5) {\n    library(splines)\n    n &lt;- length(x)\n    x &lt;- sort(x)\n    x1 &lt;- c(NA,x)\n    x2 &lt;- c(x,NA)\n    sp &lt;- (x2-x1)[2:n]\n    mid &lt;- 0.5*(x1+x2)[2:n]\n    y &lt;- n*sp\n    xx &lt;- bs(mid,df=p)\n    r &lt;- glm(y~xx,family=quasi(link=\"log\",variance=\"mu^2\"))\n    density &lt;- exp(-r$linear.predictors)\n    r &lt;- list(x=mid,density=density)\n    r\n}\n</code></pre> <p>Consider sampling from GMM model</p> \\[0.7N(2,1) + 0.3N(-2, 1)\\] <pre><code># randomly sample 500 points from given GMM\nx &lt;- ifelse(runif(500) &lt; .7, rnorm(500, 2, 1), rnorm(500, -2, 1))\n# estimate density using p = 8\nr &lt;- den.splines(x,p=8)\n# estimation\nplot(r$x,r$density,type=\"l\",xlab=\"x\",ylab=\"density\",lwd=4,col=\"red\")\n# actual\nlines(r$x,0.3*dnorm(r$x,-2,1)+0.7*dnorm(r$x,2,1),lwd=2,lty=2)\nlegend(\"topleft\", c(\"estimation\", \"actual GMM\"), fill=c(\"red\", \"black\"))\n</code></pre> <p>\u200b </p>"},{"location":"sta355/spacings.html#hazard-functions","title":"Hazard Functions","text":"<p>For \\(X\\) is a positive continuous rv, its hazard function is </p> \\[h(x) = \\frac{f(x)}{1-F(x)}\\] <p>The motivation behind is to consider \\(X\\) as the survival time, consider </p> \\[\\begin{align*} \\delta^{-1}P(x&lt;X&lt;x+\\delta\\mid X&gt;x) &amp;= \\delta^{-1}\\frac{P(x&lt;X\\leq x+\\delta)}{P(X&gt;x)}\\\\ &amp;= \\delta^{-1}\\frac{F(x+\\delta) - F(x)}{1-F(x)}\\\\ &amp;\\rightarrow_{\\delta\\rightarrow 0} \\frac{f(x)}{1-F(x)} =:h(x) \\end{align*}\\] <p>Therefore, this represents instantaneous death rate given survival to time \\(x\\). </p> <p>Also, note that </p> \\[h(x) = \\frac{f(x)}{1-F(x)} = -\\frac{d}{dx}\\ln(1-F(x))\\] <p>Therefore, </p> \\[F(x) = 1 - \\exp(-\\int_0^x h(t)dt), f(x) = h(x)\\exp(-\\int_0^x h(t)dt)\\] <p>In this case, we require \\(\\int_0^\\infty h(x)dx = \\infty\\) so that to have a \"proper\" probability distribution. </p> <p>The shape of the hazard function gives info not immediately apparent in \\(f\\) or \\(F\\).  \\(h(x)\\) increasing indicates new better than used, decreasing indicates used better than new</p>"},{"location":"sta355/statistical_models.html","title":"Statistical Models","text":""},{"location":"sta355/statistical_models.html#statistical-models_1","title":"Statistical Models","text":"<p>Assume that the data \\(x_1,...,x_n\\) are outcomes of r.v. \\(X_1,...,X_n \\sim F\\), which assumes to be unknown. </p> <p>A statistical model is a family \\(\\mathcal F\\) of probability distributions of \\((X_1,...,X_n)\\). </p> <p>Theoretically, \\(F\\in \\mathcal F\\) but in practice this is not always true, we are to find some \\(F_0 \\in \\mathcal F\\) close enough to \\(F\\) so that \\(\\mathcal F\\) is useful. </p>"},{"location":"sta355/statistical_models.html#parametric-models","title":"Parametric models","text":"<p>For a given \\(\\mathcal F\\), we can parametrize as \\(\\mathcal F = \\{F_\\theta:\\theta \\in \\Theta\\}\\)</p> <p>If \\(\\Theta \\subset \\mathbb R^p\\) then \\(\\mathcal F\\) is a parametric model and \\(\\theta \\in \\mathbb R^p = (\\theta_1,...,\\theta_p)\\)</p>"},{"location":"sta355/statistical_models.html#non-parametric-models","title":"Non-parametric models","text":"<p>If \\(\\Theta\\) is not finite dimensional then the model is said to be non-parametric (in this case, \\(\\in\\mathbb R^\\infty\\))</p> <p>Example \\(g(x)\\approx \\sum^p \\beta_k \\phi_k(x)\\) for some functions \\(\\phi_1,...,\\phi_p\\) and unknown parameters \\(\\beta_1,...,\\beta_p\\)</p>"},{"location":"sta355/statistical_models.html#semi-parametric-models","title":"Semi-parametric models","text":"<p>Non-parametric models often have a finite dimensional parametric component. </p> <p>Example \\(Y_i = g(x_i) + \\epsilon_i\\) with \\(\\{\\epsilon_i\\}\\) iid. \\(N(0,\\sigma^2)\\) and \\(g,\\sigma^2\\) are unknown</p>"},{"location":"sta355/statistical_models.html#example","title":"Example","text":"<p>Consider the linear regression \\(Y_i = \\beta_0 + \\beta_1x_i + \\epsilon_i\\) for observations \\((x_1,Y_1), ..., (x_n, Y_n)\\) where \\(\\epsilon_i \\sim N(0,\\sigma^2)\\) iid.  Such model is parametric model. </p> <p>However, if relax the assumption to \\(E(\\epsilon_i) = 0, E(\\epsilon_i^2) = \\sigma^2\\), then this will be semi-parametric model. </p>"},{"location":"sta355/statistical_models.html#example_1","title":"Example","text":"<p>Let \\(X_1,...,X_n\\) be iid. Exponential r.v. representing survival times.  </p> \\[f(x;\\lambda) = \\lambda e^{-\\lambda x}\\mathbb I(x\\geq 0)\\] <p>\\(\\lambda &gt;0\\) is unknown</p> <p>Let \\(C_1,...,C_n\\) be independent with unknown cdf \\(G\\) (or cdfs \\(G_i\\))  </p> <p>Observe \\(Z_i = \\min(X_i, C_i), \\delta_i = \\mathbb I(X_i\\leq C_i)\\)</p> <p>parameters \\(\\lambda, G\\) so that semi-parametric model.</p>"},{"location":"sta355/statistical_models.html#bayesian-models","title":"Bayesian models","text":"<p>Assume a parametric model with \\(\\Theta \\subset \\mathbb R^p\\), for each \\(\\theta \\in \\Theta\\), think of the join cdf \\(F_\\theta\\) as the conditional distribution of \\(\\mathcal X\\) given \\(\\theta\\). </p> <p>Bayesian inference put a probability distribution on \\(\\Theta\\), i.e. a prior. </p> <p>After observing \\(x_1,...,x_n\\), we can use Bayes Theorem to obtain a posterior distribution of \\(\\theta\\) given \\(X_1 = x_1,...,X_n = x_n\\)</p>"},{"location":"sta355/statistical_models.html#statistical-functionals","title":"Statistical Functionals","text":"<p>To estimate the characteristics of a model \\(F\\), we often consider \\(\\theta(F)\\), i.e. a mapping \\(\\theta: \\mathcal F\\rightarrow \\mathbb R\\)</p>"},{"location":"sta355/statistical_models.html#examples","title":"Examples","text":"<p>\\(\\theta(F) = \\mathbb E_F(X_i)= \\mathbb E_F(h(X_i))\\) \\(\\theta(F) = F^{-1}(\\tau)\\) quantiles \\(\\theta(F) = \\mathbb E_F\\big[\\frac{X_i}{\\mu(F)}\\ln(\\frac{X_i}{\\mu(F)})\\big], P(X_i &gt; 0 ) = 1, \\mu(F) = \\mathbb E_F(X_i)\\) Theil index</p>"},{"location":"sta355/statistical_models.html#substitution-principle","title":"Substitution principle","text":"<p>First estimate \\(F\\rightarrow \\hat F\\) and substitute \\(\\hat F\\) into \\(\\theta (\\hat F)\\) If \\(\\theta\\) is continuous, Using continuous mapping theorem, \\(\\theta(\\hat F) \\approx \\theta(F)\\)</p> <p>Example empirical distribution function (edf)  </p> \\[\\hat F(x) = \\frac{1}{n} \\sum^n \\mathbb I(X_i \\leq x) = \\text{proportion of observations } \\leq x\\] <p>Note that the edf is just a sample mean and WLLN, CLT holds, for each \\(\\mathbb I(X_i \\leq x)\\) is iid. Bernoulli</p> <p>Therefore, </p> <ul> <li>\\(E(\\hat F(x)) = F(x), var(\\hat F(x)) = \\frac{F(x)(1-F(x))}{n}\\) </li> <li>WLLN \\(\\hat F(x) = \\hat F_n(x) \\rightarrow^p F(x), \\forall x\\) </li> <li>CLT \\(\\sqrt n(\\hat F_n(x)-F(x))\\rightarrow^f N(0, F(x)(1-F(x)))\\)</li> </ul>"},{"location":"sta457/ARIMA.html","title":"Autoregressive Integrated Moving Average (ARIMA) model","text":""},{"location":"sta457/ARIMA.html#arima","title":"ARIMA","text":"<p>For nonstationary time series, applying difference operators repeatedly to the data \\(\\{X_t\\}\\) until the differenced observations resemble a realization of some stationary process \\(\\{W_t\\}\\)</p> <p>ARIMA(p,q,d) \\(\\{X_t\\}\\) is said to follow an ARIMA(p,q,d) model if \\(W_t = (1-B)^d X_t\\) is stationary ARMA model. i.e. </p> \\[(1-B)^d \\Phi(B)X_t = \\Theta(B)a_t, a_t \\sim NID(0,\\sigma^2)\\] <p>A series follows a stationary ARMA model after differencing \\(d\\) times, i.e. \\((1-B)^d X_t\\), such process is called an \\(I(d)\\) process. </p>"},{"location":"sta457/ARIMA.html#example","title":"Example","text":"<p>whether \\((1-B)^2 X_y\\) is stationary </p> <p>\\((1-B)^2 X_t = X_t-2B X_t + B^2 X_t = X_t - 2X_{t-1}+X_{t-2}\\)</p>"},{"location":"sta457/ARIMA.html#dickey-fuller-unit-root-test","title":"Dickey Fuller unit root test","text":"<p>The DF test is used to test \\(I(1)\\) processes. Consider \\(X_t = \\phi X_{t-1} + a_t. a_t \\sim NID(0,\\sigma^2)\\), then \\(\\Delta X_t = (\\phi - 1) X_{t-1} + a_t = \\pi X_{t-1} + a_t\\) \\(H_0: \\pi = 0, i.e. X_t\\sim I(1)\\)</p> <p>The general DF test may contain an intercept and a deterministic time trend as \\(\\Delta X_t = a+\\tau^T DR_t + \\pi X_{t-1} + a_t\\)</p>"},{"location":"sta457/ARIMA.html#augmented-dickey-fuller-test","title":"Augmented Dickey-Fuller test","text":"<p>Problems with basic DF  </p> <ul> <li>The basic DF test considers only a single unit root</li> <li>Correct model specification<ul> <li>correct specification of time trend and intercept</li> <li>The DGP may contain both AR and MA terms </li> <li>There might be structural breaks in the data</li> </ul> </li> </ul> <p>ADF test equation</p> \\[\\Delta X_t = \\tau^T DR_t + \\pi X_{t-1}+\\sum_{j=1}^k \\gamma_j\\Delta X_{t-j} + a_t\\] <p>where \\(k=p-1\\), the equation use the autoregression to take into account the presence of serial correlated errors. </p>"},{"location":"sta457/ARIMA.html#box-cov-transformation","title":"Box-cov transformation","text":"<p>If the variance is not stationary, we can try stablize it with a Box Cox transformation, i.e. include \\(\\lambda\\) as one of the parameters  \\(\\Phi(B)(X_t^\\lambda - \\mu) = \\Theta(B)a_t\\), then choose \\(\\lambda, \\Phi, \\Theta\\) based on Minimized RMSE. </p>"},{"location":"sta457/ARMA.html","title":"Autoregressive (AR) and Moving Average (MA) model","text":""},{"location":"sta457/ARMA.html#autoregressivear-and-moving-averagema-model","title":"Autoregressive(AR) and moving average(MA) model","text":"<p>A process \\(\\{X_t\\}\\) is said to be an ARMA(p,q) process if </p> <ul> <li>\\(\\{X_t\\}\\) is stationary</li> <li>\\(\\forall t. X_t - \\phi_1X_{t-1}-...-\\phi_qX_{t-p} = a_t + \\theta_1a_{t-1}+...+\\theta_qa_{t-q}\\)  using backward shift operation notation \\(B^h=x_{t-h}\\): \\(\\Phi(B)x_t = (1-\\phi_1B - ... - \\phi_p B^p)x_t = (1+\\theta_1B + ...+\\theta_qB^q)a_t = \\Theta(B)a_t\\)  where \\(a_t \\sim NID(0, \\sigma^2)\\) </li> </ul> <p>\\(\\{X_t\\}\\) is an ARMA(p,q) process with mean \\(\\mu\\) if \\(\\{X_t-\\mu\\}\\) is an ARMA(p,q) process. </p>"},{"location":"sta457/ARMA.html#moving-average-modelmaq","title":"Moving average model(MA(q))","text":"<p>MA(\\(\\infty\\)) If \\(\\{a_t\\}\\sim NID(0, \\sigma^2)\\) then we say that \\(\\{X_t\\}\\) is a MA(\\(\\infty\\)) process of \\(\\{a_t\\}\\) if \\(\\exists\\{\\psi_n\\}, \\sum^\\infty |\\psi_j|&lt;\\infty\\) and \\(X_t = \\sum^\\infty \\psi_j a_{t-j}\\) where \\(t\\in\\mathbb{Z}\\).  </p> <p>We can calculate ACF of a stochastic process \\(\\{X_t\\}\\) a.l.s. \\(\\{X_t\\}\\) can be writtin in the form of a MA(\\(\\infty\\)) process</p> <p>Also, MA(\\(\\infty\\)) is a required condition for \\(\\{X_t\\}\\) to be stationary. </p> <p>Theorem  The MA(\\(\\infty\\)) process is stationary with 0 mean and autocovariance function \\(\\gamma(k) = \\sigma^2 \\sum^\\infty \\psi_j\\psi_{j+|k|}\\) </p> <p>MA(q) \\(X_t = \\sum_{i=0}^q \\theta_i a_{t-i} = \\Theta(B)a_t\\) \\(\\theta_0 = 1, B\\) is the backward shift operator, \\(B^hX_t = X_{t-h}\\) and \\(a_t\\sim NID(0, \\sigma^2)\\)</p> <p>Under MA(q) model  </p> \\[\\begin{align*} \\gamma(1) = cov(X_t, X_{t+1})&amp;=cov(\\sum_{i=0}^q\\theta_i a_{t-i}, \\sum_{i=0}^q \\theta_ia_{t+1-i})\\\\ &amp;=E(\\sum_{i=0}^{q-1}\\theta_i\\theta_{i+1}a_{t-i}a_{t-i}) &amp;a\\sim NID, cov(a_i,a_j) =0\\\\   &amp;=\\sigma^2(\\sum_{i=0}^{q-1}\\theta_i\\theta_{i+1}) \\end{align*}\\] <p>Similarly,  </p> \\[\\begin{align*} \\gamma(k)=cov(X_t, X_{t+k}) &amp;=cov(\\sum_{i=0}^q \\theta_t a_{t-i}, \\sum_{i=0}^q \\theta_i a_{t+k - i})\\\\ &amp;=\\sigma^2 \\sum_{i=0}^{q-k}\\theta_i\\theta_{i+k}\\mathbb{I}(|k|\\leq q) \\end{align*}\\] <p>Then, the autocorrelation function (ACF) will be  </p> \\[\\begin{align*} \\rho_k &amp;= \\gamma_k/\\sqrt{var(X_t)var(X_{t+k})}\\\\ &amp; = \\gamma_k / \\sigma^2\\sum_{i=0}^{q} \\theta_i^2\\\\ &amp; =\\sigma^2 \\sum_{i=0}^{q-k}\\theta_i\\theta_{i+k}\\mathbb{I}(|k|\\leq q) / \\sigma^2\\sum_{i=0}^{q} \\theta_i^2\\\\ &amp; = \\sum_{i=0}^{q-k}\\theta_i\\theta_{i+k}\\mathbb{I}(k\\leq q) / \\sum_{i=0}^{q} \\theta_i^2 \\end{align*}\\]"},{"location":"sta457/ARMA.html#autoregressive-model-of-order-p-arp","title":"Autoregressive model of order p (AR(p))","text":"<p>\\(X_t - \\phi_1X_{t-1}-...-\\phi_pX_{t-p} = \\Phi(B)X_t = a_t\\) where \\(a_t\\sim NID(0, \\sigma^2), B^hX_t = X_{t-h}, h\\in\\mathbb{Z}, \\Phi(B)=(1-\\phi_1B-...-\\phi_p B^p)\\)</p>"},{"location":"sta457/ARMA.html#ar1","title":"AR(1)","text":"<p>Notice that for a \\(AR(1)\\) process, \\(a\\sim NID(0, \\sigma^2)\\) and \\(a_t\\) is uncorrelated with all previous \\(X_s, s&lt;t\\)</p> \\[\\begin{align*} X_t &amp;= \\phi X_{t-1} + a_t\\\\ &amp;=\\phi(\\phi X_{t-2}+a_{t-1})+a_t&amp;\\text{ replace }X_{t-1}\\\\   &amp;...&amp;\\text{repeated replacing}\\\\ &amp;=\\sum_0^\\infty \\phi^i a_{t-i} \\end{align*}\\] <p>is a \\(MA(\\infty)\\) process</p> \\[\\begin{align*} \\gamma(k)  &amp;= cov(X_t, X_{t+k})\\\\ &amp;=cov(\\sum_0^\\infty \\phi^i a_{t-i}, \\sum_0^\\infty \\phi^i a_{t+k-i})\\\\   &amp;=cov(\\sum_0^\\infty \\phi^i a_{t-i}, \\sum_0^\\infty \\phi^{i+k} a_{t-i} + \\sum_0^{k-1} \\phi^i a_{t+k-i})\\\\ &amp;= \\phi^k\\sum_0^\\infty (\\phi^ia_{t-i})^2\\\\ &amp;=\\phi^k \\gamma(0)=\\phi^k var(X_t) \\end{align*}\\] \\[\\begin{align*} \\gamma(0) &amp;=var(X_t)\\\\ &amp;=\\sum^\\infty \\phi^{2i}a^2_{t-i}\\\\ &amp;=\\sigma^2(\\sum^\\infty (\\phi^2)^i) &amp;a\\sim NID(0,\\sigma^2)\\\\ &amp;=\\sigma^2(1-\\phi^2)^{-1} &amp;\\text{when }\\phi^2&lt;1 \\text{, by Maclaurin's series} \\end{align*}\\] <p>Causal or future independent AR process when \\(|\\phi|&lt; 1\\) for an \\(AR(1)\\)</p>"},{"location":"sta457/ARMA.html#checking-stationarity-of-arp","title":"Checking stationarity of AR(p)","text":"<p>\\(\\Phi(B) = 1-\\phi_1B-...-\\phi_pB^p=0\\) must have all the roots line outside the unit circle. </p>"},{"location":"sta457/ARMA.html#acf","title":"ACF","text":""},{"location":"sta457/ARMA.html#ar1-case","title":"AR(1) Case","text":"\\[X_t = \\phi X_{t-1} + a_t, a_t\\sim NID(0,\\sigma^2)\\] <p>For \\(k\\in\\mathbb{Z}^+\\), multiply \\(X_{t-k}\\) on both sides  </p> \\[X_t X_{t-k} = \\phi X_{t-1}X_{t-k} + a_t X_{t-k}\\] <p>Taking expectation, consider \\(E(a_tX_{t-k})\\) </p> <p>\\begin{align}  cov(a_t, X_{t-k}) &amp;= E(a_t X_{t-k})-E(a_t)E(X_{t-k})\\  &amp;= E(a_t X_{t-k}) - 0\\   &amp;= cov(a_t, \\sum_0^\\infty \\phi^i a_{t-k-i}) = 0   \\end{align}</p> <p>\\(a_t\\) is uncorrelated with previous \\(a\\)'s. </p> \\[E(X_t X_{t-k}) = \\phi E(X_{t-1}X_{t-k})\\] <p>since \\(cov(X_t,X_{t-k}) = E(X_tX_{t-k})-0\\) </p> \\[\\gamma(k)=\\phi\\gamma(k-1)\\] <p>By induction, \\(\\gamma(k)=\\phi^k\\gamma(0)\\)</p>"},{"location":"sta457/ARMA.html#ar2-case","title":"AR(2) Case","text":"\\[X_t = \\phi_1 X_{t-1} + \\phi_2 X_{t-2} + a_t\\] <p>Multiple both sides by \\(X_t\\) </p> \\[X_t^2 = \\phi_1 X_{t-1}X_t + \\phi_2 X_{t-2}X_t + X_t a_t\\] <p>Taking expectation, note that \\(X_t\\) is a lin.comb of \\(a\\).   </p> \\[\\gamma(0) = \\gamma(1) + \\gamma(2) + \\sigma^2\\] \\[\\gamma(0)(1-\\phi_1\\rho(1)-\\phi_2\\rho(2)) = \\sigma^2 \\text{ since }\\rho(k)=\\gamma(k)/\\gamma(0)\\] <p>Multiple both sides by \\(X_{t-1}\\) and take expectations  </p> \\[E(X_tX_{t-1}) = \\phi_1 E(X_{t-1}X_{t-1}) + \\phi_2E(X_{t-2}X_{t-1}) + E(a_t X_{t-1})\\] \\[\\gamma(1) = \\phi_1\\gamma(0) + \\phi_2\\gamma(1)\\] \\[\\rho(1) = \\phi_1 + \\phi_2\\rho(1)\\] \\[\\rho(1) = \\frac{\\phi_1}{1-\\phi_2}\\] <p>Multiple both sides by \\(X_{t-2}\\) and take expectations</p> \\[E(X_tX_{t-2}) = \\phi_1 E(X_{t-1}X_{t-2}) + \\phi_2E(X_{t-2}X_{t-2}) + E(a_t X_{t-2})\\] \\[\\gamma(2) = \\phi_1\\gamma(1) + \\phi_2\\gamma(0)\\] \\[\\rho(2) = \\phi_1\\rho(1) + \\phi_2\\] <p>... Using this pattern  </p> \\[\\rho(h) = \\phi_1\\rho(h-1)+\\phi_2\\rho(h-2)\\] <p>with base case </p> \\[\\rho(0)=1, \\rho(1) = \\frac{\\phi_1}{1-\\phi_2}\\]"},{"location":"sta457/ARMA.html#arp-case","title":"AR(p) case","text":"<p>Given \\(X_t = (\\sum_1^p \\phi_iX_{t-i}) + a_t\\), is stationary is all \\(p\\) roots lie outside of the unit circle  </p> <p>Yule-Walker equations For the first \\(p\\) autocorrelations:</p> \\[\\rho(k) = \\sum_1^p \\phi_i\\rho_{|k-i|}\\]"},{"location":"sta457/ARMA.html#partial-autocorrelation-function-pacf","title":"Partial Autocorrelation Function (PACF)","text":"<p>\\(\\phi_{kk} = corr(X_t, X_{t+k}\\mid X_{t+1},...,X_{t+k-1})\\) the correlation between \\(X_t, X_{t+k}\\) after their mutual linear dependency on the intervening variables has been removed. </p> <p>For a given lag \\(k\\), \\(\\forall j \\in \\{1,2,...,k\\}\\).  </p> \\[\\rho_i = \\sum_1^k\\phi_{ki}\\rho_{j-i}\\] <p>We regard the ACFs are given, take regression parameters \\(\\phi_{ki}\\) and wish to solve for \\(\\phi_{kk}\\). which all together forms the Yule-Walker equations. </p> <p>Example For lag 1, \\(\\rho_1 = \\phi_{11},\\rho_0\\Rightarrow \\rho_1=\\phi_{11}\\)</p> <p>For lag 2,  </p> \\[\\rho_1 = \\phi_{21} + \\phi_{22}\\rho_1\\] \\[\\rho_2 = \\phi_{21}\\rho_1 + \\phi_{22}\\] \\[\\Rightarrow \\phi_{22} = \\frac{\\rho_2 - \\rho_1^2}{1-\\rho_1^2}\\]"},{"location":"sta457/ARMA.html#causal-and-invertible","title":"Causal and invertible","text":"<p>Causal/stationary if \\(X_t\\) can be expressed as an MA(\\(\\infty\\)) process</p> <p>Invertible if \\(X_t\\) can be expressed as an AR(\\(\\infty\\)) process. </p>"},{"location":"sta457/ARMA.html#duality-between-ar-amd-ma-processes","title":"Duality between AR amd MA processes","text":"<p>A finite-order stationary AR(p) process corresponds to a MA(\\(\\infty\\)) process, and a finite-order invertible MA(q) corresponds to an AR(\\(\\infty\\)) process. </p>"},{"location":"sta457/ARMA.html#example","title":"Example","text":"<p>Given model \\(X_t - \\phi_1 X_{t-1} - \\phi_2 X_{t-2} = a_t = \\theta a_{t-1}\\)</p> <p>Assume the process is causal, then \\(X_t = \\sum_0^\\infty \\psi_i a_{t-i} = a_t\\sum_0^\\infty \\psi_i B^i = \\psi(B)a_t\\) by causal process \\(\\phi(B)X_t = \\theta(B) a_t \\Rightarrow X_t = \\frac{\\theta(B)a_t}{\\phi(B)}\\) by ARMA model \\(\\Rightarrow \\Theta(B)/\\Phi(B)=\\Psi(B)\\) </p> <p>Replace back into the model \\(1+\\theta B = (\\sum_0^\\infty \\psi_iB^i)(1-\\phi_1B - \\phi_2B^2)\\)</p> <p>Consider \\(B\\), \\(\\theta B = \\psi_1B -\\phi_1B\\Rightarrow \\psi_1 = \\phi_1 + \\theta\\)</p> <p>Consider \\(B^2\\), \\(0 = -\\theta_2B^2-\\psi_1\\theta_1B +\\psi_2B^2\\Rightarrow \\psi_2 = \\phi_2 + \\phi_1(\\phi_1+ \\theta)\\)</p> <p>Assume the process is invertible, then \\(a_t = \\sum_0^\\infty \\pi_i X_{t-i} = X_t\\sum_0^\\infty \\pi_i B^i\\), similarly we get \\(\\Phi(B)=\\Theta(B)\\Pi(B)\\)</p>"},{"location":"sta457/ARMA.html#wold-decomposition","title":"Wold Decomposition","text":"<p>Any zero-mean process \\(\\{X_t\\}\\) wgucg us bit deterministic can be expressed as a sum of \\(X_t = U_t + V_t\\) where \\(\\{U_t\\}\\) denotes an MA(\\(\\infty\\)) process and \\(\\{V_t\\}\\) is a deterministic process which is uncorrelated with \\(\\{U_t\\}\\)  - deterministic if the values \\(X_{n+j}, j\\geq 1\\) of the process \\(\\{X_t\\}\\) were perfectly predicatable in term of \\(\\mu_n=sp\\{X_t\\}\\)  - If \\(X_n\\) comes from a deterministic process, it can be predicted (or determined) by its past observations of the process</p>"},{"location":"sta457/ARMA.html#model-identification","title":"Model identification","text":"process ACF PACF AR(p) tails off cuts off after lag p MA(q) cuts off after lag q tails off ARMA(p,q) tails off after (q-p) tails off after (p-q)"},{"location":"sta457/ARMA.html#model-adequacy","title":"Model Adequacy","text":"<p>The overall tests that check an entire group of residual autocorrelation functions are called portmanteau tests.</p> <p>Box and Pierce \\(Q = n \\sum_1^m \\hat\\rho_k^2 \\sim \\chi^2_{m-(p+q)}\\) Ljung and Box \\(Q=\\sum_1^m \\frac{n(n+2)\\hat\\rho_k^2}{n-k}\\sim \\chi^2_{m-(p+q)}\\) \\(n\\) is the number of observations \\(m\\) is the max lag \\(p,q\\) are fitted model</p>"},{"location":"sta457/ARMA.html#model-selection","title":"Model selection","text":"\\[AIC = -2\\log ML + 2k\\] \\[BIC = -2 \\log ML + k \\log n\\] <p>BIC puts more penalties on the number of parameters</p>"},{"location":"sta457/ARMA.html#example-application-of-arma-in-investment","title":"Example: Application of ARMA in Investment","text":""},{"location":"sta457/ARMA.html#alternative-assets-modeling","title":"Alternative assets modeling","text":"<p>\\(y_t\\) and \\(r_t\\) denote observable appraisal and latent economic returns. </p> <p>Goal to infer unobservable economic returns using appraisal returns </p> <p>Geltner method commercial real state </p> \\[y_t = \\phi y_{t-1} + (1-\\phi)r_t = \\sum^\\infty \\phi^j(1-\\phi)r_{t-j} = \\sum^\\infty w_jr_{t-j}\\] <p>(by substitute \\(y_{t-1}\\)) where \\(\\phi\\in (0,1), w_j := \\phi^j (1-\\phi)\\) is the weight   </p> \\[y_t = \\hat\\phi y_{t-1}+\\hat a_t , \\hat r_t = \\frac{\\hat a_t}{1-\\hat\\phi}\\] \\[var(\\hat r_t)=\\frac{\\sigma^2}{(1-\\hat\\phi)^2} \\] <p>Gertmansky, Low, &amp; Markorov </p> \\[y_t = \\sum^q w_i r_{t-i}\\] <p>where \\(w_i\\in(0,1), \\sum w_i = 1\\) Since \\(y_t\\) is a linear combination of white noise</p> \\[y_t = \\sum^q \\theta_i a_{t-i} = \\sum_i^q \\left(\\frac{\\theta_i}{\\sum_j^q \\theta_j}\\sum_j^q \\theta_j a_{t-i}\\right) = \\sum^q w_i r_{t-i}\\] <p>Factor Modeling The economic returns can be regressed by the market returns</p> \\[r_t = \\alpha + \\beta r_{Mt} + e_t\\] \\[y_t = \\sum^q w_i (\\alpha + \\beta r_{M_,t-i} + e_{t-i}) \\] \\[= \\sum^q w_i a + \\beta \\sum^q w_i r_{M,t-i} + \\sum^q w_i e_{t-i} =\\alpha + \\beta \\sum^q w_i r_{M,t-i} + \\sum^q w_i e_{t-i}\\]"},{"location":"sta457/Forecasting.html","title":"Forecasting and Transfer Function Noise Model","text":""},{"location":"sta457/Forecasting.html#minimized-mse-forecasts-for-arma-models","title":"Minimized MSE Forecasts for ARMA models","text":"<p>Consider a stationary ARMA model (casual and invertible)  </p> \\[\\Phi(B)X_t = \\Theta(B)a_t\\] <p>Rewrite as a MA process</p> \\[X_t = \\Psi(B)a_t\\] <p>Then, for \\(t = n + h\\), </p> \\[X_{n+h}=\\sum^\\infty_0 \\psi_i a_{n+ h - i}\\] <p>Suppose we have observations till \\(X_n, X_{n-1},...\\) and wish to forecast \\(h\\) step ahead of future values \\(X_{n+h}\\) as a lin.comb. of the observations. Then, we define the mean square error forecaster </p> \\[\\hat X_t(h):= \\sum_{i=0}^\\infty \\hat\\psi_i a_{t-i}\\] <p>where \\(\\hat\\psi_i\\) are parameters to be determined.  </p> <p>Then MSE of the forecast is   </p> \\[\\begin{align*} E(X_{t+h} - \\hat X_t(h))^2  &amp;= E(\\sum^\\infty \\psi_i a_{t+h-i} - \\sum^\\infty \\hat\\psi_i a _{t-i})\\\\   &amp;= E(\\sum^{j-1}\\psi_i a_{t+h-i}) + \\sum^\\infty (\\psi_{h+i} + \\hat\\psi_i)a_{t-i})^2\\\\ &amp;= \\sigma^2 \\sum^{h-1} \\psi_i^2 + \\sigma^2 \\sum^\\infty (\\psi_{h+i-\\hat\\psi_i})^2\\\\ \\arg\\min(E(X_{t+h} - \\hat X_t(h))^2)&amp;=\\arg\\min(\\sum^\\infty (\\psi_{h+i-\\hat\\psi_i})^2)\\\\ \\implies \\psi_{h+i} &amp;= \\hat\\psi_i \\end{align*}\\]"},{"location":"sta457/Forecasting.html#rule-of-calculating-conditional-expectation","title":"Rule of calculating conditional expectation","text":"\\[E_t(X_{t+h}) := E(X_{t+h}\\mid X_t, X_{t-1}, ...) = E(\\sum^\\infty \\psi_i a_{t+h-i}\\mid X_t, X_{t-1},...)\\] <p>Using the fact that \\(a_i\\)'s are uncorrelated</p> \\[E(X_{t+h}\\mid X_t, X_{t-1}, ...) =\\sum^\\infty \\psi_{h+i}a_{t-i} = \\hat X_t(h)\\] <p>For \\(h &gt; 0\\), </p> \\[\\begin{align*} E_t(X+h) &amp;= \\hat X_t (h)\\\\ E_t(X_{t-h}) &amp;= X_{t-h}\\\\ E_t(a_{t+h})&amp;=E(a_{t+h}) = 0\\\\ E_t(a_{t-h}) &amp;= X_{t-h} - \\hat X_{t-h-1} = X_{t-h} - E_{t-h-1}(X_{t-h})\\\\ \\end{align*}\\]"},{"location":"sta457/Forecasting.html#example","title":"Example","text":"<p>Consider a AR(1) model \\(X_t = 0.5X_{t-1} + a_t\\). Then  </p> \\[\\hat X_t(1)  = E_t(X_{t+1}) = 0.5 E_t(X_t) + E_t(a_{t+1}) = 0.5E_t(X_t) = 0.5X_t\\] \\[\\hat X_t(h) = 0.5 E_t(X_{t+h-1}) + E_t(a_{t+h}) = 0.5 \\hat X_t(h-1) = 0.5^h X_t\\]"},{"location":"sta457/Forecasting.html#transfer-function-noise-model","title":"Transfer Function Noise model","text":"<p>Consider the model that  \\(X_t = f(X_{t-1}, X_{t-2}, ... , Z_t, Z_{t-1},...)\\), \\(X_t\\) is a linear combination of past observations and an external variable.</p> <p>A TFN model is a time series regression that predict values of a dependent variable based on both the current and lagged values of one or more explanatory variables.</p>"},{"location":"sta457/Forecasting.html#procedure-of-building-the-single-input-tfn-model","title":"Procedure of building the single input TFN model","text":"<ol> <li>Preliminary identification of the impulse response coefficients \\(v_i\\) (prewhitening)</li> <li>Specification of the noise term \\(n_t\\)</li> <li>Specification of the transfer function using a rational polynomial in B if necessary</li> <li>Estimation of the TFN specified </li> <li>Model diagnostic checks</li> </ol> <p>Rational distributed lag model \\(v(B)\\) can be approximated by a ratio of polynomials  </p> \\[v(B)=\\frac{\\sum_0^r\\delta B^i}{1-\\sum_1^s\\theta_i B^i} = \\delta(B)/\\theta(B)\\] <p>and then, \\(y_t = \\delta(B)x_t / \\theta(B) + n_t\\)</p>"},{"location":"sta457/Forecasting.html#armax","title":"ARMAX","text":"<p>\\(y_t = v(B)x_t + n_t = v_0 x_t + v_1 x_{t-1}+v_2 x_{t-2}+...+n_t\\) where \\(v(B) = \\sum^\\infty v_j B^j\\), and \\(x_t,n_t\\) are independent. </p> <p>The coefficient \\(v_0, v_1,...\\) are referred as the impulse response function of the system.</p> <p>To make such equation to be meaningful, \\(\\sum^\\infty |v_j| = g&lt;\\infty\\), which the system is stable and \\(g\\) is called the stead-state gain. \\(g\\) represents the impact on \\(Y\\) when \\(X_{t-j}\\) are held constant over time.  </p> <p>Properties \\(x_t \\sim ARMA(p,q)\\), \\(v_i = \\phi^i (1-\\phi)\\), \\(y_t = \\sum^\\infty v_i x_{t-i}+a_t\\)</p>"},{"location":"sta457/Forecasting.html#pre-whitening","title":"Pre-whitening","text":"<p>Consider \\(x\\sim\\)ARMA, i.e. \\(\\Phi_x(B)x_t = \\Theta_x(B)a_t\\) Apply \\(\\Phi_x(B)/\\Theta_x(B)\\) on TFN model  </p> \\[\\frac{\\Phi_x(B)}{\\Theta_x(B)}y_t = v(B)\\frac{\\Phi_x(B)}{\\Theta_x(B)} x_t + \\frac{\\Phi_x(B)}{\\Theta_x(B)}\\epsilon_t\\] <p>Let \\(\\tau_t = \\frac{\\Phi_x(B)}{\\Theta_x(B)} y_t, n_t = \\frac{\\Phi_x(B)}{\\Theta_x(B)} \\epsilon_t\\), we get   </p> \\[\\tau_t = v(B)a_t + n_t \\land n_t \\perp a_t\\] <p>To get \\(\\gamma_{a\\tau}(0)\\), multiply both sides by \\(a_t\\) and take the expectations. </p> \\[\\begin{align*} \\gamma_{a\\tau}(0) &amp;=E(a_t\\tau_t)\\\\ &amp;= E(a_tv(B)a_t) + E(a_tn_t)\\\\ &amp;=E((v_0a_t + v_1e_{t-1}+...+v_ma_{t-m})a_t)\\\\ &amp;=E(v_0a_ta_t)=v_o\\gamma_a(0) = v_0\\sigma^2 \\end{align*}\\] <p>To get \\(\\gamma_{a\\tau}(1)\\), multiply both sides by \\(a_{t-1}\\)</p> \\[\\begin{align*} \\gamma_{a\\tau}(1)&amp;=E(a_{t-1}\\tau_t)\\\\ &amp;=E(a_{t-1}v(B)a_t) + E(a_{t-1}n_t)\\\\ &amp;=E(a_{t-1}(v_0a_t + v_1 a_{t-1} + v_2 a_{t-2}+...+v_m a_{t-m}))\\\\ &amp;= E(a_{t-1}v_1 a_{t-1})=v_1\\gamma_a(0) = v_1\\sigma^2 \\end{align*}\\] <p>Therefore, \\(\\gamma_{a\\tau(k)} = v_k\\sigma^2\\)</p> <p>Since \\(\\rho_{a\\tau}(k) = \\gamma_{a\\tau}(k) / \\sigma_a\\sigma_\\tau\\) </p> \\[\\rho_{a\\tau}(k) = \\frac{v_k \\sigma_a^2}{\\sigma_a\\sigma_\\tau} = \\frac{v_k \\sigma_a}{\\sigma_\\tau}\\land v_k = \\rho_{a\\tau}(k)\\frac{\\sigma_\\tau}{\\sigma_a}\\propto \\rho_{a\\tau}(k)\\]"},{"location":"sta457/Forecasting.html#box-tiao-transformation","title":"Box-Tiao Transformation","text":"<p>Similarly, since \\(n_t\\sim\\) ARMA, i.e. \\(\\Phi_n(B)n_t = \\Theta_n(B)a_t\\) Which \\(\\frac{\\Phi_n(B)}{\\Theta_n(B)}n_t = a_t\\) Then, apply \\(\\frac{\\Phi_n(B)}{\\Theta_n(B)}\\) to both sides of the equation. </p> \\[\\frac{\\Phi_n(B)}{\\Theta_n(B)} y_t = v(B) \\frac{\\Phi_n(B)}{\\Theta_n(B)} x_t + \\frac{\\Phi_n(B)}{\\Theta_n(B)} n_t\\] \\[\\tilde y_t = v(B)\\tilde x_t + a_t\\] <p>is called the Box-Tiao Transformation</p>"},{"location":"sta457/Forecasting.html#steps-of-the-estimation-procedure","title":"Steps of The Estimation Procedure","text":"<p>The steps of the estimation procedures</p> <ol> <li>Run the OLS regression on \\(y_t = \\sum_{j=1}^s v_j x_{t-j} + e_t\\) to collect the residuals \\(\\{\\hat e_t\\}\\)</li> <li>Identify an ARMA model for \\(\\hat e_t\\) </li> <li>Apply Box-Tiao transformation to filter \\(y_t, x_t\\)</li> <li>Run regression on the transformed equation</li> <li>check the correlation of regression residuals </li> </ol>"},{"location":"sta457/Introduction.html","title":"Introduction","text":""},{"location":"sta457/Introduction.html#stochastic-process","title":"Stochastic Process","text":"<p>A family of time indexed r.v. \\(Z(w,t)\\) where \\(w\\) belongs to a sample space and \\(t\\) belongs to an index set. </p> <p>For a fixed \\(t\\), \\(Z(w,t)\\) is a r.v. For a given \\(w\\), \\(Z(w,t)\\) is a function of \\(t\\), and is sample function or realization.</p>"},{"location":"sta457/Introduction.html#strongly-stationary","title":"Strongly stationary","text":"<p>Consider a finite set of r.v. \\(\\{Z_{t_1},...,Z_{t_n}\\}\\) from a stochastic process \\(\\{Z(w,t): t=0,\\pm 1, \\pm 2, ...\\}\\). The n-dimensional distribution function is defined by </p> \\[F_{Z_{t_1},...,Z_{t_n}}(x_1,...,x_n) = P\\{w:Z_{t_1}\\leq x_1,...,Z_{t_n}\\leq x_n\\}\\] <p>A process is strictly stationary if </p> \\[F_{Z_{t_1},...,Z_{t_k}}(x_1,...,x_n) = F_{Z_{t_1+k},...,Z_{t_k+k}}(x_1,...,x_n)\\] <p>for any set of \\(\\{Z_{t_1},...,Z_{t_n}\\}\\)</p>"},{"location":"sta457/Introduction.html#auto-covariance-function","title":"Auto-covariance Function","text":"<p>\\(\\gamma_X(r,s) = cov(X_r, X_s)\\) \\(\\rho(r,s) = \\gamma_X(r,s)/\\sqrt{var(X_r)var(X_s)}\\) Auto-correlation function (ACF)</p>"},{"location":"sta457/Introduction.html#cross-covariance-function","title":"Cross-covariance Function","text":"<p>\\(\\gamma_{XY}(r,s)=cov(X_r, Y_s)\\) \\(\\rho_{XY}(r,s) = \\gamma_{XY}(r,s)/\\sqrt{var(X_r)var(Y_s)}\\)</p>"},{"location":"sta457/Introduction.html#weakly-stationary","title":"Weakly stationary","text":"<p>The time series \\(\\{X_t, t\\in \\mathbb{Z}\\}\\) is said to be stationary if </p> <ul> <li>\\(var(X_t)=z&lt;\\infty\\) for all \\(t\\in Z\\) </li> <li>\\(E(X_t) = m\\) for all \\(t\\in Z\\)</li> <li>\\(y_X(r,s) = y_X(t+r, t+s)\\) for all \\(t,r,s\\in Z\\) </li> </ul> <p>the covariance being functions of the time difference alone. </p>"},{"location":"sta457/Introduction.html#classical-decomposition-model","title":"Classical decomposition model","text":"<p>decompose a time series into </p> <ul> <li>trend: loosely defined as \"long-term change in the mean level\"</li> <li>seasonal variation: exhibit variation that is annual in period.</li> <li>cyclic variation: exhibit variation at a fixed period due to some other cause. </li> <li>irregular fluctuations</li> </ul>"},{"location":"sta457/Introduction.html#steps-to-time-series-modeling","title":"Steps to time series modeling","text":"<ul> <li>plot the time series and check for trend, seasonal and other cyclic components, any apparent sharp changes in behavior, as well as any outlying observations. </li> <li>remove trend and seasonal components to get residuals</li> <li>choose a model to fit residuals</li> <li>forecasting can be carried out by forecasting residual and then inverting the transformation carried out in step 2. </li> </ul>"},{"location":"sta457/Introduction.html#sample-autocorrelation-functions-sacf","title":"Sample autocorrelation functions (SACF)","text":"<p>\\(\\hat\\gamma(h)=n^{-1}\\sum_{t=1}^{n-h} (x_{t+h}-\\bar x)(x_t - \\bar x)\\) \\(\\hat\\rho(h) = \\hat\\gamma(h)/\\hat\\gamma(0)\\)</p> <p>For a random time series, \\(\\hat\\rho_k \\sim N(0, 1/n)\\) for \\(k\\neq 0, n\\) is the length of time series. Thus, if a time series is random, we can expect 19/20 of \\(\\hat\\rho_k\\) to lie in \\([-2/\\sqrt n, 2/\\sqrt n]\\)</p>"},{"location":"sta457/Introduction.html#test-zero-cross-correlation","title":"Test Zero cross-correlation","text":"<p>\\(H_0:\\forall h. \\rho_x(h)=\\rho_y(h) = 0 \\land \\rho_{xy}(h) = 0\\) Both \\(X_t, Y_t\\) contain no serial correlation and are mutually independent. </p> <p>Test statistic: \\(\\hat\\rho_{xy}(h)\\sim N(0, 1/n)\\)</p>"},{"location":"sta457/Summary.html","title":"Summary Sheet","text":""},{"location":"sta457/Summary.html#arma-model-forecast","title":"ARMA model forecast","text":"\\[(1-0.5B)(Z_t - 3) = a_t\\] <p>Let \\(X_t = Z_t - 3\\), then \\((1-0.5B)X_t = a_t\\sim AR(1)\\). And because the root \\(B=2\\) lies outside of the unit circle, the series is stationary. Rewrite as a \\(MA(\\infty)\\)</p> \\[(1-0.5B)(1+\\psi_1B + \\psi_2B^2 + ...) = 1\\] <p>\\(\\psi_1 = 1/2\\) \\(\\psi_2 = 1/4\\) \\(\\psi_k = 2^{-k}\\)</p> <p>Then, \\(\\hat X_t(1) = \\sum_{0}^\\infty 2^{-(i+l)}a_{t-i}, \\hat Z_t(l) = 3 + \\hat X_t(l)\\)</p> <p>\\(e_t(l)=\\sum_{i=0}^{l-1} \\psi_i a_{t+l-i}\\), then \\(var(e_t(l)) = \\sum_{i=0}^{l-1} \\psi_i^2 a_{t+l-i}^2 = \\sigma^2 \\sum_0^{l-1}\\psi_i^2\\)</p> \\[(1-B+0.25B^2)(Z_t-1)=a_t\\] <p>Let \\(X_t = Z_t -1\\), \\((0.25B^2 - B + 1)=(0.5B-1)^2 \\Rightarrow B=2\\) is a stationary \\(AR(2)\\) model. </p> <p>\\(X_t = X_{t-1}+\\frac{1}{4}X_{t-2} + a_t\\), then \\(\\hat Z_t(1) = 1+X_t + \\frac{1}{4} X_{t-1}\\) \\(\\hat Z_t(2) =1+ X_t + \\frac{1}{4} X_{t-1}+ \\frac{1}{4} X_t=\\frac{5}{4}X_t + \\frac{1}{4} X_{t-1}\\) \\(\\hat Z_t(3) = 1+\\frac{5}{4} X_t + \\frac{1}{4} X_{t-1}+ \\frac{1}{4}(X_t+\\frac{1}{4}X_{t-1})=\\frac{3}{2}X_t + \\frac{5}{16}X_{t-1}\\) \\(\\hat Z_t(4) = 1+\\frac{3}{2}X_t + \\frac{5}{16}X_{t-1}+\\frac{5}{16}X_t + \\frac{1}{16}X_{t-1} = \\frac{29}{16}X_{t}+\\frac{3}{8}X_{t-1}\\)</p> <p>\\((1-B+0.25B^2)(1+\\psi_1B+\\psi_2B^2+...) = 1\\) \\(\\psi_1 -1 = 0\\Rightarrow \\psi_1 = 1\\) \\(\\psi_2 -\\psi_1 +0.25 = 0\\Rightarrow \\psi_2 = 0.75\\) \\(\\psi_3 -\\psi_2 +0.25\\psi_1 = 0\\Rightarrow \\psi_3 = 0.5\\)</p> <p>Therefore,  \\(var(e_t(1)) = \\sigma^2\\) \\(var(e_t(2)) = 2\\sigma^2\\) \\(var(e_t(3)) = \\frac{41}{16}\\sigma^2\\)</p> \\[(1-B+0.25B^2)(Z_t + 3) = (1+0.25B)a_t\\] <p>All B lies out of the unit circle, it's a stationary and invertible ARMA(2,1) model. </p> <p>\\((1-B+0.25B^2)(1+\\psi_1B+\\psi_2B^2 + ...) = 1+0.25B\\) \\(\\psi_1 = 0.25+1=1.25\\) \\(\\psi_2 = \\psi_1 = 1.25\\) \\(\\psi_3 = \\psi_2 -0.25\\psi_1 = \\frac{15}{16}\\) \\(\\psi_4 = \\psi_3 - 0.25\\psi_2 = \\frac{5}{8}\\) \\(\\psi_5 = \\psi_4 - 0.25\\psi_3 = \\frac{25}{64}\\)</p> <p>\\(var(e_t(1)) = \\sigma^2\\) \\(var(e_t(2)) = \\frac{41}{16}\\sigma^2\\) \\(var(e_t(3)) = \\frac{33}{8}\\sigma^2\\)</p>"},{"location":"sta457/Summary.html#tfn","title":"TFN","text":"<p>Consider a dynamic regression model \\(y_t = \\sum_0^k v_i x_{t-i}+n_t\\) where both \\(x_t, n_t\\) are stationary and invertible ARMA model given by \\(\\phi_x(B)x_t = \\theta(B)a_t, \\phi_n(B)n_t = \\theta_n(B)e_t\\) and \\(cov(e_t,a_s)= 0\\)</p> <p>state the prewhitening process for how to identify the value of k</p> <p>Apply \\(\\phi_x(B)/\\theta(B)\\) on the regression model, we get </p> \\[\\frac{\\phi_x(B)}{\\theta(B)}y_t = \\sum_0^k v_i \\frac{\\phi_x(B)}{\\theta(B)} x_{t-i} + \\frac{\\phi_x(B)}{\\theta(B)} n_t\\] \\[\\hat y_t = v(B)a_t + \\epsilon_t\\] <p>By having \\(\\phi_n(B)=\\phi_x(B), \\theta_n(B)=\\theta(B)\\), we have \\(\\epsilon \\sim e\\) so that </p> \\[\\hat y_t = v(B)a_t + e_t\\] <p>then, multiply both sides by \\(a_{t-k}\\) and take expectations</p> \\[E(\\hat y_t a_{t-k}) = v(B) E(a_t a_{t-k}) + E(e_t a_{t-k})\\] \\[cov(\\hat y_t, a_{t-k}) = v_k\\sigma_a^2\\] \\[v_k = cov(\\hat y_t, a_{t-k}) / \\sigma_a^2 = corr(\\hat y_t, a_{t-k})\\frac{se(\\hat y_t)}{se(a_t)}\\propto corr(\\hat y_t, a_{t-k})\\] <p>Therefore, we can test the statistical significance of \\(v_k\\) by examining the statistical significance of \\(corr(\\hat y_t, a_{t-k})\\)</p> <p>state the steps of using Box-Tiao transformation to estimate \\(v_j\\)</p> <p>The steps of the estimation procedures</p> <ol> <li>Run the OLS regression on \\(y_t = \\sum_{j=1}^s v_j x_{t-j} + e_t\\) to collect the residuals \\(\\{\\hat e_t\\}\\)</li> <li>Identify an ARMA model for \\(\\hat e_t\\) </li> <li>Apply Box-Tiao transformation to filter \\(y_t, x_t\\)</li> <li>Run regression on the transformed equation</li> <li>check the correlation of regression residuals </li> </ol> <p>Find the l-ahead optimal forecast of \\(y_{t+l}, \\hat y_t(l)\\) with \\(a_t,e_t\\).</p> <p>Since \\(v(B)=\\sum_0^k v_i B^i\\) has finite terms, it can be transformed \\(v(B)=\\delta(B)/w(B)\\), then </p> \\[y_t = \\frac{\\delta(B) \\theta(B)}{w(B)\\phi_x(B)}a_t + \\frac{\\theta_n(B)}{\\phi_n(B)}e_t\\] \\[y_t = u(B)a_t + \\psi(B)e_t\\] <p>all of them are finite order with max K of polynomials in B. Therefore, </p> \\[y_{t+l} = \\sum_0^k u_i a_{t+l-i} + \\psi_i e_{t+l-i}\\] \\[\\hat y_t(l) = \\sum_0^k u_{i+l}^* a_{t-i} + \\psi_{i+l}^* e_{t-i}\\] <p>Derive the MSE</p> <p>Consider \\(y_{t-l} - \\hat y_t(l)\\), which equals to  \\(=\\sum_0^{l-1}u_i a_{t+l-i} + \\psi_i e_{t+l - i} (i)\\)  after time lag \\(t\\) \\(-\\sum_0^k (u_{i+l}^* - u_{i+l})a_{t-i} (ii)\\) \\(-(\\psi_{i+l}^* - \\psi_{i+l})e_{t-i} (iii)\\) up to time \\(t\\) </p> <p>\\(E(y_l - \\hat y_t(l))^2 = E(i)^2 + E(ii)^2 + E(iii)^2\\) \\(=\\sum_0^{l-1} u_i^2 \\sigma_a^2 + \\psi_i^2 \\sigma_e^2\\) \\(+ \\sum_0^k \\sigma_a^2(u_{i+l^*}- u_{i+l})\\) \\(+ \\sum_0^k \\sigma_e^2(\\psi_{i+l^*}- \\psi_{i+l})\\)</p> <p>minimized when \\(u^* = u, \\psi^* = \\psi\\)</p>"},{"location":"sta457/Summary.html#var","title":"VAR","text":"<p>Consider a VAR(p) model of 2-d variables, i.e. \\(y_t = [y_{i,t},  y_{2,t}]^T\\) \\(y_t = \\sum_1^p A_i y_{t-i} + a_t\\),  and each \\(A_i = \\begin{bmatrix}\\phi_{i,11}&amp;\\phi_{i,12}\\\\\\phi_{i,21}&amp;\\phi_{i,22}\\end{bmatrix}\\)</p> <p>state how to check the stationarity</p> <p>All the roots of \\(\\det(I_k - A_1B-...-A_pB^p) = 0\\) must lie outside of the unit circle, or the companion form \\(\\xi_t = A\\xi_{t-1}+v_t\\) must have the moduli of the eigenvalues of \\(A\\) being &lt;1. </p> <p>Describe the methods to select the order for Equation (1)</p> <ul> <li>Selection by information criteria, for example, BIC, AIC, DIC, HQ, SC, FPE</li> <li>Using LRT for VAR(p) vs. VAR(p-1)</li> </ul> <p>State how to test Granger causality for that \\(X_{1t}\\) Granger causes \\(X_{2t}\\) but not the other way around. Basd on the same condition, express \\(X_{2t}\\) as the TFN model of \\(X_{1t}\\)</p> \\[X_{2,t} = \\sum_1^p \\phi_{i,21}X_{1,t-i} + \\phi_{i,22}X_{2,t-i} + a_{2,t}\\] \\[X_{2,t} - \\sum_1^p \\phi_{i,22}X_{2,t-i} = \\sum_1^p \\phi_{i,21}X_{1,t-i} + a_{2,t}\\] \\[\\Phi_{22}(B)X_{2,t} = \\Phi_{21}(B)X_{1,t}+a_{2,t}\\] \\[X_{2,t} = \\frac{\\Phi_{21}(B)}{\\Phi_{22}(B)}X_{1,t} + \\frac{a_{2,t}}{\\Phi_{22}(B)}\\] <p>Let \\(v(B) = \\frac{\\Phi_{21}(B)}{\\Phi_{22}(B)}, N_t = \\frac{a_{2,t}}{\\Phi_{22}(B)}\\), the TFN model is </p> \\[X_{2,t}=v(B)X_{1,t} + N_t\\] <p>To see whether \\(X_{1,t}\\) Granger causes \\(X_{2,t}\\), if not, then all \\(\\phi_{i,21} = 0\\)</p> <p>Describe how to test Granger causality using univariate approach. Suppose (\\(\\Phi_1(B)X_{1,t}=\\Theta_1(B)a_{1,t}, \\Phi_2(B)X_{2,t}=\\Theta_2(B)a_{2,t}\\)\\)</p> <p>Using the Portmanteau test, </p> \\[\\rho_{a_1a_2}(k) = \\frac{E(a_{1,t}, a_{2,t+k})}{\\sqrt{\\sigma_1^2\\sigma_2^2}}\\] <p>If \\(\\forall k &lt; 0. \\rho(k)=0\\), then \\(X_2\\) does not Granger cause \\(X_1\\). </p> <p>Suppose \\(X_{1,t}, X_{2,t}\\) not weakly stationary. How do you model the join dynamics of \\(\\{X_{1,t}, X_{2,t}\\}\\) using co-integration.</p> <ul> <li>differencing the two time series individually until each are stationary</li> <li>use VAR(p) to fit the two stationary process and test the Granger causality</li> <li>If \\((X_{1,t}, X_{2,t})\\) are cointegrated, use Error correction model to include lagged disequilibrium terms as explanatory variables. </li> </ul> <p>Discuss the reasons why we have to choose different models based on the condition of integration</p> <p>If cointegration exists, if we use VAR(p) model directly fitted to the differenced stationary processes, the model will be misspecified. </p> <p>Discuss the Engle-Granger approach for modeling cointegrated \\(X_{1t}\\) and \\(X_{2t}\\)</p> <ol> <li>test whether \\(X_t,Y_t\\) are I(1) using unit root test</li> <li>if both I(1), regress one against the other using least squares</li> <li>run a unit root test on regression residuals. If residuals are stationary, these two series are cointegrated. </li> <li>Where the regression line indicate the long-run equilibrium relationship between two variables, the disequilirium term is simply the regression residuals. </li> <li>Finally, consider the ECM </li> </ol> \\[\\Delta X_t = c_1 + \\rho_1(Y_{t-1} - \\hat a X_{t-1}) + \\beta_{x,1}\\Delta X_{t-1} + ... + \\beta_{y,1}\\Delta Y_{t-1}+...+\\epsilon_{x,t}\\] \\[\\Delta Y_t = c_2 + \\rho_2(Y_{t-1} - \\hat a X_{t-1}) + \\gamma_{x,1}\\Delta X_{t-1} + ... + \\gamma_{y,1}\\Delta Y_{t-1}+...+\\epsilon_{y,t}\\] <p>Discuss the implication of Granger representation theorem</p> <p>We have and should use ECM to model non-stationary time series</p>"},{"location":"sta457/Summary.html#bootstrap","title":"Bootstrap","text":"<p>Given an AR(2) model</p> \\[y_t = \\mu + \\phi y_{t-1} + \\phi_2 y_{t-2} + a_t\\] <p>(unconditional) parametric bootstrap</p> <p>\\(E(Y_t) = \\mu + E(\\phi_1 Y_{t-1}) + E(\\phi_2 Y_{t-2}) + E(a_t)\\) \\(E(Y)=\\mu + \\phi_1 E(Y) + \\phi_2 E(Y)\\) \\(E(Y)=\\frac{\\mu}{1-\\phi_1-\\phi_2}\\)</p> <p>\\(var(Y) = 0 + \\phi_1^2 var(Y) + \\phi_2^2 var(Y) + E(a)^2\\) \\(var(Y) = \\frac{\\sigma^2}{1-\\phi_1^2 - \\phi_2^2}\\)</p> <ol> <li>The unconditional distribution of \\(Y_t\\sim N(\\frac{\\mu}{1-\\phi_1-\\phi_2}, \\frac{\\sigma^2}{1-\\phi_1^2 - \\phi_2^2})\\)</li> <li>simulate \\(y_0, y_1\\) by drawing a random number from \\(Y\\). </li> <li>recursively simulate \\(y_2\\) by the model, recursively</li> </ol> <p>pros easy to compute cons don't know whether it is correlated</p>"},{"location":"sta457/Summary.html#garch","title":"GARCH","text":""},{"location":"sta457/Summary.html#arch-process","title":"ARCH process","text":"<p>ARCH(1) The first order of autoregressive conditional heteroskedastic process is  \\(e_t \\sim N(0, \\sigma_t^2)\\) where \\(\\sigma_t^2 = a_0 + a_1 e_{t-1}^2\\). On defining \\(v_t = e_t^2 - \\sigma_t^2\\), the model can also be written as  </p> \\[e_t^2 = a_0 + a_1 e_{t-1}^2 + v_t\\] <p>Since \\(E(v_t \\mid x_{t-1}, x_{t-2},...)=0\\), the model corresponds directly to an AR(1) model for the squared error \\(e_t^2\\). </p> <p>ARCH(q) Then, the ARCH(q) process is defined as </p> <p>\\(\\(\\sigma_t^2 = a_0 + \\sum_{t=1}^q a_i e_{t-i}^2\\)\\) with</p> \\[a_0 \\geq 0, a_i &gt; 0. \\sum_{i=1}^q a_i &lt; 1\\] <p>or </p> \\[e_t^2 = a_0 + \\sum_1^q a_i e_{t-i}^2 + v_t\\] \\[e_t^2 = a_0 + a(B)e_{t-1}^2 + v_t\\] <p>GARCH(p,q)</p> \\[e_t^2 = a_0 + \\sum_1^q \\beta_i \\sigma_{t-i}^2 + \\sum_1^q a_i e_{t-i}^2\\] \\[\\sigma_t^2 = a_0 + a(B)e_{t-1}^2 + \\beta(B)\\sigma_{t-1}^2\\] <p>with \\(a_0 \\geq 0, a(B)\\) and \\(\\beta(B)\\) have no common roots and that the roots of \\(1-\\beta(B)\\) all less than unit root. </p>"},{"location":"sta457/VAR.html","title":"Vector Series (VAR Models)","text":""},{"location":"sta457/VAR.html#vector-auto-regression-of-order-1-var1","title":"Vector Auto-regression of order 1 VAR(1)","text":"\\[X_t = AX_{t-1}+a_t\\] <p>where \\(A_{k\\times k}\\) is the coefficient matrix and \\(a_t\\) is a K-dimensional white noise process with time-invariant positive definite covariance matrix \\(E(aa')=\\Sigma\\)</p> <p>We can repeatedly substitute the VAR(1) above to </p> \\[y_t = \\sum A^i u_{t-i}\\] <p>For the above VMA(\\(\\infty\\)) process to be stationary. \\(A^i\\) must converge to zero, i.e. the all K eigenvalues of \\(A\\) must be less than 1.</p> <p>Then, the VAR(p) model is </p> \\[y_t = A_0 + \\sum_1^p A_i y_{t-i} +a_t\\] <p>OR the compact form </p> \\[A(B)y_t = a_t\\] <p>Then, to be stationary, all the roots of \\(\\det(I_k - A_1B-...-A_pB^p)=0\\) are greater than 1 in absolute value. </p>"},{"location":"sta457/VAR.html#companion-form","title":"Companion form","text":"<p>We can check the stability of a VAR(p) model using its companion form. </p> \\[\\xi_t = A\\xi_{t-1}+v_t\\] <p>where \\(\\xi_t = \\begin{bmatrix} y_t \\\\ ... \\\\ y_{t-p+1} \\end{bmatrix} , A = \\begin{bmatrix}         A_1 &amp; A_2 &amp; ... &amp; A_{p-1} &amp; A_p \\\\         I &amp; 0 &amp; ... &amp; 0 &amp; 0 \\\\         0 &amp; I &amp; ... &amp; 0 &amp; 0 \\\\         ... &amp; ... &amp; ... &amp; ... &amp; ... \\\\         0 &amp; 0 &amp; ... &amp; I &amp; 0 \\end{bmatrix}, v_t = \\begin{bmatrix} a_t \\\\ 0 \\\\...\\\\0\\end{bmatrix}\\)</p> <p>If the moduli of the eigenvalues of \\(A\\) are less than 1, then the process is stable</p>"},{"location":"sta457/VAR.html#example","title":"Example","text":"\\[     \\begin{bmatrix}y_1 \\\\y_2\\end{bmatrix}_t     =      \\begin{bmatrix}5 \\\\ 10\\end{bmatrix}     +      \\begin{bmatrix}0.5 &amp;-0.2 \\\\-0.2 &amp; -0.5\\end{bmatrix}     \\begin{bmatrix}y_1 \\\\y_2\\end{bmatrix}_{t-1}      +      \\begin{bmatrix}0.3 &amp;-0.7 \\\\-0.1 &amp; 0.3\\end{bmatrix}     \\begin{bmatrix}y_1 \\\\y_2\\end{bmatrix}_{t-2}      +     \\begin{bmatrix}a_1 \\\\a_2\\end{bmatrix}_t  \\] <p>Then, its companion matrix \\(A\\) will be </p> \\[\\begin{bmatrix}         0.5 &amp; 0.2 &amp; -0.3 &amp; -0.7 \\\\         -0.2 &amp; -0.5 &amp; -0.1 &amp; 0.3 \\\\         1 &amp; 0 &amp; 0 &amp; 0 \\\\         0 &amp; 1 &amp; 0 &amp; 0 \\end{bmatrix}\\]"},{"location":"sta457/VAR.html#order-selection","title":"Order selection","text":"<ul> <li>sequential likelihood ratio tests of VAR(p) vs. VAR(p-1)</li> <li>BigVAR: dimension reduction methods for multi time series</li> </ul>"},{"location":"sta457/granger_causality_and_cointegration.html","title":"Granger Causality and Co-integration","text":""},{"location":"sta457/granger_causality_and_cointegration.html#granger-causality","title":"Granger causality","text":"<p>Consider a VAR(p) model of 2-d variables, i.e. \\(y_t = [y_{i,t},  y_{2,t}]^T\\) \\(y_t = A_0 + \\sum_1^p A_i y_{t-i} + a_t\\),  and each \\(A_i = \\begin{bmatrix}\\phi_{i,11}&amp;\\phi_{i,12}\\\\\\phi_{i,21}&amp;\\phi_{i,22}\\end{bmatrix}\\)</p> <p>Then, if \\(y_{2,t}\\) does not Granger cause \\(y_{1,t}\\), then all \\(\\phi_{i, 12} = 0\\) since it will only be contributing to the equation of \\(y_{1,t}\\)</p> <p>Similarly, if \\(y_{1,t}\\) does not Granger cause \\(y_{2,t}\\), then all of \\(\\phi_{i,21}=0\\)</p>"},{"location":"sta457/granger_causality_and_cointegration.html#portmanteau-test","title":"Portmanteau test","text":"<p>Let \\(X_t, Y_t\\) be causal and invertible univariate ARMA processes and be given by </p> \\[\\Phi_1(B)(X_{1,t}-\\mu_1)=\\Theta_1(B)a_{1,t}\\] \\[\\Phi_2(B)(X_{2,t}-\\mu_2)=\\Theta_2(B)a_{2,t}\\] <p>The cross-correlation </p> \\[\\rho_{X_1X_2}(k) = \\frac{E(a_{1,t}, a_{2,t+k})}{\\sqrt{\\sigma_1^2\\sigma_2^2}}\\] <p>If \\(\\rho(k)=0\\) for \\(k&lt;0\\), then \\(X_2\\) does not cause \\(X_1\\)</p>"},{"location":"sta457/granger_causality_and_cointegration.html#cointegration","title":"Cointegration","text":"<p>I(d) process For a time series \\(X_t\\)</p> <ul> <li>A stationary and invertible time series is said to be an I(0) process</li> <li>A time series is said to be an integrated process of order one, i.e. I(1) process, if \\((1-B)X_t\\) is stationary and invertible. </li> <li>I(d) process, if \\((1-B)^d X_t\\) is stationary and invertible, \\(d&gt;0\\) and order \\(d\\) is referred to as the order of integration or the multiplicity of a unit root. </li> </ul> <p>Consider a multi time series \\(X_t\\). If \\(\\forall i, X_{i,t}\\) are I(1) but a nontrivial linear combination \\(\\beta'X_t\\) is I(0), then \\(X_t\\) is said to be cointegrated of order one and the linear combination vector \\(\\beta\\) is called a cointegrating vector. </p>"},{"location":"sta457/granger_causality_and_cointegration.html#granger-representation-theorem","title":"Granger Representation Theorem","text":"<p>\\(X_t, Y_t\\) are cointegrated, IFF exists an ECM representation. </p>"},{"location":"sta457/granger_causality_and_cointegration.html#implications","title":"Implications","text":"<ol> <li>Vector autoregression on differenced I(1) processes will be a misspecification if the component series are cointegrated</li> <li>An equilibrium specification is missing from a VAR representation</li> <li>When lagged disequilibrium terms are included as explanatory variables, the model becomes well specified</li> <li>Such as model is called an error correction model because the model is structured so that short-rum deviation from the long-run equilibrium will be corrected. </li> </ol>"}]}